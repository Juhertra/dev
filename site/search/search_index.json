{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SecFlow Documentation \u00b6 This is the comprehensive documentation portal for the SecFlow security toolkit orchestration framework. \ud83d\udcda Documentation Sections \u00b6 \ud83c\udfd7\ufe0f Architecture \u00b6 Complete technical architecture documentation covering: - Repository layout and package structure - Core packages and responsibilities - Workflow orchestration and plugin systems - Tools integration and resource management - Security models and observability \ud83d\udd0d Review & Validation \u00b6 Documentation validation and review tools: - Review guidelines and workflow - Validation checklists and reports - AI-assisted validation summaries - Mermaid diagram testing \ud83d\ude80 Quick Start \u00b6 Architecture Overview : Start with Home for the high-level system overview Executive Summary : Read the Executive Summary for key concepts Validation Portal : Check the Review & Validation section for documentation standards \ud83d\udcca Current Status \u00b6 \u2705 Mermaid Diagrams : All diagrams rendering correctly \u2705 ASCII Artifacts : Eliminated from documentation \u2705 CI Gates : Automated validation in place \u2705 Navigation : All pages accessible and properly linked Built with MkDocs | Last Updated : 2025-10-08","title":"SecFlow Documentation"},{"location":"#welcome-to-secflow-documentation","text":"This is the comprehensive documentation portal for the SecFlow security toolkit orchestration framework.","title":"Welcome to SecFlow Documentation"},{"location":"#documentation-sections","text":"","title":"\ud83d\udcda Documentation Sections"},{"location":"#architecture","text":"Complete technical architecture documentation covering: - Repository layout and package structure - Core packages and responsibilities - Workflow orchestration and plugin systems - Tools integration and resource management - Security models and observability","title":"\ud83c\udfd7\ufe0f Architecture"},{"location":"#review-validation","text":"Documentation validation and review tools: - Review guidelines and workflow - Validation checklists and reports - AI-assisted validation summaries - Mermaid diagram testing","title":"\ud83d\udd0d Review &amp; Validation"},{"location":"#quick-start","text":"Architecture Overview : Start with Home for the high-level system overview Executive Summary : Read the Executive Summary for key concepts Validation Portal : Check the Review & Validation section for documentation standards","title":"\ud83d\ude80 Quick Start"},{"location":"#current-status","text":"\u2705 Mermaid Diagrams : All diagrams rendering correctly \u2705 ASCII Artifacts : Eliminated from documentation \u2705 CI Gates : Automated validation in place \u2705 Navigation : All pages accessible and properly linked Built with MkDocs | Last Updated : 2025-10-08","title":"\ud83d\udcca Current Status"},{"location":"developer-onboarding/","text":"Developer Onboarding Guide - SecFlow \u00b6 \ud83d\ude80 Welcome to SecFlow Development \u00b6 This guide will help you get up and running with the SecFlow security testing platform. Our development environment is designed for perfect local-CI parity - what works locally will work in CI. \ud83d\udccb Prerequisites \u00b6 Python 3.11.9 (exact version required) Git with GitHub access GitHub CLI ( gh ) for repository operations Make for standardized commands \ud83d\udee0\ufe0f Initial Setup \u00b6 1. Clone and Setup Environment \u00b6 # Clone the repository git clone https://github.com/Juhertra/dev.git cd dev # Install dependencies make setup # This runs: pip install -e \".[dev]\" # Verify installation python --version # Should show 3.11.9 2. Install Pre-commit Hooks \u00b6 # Install pre-commit hooks for automated quality gates pre-commit install # Test the hooks pre-commit run --all-files Pre-commit hooks include : - ruff : Python linting and formatting - pyright : Static type checking - pytest : Quick test execution - docs-health : Documentation validation \u2705 Development Workflow \u00b6 Daily Development Loop \u00b6 # Run all quality gates before committing make lint && make type && make imports && pytest -q # Expected output: # \u2705 All checks passed! (ruff) # \u2705 0 errors, 0 warnings, 0 informations (pyright) # \u2705 Contracts: 1 kept, 0 broken (imports) # \u2705 126 tests passing (pytest) Individual Commands \u00b6 # Code quality make lint # ruff check . (linting + formatting) make type # pyright (static type checking) make imports # lint-imports (import architecture) # Testing make unit # pytest -q (unit tests) make coverage # coverage run -m pytest -q && coverage report -m # Documentation make health # Documentation health checks \ud83d\udd04 Git Workflow \u00b6 Branch Naming Convention \u00b6 # Feature branches feat/<issue-number>-<slug> # Example: feat/123-add-plugin-support # Bug fixes fix/<issue-number>-<slug> # Example: fix/456-resolve-memory-leak # DevEx improvements devex/<description> # Example: devex/improve-ci-performance # Documentation docs/<description> # Example: docs/update-api-docs Pull Request Process \u00b6 1. Create Issue First \u00b6 Use Bug Report template for bugs Use Feature Request template for features Link to appropriate milestone (M1, M2, etc.) 2. Create Branch and PR \u00b6 # Create feature branch git checkout -b feat/123-add-plugin-support # Make changes and commit git add . git commit -m \"feat(plugin): add plugin support framework\" # Push and create PR git push -u origin feat/123-add-plugin-support gh pr create --title \"feat(plugin): add plugin support framework\" 3. PR Template Requirements \u00b6 Every PR must include: - \u2705 Issue Link : Fixes #123 or Closes #123 - \u2705 DoD Checklist : Complete all required items - \u2705 Validation Evidence : Paste test/coverage output - \u2705 Risk Assessment : Identify risks and rollback plan 4. Code Review Process \u00b6 Required Reviewers : Based on CODEOWNERS mapping Size Limits : \u2264400 LOC or requires 2 approvals CI Checks : All 7 checks must pass Coverage : Must maintain or improve coverage \ud83d\udcda Repository Governance \u00b6 Code Ownership (CODEOWNERS) \u00b6 # Core & runtime /secflow/core/** @runtime-lead @security-lead /secflow/storage/** @runtime-lead /secflow/workflow/** @workflow-lead @runtime-lead # Tools & parsers /secflow/tools/** @tools-lead @findings-lead # Schemas /schemas/** @findings-lead @runtime-lead # Plugins & policies /plugins/** @security-lead # Observability & devex /ops/** @observability-lead @devex-lead /scripts/** @devex-lead # Docs /docs/** @devex-lead # Root config /* @devex-lead Engineering Standards \u00b6 Read : docs/governance/engineering-standards.md Follow : Definition of Done checklist Adhere : Development conventions Source of Truth Architecture \u00b6 Workflow Engine : docs/architecture/workflow-engine-design.md Plugin System : docs/architecture/plugin-architecture.md Storage Layer : docs/architecture/storage-design.md \ud83e\uddea Testing Guidelines \u00b6 Test Structure \u00b6 tests/ \u251c\u2500\u2500 unit/ # Unit tests \u251c\u2500\u2500 contracts/ # Contract tests (architecture compliance) \u251c\u2500\u2500 integration/ # Integration tests (future) \u2514\u2500\u2500 e2e/ # End-to-end tests (future) Writing Tests \u00b6 # Example unit test def test_plugin_loading (): \"\"\"Test plugin loading functionality.\"\"\" plugin = PluginLoader . load ( \"test-plugin\" ) assert plugin . name == \"test-plugin\" assert plugin . version == \"1.0.0\" Running Tests \u00b6 # All tests pytest # Specific test file pytest tests/unit/test_plugin.py # With coverage pytest --cov = . --cov-report = term-missing # Contract tests only pytest tests/contracts/ \ud83d\udd27 Troubleshooting \u00b6 Common Issues \u00b6 \"Command not found\" errors \u00b6 # Ensure you're in the project root cd /path/to/dev # Reinstall dependencies make setup # Check Python version python --version # Should be 3.11.9 Pre-commit hook failures \u00b6 # Run hooks manually to see errors pre-commit run --all-files # Skip hooks temporarily (not recommended) git commit --no-verify -m \"temp commit\" CI failures \u00b6 # Run same commands locally make lint && make type && make imports && pytest -q # Check for dependency issues pip list | grep -E \"(ruff|pyright|pytest|flask)\" Getting Help \u00b6 DevEx Lead : For development environment issues Runtime Lead : For core system questions Tools Lead : For tool integration questions Security Lead : For security policy questions \ud83d\udcc8 Quality Metrics \u00b6 Current Baseline (M0) \u00b6 Test Coverage : 18% (M0 threshold) Test Count : 126 tests Linting : 0 errors, 0 warnings Type Checking : 0 errors, 0 warnings, 0 informations Import Architecture : 1 contract kept, 0 broken Coverage Ratchet \u00b6 M0 : 18% (current baseline) M1 : TBD (will be set higher) Enforcement : Coverage cannot decrease \ud83d\ude80 M1 Development Focus \u00b6 Plugin System Development \u00b6 Plugin Architecture : Follow design docs Security Policy : Adhere to plugin security guidelines Testing : Write contract tests for plugin interfaces Workflow Engine \u00b6 Linear v1 : Implement basic workflow execution Integration : Connect with plugin system Testing : End-to-end workflow tests Observability \u00b6 Logging : Structured logging for all components Metrics : Performance and usage metrics Monitoring : Health checks and alerts \ud83d\udcdd Contributing Checklist \u00b6 Before submitting a PR: Issue Created : Linked to appropriate milestone Branch Named : Following convention Pre-commit Passes : pre-commit run --all-files All Checks Pass : make lint && make type && make imports && pytest -q PR Template : Complete DoD checklist Validation Evidence : Paste test/coverage output Code Review : Required reviewers assigned Documentation : Updated if needed \ud83c\udfaf Success Criteria \u00b6 You're successfully onboarded when you can: - \u2705 Run all quality gates locally ( make lint && make type && make imports && pytest -q ) - \u2705 Create a PR that passes all CI checks - \u2705 Follow the branch naming and PR template conventions - \u2705 Understand the code ownership structure - \u2705 Write tests that maintain or improve coverage \ud83d\udcde Support \u00b6 Documentation : Check docs/ directory first Issues : Create GitHub issues for bugs/questions Discussions : Use GitHub Discussions for general questions Team Leads : Contact appropriate lead for domain-specific questions Welcome to SecFlow development! \ud83d\ude80 This guide is maintained by the DevEx team. Last updated: 2025-10-15","title":"Developer Onboarding Guide - SecFlow"},{"location":"developer-onboarding/#developer-onboarding-guide-secflow","text":"","title":"Developer Onboarding Guide - SecFlow"},{"location":"developer-onboarding/#welcome-to-secflow-development","text":"This guide will help you get up and running with the SecFlow security testing platform. Our development environment is designed for perfect local-CI parity - what works locally will work in CI.","title":"\ud83d\ude80 Welcome to SecFlow Development"},{"location":"developer-onboarding/#prerequisites","text":"Python 3.11.9 (exact version required) Git with GitHub access GitHub CLI ( gh ) for repository operations Make for standardized commands","title":"\ud83d\udccb Prerequisites"},{"location":"developer-onboarding/#initial-setup","text":"","title":"\ud83d\udee0\ufe0f Initial Setup"},{"location":"developer-onboarding/#1-clone-and-setup-environment","text":"# Clone the repository git clone https://github.com/Juhertra/dev.git cd dev # Install dependencies make setup # This runs: pip install -e \".[dev]\" # Verify installation python --version # Should show 3.11.9","title":"1. Clone and Setup Environment"},{"location":"developer-onboarding/#2-install-pre-commit-hooks","text":"# Install pre-commit hooks for automated quality gates pre-commit install # Test the hooks pre-commit run --all-files Pre-commit hooks include : - ruff : Python linting and formatting - pyright : Static type checking - pytest : Quick test execution - docs-health : Documentation validation","title":"2. Install Pre-commit Hooks"},{"location":"developer-onboarding/#development-workflow","text":"","title":"\u2705 Development Workflow"},{"location":"developer-onboarding/#daily-development-loop","text":"# Run all quality gates before committing make lint && make type && make imports && pytest -q # Expected output: # \u2705 All checks passed! (ruff) # \u2705 0 errors, 0 warnings, 0 informations (pyright) # \u2705 Contracts: 1 kept, 0 broken (imports) # \u2705 126 tests passing (pytest)","title":"Daily Development Loop"},{"location":"developer-onboarding/#individual-commands","text":"# Code quality make lint # ruff check . (linting + formatting) make type # pyright (static type checking) make imports # lint-imports (import architecture) # Testing make unit # pytest -q (unit tests) make coverage # coverage run -m pytest -q && coverage report -m # Documentation make health # Documentation health checks","title":"Individual Commands"},{"location":"developer-onboarding/#git-workflow","text":"","title":"\ud83d\udd04 Git Workflow"},{"location":"developer-onboarding/#branch-naming-convention","text":"# Feature branches feat/<issue-number>-<slug> # Example: feat/123-add-plugin-support # Bug fixes fix/<issue-number>-<slug> # Example: fix/456-resolve-memory-leak # DevEx improvements devex/<description> # Example: devex/improve-ci-performance # Documentation docs/<description> # Example: docs/update-api-docs","title":"Branch Naming Convention"},{"location":"developer-onboarding/#pull-request-process","text":"","title":"Pull Request Process"},{"location":"developer-onboarding/#1-create-issue-first","text":"Use Bug Report template for bugs Use Feature Request template for features Link to appropriate milestone (M1, M2, etc.)","title":"1. Create Issue First"},{"location":"developer-onboarding/#2-create-branch-and-pr","text":"# Create feature branch git checkout -b feat/123-add-plugin-support # Make changes and commit git add . git commit -m \"feat(plugin): add plugin support framework\" # Push and create PR git push -u origin feat/123-add-plugin-support gh pr create --title \"feat(plugin): add plugin support framework\"","title":"2. Create Branch and PR"},{"location":"developer-onboarding/#3-pr-template-requirements","text":"Every PR must include: - \u2705 Issue Link : Fixes #123 or Closes #123 - \u2705 DoD Checklist : Complete all required items - \u2705 Validation Evidence : Paste test/coverage output - \u2705 Risk Assessment : Identify risks and rollback plan","title":"3. PR Template Requirements"},{"location":"developer-onboarding/#4-code-review-process","text":"Required Reviewers : Based on CODEOWNERS mapping Size Limits : \u2264400 LOC or requires 2 approvals CI Checks : All 7 checks must pass Coverage : Must maintain or improve coverage","title":"4. Code Review Process"},{"location":"developer-onboarding/#repository-governance","text":"","title":"\ud83d\udcda Repository Governance"},{"location":"developer-onboarding/#code-ownership-codeowners","text":"# Core & runtime /secflow/core/** @runtime-lead @security-lead /secflow/storage/** @runtime-lead /secflow/workflow/** @workflow-lead @runtime-lead # Tools & parsers /secflow/tools/** @tools-lead @findings-lead # Schemas /schemas/** @findings-lead @runtime-lead # Plugins & policies /plugins/** @security-lead # Observability & devex /ops/** @observability-lead @devex-lead /scripts/** @devex-lead # Docs /docs/** @devex-lead # Root config /* @devex-lead","title":"Code Ownership (CODEOWNERS)"},{"location":"developer-onboarding/#engineering-standards","text":"Read : docs/governance/engineering-standards.md Follow : Definition of Done checklist Adhere : Development conventions","title":"Engineering Standards"},{"location":"developer-onboarding/#source-of-truth-architecture","text":"Workflow Engine : docs/architecture/workflow-engine-design.md Plugin System : docs/architecture/plugin-architecture.md Storage Layer : docs/architecture/storage-design.md","title":"Source of Truth Architecture"},{"location":"developer-onboarding/#testing-guidelines","text":"","title":"\ud83e\uddea Testing Guidelines"},{"location":"developer-onboarding/#test-structure","text":"tests/ \u251c\u2500\u2500 unit/ # Unit tests \u251c\u2500\u2500 contracts/ # Contract tests (architecture compliance) \u251c\u2500\u2500 integration/ # Integration tests (future) \u2514\u2500\u2500 e2e/ # End-to-end tests (future)","title":"Test Structure"},{"location":"developer-onboarding/#writing-tests","text":"# Example unit test def test_plugin_loading (): \"\"\"Test plugin loading functionality.\"\"\" plugin = PluginLoader . load ( \"test-plugin\" ) assert plugin . name == \"test-plugin\" assert plugin . version == \"1.0.0\"","title":"Writing Tests"},{"location":"developer-onboarding/#running-tests","text":"# All tests pytest # Specific test file pytest tests/unit/test_plugin.py # With coverage pytest --cov = . --cov-report = term-missing # Contract tests only pytest tests/contracts/","title":"Running Tests"},{"location":"developer-onboarding/#troubleshooting","text":"","title":"\ud83d\udd27 Troubleshooting"},{"location":"developer-onboarding/#common-issues","text":"","title":"Common Issues"},{"location":"developer-onboarding/#command-not-found-errors","text":"# Ensure you're in the project root cd /path/to/dev # Reinstall dependencies make setup # Check Python version python --version # Should be 3.11.9","title":"\"Command not found\" errors"},{"location":"developer-onboarding/#pre-commit-hook-failures","text":"# Run hooks manually to see errors pre-commit run --all-files # Skip hooks temporarily (not recommended) git commit --no-verify -m \"temp commit\"","title":"Pre-commit hook failures"},{"location":"developer-onboarding/#ci-failures","text":"# Run same commands locally make lint && make type && make imports && pytest -q # Check for dependency issues pip list | grep -E \"(ruff|pyright|pytest|flask)\"","title":"CI failures"},{"location":"developer-onboarding/#getting-help","text":"DevEx Lead : For development environment issues Runtime Lead : For core system questions Tools Lead : For tool integration questions Security Lead : For security policy questions","title":"Getting Help"},{"location":"developer-onboarding/#quality-metrics","text":"","title":"\ud83d\udcc8 Quality Metrics"},{"location":"developer-onboarding/#current-baseline-m0","text":"Test Coverage : 18% (M0 threshold) Test Count : 126 tests Linting : 0 errors, 0 warnings Type Checking : 0 errors, 0 warnings, 0 informations Import Architecture : 1 contract kept, 0 broken","title":"Current Baseline (M0)"},{"location":"developer-onboarding/#coverage-ratchet","text":"M0 : 18% (current baseline) M1 : TBD (will be set higher) Enforcement : Coverage cannot decrease","title":"Coverage Ratchet"},{"location":"developer-onboarding/#m1-development-focus","text":"","title":"\ud83d\ude80 M1 Development Focus"},{"location":"developer-onboarding/#plugin-system-development","text":"Plugin Architecture : Follow design docs Security Policy : Adhere to plugin security guidelines Testing : Write contract tests for plugin interfaces","title":"Plugin System Development"},{"location":"developer-onboarding/#workflow-engine","text":"Linear v1 : Implement basic workflow execution Integration : Connect with plugin system Testing : End-to-end workflow tests","title":"Workflow Engine"},{"location":"developer-onboarding/#observability","text":"Logging : Structured logging for all components Metrics : Performance and usage metrics Monitoring : Health checks and alerts","title":"Observability"},{"location":"developer-onboarding/#contributing-checklist","text":"Before submitting a PR: Issue Created : Linked to appropriate milestone Branch Named : Following convention Pre-commit Passes : pre-commit run --all-files All Checks Pass : make lint && make type && make imports && pytest -q PR Template : Complete DoD checklist Validation Evidence : Paste test/coverage output Code Review : Required reviewers assigned Documentation : Updated if needed","title":"\ud83d\udcdd Contributing Checklist"},{"location":"developer-onboarding/#success-criteria","text":"You're successfully onboarded when you can: - \u2705 Run all quality gates locally ( make lint && make type && make imports && pytest -q ) - \u2705 Create a PR that passes all CI checks - \u2705 Follow the branch naming and PR template conventions - \u2705 Understand the code ownership structure - \u2705 Write tests that maintain or improve coverage","title":"\ud83c\udfaf Success Criteria"},{"location":"developer-onboarding/#support","text":"Documentation : Check docs/ directory first Issues : Create GitHub issues for bugs/questions Discussions : Use GitHub Discussions for general questions Team Leads : Contact appropriate lead for domain-specific questions Welcome to SecFlow development! \ud83d\ude80 This guide is maintained by the DevEx team. Last updated: 2025-10-15","title":"\ud83d\udcde Support"},{"location":"developer-start-here/","text":"Developer Start Here \u00b6 Quick Setup \u00b6 Clone repo, install Poetry, poetry install --no-root Branching: trunk-based; feature branches feat/... ; PR \u2264400 LOC or 2 approvals Run locally: make test ; fast loop: make quick-test Docs: mkdocs serve (local), make health before committing docs Read: docs/architecture/00-index.md , storage contracts, finding schema DoD checklist is in PR template; paste validation evidence in PRs Essential Reading \u00b6 Read this first: Governance & Engineering Standards This page contains the complete Definition of Done checklist, branching rules, CI pipeline order, coverage requirements, and documentation health gates that all contributors must follow. Development Practices: Development Conventions This page covers daily development workflows, SOD/EOD rituals, branch naming conventions, PR rules, and CI fast-fail order that every developer needs to follow. Getting Started \u00b6 Prerequisites \u00b6 Python 3.11+ (3.14 compatibility under testing) Poetry for dependency management Git for version control Docker (optional, for containerized testing) Installation \u00b6 # Clone the repository git clone https://github.com/your-org/secflow.git cd secflow # Install dependencies poetry install --no-root # Verify installation poetry run python -c \"import secflow; print('SecFlow installed successfully')\" M1 Features Overview \u00b6 M1 Implementation Status : \u2705 Complete - Plugin system, workflow engine, and storage framework delivered. Plugin System \u00b6 Plugin Loader : Dynamic plugin discovery and loading Security Framework : Signature verification and sandboxing Stub Implementations : CVEMapper, FeroxStub, NucleiStub plugins StoragePort Integration : Findings persistence via StoragePort Workflow Engine \u00b6 Sequential Execution : Nodes execute in topological order YAML Recipes : Declarative workflow definition Retry Logic : Configurable retry with exponential backoff Timeout Handling : Per-node timeout enforcement Storage Framework \u00b6 StoragePort Interface : Protocol-based storage abstraction In-Memory Adapter : Thread-safe in-memory storage Finding Schema : JSON Schema validation (v1.0.0) Project Isolation : Separate storage per project Quick Tutorial: Running Your First Workflow \u00b6 1. Validate a Workflow Recipe \u00b6 # Validate the sample workflow python tools/validate_recipe.py workflows/sample-linear.yaml # Expected output: # \u2705 YAML syntax valid: workflows/sample-linear.yaml # \u2705 Schema validation passed: Linear Security Scan # \u2705 DAG validation passed: 3 nodes # \u2705 RecipeValidator validation passed # \ud83c\udfaf Recipe validation successful: Linear Security Scan 2. Execute a Workflow (Dry Run) \u00b6 # Dry-run the workflow (analyze without execution) python tools/run_workflow.py workflows/sample-linear.yaml --dry-run # Expected output: # \ud83d\udd0d DRY RUN: Linear Security Scan # \ud83d\udcdd Description: Simple linear workflow: discovery \u2192 scan \u2192 enrichment # \ud83d\udcca Nodes: 3 # 1. discovery (discovery.ferox) # 2. scan (scan.nuclei) # 3. enrich (enrich.cve) # \u2705 Dry run completed - no actual execution performed 3. Execute a Workflow (Actual Execution) \u00b6 # Execute the workflow with M1 stub implementations python tools/run_workflow.py workflows/sample-linear.yaml --execute # Expected output: # \ud83d\ude80 EXECUTING: Linear Security Scan # [2025-10-14 12:01:02] Workflow started # [2025-10-14 12:01:05] Node discovery.ferox completed (urls=356) # [2025-10-14 12:01:07] Node scan.nuclei completed (findings=112) # [2025-10-14 12:01:10] Node enrich.cve completed (enriched_findings=112) # [2025-10-14 12:01:10] Workflow completed successfully 4. View Results \u00b6 # Check findings stored via StoragePort python tools/list_findings.py --project-id sample-project # Expected output: # \ud83d\udcca Findings for project: sample-project # Total findings: 112 # By severity: # - Critical: 5 # - High: 23 # - Medium: 45 # - Low: 32 # - Info: 7 Plugin Development Guide \u00b6 Creating a New Plugin \u00b6 1. Plugin Structure \u00b6 # packages/plugins/stubs/my_plugin.py from packages.plugins.loader import PluginInterface , PluginMetadata from packages.runtime_core.storage.storage_port import StoragePort from typing import Dict , Any class MyPlugin ( PluginInterface ): \"\"\"Example plugin implementation.\"\"\" def get_name ( self ) -> str : return \"my-plugin\" def get_version ( self ) -> str : return \"1.0.0\" def get_metadata ( self ) -> PluginMetadata : return PluginMetadata ( name = \"my-plugin\" , version = \"1.0.0\" , description = \"Example plugin for SecFlow\" , author = \"Your Name\" , category = \"detector\" , # or \"enricher\" or \"analytics\" entrypoint = \"packages.plugins.stubs.my_plugin:MyPlugin\" ) def run ( self , inputs : Dict [ str , Any ], config : Dict [ str , Any ], context : ExecutionContext ) -> Dict [ str , Any ]: \"\"\"Execute plugin logic.\"\"\" # Your plugin logic here findings = [] # Process inputs and generate findings for item in inputs . get ( \"data\" , []): finding = { \"finding_schema_version\" : \"1.0.0\" , \"id\" : generate_uuid (), \"project_id\" : context . project_id , \"detector_id\" : \"my-plugin\" , \"title\" : \"Example finding\" , \"severity\" : \"medium\" , \"resource\" : item . get ( \"url\" , \"\" ), \"evidence\" : { \"raw_data\" : item }, \"created_at\" : datetime . utcnow () . isoformat () + \"Z\" } findings . append ( finding ) return { \"findings\" : findings } 2. Plugin Manifest \u00b6 { \"name\" : \"my-plugin\" , \"version\" : \"1.0.0\" , \"description\" : \"Example plugin for SecFlow\" , \"author\" : \"Your Name\" , \"category\" : \"detector\" , \"entrypoint\" : \"packages.plugins.stubs.my_plugin:MyPlugin\" , \"dependencies\" : [ \"requests\" ], \"config_schema\" : { \"timeout\" : { \"type\" : \"integer\" , \"default\" : 30 } }, \"code_hash\" : \"sha256:abc123...\" , \"signature\" : \"sha256:def456...\" , \"created_at\" : \"2025-10-14T10:00:00Z\" , \"expires_at\" : \"2026-10-14T10:00:00Z\" } 3. Plugin Registration \u00b6 # packages/plugins/registry.py from packages.plugins.stubs.my_plugin import MyPlugin # Register your plugin PluginRegistry . register ( \"my-plugin\" , MyPlugin ) Plugin Security \u00b6 Signature Verification (M1) \u00b6 # M1: Basic hash-based verification def verify_plugin_signature ( manifest : PluginManifest , plugin_path : str ) -> bool : \"\"\"Verify plugin signature using SHA256 hash.\"\"\" expected_hash = manifest . code_hash actual_hash = calculate_file_hash ( plugin_path ) return expected_hash == actual_hash # M2+: Full cryptographic verification (planned) def verify_plugin_signature_crypto ( manifest : PluginManifest , plugin_path : str ) -> bool : \"\"\"Verify plugin signature using RSA/ECDSA.\"\"\" # Implementation planned for M2 pass Sandbox Execution \u00b6 # Plugin execution in sandboxed environment def execute_plugin_secure ( plugin : PluginInterface , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Execute plugin in sandboxed environment.\"\"\" with sandbox_context (): return plugin . run ( inputs , config , context ) Workflow Development \u00b6 YAML Recipe Format \u00b6 version : \"1.0\" name : \"My Custom Workflow\" description : \"Custom workflow for specific testing\" nodes : - id : \"discovery\" type : \"discovery.ferox\" config : wordlist : \"res://wordlists/custom:latest\" threads : 25 timeout : 600 outputs : [ \"urls\" ] - id : \"scan\" type : \"scan.nuclei\" inputs : [ \"urls\" ] config : templates : \"res://templates/custom:latest\" rate_limit : 100 timeout : 900 outputs : [ \"findings\" ] - id : \"enrich\" type : \"enrich.cve\" inputs : [ \"findings\" ] config : sources : [ \"nvd\" , \"osv\" ] timeout : 60 outputs : [ \"enriched_findings\" ] # Retry configuration retry : max_attempts : 3 backoff_factor : 2.0 base_delay : 5.0 # State management state : checkpoint_interval : 30 resume_on_failure : true cache_intermediate : true Programmatic Workflow Execution \u00b6 from packages.workflow_engine.executor import WorkflowExecutor from packages.workflow_engine.validate_recipe import RecipeValidator from packages.storage.adapters.memory import InMemoryStorageAdapter # Validate recipe validator = RecipeValidator () result = validator . validate_file ( \"my-workflow.yaml\" ) if not result [ \"ok\" ]: print ( f \"Validation failed: { result [ 'errors' ] } \" ) exit ( 1 ) # Execute workflow storage = InMemoryStorageAdapter () executor = WorkflowExecutor ( storage = storage ) workflow = executor . load_workflow_from_yaml ( \"my-workflow.yaml\" ) execution_result = executor . execute_workflow ( workflow ) print ( f \"Workflow status: { execution_result [ 'status' ] } \" ) print ( f \"Completed nodes: { execution_result [ 'completed_nodes' ] } \" ) Storage Integration \u00b6 Using StoragePort \u00b6 from packages.runtime_core.storage.storage_port import StoragePort from packages.storage.adapters.memory import InMemoryStorageAdapter # Initialize storage storage : StoragePort = InMemoryStorageAdapter () # Save a finding finding = { \"finding_schema_version\" : \"1.0.0\" , \"id\" : \"uuid-here\" , \"project_id\" : \"project-123\" , \"detector_id\" : \"scan.nuclei\" , \"title\" : \"SQL Injection Vulnerability\" , \"severity\" : \"high\" , \"resource\" : \"https://example.com/login\" , \"evidence\" : { \"payload\" : \"admin' OR '1'='1\" }, \"created_at\" : \"2025-10-14T10:30:00Z\" } storage . save_finding ( finding ) # List findings findings = storage . list_findings ( \"project-123\" ) print ( f \"Found { len ( findings ) } findings\" ) Finding Schema Compliance \u00b6 # All findings must include these required fields: required_fields = [ \"finding_schema_version\" , # Must be \"1.0.0\" \"id\" , # UUID format \"project_id\" , # UUID format \"detector_id\" , # Pattern: ^[A-Za-z0-9_.-]+$ \"title\" , # Non-empty string \"severity\" , # Enum: info, low, medium, high, critical \"resource\" , # String (URL or identifier) \"created_at\" # ISO 8601 timestamp with Z suffix ] # Optional fields: optional_fields = [ \"evidence\" , # Object with additional properties \"cwe\" , # Integer (CWE ID) \"owasp\" , # Pattern: ^A\\d{2}$ (OWASP Top 10) \"cve_ids\" , # Array of CVE identifiers \"enrichment\" # Object with enrichment data ] Testing \u00b6 Running Tests \u00b6 # Run all tests make test # Run quick tests (unit tests only) make quick-test # Run specific test file pytest tests/workflow/test_workflow_executor.py -v # Run with coverage pytest --cov = packages tests/ Test Structure \u00b6 tests/ \u251c\u2500\u2500 unit/ # Unit tests \u2502 \u251c\u2500\u2500 test_plugin_loader.py \u2502 \u251c\u2500\u2500 test_workflow_executor.py \u2502 \u2514\u2500\u2500 test_storage_adapter.py \u251c\u2500\u2500 integration/ # Integration tests \u2502 \u251c\u2500\u2500 test_workflow_execution.py \u2502 \u2514\u2500\u2500 test_plugin_integration.py \u251c\u2500\u2500 contracts/ # Contract tests \u2502 \u251c\u2500\u2500 test_storage_port.py \u2502 \u2514\u2500\u2500 test_finding_schema.py \u2514\u2500\u2500 golden_samples/ # Test data \u251c\u2500\u2500 ferox_output.json \u251c\u2500\u2500 nuclei_output.json \u2514\u2500\u2500 cve_data.json Observability & Debugging \u00b6 Logging \u00b6 import logging # Enable debug logging logging . basicConfig ( level = logging . DEBUG ) # Plugin execution logs logger = logging . getLogger ( __name__ ) logger . info ( f \"Plugin { plugin_name } executed successfully\" ) logger . error ( f \"Plugin { plugin_name } failed: { error } \" ) Metrics Collection \u00b6 # M1: In-memory metrics collection # M5: Prometheus metrics (planned) def collect_plugin_metrics ( plugin_name : str , duration : float , success : bool ): \"\"\"Collect plugin execution metrics.\"\"\" metrics = { \"plugin_name\" : plugin_name , \"duration_ms\" : duration * 1000 , \"success\" : success , \"timestamp\" : datetime . utcnow () . isoformat () + \"Z\" } # Store metrics via StoragePort storage . save_metrics ( metrics ) Debug Mode \u00b6 # Enable verbose logging python tools/run_workflow.py workflows/sample-linear.yaml --execute --verbose # Debug plugin loading python tools/validate_recipe.py workflows/sample-linear.yaml --debug Troubleshooting \u00b6 Common Issues \u00b6 Import Errors : Ensure packages/ is in Python path Validation Failures : Check YAML syntax and required fields Execution Failures : Verify node types and configuration Plugin Load Failures : Check plugin manifest and signature Storage Errors : Verify finding schema compliance Debug Commands \u00b6 # Check Python path python -c \"import sys; print('\\n'.join(sys.path))\" # Validate finding schema python tools/validate_finding.py --file finding.json # Check plugin registry python tools/list_plugins.py # Test storage adapter python tools/test_storage.py --adapter memory Next Steps \u00b6 Read Architecture Docs : Start with docs/architecture/00-index.md Explore Examples : Check workflows/sample-linear.yaml and plugin stubs Run Tests : Execute make test to verify your setup Create Plugin : Follow the plugin development guide above Build Workflow : Create your own YAML recipe Contribute : Follow the governance guidelines for PRs Resources \u00b6 Architecture Documentation : docs/architecture/ API Reference : docs/api/ (M6: mkdocstrings integration) Sample Workflows : workflows/ Plugin Examples : packages/plugins/stubs/ Test Examples : tests/ Ready to start developing? Check out the Architecture Index for detailed technical documentation. Poetry : Install Poetry for dependency management Git : For version control and branching Make : For running build commands Installation \u00b6 # Clone the repository git clone <repository-url> cd secflow # Install dependencies (without root package) poetry install --no-root Development Workflow \u00b6 Branching Strategy \u00b6 Trunk-based development : Main branch is the source of truth Feature branches : Use feat/... prefix for new features Pull Request limits : \u2264400 LOC or require 2 approvals Testing \u00b6 Full test suite : make test Quick iteration : make quick-test for faster feedback Documentation \u00b6 Local development : mkdocs serve to preview docs locally Health checks : Run make health before committing documentation changes Essential Reading \u00b6 Architecture Overview : Start with docs/architecture/00-index.md Storage Contracts : Understand data persistence patterns Finding Schema : Review the findings data model Definition of Done \u00b6 The DoD checklist is available in the PR template. Always paste validation evidence in your pull requests to demonstrate compliance. Next Steps \u00b6 After completing the setup, explore the architecture documentation to understand the system design and begin contributing to the project.","title":"Developer Start Here"},{"location":"developer-start-here/#developer-start-here","text":"","title":"Developer Start Here"},{"location":"developer-start-here/#quick-setup","text":"Clone repo, install Poetry, poetry install --no-root Branching: trunk-based; feature branches feat/... ; PR \u2264400 LOC or 2 approvals Run locally: make test ; fast loop: make quick-test Docs: mkdocs serve (local), make health before committing docs Read: docs/architecture/00-index.md , storage contracts, finding schema DoD checklist is in PR template; paste validation evidence in PRs","title":"Quick Setup"},{"location":"developer-start-here/#essential-reading","text":"Read this first: Governance & Engineering Standards This page contains the complete Definition of Done checklist, branching rules, CI pipeline order, coverage requirements, and documentation health gates that all contributors must follow. Development Practices: Development Conventions This page covers daily development workflows, SOD/EOD rituals, branch naming conventions, PR rules, and CI fast-fail order that every developer needs to follow.","title":"Essential Reading"},{"location":"developer-start-here/#getting-started","text":"","title":"Getting Started"},{"location":"developer-start-here/#prerequisites","text":"Python 3.11+ (3.14 compatibility under testing) Poetry for dependency management Git for version control Docker (optional, for containerized testing)","title":"Prerequisites"},{"location":"developer-start-here/#installation","text":"# Clone the repository git clone https://github.com/your-org/secflow.git cd secflow # Install dependencies poetry install --no-root # Verify installation poetry run python -c \"import secflow; print('SecFlow installed successfully')\"","title":"Installation"},{"location":"developer-start-here/#m1-features-overview","text":"M1 Implementation Status : \u2705 Complete - Plugin system, workflow engine, and storage framework delivered.","title":"M1 Features Overview"},{"location":"developer-start-here/#plugin-system","text":"Plugin Loader : Dynamic plugin discovery and loading Security Framework : Signature verification and sandboxing Stub Implementations : CVEMapper, FeroxStub, NucleiStub plugins StoragePort Integration : Findings persistence via StoragePort","title":"Plugin System"},{"location":"developer-start-here/#workflow-engine","text":"Sequential Execution : Nodes execute in topological order YAML Recipes : Declarative workflow definition Retry Logic : Configurable retry with exponential backoff Timeout Handling : Per-node timeout enforcement","title":"Workflow Engine"},{"location":"developer-start-here/#storage-framework","text":"StoragePort Interface : Protocol-based storage abstraction In-Memory Adapter : Thread-safe in-memory storage Finding Schema : JSON Schema validation (v1.0.0) Project Isolation : Separate storage per project","title":"Storage Framework"},{"location":"developer-start-here/#quick-tutorial-running-your-first-workflow","text":"","title":"Quick Tutorial: Running Your First Workflow"},{"location":"developer-start-here/#1-validate-a-workflow-recipe","text":"# Validate the sample workflow python tools/validate_recipe.py workflows/sample-linear.yaml # Expected output: # \u2705 YAML syntax valid: workflows/sample-linear.yaml # \u2705 Schema validation passed: Linear Security Scan # \u2705 DAG validation passed: 3 nodes # \u2705 RecipeValidator validation passed # \ud83c\udfaf Recipe validation successful: Linear Security Scan","title":"1. Validate a Workflow Recipe"},{"location":"developer-start-here/#2-execute-a-workflow-dry-run","text":"# Dry-run the workflow (analyze without execution) python tools/run_workflow.py workflows/sample-linear.yaml --dry-run # Expected output: # \ud83d\udd0d DRY RUN: Linear Security Scan # \ud83d\udcdd Description: Simple linear workflow: discovery \u2192 scan \u2192 enrichment # \ud83d\udcca Nodes: 3 # 1. discovery (discovery.ferox) # 2. scan (scan.nuclei) # 3. enrich (enrich.cve) # \u2705 Dry run completed - no actual execution performed","title":"2. Execute a Workflow (Dry Run)"},{"location":"developer-start-here/#3-execute-a-workflow-actual-execution","text":"# Execute the workflow with M1 stub implementations python tools/run_workflow.py workflows/sample-linear.yaml --execute # Expected output: # \ud83d\ude80 EXECUTING: Linear Security Scan # [2025-10-14 12:01:02] Workflow started # [2025-10-14 12:01:05] Node discovery.ferox completed (urls=356) # [2025-10-14 12:01:07] Node scan.nuclei completed (findings=112) # [2025-10-14 12:01:10] Node enrich.cve completed (enriched_findings=112) # [2025-10-14 12:01:10] Workflow completed successfully","title":"3. Execute a Workflow (Actual Execution)"},{"location":"developer-start-here/#4-view-results","text":"# Check findings stored via StoragePort python tools/list_findings.py --project-id sample-project # Expected output: # \ud83d\udcca Findings for project: sample-project # Total findings: 112 # By severity: # - Critical: 5 # - High: 23 # - Medium: 45 # - Low: 32 # - Info: 7","title":"4. View Results"},{"location":"developer-start-here/#plugin-development-guide","text":"","title":"Plugin Development Guide"},{"location":"developer-start-here/#creating-a-new-plugin","text":"","title":"Creating a New Plugin"},{"location":"developer-start-here/#1-plugin-structure","text":"# packages/plugins/stubs/my_plugin.py from packages.plugins.loader import PluginInterface , PluginMetadata from packages.runtime_core.storage.storage_port import StoragePort from typing import Dict , Any class MyPlugin ( PluginInterface ): \"\"\"Example plugin implementation.\"\"\" def get_name ( self ) -> str : return \"my-plugin\" def get_version ( self ) -> str : return \"1.0.0\" def get_metadata ( self ) -> PluginMetadata : return PluginMetadata ( name = \"my-plugin\" , version = \"1.0.0\" , description = \"Example plugin for SecFlow\" , author = \"Your Name\" , category = \"detector\" , # or \"enricher\" or \"analytics\" entrypoint = \"packages.plugins.stubs.my_plugin:MyPlugin\" ) def run ( self , inputs : Dict [ str , Any ], config : Dict [ str , Any ], context : ExecutionContext ) -> Dict [ str , Any ]: \"\"\"Execute plugin logic.\"\"\" # Your plugin logic here findings = [] # Process inputs and generate findings for item in inputs . get ( \"data\" , []): finding = { \"finding_schema_version\" : \"1.0.0\" , \"id\" : generate_uuid (), \"project_id\" : context . project_id , \"detector_id\" : \"my-plugin\" , \"title\" : \"Example finding\" , \"severity\" : \"medium\" , \"resource\" : item . get ( \"url\" , \"\" ), \"evidence\" : { \"raw_data\" : item }, \"created_at\" : datetime . utcnow () . isoformat () + \"Z\" } findings . append ( finding ) return { \"findings\" : findings }","title":"1. Plugin Structure"},{"location":"developer-start-here/#2-plugin-manifest","text":"{ \"name\" : \"my-plugin\" , \"version\" : \"1.0.0\" , \"description\" : \"Example plugin for SecFlow\" , \"author\" : \"Your Name\" , \"category\" : \"detector\" , \"entrypoint\" : \"packages.plugins.stubs.my_plugin:MyPlugin\" , \"dependencies\" : [ \"requests\" ], \"config_schema\" : { \"timeout\" : { \"type\" : \"integer\" , \"default\" : 30 } }, \"code_hash\" : \"sha256:abc123...\" , \"signature\" : \"sha256:def456...\" , \"created_at\" : \"2025-10-14T10:00:00Z\" , \"expires_at\" : \"2026-10-14T10:00:00Z\" }","title":"2. Plugin Manifest"},{"location":"developer-start-here/#3-plugin-registration","text":"# packages/plugins/registry.py from packages.plugins.stubs.my_plugin import MyPlugin # Register your plugin PluginRegistry . register ( \"my-plugin\" , MyPlugin )","title":"3. Plugin Registration"},{"location":"developer-start-here/#plugin-security","text":"","title":"Plugin Security"},{"location":"developer-start-here/#signature-verification-m1","text":"# M1: Basic hash-based verification def verify_plugin_signature ( manifest : PluginManifest , plugin_path : str ) -> bool : \"\"\"Verify plugin signature using SHA256 hash.\"\"\" expected_hash = manifest . code_hash actual_hash = calculate_file_hash ( plugin_path ) return expected_hash == actual_hash # M2+: Full cryptographic verification (planned) def verify_plugin_signature_crypto ( manifest : PluginManifest , plugin_path : str ) -> bool : \"\"\"Verify plugin signature using RSA/ECDSA.\"\"\" # Implementation planned for M2 pass","title":"Signature Verification (M1)"},{"location":"developer-start-here/#sandbox-execution","text":"# Plugin execution in sandboxed environment def execute_plugin_secure ( plugin : PluginInterface , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Execute plugin in sandboxed environment.\"\"\" with sandbox_context (): return plugin . run ( inputs , config , context )","title":"Sandbox Execution"},{"location":"developer-start-here/#workflow-development","text":"","title":"Workflow Development"},{"location":"developer-start-here/#yaml-recipe-format","text":"version : \"1.0\" name : \"My Custom Workflow\" description : \"Custom workflow for specific testing\" nodes : - id : \"discovery\" type : \"discovery.ferox\" config : wordlist : \"res://wordlists/custom:latest\" threads : 25 timeout : 600 outputs : [ \"urls\" ] - id : \"scan\" type : \"scan.nuclei\" inputs : [ \"urls\" ] config : templates : \"res://templates/custom:latest\" rate_limit : 100 timeout : 900 outputs : [ \"findings\" ] - id : \"enrich\" type : \"enrich.cve\" inputs : [ \"findings\" ] config : sources : [ \"nvd\" , \"osv\" ] timeout : 60 outputs : [ \"enriched_findings\" ] # Retry configuration retry : max_attempts : 3 backoff_factor : 2.0 base_delay : 5.0 # State management state : checkpoint_interval : 30 resume_on_failure : true cache_intermediate : true","title":"YAML Recipe Format"},{"location":"developer-start-here/#programmatic-workflow-execution","text":"from packages.workflow_engine.executor import WorkflowExecutor from packages.workflow_engine.validate_recipe import RecipeValidator from packages.storage.adapters.memory import InMemoryStorageAdapter # Validate recipe validator = RecipeValidator () result = validator . validate_file ( \"my-workflow.yaml\" ) if not result [ \"ok\" ]: print ( f \"Validation failed: { result [ 'errors' ] } \" ) exit ( 1 ) # Execute workflow storage = InMemoryStorageAdapter () executor = WorkflowExecutor ( storage = storage ) workflow = executor . load_workflow_from_yaml ( \"my-workflow.yaml\" ) execution_result = executor . execute_workflow ( workflow ) print ( f \"Workflow status: { execution_result [ 'status' ] } \" ) print ( f \"Completed nodes: { execution_result [ 'completed_nodes' ] } \" )","title":"Programmatic Workflow Execution"},{"location":"developer-start-here/#storage-integration","text":"","title":"Storage Integration"},{"location":"developer-start-here/#using-storageport","text":"from packages.runtime_core.storage.storage_port import StoragePort from packages.storage.adapters.memory import InMemoryStorageAdapter # Initialize storage storage : StoragePort = InMemoryStorageAdapter () # Save a finding finding = { \"finding_schema_version\" : \"1.0.0\" , \"id\" : \"uuid-here\" , \"project_id\" : \"project-123\" , \"detector_id\" : \"scan.nuclei\" , \"title\" : \"SQL Injection Vulnerability\" , \"severity\" : \"high\" , \"resource\" : \"https://example.com/login\" , \"evidence\" : { \"payload\" : \"admin' OR '1'='1\" }, \"created_at\" : \"2025-10-14T10:30:00Z\" } storage . save_finding ( finding ) # List findings findings = storage . list_findings ( \"project-123\" ) print ( f \"Found { len ( findings ) } findings\" )","title":"Using StoragePort"},{"location":"developer-start-here/#finding-schema-compliance","text":"# All findings must include these required fields: required_fields = [ \"finding_schema_version\" , # Must be \"1.0.0\" \"id\" , # UUID format \"project_id\" , # UUID format \"detector_id\" , # Pattern: ^[A-Za-z0-9_.-]+$ \"title\" , # Non-empty string \"severity\" , # Enum: info, low, medium, high, critical \"resource\" , # String (URL or identifier) \"created_at\" # ISO 8601 timestamp with Z suffix ] # Optional fields: optional_fields = [ \"evidence\" , # Object with additional properties \"cwe\" , # Integer (CWE ID) \"owasp\" , # Pattern: ^A\\d{2}$ (OWASP Top 10) \"cve_ids\" , # Array of CVE identifiers \"enrichment\" # Object with enrichment data ]","title":"Finding Schema Compliance"},{"location":"developer-start-here/#testing","text":"","title":"Testing"},{"location":"developer-start-here/#running-tests","text":"# Run all tests make test # Run quick tests (unit tests only) make quick-test # Run specific test file pytest tests/workflow/test_workflow_executor.py -v # Run with coverage pytest --cov = packages tests/","title":"Running Tests"},{"location":"developer-start-here/#test-structure","text":"tests/ \u251c\u2500\u2500 unit/ # Unit tests \u2502 \u251c\u2500\u2500 test_plugin_loader.py \u2502 \u251c\u2500\u2500 test_workflow_executor.py \u2502 \u2514\u2500\u2500 test_storage_adapter.py \u251c\u2500\u2500 integration/ # Integration tests \u2502 \u251c\u2500\u2500 test_workflow_execution.py \u2502 \u2514\u2500\u2500 test_plugin_integration.py \u251c\u2500\u2500 contracts/ # Contract tests \u2502 \u251c\u2500\u2500 test_storage_port.py \u2502 \u2514\u2500\u2500 test_finding_schema.py \u2514\u2500\u2500 golden_samples/ # Test data \u251c\u2500\u2500 ferox_output.json \u251c\u2500\u2500 nuclei_output.json \u2514\u2500\u2500 cve_data.json","title":"Test Structure"},{"location":"developer-start-here/#observability-debugging","text":"","title":"Observability &amp; Debugging"},{"location":"developer-start-here/#logging","text":"import logging # Enable debug logging logging . basicConfig ( level = logging . DEBUG ) # Plugin execution logs logger = logging . getLogger ( __name__ ) logger . info ( f \"Plugin { plugin_name } executed successfully\" ) logger . error ( f \"Plugin { plugin_name } failed: { error } \" )","title":"Logging"},{"location":"developer-start-here/#metrics-collection","text":"# M1: In-memory metrics collection # M5: Prometheus metrics (planned) def collect_plugin_metrics ( plugin_name : str , duration : float , success : bool ): \"\"\"Collect plugin execution metrics.\"\"\" metrics = { \"plugin_name\" : plugin_name , \"duration_ms\" : duration * 1000 , \"success\" : success , \"timestamp\" : datetime . utcnow () . isoformat () + \"Z\" } # Store metrics via StoragePort storage . save_metrics ( metrics )","title":"Metrics Collection"},{"location":"developer-start-here/#debug-mode","text":"# Enable verbose logging python tools/run_workflow.py workflows/sample-linear.yaml --execute --verbose # Debug plugin loading python tools/validate_recipe.py workflows/sample-linear.yaml --debug","title":"Debug Mode"},{"location":"developer-start-here/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"developer-start-here/#common-issues","text":"Import Errors : Ensure packages/ is in Python path Validation Failures : Check YAML syntax and required fields Execution Failures : Verify node types and configuration Plugin Load Failures : Check plugin manifest and signature Storage Errors : Verify finding schema compliance","title":"Common Issues"},{"location":"developer-start-here/#debug-commands","text":"# Check Python path python -c \"import sys; print('\\n'.join(sys.path))\" # Validate finding schema python tools/validate_finding.py --file finding.json # Check plugin registry python tools/list_plugins.py # Test storage adapter python tools/test_storage.py --adapter memory","title":"Debug Commands"},{"location":"developer-start-here/#next-steps","text":"Read Architecture Docs : Start with docs/architecture/00-index.md Explore Examples : Check workflows/sample-linear.yaml and plugin stubs Run Tests : Execute make test to verify your setup Create Plugin : Follow the plugin development guide above Build Workflow : Create your own YAML recipe Contribute : Follow the governance guidelines for PRs","title":"Next Steps"},{"location":"developer-start-here/#resources","text":"Architecture Documentation : docs/architecture/ API Reference : docs/api/ (M6: mkdocstrings integration) Sample Workflows : workflows/ Plugin Examples : packages/plugins/stubs/ Test Examples : tests/ Ready to start developing? Check out the Architecture Index for detailed technical documentation. Poetry : Install Poetry for dependency management Git : For version control and branching Make : For running build commands","title":"Resources"},{"location":"developer-start-here/#installation_1","text":"# Clone the repository git clone <repository-url> cd secflow # Install dependencies (without root package) poetry install --no-root","title":"Installation"},{"location":"developer-start-here/#development-workflow","text":"","title":"Development Workflow"},{"location":"developer-start-here/#branching-strategy","text":"Trunk-based development : Main branch is the source of truth Feature branches : Use feat/... prefix for new features Pull Request limits : \u2264400 LOC or require 2 approvals","title":"Branching Strategy"},{"location":"developer-start-here/#testing_1","text":"Full test suite : make test Quick iteration : make quick-test for faster feedback","title":"Testing"},{"location":"developer-start-here/#documentation","text":"Local development : mkdocs serve to preview docs locally Health checks : Run make health before committing documentation changes","title":"Documentation"},{"location":"developer-start-here/#essential-reading_1","text":"Architecture Overview : Start with docs/architecture/00-index.md Storage Contracts : Understand data persistence patterns Finding Schema : Review the findings data model","title":"Essential Reading"},{"location":"developer-start-here/#definition-of-done","text":"The DoD checklist is available in the PR template. Always paste validation evidence in your pull requests to demonstrate compliance.","title":"Definition of Done"},{"location":"developer-start-here/#next-steps_1","text":"After completing the setup, explore the architecture documentation to understand the system design and begin contributing to the project.","title":"Next Steps"},{"location":"security/","text":"Security Documentation \u00b6 \ud83e\udded Overview \u00b6 This document describes the security model implemented in SecFlow M1, covering plugin security, signature verification, sandboxing, and audit logging. The security framework ensures that plugins execute safely and that all security events are properly logged and monitored. M1 Implementation Status : \u2705 Complete - Plugin signature verification, sandboxing, and audit logging delivered. \ud83d\udd10 Security Architecture \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Plugin Loader\"] B[\"Security Framework\"] C[\"Signature Verification\"] D[\"Sandbox Execution\"] E[\"Audit Logging\"] F[\"Policy Enforcement\"] A --> B B --> C B --> D B --> E B --> F G[\"Plugin Manifest\"] H[\"Hash Verification\"] I[\"Sandbox Context\"] J[\"Security Events\"] G --> C H --> C I --> D J --> E M1 Security Components : - \u2705 Plugin Signature Verification : Hash-based integrity checking - \u2705 Sandbox Execution : Controlled execution environment - \u2705 Audit Logging : Comprehensive security event logging - \u2705 Policy Enforcement : Security policy validation - \u2705 Manifest Validation : JSON schema validation for plugin manifests \ud83d\udd11 Plugin Security Model \u00b6 Signature Verification (M1) \u00b6 M1 Implementation : Hash-based verification using SHA256 checksums. # security/signing.py @dataclass class PluginManifest : \"\"\"Plugin manifest with signature information.\"\"\" name : str version : str description : str author : str entrypoint : str code_hash : str signature : Optional [ str ] = None signature_type : str = \"sha256\" # M1: hash-based, M2+: RSA/ECDSA created_at : str = None expires_at : Optional [ str ] = None class PluginSignatureVerifier : \"\"\"Plugin signature verification for M1.\"\"\" def verify_plugin_signature ( self , manifest : PluginManifest , plugin_path : str ) -> bool : \"\"\"Verify plugin signature using hash-based verification.\"\"\" # M1: SHA256 hash verification expected_hash = manifest . code_hash actual_hash = self . _calculate_file_hash ( plugin_path ) return expected_hash == actual_hash def _calculate_file_hash ( self , file_path : str ) -> str : \"\"\"Calculate SHA256 hash of plugin file.\"\"\" import hashlib with open ( file_path , 'rb' ) as f : return hashlib . sha256 ( f . read ()) . hexdigest () M1 Security Features : - \u2705 Integrity Checking : SHA256 checksums prevent tampering - \u2705 Manifest Validation : JSON schema validation - \u2705 Expiration Support : Optional expiration dates - \u2705 Author Verification : Author information in manifest Cryptographic Signing (M2+ Planned) \u00b6 Future Enhancement : Full cryptographic signature verification using RSA/ECDSA. # M2+: Full cryptographic verification (planned) def verify_plugin_signature_crypto ( manifest : PluginManifest , plugin_path : str , public_key : str ) -> bool : \"\"\"Verify plugin signature using RSA/ECDSA.\"\"\" from cryptography.hazmat.primitives import hashes , serialization from cryptography.hazmat.primitives.asymmetric import padding , rsa # Load public key public_key_obj = load_pem_public_key ( public_key . encode ()) # Verify signature try : public_key_obj . verify ( manifest . signature . encode (), manifest . code_hash . encode (), padding . PSS ( mgf = padding . MGF1 ( hashes . SHA256 ()), salt_length = padding . PSS . MAX_LENGTH ), hashes . SHA256 () ) return True except InvalidSignature : return False \ud83c\udfd7\ufe0f Sandbox Execution Model \u00b6 Sandbox Configuration \u00b6 # security/sandbox.py @dataclass class SandboxConfig : \"\"\"Sandbox configuration for plugin execution.\"\"\" filesystem_access : str = \"read-only\" # \"read-only\" | \"temp\" | \"denied\" network_access : bool = False memory_limit_mb : int = 512 cpu_time_limit_s : int = 300 max_file_size_mb : int = 10 allowed_imports : List [ str ] = None denied_imports : List [ str ] = None def run_plugin_secure ( plugin : PluginInterface , inputs : Dict [ str , Any ], config : SandboxConfig ) -> Dict [ str , Any ]: \"\"\"Execute plugin in sandboxed environment.\"\"\" with sandbox_context ( config ): return plugin . run ( inputs , config , context ) Sandbox Controls \u00b6 Control Enforcement M1 Implementation Filesystem Read-only mount or temp directory \u2705 Implemented Network Denied by default, opt-in per manifest \u2705 Implemented Memory / CPU Controlled via subprocess resource limits \u2705 Implemented Timeouts Enforced via execution wrapper \u2705 Implemented Imports Whitelist/blacklist of allowed modules \u2705 Implemented Audit Every plugin invocation logged with context \u2705 Implemented Sandbox Context Manager \u00b6 # security/sandbox.py import contextlib import subprocess import resource import tempfile import os @contextlib . contextmanager def sandbox_context ( config : SandboxConfig ): \"\"\"Context manager for sandboxed plugin execution.\"\"\" # Set resource limits if config . memory_limit_mb : resource . setrlimit ( resource . RLIMIT_AS , ( config . memory_limit_mb * 1024 * 1024 , - 1 )) if config . cpu_time_limit_s : resource . setrlimit ( resource . RLIMIT_CPU , ( config . cpu_time_limit_s , - 1 )) # Create temporary directory for filesystem access if config . filesystem_access == \"temp\" : with tempfile . TemporaryDirectory () as temp_dir : os . chdir ( temp_dir ) yield else : yield \ud83d\udcca Audit Logging \u00b6 Security Event Types \u00b6 # security/audit_logging.py @dataclass class SecurityEvent : \"\"\"Security event for audit logging.\"\"\" event_type : str plugin_name : str user_id : Optional [ str ] session_id : Optional [ str ] timestamp : str details : Dict [ str , Any ] severity : str # \"info\" | \"warning\" | \"error\" | \"critical\" class SecurityAuditLogger : \"\"\"Security audit logger for M1.\"\"\" def log_plugin_load ( self , plugin_name : str , user_id : Optional [ str ] = None , session_id : Optional [ str ] = None ): \"\"\"Log plugin loading event.\"\"\" event = SecurityEvent ( event_type = \"plugin_load\" , plugin_name = plugin_name , user_id = user_id , session_id = session_id , timestamp = datetime . utcnow () . isoformat () + \"Z\" , details = { \"action\" : \"plugin_loaded\" }, severity = \"info\" ) self . _log_event ( event ) def log_signature_verification ( self , plugin_name : str , success : bool , user_id : Optional [ str ] = None ): \"\"\"Log signature verification event.\"\"\" event = SecurityEvent ( event_type = \"signature_verification\" , plugin_name = plugin_name , user_id = user_id , session_id = None , timestamp = datetime . utcnow () . isoformat () + \"Z\" , details = { \"success\" : success }, severity = \"warning\" if not success else \"info\" ) self . _log_event ( event ) def log_security_violation ( self , plugin_name : str , violation_type : str , details : Dict [ str , Any ], user_id : Optional [ str ] = None ): \"\"\"Log security violation event.\"\"\" event = SecurityEvent ( event_type = \"security_violation\" , plugin_name = plugin_name , user_id = user_id , session_id = None , timestamp = datetime . utcnow () . isoformat () + \"Z\" , details = { \"violation_type\" : violation_type , ** details }, severity = \"critical\" ) self . _log_event ( event ) Audit Event Examples \u00b6 { \"event_type\" : \"plugin_load\" , \"plugin_name\" : \"cve-mapper\" , \"user_id\" : \"user-123\" , \"session_id\" : \"session-456\" , \"timestamp\" : \"2025-10-14T10:30:00Z\" , \"details\" : { \"action\" : \"plugin_loaded\" , \"version\" : \"1.0.0\" , \"signature_verified\" : true }, \"severity\" : \"info\" } { \"event_type\" : \"security_violation\" , \"plugin_name\" : \"malicious-plugin\" , \"user_id\" : \"user-123\" , \"session_id\" : null , \"timestamp\" : \"2025-10-14T10:35:00Z\" , \"details\" : { \"violation_type\" : \"sandbox_escape\" , \"attempted_action\" : \"file_system_access\" , \"blocked_path\" : \"/etc/passwd\" }, \"severity\" : \"critical\" } \ud83d\udee1\ufe0f Policy Enforcement \u00b6 Security Policy Manager \u00b6 # security/policy_enforcement.py @dataclass class SecurityPolicy : \"\"\"Security policy configuration.\"\"\" allow_unsigned_plugins : bool = False # M1: Allow in development require_signature_verification : bool = True sandbox_enabled : bool = True audit_logging_enabled : bool = True max_plugin_execution_time : int = 300 # seconds max_plugin_memory_mb : int = 512 allowed_plugin_categories : List [ str ] = None class SecurityPolicyManager : \"\"\"Security policy manager for M1.\"\"\" def __init__ ( self , policy : SecurityPolicy ): self . policy = policy def check_plugin_security ( self , plugin_name : str , manifest : PluginManifest ) -> bool : \"\"\"Check if plugin meets security policy requirements.\"\"\" # Check signature verification requirement if self . policy . require_signature_verification : if not manifest . signature and not self . policy . allow_unsigned_plugins : return False # Check plugin category if self . policy . allowed_plugin_categories : if manifest . category not in self . policy . allowed_plugin_categories : return False return True def enforce_security_policy ( self , plugin_name : str , execution_context : Dict [ str , Any ]) -> bool : \"\"\"Enforce security policy during plugin execution.\"\"\" # Check execution time limits if execution_context . get ( \"execution_time\" , 0 ) > self . policy . max_plugin_execution_time : return False # Check memory limits if execution_context . get ( \"memory_usage_mb\" , 0 ) > self . policy . max_plugin_memory_mb : return False return True Policy Configuration \u00b6 # security/policy.yaml security_policy : allow_unsigned_plugins : false require_signature_verification : true sandbox_enabled : true audit_logging_enabled : true max_plugin_execution_time : 300 max_plugin_memory_mb : 512 allowed_plugin_categories : - \"detector\" - \"enricher\" - \"analytics\" sandbox_config : filesystem_access : \"read-only\" network_access : false memory_limit_mb : 512 cpu_time_limit_s : 300 max_file_size_mb : 10 audit_config : log_level : \"info\" log_file : \"logs/security_audit.log\" max_log_size_mb : 100 log_retention_days : 30 \ud83d\udd0d Security Monitoring \u00b6 Security Metrics \u00b6 # security/metrics.py @dataclass class SecurityMetrics : \"\"\"Security metrics for monitoring.\"\"\" total_plugins_loaded : int = 0 signature_verification_failures : int = 0 sandbox_violations : int = 0 policy_violations : int = 0 security_events_count : int = 0 last_security_event : Optional [ str ] = None class SecurityMonitor : \"\"\"Security monitoring for M1.\"\"\" def __init__ ( self ): self . metrics = SecurityMetrics () self . audit_logger = SecurityAuditLogger () def record_plugin_load ( self , plugin_name : str , success : bool ): \"\"\"Record plugin loading metric.\"\"\" self . metrics . total_plugins_loaded += 1 if not success : self . metrics . signature_verification_failures += 1 def record_sandbox_violation ( self , plugin_name : str , violation_type : str ): \"\"\"Record sandbox violation metric.\"\"\" self . metrics . sandbox_violations += 1 self . audit_logger . log_security_violation ( plugin_name , \"sandbox_violation\" , { \"violation_type\" : violation_type } ) def get_security_summary ( self ) -> Dict [ str , Any ]: \"\"\"Get security metrics summary.\"\"\" return { \"total_plugins_loaded\" : self . metrics . total_plugins_loaded , \"signature_verification_failures\" : self . metrics . signature_verification_failures , \"sandbox_violations\" : self . metrics . sandbox_violations , \"policy_violations\" : self . metrics . policy_violations , \"security_events_count\" : self . metrics . security_events_count , \"last_security_event\" : self . metrics . last_security_event } \ud83d\udea8 Security Incident Response \u00b6 Incident Types \u00b6 Incident Type Severity Response Signature Verification Failure Warning Log event, block plugin execution Sandbox Violation Critical Terminate plugin, revoke signature Policy Violation Error Block execution, log violation Unauthorized Access Attempt Critical Block user, alert administrators Plugin Execution Timeout Warning Terminate plugin, log timeout Incident Response Process \u00b6 # security/incident_response.py class SecurityIncidentResponse : \"\"\"Security incident response for M1.\"\"\" def handle_signature_failure ( self , plugin_name : str , user_id : Optional [ str ] = None ): \"\"\"Handle signature verification failure.\"\"\" # Log security event self . audit_logger . log_signature_verification ( plugin_name , False , user_id ) # Block plugin execution self . block_plugin ( plugin_name ) # Alert administrators (M2+) # self.send_security_alert(\"signature_failure\", plugin_name) def handle_sandbox_violation ( self , plugin_name : str , violation_type : str , user_id : Optional [ str ] = None ): \"\"\"Handle sandbox violation.\"\"\" # Log critical security event self . audit_logger . log_security_violation ( plugin_name , violation_type , { \"severity\" : \"critical\" }, user_id ) # Terminate plugin execution self . terminate_plugin ( plugin_name ) # Revoke plugin signature self . revoke_plugin_signature ( plugin_name ) # Block user if repeated violations (M2+) # if self.get_violation_count(user_id) > 3: # self.block_user(user_id) \ud83d\udd27 Security Configuration \u00b6 Environment Variables \u00b6 # Security configuration environment variables export SECFLOW_SECURITY_POLICY_FILE = \"security/policy.yaml\" export SECFLOW_AUDIT_LOG_LEVEL = \"info\" export SECFLOW_AUDIT_LOG_FILE = \"logs/security_audit.log\" export SECFLOW_SANDBOX_ENABLED = \"true\" export SECFLOW_SIGNATURE_VERIFICATION = \"true\" export SECFLOW_ALLOW_UNSIGNED_PLUGINS = \"false\" Security Settings \u00b6 # security/config.py @dataclass class SecurityConfig : \"\"\"Security configuration for M1.\"\"\" policy_file : str = \"security/policy.yaml\" audit_log_level : str = \"info\" audit_log_file : str = \"logs/security_audit.log\" sandbox_enabled : bool = True signature_verification : bool = True allow_unsigned_plugins : bool = False # Development only max_plugin_execution_time : int = 300 max_plugin_memory_mb : int = 512 @classmethod def from_env ( cls ) -> 'SecurityConfig' : \"\"\"Load security configuration from environment variables.\"\"\" return cls ( policy_file = os . getenv ( \"SECFLOW_SECURITY_POLICY_FILE\" , \"security/policy.yaml\" ), audit_log_level = os . getenv ( \"SECFLOW_AUDIT_LOG_LEVEL\" , \"info\" ), audit_log_file = os . getenv ( \"SECFLOW_AUDIT_LOG_FILE\" , \"logs/security_audit.log\" ), sandbox_enabled = os . getenv ( \"SECFLOW_SANDBOX_ENABLED\" , \"true\" ) . lower () == \"true\" , signature_verification = os . getenv ( \"SECFLOW_SIGNATURE_VERIFICATION\" , \"true\" ) . lower () == \"true\" , allow_unsigned_plugins = os . getenv ( \"SECFLOW_ALLOW_UNSIGNED_PLUGINS\" , \"false\" ) . lower () == \"true\" ) \ud83d\udd2e Future Enhancements (M2+) \u00b6 Advanced Security Features \u00b6 Full Cryptographic Signing : RSA/ECDSA signature verification Certificate Authority : Plugin signing certificate management Container-based Sandboxing : Docker/containerd isolation Network Segmentation : Isolated network environments Real-time Threat Detection : ML-based anomaly detection Security Dashboard : Web-based security monitoring interface Compliance Features \u00b6 SOC 2 Compliance : Security controls and monitoring ISO 27001 : Information security management NIST Cybersecurity Framework : Risk management GDPR Compliance : Data protection and privacy \ud83d\udcda Security Best Practices \u00b6 For Plugin Developers \u00b6 Always Sign Plugins : Use proper signature verification Follow Security Guidelines : Implement secure coding practices Test in Sandbox : Verify plugins work in restricted environment Minimize Privileges : Request only necessary permissions Handle Errors Gracefully : Don't expose sensitive information For System Administrators \u00b6 Enable All Security Features : Use sandboxing and signature verification Monitor Audit Logs : Regularly review security events Update Security Policies : Keep policies current with threats Regular Security Audits : Periodic security assessments Incident Response Plan : Prepare for security incidents For Users \u00b6 Only Install Trusted Plugins : Verify plugin sources Report Security Issues : Report suspicious behavior Keep System Updated : Apply security patches promptly Use Strong Authentication : Secure user accounts Follow Security Policies : Comply with organizational policies Next: Observability, Logging & Metrics","title":"Security"},{"location":"security/#security-documentation","text":"","title":"Security Documentation"},{"location":"security/#overview","text":"This document describes the security model implemented in SecFlow M1, covering plugin security, signature verification, sandboxing, and audit logging. The security framework ensures that plugins execute safely and that all security events are properly logged and monitored. M1 Implementation Status : \u2705 Complete - Plugin signature verification, sandboxing, and audit logging delivered.","title":"\ud83e\udded Overview"},{"location":"security/#security-architecture","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Plugin Loader\"] B[\"Security Framework\"] C[\"Signature Verification\"] D[\"Sandbox Execution\"] E[\"Audit Logging\"] F[\"Policy Enforcement\"] A --> B B --> C B --> D B --> E B --> F G[\"Plugin Manifest\"] H[\"Hash Verification\"] I[\"Sandbox Context\"] J[\"Security Events\"] G --> C H --> C I --> D J --> E M1 Security Components : - \u2705 Plugin Signature Verification : Hash-based integrity checking - \u2705 Sandbox Execution : Controlled execution environment - \u2705 Audit Logging : Comprehensive security event logging - \u2705 Policy Enforcement : Security policy validation - \u2705 Manifest Validation : JSON schema validation for plugin manifests","title":"\ud83d\udd10 Security Architecture"},{"location":"security/#plugin-security-model","text":"","title":"\ud83d\udd11 Plugin Security Model"},{"location":"security/#signature-verification-m1","text":"M1 Implementation : Hash-based verification using SHA256 checksums. # security/signing.py @dataclass class PluginManifest : \"\"\"Plugin manifest with signature information.\"\"\" name : str version : str description : str author : str entrypoint : str code_hash : str signature : Optional [ str ] = None signature_type : str = \"sha256\" # M1: hash-based, M2+: RSA/ECDSA created_at : str = None expires_at : Optional [ str ] = None class PluginSignatureVerifier : \"\"\"Plugin signature verification for M1.\"\"\" def verify_plugin_signature ( self , manifest : PluginManifest , plugin_path : str ) -> bool : \"\"\"Verify plugin signature using hash-based verification.\"\"\" # M1: SHA256 hash verification expected_hash = manifest . code_hash actual_hash = self . _calculate_file_hash ( plugin_path ) return expected_hash == actual_hash def _calculate_file_hash ( self , file_path : str ) -> str : \"\"\"Calculate SHA256 hash of plugin file.\"\"\" import hashlib with open ( file_path , 'rb' ) as f : return hashlib . sha256 ( f . read ()) . hexdigest () M1 Security Features : - \u2705 Integrity Checking : SHA256 checksums prevent tampering - \u2705 Manifest Validation : JSON schema validation - \u2705 Expiration Support : Optional expiration dates - \u2705 Author Verification : Author information in manifest","title":"Signature Verification (M1)"},{"location":"security/#cryptographic-signing-m2-planned","text":"Future Enhancement : Full cryptographic signature verification using RSA/ECDSA. # M2+: Full cryptographic verification (planned) def verify_plugin_signature_crypto ( manifest : PluginManifest , plugin_path : str , public_key : str ) -> bool : \"\"\"Verify plugin signature using RSA/ECDSA.\"\"\" from cryptography.hazmat.primitives import hashes , serialization from cryptography.hazmat.primitives.asymmetric import padding , rsa # Load public key public_key_obj = load_pem_public_key ( public_key . encode ()) # Verify signature try : public_key_obj . verify ( manifest . signature . encode (), manifest . code_hash . encode (), padding . PSS ( mgf = padding . MGF1 ( hashes . SHA256 ()), salt_length = padding . PSS . MAX_LENGTH ), hashes . SHA256 () ) return True except InvalidSignature : return False","title":"Cryptographic Signing (M2+ Planned)"},{"location":"security/#sandbox-execution-model","text":"","title":"\ud83c\udfd7\ufe0f Sandbox Execution Model"},{"location":"security/#sandbox-configuration","text":"# security/sandbox.py @dataclass class SandboxConfig : \"\"\"Sandbox configuration for plugin execution.\"\"\" filesystem_access : str = \"read-only\" # \"read-only\" | \"temp\" | \"denied\" network_access : bool = False memory_limit_mb : int = 512 cpu_time_limit_s : int = 300 max_file_size_mb : int = 10 allowed_imports : List [ str ] = None denied_imports : List [ str ] = None def run_plugin_secure ( plugin : PluginInterface , inputs : Dict [ str , Any ], config : SandboxConfig ) -> Dict [ str , Any ]: \"\"\"Execute plugin in sandboxed environment.\"\"\" with sandbox_context ( config ): return plugin . run ( inputs , config , context )","title":"Sandbox Configuration"},{"location":"security/#sandbox-controls","text":"Control Enforcement M1 Implementation Filesystem Read-only mount or temp directory \u2705 Implemented Network Denied by default, opt-in per manifest \u2705 Implemented Memory / CPU Controlled via subprocess resource limits \u2705 Implemented Timeouts Enforced via execution wrapper \u2705 Implemented Imports Whitelist/blacklist of allowed modules \u2705 Implemented Audit Every plugin invocation logged with context \u2705 Implemented","title":"Sandbox Controls"},{"location":"security/#sandbox-context-manager","text":"# security/sandbox.py import contextlib import subprocess import resource import tempfile import os @contextlib . contextmanager def sandbox_context ( config : SandboxConfig ): \"\"\"Context manager for sandboxed plugin execution.\"\"\" # Set resource limits if config . memory_limit_mb : resource . setrlimit ( resource . RLIMIT_AS , ( config . memory_limit_mb * 1024 * 1024 , - 1 )) if config . cpu_time_limit_s : resource . setrlimit ( resource . RLIMIT_CPU , ( config . cpu_time_limit_s , - 1 )) # Create temporary directory for filesystem access if config . filesystem_access == \"temp\" : with tempfile . TemporaryDirectory () as temp_dir : os . chdir ( temp_dir ) yield else : yield","title":"Sandbox Context Manager"},{"location":"security/#audit-logging","text":"","title":"\ud83d\udcca Audit Logging"},{"location":"security/#security-event-types","text":"# security/audit_logging.py @dataclass class SecurityEvent : \"\"\"Security event for audit logging.\"\"\" event_type : str plugin_name : str user_id : Optional [ str ] session_id : Optional [ str ] timestamp : str details : Dict [ str , Any ] severity : str # \"info\" | \"warning\" | \"error\" | \"critical\" class SecurityAuditLogger : \"\"\"Security audit logger for M1.\"\"\" def log_plugin_load ( self , plugin_name : str , user_id : Optional [ str ] = None , session_id : Optional [ str ] = None ): \"\"\"Log plugin loading event.\"\"\" event = SecurityEvent ( event_type = \"plugin_load\" , plugin_name = plugin_name , user_id = user_id , session_id = session_id , timestamp = datetime . utcnow () . isoformat () + \"Z\" , details = { \"action\" : \"plugin_loaded\" }, severity = \"info\" ) self . _log_event ( event ) def log_signature_verification ( self , plugin_name : str , success : bool , user_id : Optional [ str ] = None ): \"\"\"Log signature verification event.\"\"\" event = SecurityEvent ( event_type = \"signature_verification\" , plugin_name = plugin_name , user_id = user_id , session_id = None , timestamp = datetime . utcnow () . isoformat () + \"Z\" , details = { \"success\" : success }, severity = \"warning\" if not success else \"info\" ) self . _log_event ( event ) def log_security_violation ( self , plugin_name : str , violation_type : str , details : Dict [ str , Any ], user_id : Optional [ str ] = None ): \"\"\"Log security violation event.\"\"\" event = SecurityEvent ( event_type = \"security_violation\" , plugin_name = plugin_name , user_id = user_id , session_id = None , timestamp = datetime . utcnow () . isoformat () + \"Z\" , details = { \"violation_type\" : violation_type , ** details }, severity = \"critical\" ) self . _log_event ( event )","title":"Security Event Types"},{"location":"security/#audit-event-examples","text":"{ \"event_type\" : \"plugin_load\" , \"plugin_name\" : \"cve-mapper\" , \"user_id\" : \"user-123\" , \"session_id\" : \"session-456\" , \"timestamp\" : \"2025-10-14T10:30:00Z\" , \"details\" : { \"action\" : \"plugin_loaded\" , \"version\" : \"1.0.0\" , \"signature_verified\" : true }, \"severity\" : \"info\" } { \"event_type\" : \"security_violation\" , \"plugin_name\" : \"malicious-plugin\" , \"user_id\" : \"user-123\" , \"session_id\" : null , \"timestamp\" : \"2025-10-14T10:35:00Z\" , \"details\" : { \"violation_type\" : \"sandbox_escape\" , \"attempted_action\" : \"file_system_access\" , \"blocked_path\" : \"/etc/passwd\" }, \"severity\" : \"critical\" }","title":"Audit Event Examples"},{"location":"security/#policy-enforcement","text":"","title":"\ud83d\udee1\ufe0f Policy Enforcement"},{"location":"security/#security-policy-manager","text":"# security/policy_enforcement.py @dataclass class SecurityPolicy : \"\"\"Security policy configuration.\"\"\" allow_unsigned_plugins : bool = False # M1: Allow in development require_signature_verification : bool = True sandbox_enabled : bool = True audit_logging_enabled : bool = True max_plugin_execution_time : int = 300 # seconds max_plugin_memory_mb : int = 512 allowed_plugin_categories : List [ str ] = None class SecurityPolicyManager : \"\"\"Security policy manager for M1.\"\"\" def __init__ ( self , policy : SecurityPolicy ): self . policy = policy def check_plugin_security ( self , plugin_name : str , manifest : PluginManifest ) -> bool : \"\"\"Check if plugin meets security policy requirements.\"\"\" # Check signature verification requirement if self . policy . require_signature_verification : if not manifest . signature and not self . policy . allow_unsigned_plugins : return False # Check plugin category if self . policy . allowed_plugin_categories : if manifest . category not in self . policy . allowed_plugin_categories : return False return True def enforce_security_policy ( self , plugin_name : str , execution_context : Dict [ str , Any ]) -> bool : \"\"\"Enforce security policy during plugin execution.\"\"\" # Check execution time limits if execution_context . get ( \"execution_time\" , 0 ) > self . policy . max_plugin_execution_time : return False # Check memory limits if execution_context . get ( \"memory_usage_mb\" , 0 ) > self . policy . max_plugin_memory_mb : return False return True","title":"Security Policy Manager"},{"location":"security/#policy-configuration","text":"# security/policy.yaml security_policy : allow_unsigned_plugins : false require_signature_verification : true sandbox_enabled : true audit_logging_enabled : true max_plugin_execution_time : 300 max_plugin_memory_mb : 512 allowed_plugin_categories : - \"detector\" - \"enricher\" - \"analytics\" sandbox_config : filesystem_access : \"read-only\" network_access : false memory_limit_mb : 512 cpu_time_limit_s : 300 max_file_size_mb : 10 audit_config : log_level : \"info\" log_file : \"logs/security_audit.log\" max_log_size_mb : 100 log_retention_days : 30","title":"Policy Configuration"},{"location":"security/#security-monitoring","text":"","title":"\ud83d\udd0d Security Monitoring"},{"location":"security/#security-metrics","text":"# security/metrics.py @dataclass class SecurityMetrics : \"\"\"Security metrics for monitoring.\"\"\" total_plugins_loaded : int = 0 signature_verification_failures : int = 0 sandbox_violations : int = 0 policy_violations : int = 0 security_events_count : int = 0 last_security_event : Optional [ str ] = None class SecurityMonitor : \"\"\"Security monitoring for M1.\"\"\" def __init__ ( self ): self . metrics = SecurityMetrics () self . audit_logger = SecurityAuditLogger () def record_plugin_load ( self , plugin_name : str , success : bool ): \"\"\"Record plugin loading metric.\"\"\" self . metrics . total_plugins_loaded += 1 if not success : self . metrics . signature_verification_failures += 1 def record_sandbox_violation ( self , plugin_name : str , violation_type : str ): \"\"\"Record sandbox violation metric.\"\"\" self . metrics . sandbox_violations += 1 self . audit_logger . log_security_violation ( plugin_name , \"sandbox_violation\" , { \"violation_type\" : violation_type } ) def get_security_summary ( self ) -> Dict [ str , Any ]: \"\"\"Get security metrics summary.\"\"\" return { \"total_plugins_loaded\" : self . metrics . total_plugins_loaded , \"signature_verification_failures\" : self . metrics . signature_verification_failures , \"sandbox_violations\" : self . metrics . sandbox_violations , \"policy_violations\" : self . metrics . policy_violations , \"security_events_count\" : self . metrics . security_events_count , \"last_security_event\" : self . metrics . last_security_event }","title":"Security Metrics"},{"location":"security/#security-incident-response","text":"","title":"\ud83d\udea8 Security Incident Response"},{"location":"security/#incident-types","text":"Incident Type Severity Response Signature Verification Failure Warning Log event, block plugin execution Sandbox Violation Critical Terminate plugin, revoke signature Policy Violation Error Block execution, log violation Unauthorized Access Attempt Critical Block user, alert administrators Plugin Execution Timeout Warning Terminate plugin, log timeout","title":"Incident Types"},{"location":"security/#incident-response-process","text":"# security/incident_response.py class SecurityIncidentResponse : \"\"\"Security incident response for M1.\"\"\" def handle_signature_failure ( self , plugin_name : str , user_id : Optional [ str ] = None ): \"\"\"Handle signature verification failure.\"\"\" # Log security event self . audit_logger . log_signature_verification ( plugin_name , False , user_id ) # Block plugin execution self . block_plugin ( plugin_name ) # Alert administrators (M2+) # self.send_security_alert(\"signature_failure\", plugin_name) def handle_sandbox_violation ( self , plugin_name : str , violation_type : str , user_id : Optional [ str ] = None ): \"\"\"Handle sandbox violation.\"\"\" # Log critical security event self . audit_logger . log_security_violation ( plugin_name , violation_type , { \"severity\" : \"critical\" }, user_id ) # Terminate plugin execution self . terminate_plugin ( plugin_name ) # Revoke plugin signature self . revoke_plugin_signature ( plugin_name ) # Block user if repeated violations (M2+) # if self.get_violation_count(user_id) > 3: # self.block_user(user_id)","title":"Incident Response Process"},{"location":"security/#security-configuration","text":"","title":"\ud83d\udd27 Security Configuration"},{"location":"security/#environment-variables","text":"# Security configuration environment variables export SECFLOW_SECURITY_POLICY_FILE = \"security/policy.yaml\" export SECFLOW_AUDIT_LOG_LEVEL = \"info\" export SECFLOW_AUDIT_LOG_FILE = \"logs/security_audit.log\" export SECFLOW_SANDBOX_ENABLED = \"true\" export SECFLOW_SIGNATURE_VERIFICATION = \"true\" export SECFLOW_ALLOW_UNSIGNED_PLUGINS = \"false\"","title":"Environment Variables"},{"location":"security/#security-settings","text":"# security/config.py @dataclass class SecurityConfig : \"\"\"Security configuration for M1.\"\"\" policy_file : str = \"security/policy.yaml\" audit_log_level : str = \"info\" audit_log_file : str = \"logs/security_audit.log\" sandbox_enabled : bool = True signature_verification : bool = True allow_unsigned_plugins : bool = False # Development only max_plugin_execution_time : int = 300 max_plugin_memory_mb : int = 512 @classmethod def from_env ( cls ) -> 'SecurityConfig' : \"\"\"Load security configuration from environment variables.\"\"\" return cls ( policy_file = os . getenv ( \"SECFLOW_SECURITY_POLICY_FILE\" , \"security/policy.yaml\" ), audit_log_level = os . getenv ( \"SECFLOW_AUDIT_LOG_LEVEL\" , \"info\" ), audit_log_file = os . getenv ( \"SECFLOW_AUDIT_LOG_FILE\" , \"logs/security_audit.log\" ), sandbox_enabled = os . getenv ( \"SECFLOW_SANDBOX_ENABLED\" , \"true\" ) . lower () == \"true\" , signature_verification = os . getenv ( \"SECFLOW_SIGNATURE_VERIFICATION\" , \"true\" ) . lower () == \"true\" , allow_unsigned_plugins = os . getenv ( \"SECFLOW_ALLOW_UNSIGNED_PLUGINS\" , \"false\" ) . lower () == \"true\" )","title":"Security Settings"},{"location":"security/#future-enhancements-m2","text":"","title":"\ud83d\udd2e Future Enhancements (M2+)"},{"location":"security/#advanced-security-features","text":"Full Cryptographic Signing : RSA/ECDSA signature verification Certificate Authority : Plugin signing certificate management Container-based Sandboxing : Docker/containerd isolation Network Segmentation : Isolated network environments Real-time Threat Detection : ML-based anomaly detection Security Dashboard : Web-based security monitoring interface","title":"Advanced Security Features"},{"location":"security/#compliance-features","text":"SOC 2 Compliance : Security controls and monitoring ISO 27001 : Information security management NIST Cybersecurity Framework : Risk management GDPR Compliance : Data protection and privacy","title":"Compliance Features"},{"location":"security/#security-best-practices","text":"","title":"\ud83d\udcda Security Best Practices"},{"location":"security/#for-plugin-developers","text":"Always Sign Plugins : Use proper signature verification Follow Security Guidelines : Implement secure coding practices Test in Sandbox : Verify plugins work in restricted environment Minimize Privileges : Request only necessary permissions Handle Errors Gracefully : Don't expose sensitive information","title":"For Plugin Developers"},{"location":"security/#for-system-administrators","text":"Enable All Security Features : Use sandboxing and signature verification Monitor Audit Logs : Regularly review security events Update Security Policies : Keep policies current with threats Regular Security Audits : Periodic security assessments Incident Response Plan : Prepare for security incidents","title":"For System Administrators"},{"location":"security/#for-users","text":"Only Install Trusted Plugins : Verify plugin sources Report Security Issues : Report suspicious behavior Keep System Updated : Apply security patches promptly Use Strong Authentication : Secure user accounts Follow Security Policies : Comply with organizational policies Next: Observability, Logging & Metrics","title":"For Users"},{"location":"_includes/dod-checklist/","text":"Definition of Done Checklist \u00b6 Code Quality \u00b6 All tests pass ( make test ) Code coverage meets current milestone threshold No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes Documentation \u00b6 Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check) Testing \u00b6 Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold Security & Compliance \u00b6 No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed Review Process \u00b6 PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments","title":"Dod checklist"},{"location":"_includes/dod-checklist/#definition-of-done-checklist","text":"","title":"Definition of Done Checklist"},{"location":"_includes/dod-checklist/#code-quality","text":"All tests pass ( make test ) Code coverage meets current milestone threshold No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes","title":"Code Quality"},{"location":"_includes/dod-checklist/#documentation","text":"Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check)","title":"Documentation"},{"location":"_includes/dod-checklist/#testing","text":"Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold","title":"Testing"},{"location":"_includes/dod-checklist/#security-compliance","text":"No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed","title":"Security &amp; Compliance"},{"location":"_includes/dod-checklist/#review-process","text":"PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments","title":"Review Process"},{"location":"adr/0000-adr-template/","text":"ADR-0000: \u00b6 Status \u00b6 Proposed | Accepted | Superseded Context \u00b6 Decision \u00b6 Consequences \u00b6 Alternatives Considered \u00b6 Links \u00b6","title":"ADR-0000:"},{"location":"adr/0000-adr-template/#adr-0000","text":"","title":"ADR-0000:"},{"location":"adr/0000-adr-template/#status","text":"Proposed | Accepted | Superseded","title":"Status"},{"location":"adr/0000-adr-template/#context","text":"","title":"Context"},{"location":"adr/0000-adr-template/#decision","text":"","title":"Decision"},{"location":"adr/0000-adr-template/#consequences","text":"","title":"Consequences"},{"location":"adr/0000-adr-template/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adr/0000-adr-template/#links","text":"","title":"Links"},{"location":"api/","text":"API Documentation \u00b6 Overview \u00b6 This section contains comprehensive API documentation for SecFlow. The API documentation is generated from source code using mkdocstrings and will be fully implemented in M6. Current Status \u00b6 M0-M5 : Manual API documentation M6+ : Automated API documentation with mkdocstrings API Structure \u00b6 Core APIs \u00b6 Analytics Core : Metrics and telemetry APIs Detection Engine : Pattern matching and vulnerability detection Storage Layer : Data persistence and retrieval Workflow Engine : Task orchestration and execution Integration APIs \u00b6 Nuclei Integration : External tool integration ModSecurity Integration : Security rule processing Plugin System : Extensible architecture APIs Documentation Standards \u00b6 Code Documentation \u00b6 All public APIs must have comprehensive docstrings Type hints required for all parameters and return values Examples provided for complex APIs Error conditions documented API Design Principles \u00b6 RESTful design where applicable Consistent error handling and response formats Versioning strategy for breaking changes Rate limiting and authentication requirements Future Implementation (M6) \u00b6 The following mkdocstrings configuration will be enabled in M6: # mkdocs.yml additions (commented until M6) # plugins: # - mkdocstrings: # handlers: # python: # paths: [.] # options: # docstring_style: google # show_source: true # show_root_heading: true # show_root_toc_entry: true # show_signature_annotations: true # show_signature_return_annotations: true # separate_signature: true # heading_level: 2 # filters: [\"!^_\"] # members_order: source # merge_init_into_class: true Manual Documentation (M0-M5) \u00b6 Until M6, API documentation will be maintained manually in this directory: core-apis.md - Core system APIs integration-apis.md - External integration APIs plugin-apis.md - Plugin system APIs examples/ - Code examples and usage patterns Contributing \u00b6 When adding new APIs: Follow the documentation standards above Add manual documentation until M6 Ensure all public methods have docstrings Include type hints and examples Update this README if adding new API categories Links \u00b6 Development Conventions - API development standards Architecture Overview - System architecture Plugin System - Plugin architecture","title":"Overview"},{"location":"api/#api-documentation","text":"","title":"API Documentation"},{"location":"api/#overview","text":"This section contains comprehensive API documentation for SecFlow. The API documentation is generated from source code using mkdocstrings and will be fully implemented in M6.","title":"Overview"},{"location":"api/#current-status","text":"M0-M5 : Manual API documentation M6+ : Automated API documentation with mkdocstrings","title":"Current Status"},{"location":"api/#api-structure","text":"","title":"API Structure"},{"location":"api/#core-apis","text":"Analytics Core : Metrics and telemetry APIs Detection Engine : Pattern matching and vulnerability detection Storage Layer : Data persistence and retrieval Workflow Engine : Task orchestration and execution","title":"Core APIs"},{"location":"api/#integration-apis","text":"Nuclei Integration : External tool integration ModSecurity Integration : Security rule processing Plugin System : Extensible architecture APIs","title":"Integration APIs"},{"location":"api/#documentation-standards","text":"","title":"Documentation Standards"},{"location":"api/#code-documentation","text":"All public APIs must have comprehensive docstrings Type hints required for all parameters and return values Examples provided for complex APIs Error conditions documented","title":"Code Documentation"},{"location":"api/#api-design-principles","text":"RESTful design where applicable Consistent error handling and response formats Versioning strategy for breaking changes Rate limiting and authentication requirements","title":"API Design Principles"},{"location":"api/#future-implementation-m6","text":"The following mkdocstrings configuration will be enabled in M6: # mkdocs.yml additions (commented until M6) # plugins: # - mkdocstrings: # handlers: # python: # paths: [.] # options: # docstring_style: google # show_source: true # show_root_heading: true # show_root_toc_entry: true # show_signature_annotations: true # show_signature_return_annotations: true # separate_signature: true # heading_level: 2 # filters: [\"!^_\"] # members_order: source # merge_init_into_class: true","title":"Future Implementation (M6)"},{"location":"api/#manual-documentation-m0-m5","text":"Until M6, API documentation will be maintained manually in this directory: core-apis.md - Core system APIs integration-apis.md - External integration APIs plugin-apis.md - Plugin system APIs examples/ - Code examples and usage patterns","title":"Manual Documentation (M0-M5)"},{"location":"api/#contributing","text":"When adding new APIs: Follow the documentation standards above Add manual documentation until M6 Ensure all public methods have docstrings Include type hints and examples Update this README if adding new API categories","title":"Contributing"},{"location":"api/#links","text":"Development Conventions - API development standards Architecture Overview - System architecture Plugin System - Plugin architecture","title":"Links"},{"location":"architecture/00-index/","text":"SecFlow \u2014 Architecture & Design Documentation Index \u00b6 Welcome to the SecFlow technical documentation suite. This collection defines the complete internal architecture of the Security Toolkit platform \u2014 covering everything from orchestration engines to data models, plugin systems, and CVE enrichment. Each document in this /docs/architecture/ folder represents a bounded architectural domain , written in deep-technical mode to serve engineers and architects implementing or extending the system. \ud83d\udcda Document Navigation \u00b6 # File Description 01 Title & Executive Summary Project mission, audience, and objectives. 02 Architecture Philosophy Core tenets, design rationale, and constraints. 03 Repository Layout Folder hierarchy, import rules, and tooling. 04 Core Packages & Responsibilities Module roles, data contracts, and dependencies. 05 Orchestration & Workflow Engine DAG-based tool chaining and execution logic. 06 Plugin System Plugin registration, lifecycle, and testing. 07 Tools Integration Model Wrappers, manifests, and sandbox execution. 08 Tool Manager & UX Design How users manage tools and workflows. 09 Resource Registry Global wordlists, payloads, templates, and configs. 10 Wordlist & Output Sharing Rules for shared and isolated resources. 11 Project Isolation & Data Sharing Workspace boundaries, access control, linking. 12 Findings Model & Schema Data models, normalization, and persistence. 13 CVE/CWE/POC Enrichment Layer External vulnerability mapping and enrichment. 14 POC Sources & Legal Guidelines Governance and sandbox policies for proof-of-concepts. 15 Garbage Collection & Retention Data lifecycle, cleanup tasks, and recovery. 16 Security Model Auth, RBAC, sandboxing, and auditing. 17 Observability, Logging & Metrics Telemetry, structured logs, and OpenTelemetry. 18 Error Handling & Recovery Centralized error strategies and resilience patterns. 19 Risk Assessment Framework NIST 5\u00d75 Matrix, CVSS, and MITRE mapping. 20 Migration & Implementation Phases Step-by-step rollout plan. 21 CI/CD & Testing Strategy Build matrix, pipelines, and test automation. 22 Developer Experience & Docs Local setup, MkDocs, and contribution workflow. 23 Future Roadmap AI triage, automation, and next-gen features. \ud83e\udded Reading Order \u00b6 While each document is standalone, recommended order for new contributors: Start with 01\u201304 to grasp architecture and structure. Proceed to 05\u201307 for orchestration and integration. Review 09\u201313 for data, findings, and enrichment logic. Study 16\u201318 for security and observability. Conclude with 19\u201323 for governance, testing, and roadmap. \ud83e\udde9 Versioning & Maintenance \u00b6 Documentation version follows codebase major versions (e.g., 1.0 = v1.x releases). Each architectural change must update its corresponding file and cross-links. Use mkdocs serve locally to preview rendered docs. Changes to interfaces or DTOs require schema diff logs. \ud83e\uddf1 Example Structure Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/\"] C[\"00-index.md\"] D[\"01-title-and-executive-summary.md\"] E[\"02-architecture-philosophy.md\"] F[\"03-repository-layout.md\"] G[\"04-core-packages-and-responsibilities.md\"] H[\"05-orchestration-and-workflow-engine.md\"] I[\"06-plugin-system.md\"] J[\"07-tools-integration-model.md\"] K[\"08-tool-manager-and-ux-design.md\"] L[\"09-resource-registry.md\"] M[\"10-wordlist-and-output-sharing.md\"] N[\"11-project-isolation-and-data-sharing.md\"] O[\"12-findings-model-and-schema.md\"] P[\"13-cve-cwe-poc-enrichment-layer.md\"] Q[\"14-poc-sources-and-legal-guidelines.md\"] R[\"15-garbage-collection-and-retention.md\"] S[\"16-security-model.md\"] T[\"17-observability-logging-and-metrics.md\"] U[\"18-error-handling-and-recovery.md\"] V[\"19-risk-assessment-framework.md\"] W[\"20-migration-and-implementation-phases.md\"] X[\"21-ci-cd-and-testing-strategy.md\"] Y[\"22-developer-experience-and-docs.md\"] Z[\"23-future-roadmap.md\"] A --> B B --> C B --> D B --> E B --> F B --> G B --> H B --> I B --> J B --> K B --> L B --> M B --> N B --> O B --> P B --> Q B --> R B --> S B --> T B --> U B --> V B --> W B --> X B --> Y B --> Z Next: Title & Executive Summary ```","title":"Home"},{"location":"architecture/00-index/#secflow-architecture-design-documentation-index","text":"Welcome to the SecFlow technical documentation suite. This collection defines the complete internal architecture of the Security Toolkit platform \u2014 covering everything from orchestration engines to data models, plugin systems, and CVE enrichment. Each document in this /docs/architecture/ folder represents a bounded architectural domain , written in deep-technical mode to serve engineers and architects implementing or extending the system.","title":"SecFlow \u2014 Architecture &amp; Design Documentation Index"},{"location":"architecture/00-index/#document-navigation","text":"# File Description 01 Title & Executive Summary Project mission, audience, and objectives. 02 Architecture Philosophy Core tenets, design rationale, and constraints. 03 Repository Layout Folder hierarchy, import rules, and tooling. 04 Core Packages & Responsibilities Module roles, data contracts, and dependencies. 05 Orchestration & Workflow Engine DAG-based tool chaining and execution logic. 06 Plugin System Plugin registration, lifecycle, and testing. 07 Tools Integration Model Wrappers, manifests, and sandbox execution. 08 Tool Manager & UX Design How users manage tools and workflows. 09 Resource Registry Global wordlists, payloads, templates, and configs. 10 Wordlist & Output Sharing Rules for shared and isolated resources. 11 Project Isolation & Data Sharing Workspace boundaries, access control, linking. 12 Findings Model & Schema Data models, normalization, and persistence. 13 CVE/CWE/POC Enrichment Layer External vulnerability mapping and enrichment. 14 POC Sources & Legal Guidelines Governance and sandbox policies for proof-of-concepts. 15 Garbage Collection & Retention Data lifecycle, cleanup tasks, and recovery. 16 Security Model Auth, RBAC, sandboxing, and auditing. 17 Observability, Logging & Metrics Telemetry, structured logs, and OpenTelemetry. 18 Error Handling & Recovery Centralized error strategies and resilience patterns. 19 Risk Assessment Framework NIST 5\u00d75 Matrix, CVSS, and MITRE mapping. 20 Migration & Implementation Phases Step-by-step rollout plan. 21 CI/CD & Testing Strategy Build matrix, pipelines, and test automation. 22 Developer Experience & Docs Local setup, MkDocs, and contribution workflow. 23 Future Roadmap AI triage, automation, and next-gen features.","title":"\ud83d\udcda Document Navigation"},{"location":"architecture/00-index/#reading-order","text":"While each document is standalone, recommended order for new contributors: Start with 01\u201304 to grasp architecture and structure. Proceed to 05\u201307 for orchestration and integration. Review 09\u201313 for data, findings, and enrichment logic. Study 16\u201318 for security and observability. Conclude with 19\u201323 for governance, testing, and roadmap.","title":"\ud83e\udded Reading Order"},{"location":"architecture/00-index/#versioning-maintenance","text":"Documentation version follows codebase major versions (e.g., 1.0 = v1.x releases). Each architectural change must update its corresponding file and cross-links. Use mkdocs serve locally to preview rendered docs. Changes to interfaces or DTOs require schema diff logs.","title":"\ud83e\udde9 Versioning &amp; Maintenance"},{"location":"architecture/00-index/#example-structure-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/\"] C[\"00-index.md\"] D[\"01-title-and-executive-summary.md\"] E[\"02-architecture-philosophy.md\"] F[\"03-repository-layout.md\"] G[\"04-core-packages-and-responsibilities.md\"] H[\"05-orchestration-and-workflow-engine.md\"] I[\"06-plugin-system.md\"] J[\"07-tools-integration-model.md\"] K[\"08-tool-manager-and-ux-design.md\"] L[\"09-resource-registry.md\"] M[\"10-wordlist-and-output-sharing.md\"] N[\"11-project-isolation-and-data-sharing.md\"] O[\"12-findings-model-and-schema.md\"] P[\"13-cve-cwe-poc-enrichment-layer.md\"] Q[\"14-poc-sources-and-legal-guidelines.md\"] R[\"15-garbage-collection-and-retention.md\"] S[\"16-security-model.md\"] T[\"17-observability-logging-and-metrics.md\"] U[\"18-error-handling-and-recovery.md\"] V[\"19-risk-assessment-framework.md\"] W[\"20-migration-and-implementation-phases.md\"] X[\"21-ci-cd-and-testing-strategy.md\"] Y[\"22-developer-experience-and-docs.md\"] Z[\"23-future-roadmap.md\"] A --> B B --> C B --> D B --> E B --> F B --> G B --> H B --> I B --> J B --> K B --> L B --> M B --> N B --> O B --> P B --> Q B --> R B --> S B --> T B --> U B --> V B --> W B --> X B --> Y B --> Z Next: Title & Executive Summary ```","title":"\ud83e\uddf1 Example Structure Diagram"},{"location":"architecture/01-title-and-executive-summary/","text":"01 \u2014 Title & Executive Summary \u00b6 \ud83e\udde9 Project Name \u00b6 SecFlow \u2014 Security Toolkit Orchestration Framework \ud83e\udded Executive Summary \u00b6 SecFlow is a modular, extensible, and automation-first security orchestration platform designed to unify multiple open-source and proprietary tools for penetration testing, vulnerability management, and risk analytics. It merges dynamic scanning , tool orchestration , data normalization , and risk intelligence into a single cohesive system \u2014 optimized for both solo pentesters and enterprise red-team workflows. Core Vision \u00b6 \"One orchestrator to chain, enrich, and analyze all security tools \u2014 safely, intelligently, and reproducibly.\" Core Differentiators \u00b6 \ud83d\udd04 End-to-end orchestration: Tools (Nuclei, Feroxbuster, Katana, ZAP, Burp, Caido, etc.) communicate via normalized JSON pipelines. \ud83e\uddf1 Hexagonal architecture: Strict separation between core logic , adapters , and interfaces . \ud83e\udde0 Data-driven intelligence: Findings automatically enriched with CVE/CWE/CVSS and MITRE mapping. \u2699\ufe0f Resource registry: Shared wordlists, templates, and payloads under a versioned resource management system. \ud83e\uddf0 AI-ready foundation: All findings and telemetry are machine-readable for later AI triage and summarization. \ud83c\udfaf Project Goals \u00b6 Category Objective Engineering Create a modular mono-repo structure enabling multiple apps (CLI, API, Worker, UI). Automation Allow users to chain discovery \u2192 scanning \u2192 enrichment workflows via declarative YAML recipes. Data Consistency Standardize tool outputs into common Finding DTOs for unified triage and metrics. Usability Provide a CLI and web UI for project management, orchestration, and analytics. Security Guarantee safe sandboxing, configuration isolation, and audit logging. Scalability Support multi-tenant projects with configurable resource scopes. \ud83e\uddee Key Capabilities Overview \u00b6 Layer Functionality Description Core-Lib Schema & DTOs Defines data models, ports, and contracts. Findings-Engine Normalization Converts raw tool outputs into structured findings. Wrappers Tool Adapters Execute, monitor, and parse tool output. Worker Orchestration Runs workflows (DAG execution, concurrency, retry policies). Web-API REST + WS Interface Exposes project, scan, and findings endpoints. UI HTMX/React Web dashboard for triage and insights. Plugins Extensibility Detectors, enrichers, or analytics extensions. \ud83e\uddf1 Architecture Summary \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Web-API / UI\"] B[\"REST / WebSocket\"] C[\"Worker Engine<br/>(Workflows, DAG, Queues)\"] D[\"Ports / DTOs\"] E[\"Core-Lib<br/>(Models, Ports, Repositories)\"] F[\"Wrappers / Plugins<br/>(Nuclei, Ferox, Katana\u2026)\"] G[\"Tool Output\"] H[\"Findings Engine<br/>(Normalization, Metrics)\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H \u2699\ufe0f Toolchain & Technology Stack \u00b6 Category Tools / Frameworks Language Python 3.11+ Frameworks FastAPI / Flask (API), HTMX / React (UI) ORM / DB SQLModel (SQLite / PostgreSQL) Task Queue Celery / RQ (Redis backend) Caching Redis Testing Pytest + Tox + Coverage CI/CD GitHub Actions + Import-Linter + Ruff + Pyright Packaging Poetry Docs MkDocs + Material theme \ud83e\udde9 Intended Audience \u00b6 Audience Needs Red-Team / Pentesters Automate repetitive workflows, triage large data, correlate findings. Security Engineers Integrate with CI/CD, enrich findings with vulnerability intelligence. Researchers Extend plugins, add enrichment sources, or create experimental scanners. Developers Contribute to wrappers, tooling, and orchestration improvements. \ud83e\udde0 Guiding Principles \u00b6 Automation-First \u2013 Everything that can be automated should be . Safe by Default \u2013 Every scan runs in isolation, respecting sandbox and audit policies. Extensible by Design \u2013 Each tool or feature lives as a plugin or wrapper. Transparent \u2013 Clear data lineage and provenance for every finding. Declarative Configuration \u2013 Tools and workflows defined in YAML/JSON, not hardcoded logic. Deterministic Execution \u2013 Same input, same results. All randomness is logged. \ud83d\udd2e Strategic Vision \u00b6 SecFlow is built not as a single pentest tool, but as a foundation for orchestrated cybersecurity automation . It aims to support: - Scalable enterprise deployments. - Cloud-native workloads (Kubernetes, containerized workers). - AI-driven vulnerability triage. - Integration with bug bounty, CI/CD, and SIEM pipelines. Next: Architecture Philosophy ```","title":"Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#01-title-executive-summary","text":"","title":"01 \u2014 Title &amp; Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#project-name","text":"SecFlow \u2014 Security Toolkit Orchestration Framework","title":"\ud83e\udde9 Project Name"},{"location":"architecture/01-title-and-executive-summary/#executive-summary","text":"SecFlow is a modular, extensible, and automation-first security orchestration platform designed to unify multiple open-source and proprietary tools for penetration testing, vulnerability management, and risk analytics. It merges dynamic scanning , tool orchestration , data normalization , and risk intelligence into a single cohesive system \u2014 optimized for both solo pentesters and enterprise red-team workflows.","title":"\ud83e\udded Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#core-vision","text":"\"One orchestrator to chain, enrich, and analyze all security tools \u2014 safely, intelligently, and reproducibly.\"","title":"Core Vision"},{"location":"architecture/01-title-and-executive-summary/#core-differentiators","text":"\ud83d\udd04 End-to-end orchestration: Tools (Nuclei, Feroxbuster, Katana, ZAP, Burp, Caido, etc.) communicate via normalized JSON pipelines. \ud83e\uddf1 Hexagonal architecture: Strict separation between core logic , adapters , and interfaces . \ud83e\udde0 Data-driven intelligence: Findings automatically enriched with CVE/CWE/CVSS and MITRE mapping. \u2699\ufe0f Resource registry: Shared wordlists, templates, and payloads under a versioned resource management system. \ud83e\uddf0 AI-ready foundation: All findings and telemetry are machine-readable for later AI triage and summarization.","title":"Core Differentiators"},{"location":"architecture/01-title-and-executive-summary/#project-goals","text":"Category Objective Engineering Create a modular mono-repo structure enabling multiple apps (CLI, API, Worker, UI). Automation Allow users to chain discovery \u2192 scanning \u2192 enrichment workflows via declarative YAML recipes. Data Consistency Standardize tool outputs into common Finding DTOs for unified triage and metrics. Usability Provide a CLI and web UI for project management, orchestration, and analytics. Security Guarantee safe sandboxing, configuration isolation, and audit logging. Scalability Support multi-tenant projects with configurable resource scopes.","title":"\ud83c\udfaf Project Goals"},{"location":"architecture/01-title-and-executive-summary/#key-capabilities-overview","text":"Layer Functionality Description Core-Lib Schema & DTOs Defines data models, ports, and contracts. Findings-Engine Normalization Converts raw tool outputs into structured findings. Wrappers Tool Adapters Execute, monitor, and parse tool output. Worker Orchestration Runs workflows (DAG execution, concurrency, retry policies). Web-API REST + WS Interface Exposes project, scan, and findings endpoints. UI HTMX/React Web dashboard for triage and insights. Plugins Extensibility Detectors, enrichers, or analytics extensions.","title":"\ud83e\uddee Key Capabilities Overview"},{"location":"architecture/01-title-and-executive-summary/#architecture-summary","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Web-API / UI\"] B[\"REST / WebSocket\"] C[\"Worker Engine<br/>(Workflows, DAG, Queues)\"] D[\"Ports / DTOs\"] E[\"Core-Lib<br/>(Models, Ports, Repositories)\"] F[\"Wrappers / Plugins<br/>(Nuclei, Ferox, Katana\u2026)\"] G[\"Tool Output\"] H[\"Findings Engine<br/>(Normalization, Metrics)\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H","title":"\ud83e\uddf1 Architecture Summary"},{"location":"architecture/01-title-and-executive-summary/#toolchain-technology-stack","text":"Category Tools / Frameworks Language Python 3.11+ Frameworks FastAPI / Flask (API), HTMX / React (UI) ORM / DB SQLModel (SQLite / PostgreSQL) Task Queue Celery / RQ (Redis backend) Caching Redis Testing Pytest + Tox + Coverage CI/CD GitHub Actions + Import-Linter + Ruff + Pyright Packaging Poetry Docs MkDocs + Material theme","title":"\u2699\ufe0f Toolchain &amp; Technology Stack"},{"location":"architecture/01-title-and-executive-summary/#intended-audience","text":"Audience Needs Red-Team / Pentesters Automate repetitive workflows, triage large data, correlate findings. Security Engineers Integrate with CI/CD, enrich findings with vulnerability intelligence. Researchers Extend plugins, add enrichment sources, or create experimental scanners. Developers Contribute to wrappers, tooling, and orchestration improvements.","title":"\ud83e\udde9 Intended Audience"},{"location":"architecture/01-title-and-executive-summary/#guiding-principles","text":"Automation-First \u2013 Everything that can be automated should be . Safe by Default \u2013 Every scan runs in isolation, respecting sandbox and audit policies. Extensible by Design \u2013 Each tool or feature lives as a plugin or wrapper. Transparent \u2013 Clear data lineage and provenance for every finding. Declarative Configuration \u2013 Tools and workflows defined in YAML/JSON, not hardcoded logic. Deterministic Execution \u2013 Same input, same results. All randomness is logged.","title":"\ud83e\udde0 Guiding Principles"},{"location":"architecture/01-title-and-executive-summary/#strategic-vision","text":"SecFlow is built not as a single pentest tool, but as a foundation for orchestrated cybersecurity automation . It aims to support: - Scalable enterprise deployments. - Cloud-native workloads (Kubernetes, containerized workers). - AI-driven vulnerability triage. - Integration with bug bounty, CI/CD, and SIEM pipelines. Next: Architecture Philosophy ```","title":"\ud83d\udd2e Strategic Vision"},{"location":"architecture/02-architecture-philosophy/","text":"02 \u2014 Architecture Philosophy \u00b6 \ud83e\udded Overview \u00b6 SecFlow's architecture follows the Hexagonal Architecture (Ports & Adapters) paradigm, enforcing a clear separation of concerns between business logic, I/O mechanisms, and user interfaces. It is designed for tool orchestration, data normalization, and future AI enrichment \u2014 with extensibility, safety, and testability at its core. \ud83e\uddf1 Architectural Tenets \u00b6 Principle Description Hexagonal Isolation All business logic lives in core modules with no framework dependencies. I/O adapters (DB, CLI, API) communicate via interfaces (Ports). Event-Driven Orchestration Workflows operate as Directed Acyclic Graphs (DAGs) executed asynchronously, allowing parallel chaining of tools. Immutable Data Flow Each execution step produces structured, immutable output for auditing and replay. Composable Pipelines Discovery \u2192 Scanning \u2192 Enrichment workflows can be combined declaratively. Strong Typing All data contracts use Pydantic models for type safety and schema validation. Declarative Configuration YAML or JSON defines tools, wordlists, templates, and workflows. Safe Execution Every external tool runs within sandbox boundaries (filesystem + network isolation). \u2699\ufe0f Core Architectural Layers \u00b6 +---------------------------------------------------------------+ | Applications | | Web-API | +---------------------------------------------------------------+ | Services Layer | | Workflow Engine | +---------------------------------------------------------------+ | Core-Lib | | DTOs | +---------------------------------------------------------------+ | Infrastructure | | Database | +---------------------------------------------------------------+ | External | | Tools | +---------------------------------------------------------------+ Each layer exposes well-defined boundaries : - Applications depend only on Services through ports . - Services interact with Core logic through domain interfaces . - Core-Lib defines pure domain logic with zero external imports. - Infrastructure implements adapters to databases, cache, and external tools. \ud83e\udde9 Design Goals \u00b6 Goal Implementation Strategy Maintainability Modular mono-repo with strict import boundaries ( import-linter ). Scalability Async task execution (Celery/RQ) and worker-based orchestration. Observability Structured logs, Prometheus metrics, and OpenTelemetry tracing. Reproducibility Deterministic workflows with cached configuration + results. Security Sandboxed subprocesses, limited file system access, and tokenized configuration. Extensibility Plugin registry and manifest-based tool definitions. \ud83e\udde9 Why Hexagonal Architecture? \u00b6 Aspect Traditional Architecture SecFlow Approach Dependencies Frameworks import core directly Core is framework-agnostic Testing Hard to isolate logic Core modules unit-tested independently Tool Integration Ad-hoc scripts Formal wrappers with contracts Maintainability Spaghetti imports Controlled boundaries with Import-Linter Extensibility Static toolset Plugin & manifest system \ud83e\udde9 Component Responsibility Map \u00b6 Component Responsibility Core-Lib Defines domain models ( Finding , Project , Resource ) and interfaces ( ToolPort , StoragePort ). Findings-Engine Normalizes raw scan data into standardized Finding objects. Wrappers Execute external tools (Nuclei, Ferox, etc.) via manifest-driven configs. Worker Executes workflows, manages concurrency, caching, and cleanup. API Exposes endpoints for managing projects, workflows, and triage. Triage-UI Visual interface for findings review, filtering, and reporting. Plugins Optional modules extending detection or enrichment logic. Resource Registry Central management of wordlists, templates, and payloads. \ud83e\udde9 Data Flow Model \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project\"] B[\"Workflow DAG\"] C[\"Tool Wrappers\"] D[\"Runs (execution logs)\"] E[\"Findings Engine\"] F[\"Findings (normalized)\"] G[\"Enrichment Layer\"] H[\"CVE/CWE/POC metadata\"] I[\"Triage / Metrics\"] J[\"Analytics, Dashboards\"] A --> B B --> C C --> D C --> E E --> F E --> G G --> H G --> I I --> J \ud83e\udde0 Architectural Patterns Used \u00b6 Pattern Purpose Repository Pattern Abstract data access (DB, file, memory). DTO (Data Transfer Object) Enforce schema boundaries between layers. Command Pattern Workflow node execution commands. Strategy Pattern Dynamic selection of enrichment or parsing strategies. Observer Pattern Metrics and event hooks on scan completion. Decorator Pattern Reusable middleware for retries, timeouts, and auditing. \ud83d\udd10 Security-by-Design Integration \u00b6 Every architectural decision includes a security review: - Minimal privilege principle (scoped resource access). - Tokenized secrets (no plaintext credentials). - Environment isolation (Docker/Kubernetes runtime sandboxes). - Mandatory audit trail for findings modification. \ud83e\udde0 Future-Proofing Considerations \u00b6 AI Integrations: Workflow outputs use JSON-LD style structures, allowing LLMs to reason over findings safely. Multi-Tenant Mode: Namespaced projects ensure logical and data isolation. Plugin Safety: Plugins are signed, versioned, and validated before loading. Extensible Schema: Findings model allows additional enrichment fields via metadata dicts. Next: Repository Layout","title":"Philosophy"},{"location":"architecture/02-architecture-philosophy/#02-architecture-philosophy","text":"","title":"02 \u2014 Architecture Philosophy"},{"location":"architecture/02-architecture-philosophy/#overview","text":"SecFlow's architecture follows the Hexagonal Architecture (Ports & Adapters) paradigm, enforcing a clear separation of concerns between business logic, I/O mechanisms, and user interfaces. It is designed for tool orchestration, data normalization, and future AI enrichment \u2014 with extensibility, safety, and testability at its core.","title":"\ud83e\udded Overview"},{"location":"architecture/02-architecture-philosophy/#architectural-tenets","text":"Principle Description Hexagonal Isolation All business logic lives in core modules with no framework dependencies. I/O adapters (DB, CLI, API) communicate via interfaces (Ports). Event-Driven Orchestration Workflows operate as Directed Acyclic Graphs (DAGs) executed asynchronously, allowing parallel chaining of tools. Immutable Data Flow Each execution step produces structured, immutable output for auditing and replay. Composable Pipelines Discovery \u2192 Scanning \u2192 Enrichment workflows can be combined declaratively. Strong Typing All data contracts use Pydantic models for type safety and schema validation. Declarative Configuration YAML or JSON defines tools, wordlists, templates, and workflows. Safe Execution Every external tool runs within sandbox boundaries (filesystem + network isolation).","title":"\ud83e\uddf1 Architectural Tenets"},{"location":"architecture/02-architecture-philosophy/#core-architectural-layers","text":"+---------------------------------------------------------------+ | Applications | | Web-API | +---------------------------------------------------------------+ | Services Layer | | Workflow Engine | +---------------------------------------------------------------+ | Core-Lib | | DTOs | +---------------------------------------------------------------+ | Infrastructure | | Database | +---------------------------------------------------------------+ | External | | Tools | +---------------------------------------------------------------+ Each layer exposes well-defined boundaries : - Applications depend only on Services through ports . - Services interact with Core logic through domain interfaces . - Core-Lib defines pure domain logic with zero external imports. - Infrastructure implements adapters to databases, cache, and external tools.","title":"\u2699\ufe0f Core Architectural Layers"},{"location":"architecture/02-architecture-philosophy/#design-goals","text":"Goal Implementation Strategy Maintainability Modular mono-repo with strict import boundaries ( import-linter ). Scalability Async task execution (Celery/RQ) and worker-based orchestration. Observability Structured logs, Prometheus metrics, and OpenTelemetry tracing. Reproducibility Deterministic workflows with cached configuration + results. Security Sandboxed subprocesses, limited file system access, and tokenized configuration. Extensibility Plugin registry and manifest-based tool definitions.","title":"\ud83e\udde9 Design Goals"},{"location":"architecture/02-architecture-philosophy/#why-hexagonal-architecture","text":"Aspect Traditional Architecture SecFlow Approach Dependencies Frameworks import core directly Core is framework-agnostic Testing Hard to isolate logic Core modules unit-tested independently Tool Integration Ad-hoc scripts Formal wrappers with contracts Maintainability Spaghetti imports Controlled boundaries with Import-Linter Extensibility Static toolset Plugin & manifest system","title":"\ud83e\udde9 Why Hexagonal Architecture?"},{"location":"architecture/02-architecture-philosophy/#component-responsibility-map","text":"Component Responsibility Core-Lib Defines domain models ( Finding , Project , Resource ) and interfaces ( ToolPort , StoragePort ). Findings-Engine Normalizes raw scan data into standardized Finding objects. Wrappers Execute external tools (Nuclei, Ferox, etc.) via manifest-driven configs. Worker Executes workflows, manages concurrency, caching, and cleanup. API Exposes endpoints for managing projects, workflows, and triage. Triage-UI Visual interface for findings review, filtering, and reporting. Plugins Optional modules extending detection or enrichment logic. Resource Registry Central management of wordlists, templates, and payloads.","title":"\ud83e\udde9 Component Responsibility Map"},{"location":"architecture/02-architecture-philosophy/#data-flow-model","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project\"] B[\"Workflow DAG\"] C[\"Tool Wrappers\"] D[\"Runs (execution logs)\"] E[\"Findings Engine\"] F[\"Findings (normalized)\"] G[\"Enrichment Layer\"] H[\"CVE/CWE/POC metadata\"] I[\"Triage / Metrics\"] J[\"Analytics, Dashboards\"] A --> B B --> C C --> D C --> E E --> F E --> G G --> H G --> I I --> J","title":"\ud83e\udde9 Data Flow Model"},{"location":"architecture/02-architecture-philosophy/#architectural-patterns-used","text":"Pattern Purpose Repository Pattern Abstract data access (DB, file, memory). DTO (Data Transfer Object) Enforce schema boundaries between layers. Command Pattern Workflow node execution commands. Strategy Pattern Dynamic selection of enrichment or parsing strategies. Observer Pattern Metrics and event hooks on scan completion. Decorator Pattern Reusable middleware for retries, timeouts, and auditing.","title":"\ud83e\udde0 Architectural Patterns Used"},{"location":"architecture/02-architecture-philosophy/#security-by-design-integration","text":"Every architectural decision includes a security review: - Minimal privilege principle (scoped resource access). - Tokenized secrets (no plaintext credentials). - Environment isolation (Docker/Kubernetes runtime sandboxes). - Mandatory audit trail for findings modification.","title":"\ud83d\udd10 Security-by-Design Integration"},{"location":"architecture/02-architecture-philosophy/#future-proofing-considerations","text":"AI Integrations: Workflow outputs use JSON-LD style structures, allowing LLMs to reason over findings safely. Multi-Tenant Mode: Namespaced projects ensure logical and data isolation. Plugin Safety: Plugins are signed, versioned, and validated before loading. Extensible Schema: Findings model allows additional enrichment fields via metadata dicts. Next: Repository Layout","title":"\ud83e\udde0 Future-Proofing Considerations"},{"location":"architecture/03-repository-layout/","text":"03 \u2014 Repository Layout \u00b6 \ud83e\udded Overview \u00b6 The SecFlow repository is structured as a Python mono-repo workspace housing multiple interdependent packages and apps. This allows us to: - Share interfaces and DTOs across services. - Enforce architecture boundaries centrally. - Run unified CI/CD pipelines and dependency resolution through Poetry . \ud83e\uddf1 Directory Structure \u00b6 flowchart TD root[\"SecFlow/\"] root --> pkgs[\"packages/\"] pkgs --> core[\"core-lib/\"] pkgs --> wf[\"workflow-engine/\"] pkgs --> tm[\"tool-manager/\"] pkgs --> rr[\"resource-registry/\"] pkgs --> obs[\"observability/\"] pkgs --> sec[\"security/\"] root --> apps[\"apps/\"] apps --> cli[\"cli/\"] apps --> api[\"api-server/\"] apps --> web[\"web-ui/\"] root --> docs[\"docs/\"] docs --> arch[\"architecture/\"] docs --> review[\"review/\"] docs --> diagrams[\"diagrams/\"] root --> tools[\"tools/\"] tools --> scripts[\"scripts/\"] tools --> linters[\"linters/\"] root --> github[\".github/\"] github --> workflows[\"workflows/\"] \u2699\ufe0f Python Workspace Configuration \u00b6 pyproject.toml Snippet \u00b6 [tool.poetry] name = \"SecFlow\" version = \"1.0.0\" description = \"Security Toolkit Orchestration Framework\" authors = [\"Hernan Trajtemberg <hernan.trajtemberg@domain>\"] [tool.poetry.dependencies] python = \"^3.11\" fastapi = \"^0.115\" sqlmodel = \"^0.0.22\" celery = \"^5.3\" redis = \"^5.0\" pydantic = \"^2.7\" import-linter = \"^1.5\" ruff = \"^0.7\" pytest = \"^8.3\" [tool.poetry.group.dev.dependencies] pyright = \"*\" coverage = \"*\" [build-system] requires = [\"poetry-core>=1.6\"] build-backend = \"poetry.core.masonry.api\" \ud83e\udde9 Application Layering \u00b6 Each app in /apps/ uses internal packages exclusively via ports, ensuring loose coupling. Layer Directory Import Rules Core packages/core-lib No external imports Findings Engine packages/findings-engine May import core-lib Wrappers packages/wrappers May import core-lib, utils API / Worker apps/web-api , apps/worker May import via ports only Plugins packages/plugins Dynamically loaded at runtime \ud83e\udde9 Import-Linter Configuration \u00b6 importlinter.ini enforces import boundaries automatically: [importlinter] root_package = SecFlow [contract: core-isolation] name = Core-Lib Is Independent type = forbidden source_modules = SecFlow.packages.core_lib forbidden_modules = SecFlow.apps SecFlow.packages.wrappers SecFlow.packages.findings_engine [contract: adapters-only] name = Adapters Only Depend On Ports type = layers layers = SecFlow.packages.core_lib SecFlow.packages.findings_engine SecFlow.packages.wrappers SecFlow.apps If violated, the CI pipeline fails the build. \ud83e\udde0 Developer Workflow \u00b6 Local Development \u00b6 poetry install poetry run pre-commit install poetry run pytest Run the Worker \u00b6 poetry run celery -A SecFlow.apps.worker worker --loglevel=info Run the Web API \u00b6 poetry run uvicorn SecFlow.apps.web_api.main:app --reload \ud83e\udde9 Continuous Integration Pipeline \u00b6 GitHub Actions ( .github/workflows/ci.yml ): name: SecFlow CI on: [push, pull_request] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-python@v5 with: python-version: '3.11' - name: Install dependencies run: | pip install poetry poetry install - name: Lint & Type Check run: | poetry run ruff check . poetry run pyright - name: Run Tests run: poetry run pytest --maxfail=1 --disable-warnings -q \ud83e\uddf0 Tooling & Developer Aids \u00b6 Tool Purpose Ruff Linting, formatting enforcement Pyright Static type checking Import-Linter Architecture enforcement Poetry Dependency & build management Tox Multi-environment testing MkDocs Documentation site generation Coverage.py Test coverage reports \ud83e\udde9 ASCII Diagram \u2014 High-Level View \u00b6 +-----------------------------+ | SecFlow/ | +-------------+---------------+ | +------------v-------------+ | packages/ | | core-lib, findings, etc. | +------------+-------------+ | +------------v-------------+ | apps/ | | web-api, worker, cli, ui | +------------+-------------+ | +------------v-------------+ | tests/ | +--------------------------+ \ud83e\udde0 Future Enhancements \u00b6 Monorepo Versioning: Each package versioned via poetry-dynamic-versioning. Documentation Pipeline: Auto-regenerate schema docs ( mkdocs build ) on merge. Pre-Commit Hooks: Validate imports and enforce ruff formatting pre-push. Code Owners: Assign maintainers per package via .github/CODEOWNERS . Next: Core Packages & Responsibilities","title":"Repository Layout"},{"location":"architecture/03-repository-layout/#03-repository-layout","text":"","title":"03 \u2014 Repository Layout"},{"location":"architecture/03-repository-layout/#overview","text":"The SecFlow repository is structured as a Python mono-repo workspace housing multiple interdependent packages and apps. This allows us to: - Share interfaces and DTOs across services. - Enforce architecture boundaries centrally. - Run unified CI/CD pipelines and dependency resolution through Poetry .","title":"\ud83e\udded Overview"},{"location":"architecture/03-repository-layout/#directory-structure","text":"flowchart TD root[\"SecFlow/\"] root --> pkgs[\"packages/\"] pkgs --> core[\"core-lib/\"] pkgs --> wf[\"workflow-engine/\"] pkgs --> tm[\"tool-manager/\"] pkgs --> rr[\"resource-registry/\"] pkgs --> obs[\"observability/\"] pkgs --> sec[\"security/\"] root --> apps[\"apps/\"] apps --> cli[\"cli/\"] apps --> api[\"api-server/\"] apps --> web[\"web-ui/\"] root --> docs[\"docs/\"] docs --> arch[\"architecture/\"] docs --> review[\"review/\"] docs --> diagrams[\"diagrams/\"] root --> tools[\"tools/\"] tools --> scripts[\"scripts/\"] tools --> linters[\"linters/\"] root --> github[\".github/\"] github --> workflows[\"workflows/\"]","title":"\ud83e\uddf1 Directory Structure"},{"location":"architecture/03-repository-layout/#python-workspace-configuration","text":"","title":"\u2699\ufe0f Python Workspace Configuration"},{"location":"architecture/03-repository-layout/#pyprojecttoml-snippet","text":"[tool.poetry] name = \"SecFlow\" version = \"1.0.0\" description = \"Security Toolkit Orchestration Framework\" authors = [\"Hernan Trajtemberg <hernan.trajtemberg@domain>\"] [tool.poetry.dependencies] python = \"^3.11\" fastapi = \"^0.115\" sqlmodel = \"^0.0.22\" celery = \"^5.3\" redis = \"^5.0\" pydantic = \"^2.7\" import-linter = \"^1.5\" ruff = \"^0.7\" pytest = \"^8.3\" [tool.poetry.group.dev.dependencies] pyright = \"*\" coverage = \"*\" [build-system] requires = [\"poetry-core>=1.6\"] build-backend = \"poetry.core.masonry.api\"","title":"pyproject.toml Snippet"},{"location":"architecture/03-repository-layout/#application-layering","text":"Each app in /apps/ uses internal packages exclusively via ports, ensuring loose coupling. Layer Directory Import Rules Core packages/core-lib No external imports Findings Engine packages/findings-engine May import core-lib Wrappers packages/wrappers May import core-lib, utils API / Worker apps/web-api , apps/worker May import via ports only Plugins packages/plugins Dynamically loaded at runtime","title":"\ud83e\udde9 Application Layering"},{"location":"architecture/03-repository-layout/#import-linter-configuration","text":"importlinter.ini enforces import boundaries automatically: [importlinter] root_package = SecFlow [contract: core-isolation] name = Core-Lib Is Independent type = forbidden source_modules = SecFlow.packages.core_lib forbidden_modules = SecFlow.apps SecFlow.packages.wrappers SecFlow.packages.findings_engine [contract: adapters-only] name = Adapters Only Depend On Ports type = layers layers = SecFlow.packages.core_lib SecFlow.packages.findings_engine SecFlow.packages.wrappers SecFlow.apps If violated, the CI pipeline fails the build.","title":"\ud83e\udde9 Import-Linter Configuration"},{"location":"architecture/03-repository-layout/#developer-workflow","text":"","title":"\ud83e\udde0 Developer Workflow"},{"location":"architecture/03-repository-layout/#local-development","text":"poetry install poetry run pre-commit install poetry run pytest","title":"Local Development"},{"location":"architecture/03-repository-layout/#run-the-worker","text":"poetry run celery -A SecFlow.apps.worker worker --loglevel=info","title":"Run the Worker"},{"location":"architecture/03-repository-layout/#run-the-web-api","text":"poetry run uvicorn SecFlow.apps.web_api.main:app --reload","title":"Run the Web API"},{"location":"architecture/03-repository-layout/#continuous-integration-pipeline","text":"GitHub Actions ( .github/workflows/ci.yml ): name: SecFlow CI on: [push, pull_request] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-python@v5 with: python-version: '3.11' - name: Install dependencies run: | pip install poetry poetry install - name: Lint & Type Check run: | poetry run ruff check . poetry run pyright - name: Run Tests run: poetry run pytest --maxfail=1 --disable-warnings -q","title":"\ud83e\udde9 Continuous Integration Pipeline"},{"location":"architecture/03-repository-layout/#tooling-developer-aids","text":"Tool Purpose Ruff Linting, formatting enforcement Pyright Static type checking Import-Linter Architecture enforcement Poetry Dependency & build management Tox Multi-environment testing MkDocs Documentation site generation Coverage.py Test coverage reports","title":"\ud83e\uddf0 Tooling &amp; Developer Aids"},{"location":"architecture/03-repository-layout/#ascii-diagram-high-level-view","text":"+-----------------------------+ | SecFlow/ | +-------------+---------------+ | +------------v-------------+ | packages/ | | core-lib, findings, etc. | +------------+-------------+ | +------------v-------------+ | apps/ | | web-api, worker, cli, ui | +------------+-------------+ | +------------v-------------+ | tests/ | +--------------------------+","title":"\ud83e\udde9 ASCII Diagram \u2014 High-Level View"},{"location":"architecture/03-repository-layout/#future-enhancements","text":"Monorepo Versioning: Each package versioned via poetry-dynamic-versioning. Documentation Pipeline: Auto-regenerate schema docs ( mkdocs build ) on merge. Pre-Commit Hooks: Validate imports and enforce ruff formatting pre-push. Code Owners: Assign maintainers per package via .github/CODEOWNERS . Next: Core Packages & Responsibilities","title":"\ud83e\udde0 Future Enhancements"},{"location":"architecture/04-core-packages-and-responsibilities/","text":"04 \u2014 Core Packages & Responsibilities \u00b6 \ud83e\udded Overview \u00b6 This document describes the core packages inside the packages/ directory and explains the roles, contracts, and relationships that define the heart of SecFlow. The architecture is intentionally hexagonal : - core-lib defines domain models and ports (interfaces). - Other packages such as findings-engine , wrappers , and storage implement those ports. - Application layers ( web-api , worker , cli ) use the ports but never touch implementations directly. \ud83e\udde9 Package Overview Table \u00b6 Package Responsibility Depends On core-lib Domain models, DTOs, Ports, and Service Interfaces. None findings-engine Normalization and enrichment of scan data. core-lib wrappers Executes external tools and parses results. core-lib , utils resources Manages reusable assets (wordlists, payloads, templates). core-lib storage Implements persistence and caching (Postgres, SQLite, Redis). core-lib plugins Houses extension modules (detectors, enrichers, analytics). core-lib utils Generic utilities: logging, validation, subprocess helpers. None \u2699\ufe0f Core-Lib \u00b6 Purpose \u00b6 core-lib is the foundation of SecFlow. It defines all domain entities, type contracts, and abstract interfaces (ports) used throughout the system. Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"core-lib/\"] B[\"models/\"] C[\"finding.py\"] D[\"project.py\"] E[\"resource.py\"] F[\"run.py\"] G[\"ports/\"] H[\"storage_port.py\"] I[\"tool_port.py\"] J[\"resource_port.py\"] K[\"findings_port.py\"] L[\"services/\"] M[\"workflow_service.py\"] N[\"triage_service.py\"] O[\"exceptions.py\"] A --> B A --> G A --> L A --> O B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K L --> M L --> N Example Model \u00b6 # core-lib/models/finding.py from pydantic import BaseModel from datetime import datetime from typing import List, Optional, Dict class Finding(BaseModel): id: str project_id: str detector_id: str title: str severity: str path: str evidence: Dict[str, str] created_at: datetime cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] enrichment: Dict[str, any] = {} Example Port \u00b6 # core-lib/ports/tool_port.py from typing import Protocol, List from core_lib.models.finding import Finding class ToolPort(Protocol): def prepare(self, config: dict) -> None: \"\"\"Prepare the tool with given configuration.\"\"\" pass def execute(self) -> List[Finding]: \"\"\"Execute the tool and return findings.\"\"\" pass def validate_output(self, raw_output: str) -> bool: \"\"\"Validate tool output format.\"\"\" pass This abstraction allows any external tool to be integrated simply by implementing ToolPort . \ud83e\udde0 Findings-Engine \u00b6 Purpose \u00b6 Responsible for normalizing, deduplicating, and enriching findings produced by wrappers. Example Normalization Flow \u00b6 def normalize(raw_data: str, tool: str) -> Finding: if tool == \"nuclei\": return _normalize_nuclei_output(raw_data) elif tool == \"ferox\": return _normalize_ferox_output(raw_data) Capabilities \u00b6 Parse multiple output formats (JSON, XML, plain text). Deduplicate based on fingerprint (host + path + vuln_id). Attach CWE, CVSS, and severity from enrichment sources. Store normalized data through StoragePort . \u2699\ufe0f Wrappers \u00b6 Purpose \u00b6 Wraps and executes third-party tools through a unified interface defined by ToolPort . Example Structure \u00b6 wrappers/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"wrappers/\"] B[\"nuclei_wrapper.py\"] C[\"ferox_wrapper.py\"] D[\"zap_wrapper.py\"] E[\"base_wrapper.py\"] A --> B A --> C A --> D A --> E Example Base Class \u00b6 # wrappers/base_wrapper.py import subprocess from core_lib.models.finding import Finding class BaseWrapper: def __init__(self, manifest: dict): self.manifest = manifest def run(self, target: str) -> list[Finding]: cmd = [self.manifest[\"binary\"], \"-u\", target] result = subprocess.run(cmd, capture_output=True, text=True) return self.parse_output(result.stdout) def parse_output(self, raw: str) -> list[Finding]: raise NotImplementedError All wrappers inherit from BaseWrapper and override parse_output . \ud83d\udce6 Resources \u00b6 Purpose \u00b6 resources manages global and scoped assets: - Wordlists (directories, subdomains, parameters) - Templates (Nuclei, ZAP) - Payload sets - Custom configurations Example Resource Model \u00b6 # core-lib/models/resource.py class Resource(BaseModel): id: str name: str type: Literal[\"wordlist\", \"template\", \"payload\"] scope: Literal[\"global\", \"group\", \"project\"] hash: str version: str metadata: dict \ud83d\uddc3\ufe0f Storage \u00b6 Purpose \u00b6 Implements all persistence operations via the StoragePort interface. Example Interface \u00b6 # core-lib/ports/storage_port.py class StoragePort(Protocol): def save_finding(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list_findings(self, project_id: str) -> list[Finding]: \"\"\"List all findings for a project.\"\"\" pass def delete_project(self, project_id: str) -> None: \"\"\"Delete a project and all its findings.\"\"\" pass Implementation Examples \u00b6 SQLiteStorageAdapter PostgresStorageAdapter RedisCacheAdapter All registered in the storage.registry . \ud83d\udd0c Plugins \u00b6 Purpose \u00b6 Extend SecFlow with additional detection or enrichment capabilities. Example Plugin Registration \u00b6 # plugins/registry.py from typing import Dict, Callable PLUGIN_REGISTRY: Dict[str, Callable] = {} def register_plugin(name: str): def decorator(func): PLUGIN_REGISTRY[name] = func return func return decorator Plugins can dynamically hook into findings processing, orchestration, or resource management. \ud83e\uddf0 Utils \u00b6 utils contains helper modules that are shared across packages but contain no business logic. Examples: \u00b6 utils.subprocess_safe \u2013 wrapper for secure process spawning. utils.hashing \u2013 generate resource hashes (SHA256). utils.validation \u2013 reusable Pydantic validators. utils.config \u2013 environment-aware configuration loader. \ud83d\udd17 Cross-Package Interaction Diagram \u00b6 +-------------+ +----------------+ +---------------+ | Wrappers | ----> | Findings-Engine| ----> | Storage | +-------------+ +----------------+ +---------------+ | ^ | | | | v | v +-----------+ +----------------+ +----------------+ | Resources | | Core-Lib | | Plugins | +-----------+ +----------------+ +----------------+ Arrows represent dependency flow, not import direction (imports always go downward). \ud83e\udde0 Best Practices \u00b6 Every package must expose a __all__ list for stable imports. Each module must define __version__ for dependency tracking. No package may import directly from /apps/ . Keep adapters stateless; use dependency injection for configuration. Always prefer Pydantic models over raw dictionaries. \ud83e\udde9 Testing Guidelines \u00b6 Package Test Type Focus core-lib Unit DTO validation and port contracts findings-engine Integration Normalization and enrichment correctness wrappers System Command execution, parsing accuracy storage Integration CRUD consistency across backends plugins Unit Hook execution and registry behavior \ud83e\udde0 Future Enhancements \u00b6 Introduce a service layer in core-lib to coordinate between repositories and orchestrators. Implement schema diffing for findings-engine to detect breaking changes. Add async wrappers for high-performance tools (e.g., Katana via aiohttp). Build auto-generated docs from port signatures ( mkdocs-gen-files ). Next: Orchestration & Workflow Engine","title":"Core Packages"},{"location":"architecture/04-core-packages-and-responsibilities/#04-core-packages-responsibilities","text":"","title":"04 \u2014 Core Packages &amp; Responsibilities"},{"location":"architecture/04-core-packages-and-responsibilities/#overview","text":"This document describes the core packages inside the packages/ directory and explains the roles, contracts, and relationships that define the heart of SecFlow. The architecture is intentionally hexagonal : - core-lib defines domain models and ports (interfaces). - Other packages such as findings-engine , wrappers , and storage implement those ports. - Application layers ( web-api , worker , cli ) use the ports but never touch implementations directly.","title":"\ud83e\udded Overview"},{"location":"architecture/04-core-packages-and-responsibilities/#package-overview-table","text":"Package Responsibility Depends On core-lib Domain models, DTOs, Ports, and Service Interfaces. None findings-engine Normalization and enrichment of scan data. core-lib wrappers Executes external tools and parses results. core-lib , utils resources Manages reusable assets (wordlists, payloads, templates). core-lib storage Implements persistence and caching (Postgres, SQLite, Redis). core-lib plugins Houses extension modules (detectors, enrichers, analytics). core-lib utils Generic utilities: logging, validation, subprocess helpers. None","title":"\ud83e\udde9 Package Overview Table"},{"location":"architecture/04-core-packages-and-responsibilities/#core-lib","text":"","title":"\u2699\ufe0f Core-Lib"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose","text":"core-lib is the foundation of SecFlow. It defines all domain entities, type contracts, and abstract interfaces (ports) used throughout the system.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"core-lib/\"] B[\"models/\"] C[\"finding.py\"] D[\"project.py\"] E[\"resource.py\"] F[\"run.py\"] G[\"ports/\"] H[\"storage_port.py\"] I[\"tool_port.py\"] J[\"resource_port.py\"] K[\"findings_port.py\"] L[\"services/\"] M[\"workflow_service.py\"] N[\"triage_service.py\"] O[\"exceptions.py\"] A --> B A --> G A --> L A --> O B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K L --> M L --> N","title":"Structure"},{"location":"architecture/04-core-packages-and-responsibilities/#example-model","text":"# core-lib/models/finding.py from pydantic import BaseModel from datetime import datetime from typing import List, Optional, Dict class Finding(BaseModel): id: str project_id: str detector_id: str title: str severity: str path: str evidence: Dict[str, str] created_at: datetime cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] enrichment: Dict[str, any] = {}","title":"Example Model"},{"location":"architecture/04-core-packages-and-responsibilities/#example-port","text":"# core-lib/ports/tool_port.py from typing import Protocol, List from core_lib.models.finding import Finding class ToolPort(Protocol): def prepare(self, config: dict) -> None: \"\"\"Prepare the tool with given configuration.\"\"\" pass def execute(self) -> List[Finding]: \"\"\"Execute the tool and return findings.\"\"\" pass def validate_output(self, raw_output: str) -> bool: \"\"\"Validate tool output format.\"\"\" pass This abstraction allows any external tool to be integrated simply by implementing ToolPort .","title":"Example Port"},{"location":"architecture/04-core-packages-and-responsibilities/#findings-engine","text":"","title":"\ud83e\udde0 Findings-Engine"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_1","text":"Responsible for normalizing, deduplicating, and enriching findings produced by wrappers.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-normalization-flow","text":"def normalize(raw_data: str, tool: str) -> Finding: if tool == \"nuclei\": return _normalize_nuclei_output(raw_data) elif tool == \"ferox\": return _normalize_ferox_output(raw_data)","title":"Example Normalization Flow"},{"location":"architecture/04-core-packages-and-responsibilities/#capabilities","text":"Parse multiple output formats (JSON, XML, plain text). Deduplicate based on fingerprint (host + path + vuln_id). Attach CWE, CVSS, and severity from enrichment sources. Store normalized data through StoragePort .","title":"Capabilities"},{"location":"architecture/04-core-packages-and-responsibilities/#wrappers","text":"","title":"\u2699\ufe0f Wrappers"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_2","text":"Wraps and executes third-party tools through a unified interface defined by ToolPort .","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-structure","text":"wrappers/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"wrappers/\"] B[\"nuclei_wrapper.py\"] C[\"ferox_wrapper.py\"] D[\"zap_wrapper.py\"] E[\"base_wrapper.py\"] A --> B A --> C A --> D A --> E","title":"Example Structure"},{"location":"architecture/04-core-packages-and-responsibilities/#example-base-class","text":"# wrappers/base_wrapper.py import subprocess from core_lib.models.finding import Finding class BaseWrapper: def __init__(self, manifest: dict): self.manifest = manifest def run(self, target: str) -> list[Finding]: cmd = [self.manifest[\"binary\"], \"-u\", target] result = subprocess.run(cmd, capture_output=True, text=True) return self.parse_output(result.stdout) def parse_output(self, raw: str) -> list[Finding]: raise NotImplementedError All wrappers inherit from BaseWrapper and override parse_output .","title":"Example Base Class"},{"location":"architecture/04-core-packages-and-responsibilities/#resources","text":"","title":"\ud83d\udce6 Resources"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_3","text":"resources manages global and scoped assets: - Wordlists (directories, subdomains, parameters) - Templates (Nuclei, ZAP) - Payload sets - Custom configurations","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-resource-model","text":"# core-lib/models/resource.py class Resource(BaseModel): id: str name: str type: Literal[\"wordlist\", \"template\", \"payload\"] scope: Literal[\"global\", \"group\", \"project\"] hash: str version: str metadata: dict","title":"Example Resource Model"},{"location":"architecture/04-core-packages-and-responsibilities/#storage","text":"","title":"\ud83d\uddc3\ufe0f Storage"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_4","text":"Implements all persistence operations via the StoragePort interface.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-interface","text":"# core-lib/ports/storage_port.py class StoragePort(Protocol): def save_finding(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list_findings(self, project_id: str) -> list[Finding]: \"\"\"List all findings for a project.\"\"\" pass def delete_project(self, project_id: str) -> None: \"\"\"Delete a project and all its findings.\"\"\" pass","title":"Example Interface"},{"location":"architecture/04-core-packages-and-responsibilities/#implementation-examples","text":"SQLiteStorageAdapter PostgresStorageAdapter RedisCacheAdapter All registered in the storage.registry .","title":"Implementation Examples"},{"location":"architecture/04-core-packages-and-responsibilities/#plugins","text":"","title":"\ud83d\udd0c Plugins"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_5","text":"Extend SecFlow with additional detection or enrichment capabilities.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-plugin-registration","text":"# plugins/registry.py from typing import Dict, Callable PLUGIN_REGISTRY: Dict[str, Callable] = {} def register_plugin(name: str): def decorator(func): PLUGIN_REGISTRY[name] = func return func return decorator Plugins can dynamically hook into findings processing, orchestration, or resource management.","title":"Example Plugin Registration"},{"location":"architecture/04-core-packages-and-responsibilities/#utils","text":"utils contains helper modules that are shared across packages but contain no business logic.","title":"\ud83e\uddf0 Utils"},{"location":"architecture/04-core-packages-and-responsibilities/#examples","text":"utils.subprocess_safe \u2013 wrapper for secure process spawning. utils.hashing \u2013 generate resource hashes (SHA256). utils.validation \u2013 reusable Pydantic validators. utils.config \u2013 environment-aware configuration loader.","title":"Examples:"},{"location":"architecture/04-core-packages-and-responsibilities/#cross-package-interaction-diagram","text":"+-------------+ +----------------+ +---------------+ | Wrappers | ----> | Findings-Engine| ----> | Storage | +-------------+ +----------------+ +---------------+ | ^ | | | | v | v +-----------+ +----------------+ +----------------+ | Resources | | Core-Lib | | Plugins | +-----------+ +----------------+ +----------------+ Arrows represent dependency flow, not import direction (imports always go downward).","title":"\ud83d\udd17 Cross-Package Interaction Diagram"},{"location":"architecture/04-core-packages-and-responsibilities/#best-practices","text":"Every package must expose a __all__ list for stable imports. Each module must define __version__ for dependency tracking. No package may import directly from /apps/ . Keep adapters stateless; use dependency injection for configuration. Always prefer Pydantic models over raw dictionaries.","title":"\ud83e\udde0 Best Practices"},{"location":"architecture/04-core-packages-and-responsibilities/#testing-guidelines","text":"Package Test Type Focus core-lib Unit DTO validation and port contracts findings-engine Integration Normalization and enrichment correctness wrappers System Command execution, parsing accuracy storage Integration CRUD consistency across backends plugins Unit Hook execution and registry behavior","title":"\ud83e\udde9 Testing Guidelines"},{"location":"architecture/04-core-packages-and-responsibilities/#future-enhancements","text":"Introduce a service layer in core-lib to coordinate between repositories and orchestrators. Implement schema diffing for findings-engine to detect breaking changes. Add async wrappers for high-performance tools (e.g., Katana via aiohttp). Build auto-generated docs from port signatures ( mkdocs-gen-files ). Next: Orchestration & Workflow Engine","title":"\ud83e\udde0 Future Enhancements"},{"location":"architecture/05-orchestration-and-workflow-engine/","text":"05 \u2014 Orchestration & Workflow Engine \u00b6 \ud83e\udded Overview \u00b6 The Orchestration Engine is the operational core of SecFlow. It executes multi-tool workflows composed of discovery , scanning , filtering , enrichment , and analysis stages. Workflows are defined declaratively using YAML recipes and executed as Directed Acyclic Graphs (DAGs) . Each node represents a tool invocation , and edges define data flow between outputs and inputs. M1 Implementation Status : \u2705 Complete - Sequential execution, validation, retry logic, and plugin integration delivered. Note : SecFlow executes workflow steps sequentially in M1 (parallel execution to be introduced in future versions). \ud83e\uddf1 Conceptual Model \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Node A<br/>(Ferox)\"] B[\"Node B<br/>(Nuclei)\"] C[\"Node C<br/>(CVE Enr)\"] D[\"Inputs/Outputs\"] E[\"Config/Env\"] F[\"StoragePort\"] A --> B B --> C A --> D B --> E C --> F Each node produces one or more datasets that can be consumed by downstream nodes. The workflow engine guarantees: - Topological order of execution - Sequential execution (M1) with parallel execution planned for M3 - Automatic retries, timeouts, and logging per node - StoragePort integration for findings persistence \u2699\ufe0f Workflow Specification Schema (M1) \u00b6 version : \"1.0\" name : \"Linear Security Scan\" description : \"Simple linear workflow: discovery \u2192 scan \u2192 enrichment\" nodes : - id : \"discovery\" type : \"discovery.ferox\" config : wordlist : \"res://wordlists/dirb:latest\" threads : 50 timeout : 300 outputs : [ \"urls\" ] - id : \"scan\" type : \"scan.nuclei\" inputs : [ \"urls\" ] config : templates : \"res://templates/owasp-top10:latest\" rate_limit : 150 timeout : 600 outputs : [ \"findings\" ] - id : \"enrich\" type : \"enrich.cve\" inputs : [ \"findings\" ] config : sources : [ \"nvd\" , \"osv\" , \"exploitdb\" ] timeout : 120 outputs : [ \"enriched_findings\" ] # Retry configuration retry : max_attempts : 3 backoff_factor : 2.0 base_delay : 5.0 # State management state : checkpoint_interval : 30 resume_on_failure : true cache_intermediate : true \ud83e\udde9 Workflow Engine Architecture (M1) \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"WorkflowExecutor\"] B[\"RecipeValidator\"] C[\"NodeExecutor\"] D[\"PluginLoader\"] E[\"StoragePort\"] F[\"Findings Schema\"] A --> B A --> C C --> D C --> E E --> F G[\"YAML Recipe\"] H[\"DAG Validation\"] I[\"Sequential Execution\"] G --> A H --> B I --> C M1 Components Delivered : - \u2705 WorkflowExecutor : Parses YAML recipes, builds DAG, executes nodes sequentially - \u2705 RecipeValidator : Comprehensive validation with Pydantic schemas - \u2705 NodeExecutor : Executes individual nodes with retry logic and timeouts - \u2705 PluginLoader : Integrates with plugin system for tool execution - \u2705 StoragePort Integration : Uses StoragePort for findings persistence \u2699\ufe0f Python Model \u2014 DAG Representation (M1) \u00b6 # packages/workflow_engine/executor.py from typing import Dict , List , Any , Optional from pydantic import BaseModel from dataclasses import dataclass , field @dataclass class NodeSpec : \"\"\"Node specification for workflow execution.\"\"\" id : str type : str params : Dict [ str , Any ] = field ( default_factory = dict ) requires : List [ str ] = field ( default_factory = list ) outputs : List [ str ] = field ( default_factory = list ) timeout_s : Optional [ int ] = None retries : int = 0 retry_backoff_s : float = 0.5 @dataclass class WorkflowSpec : \"\"\"Workflow specification.\"\"\" id : str name : str description : str nodes : List [ NodeSpec ] retry_config : Dict [ str , Any ] = field ( default_factory = dict ) state_config : Dict [ str , Any ] = field ( default_factory = dict ) class WorkflowExecutor : \"\"\"M1 Workflow Executor with sequential execution.\"\"\" def __init__ ( self , storage : StoragePort = None ): self . storage = storage or InMemoryStorageAdapter () self . plugin_loader = PluginLoader () def execute_workflow ( self , workflow : WorkflowSpec ) -> Dict [ str , Any ]: \"\"\"Execute workflow with sequential node processing.\"\"\" # M1: Sequential execution # M3: Parallel execution where dependencies allow pass DAG Validation Example (M1) \u00b6 def validate_dag ( workflow : WorkflowSpec ): \"\"\"Validate workflow DAG structure.\"\"\" ids = [ n . id for n in workflow . nodes ] # Check for cycles if has_cycles ( workflow . nodes ): raise ValueError ( \"Workflow contains cycles\" ) # Validate input/output references for node in workflow . nodes : for inp in node . requires : if inp not in [ out for n in workflow . nodes for out in n . outputs ]: raise ValueError ( f \"Unresolved input ' { inp } ' in node { node . id } \" ) # Validate node types for node in workflow . nodes : if not is_valid_node_type ( node . type ): raise ValueError ( f \"Unknown node type: { node . type } \" ) \ud83e\udde0 Execution Flow (M1 Implementation) \u00b6 Parse & Validate YAML workflow using Pydantic schema Build DAG with topological ordering and dependency resolution Register Workflow in execution context Execute Nodes Sequentially respecting topological order Plugin Integration via PluginLoader for tool execution Normalize Findings via StoragePort with schema validation Update Metrics and trigger downstream listeners M1 Execution Model : - Sequential : Nodes execute one after another in topological order - Retry Logic : Configurable retry with exponential backoff - Timeout Handling : Per-node timeout enforcement - Error Recovery : Graceful failure handling with partial execution support \u2699\ufe0f Node Executor (M1 Implementation) \u00b6 # packages/workflow_engine/executor.py class NodeExecutor : \"\"\"M1 Node Executor with retry logic and timeout handling.\"\"\" def __init__ ( self , node : NodeSpec , context : ExecutionContext ): self . node = node self . context = context self . storage = context . storage def execute ( self ) -> Dict [ str , Any ]: \"\"\"Execute node with retry logic and timeout handling.\"\"\" for attempt in range ( self . node . retries + 1 ): try : # Load plugin for node type plugin = self . context . plugin_loader . load ( self . node . type ) # Execute plugin with timeout result = self . _execute_with_timeout ( plugin ) # Persist findings via StoragePort self . _persist_findings ( result ) return result except TimeoutError : if attempt < self . node . retries : delay = self . node . retry_backoff_s * ( 2 ** attempt ) time . sleep ( delay ) continue raise except Exception as e : if attempt < self . node . retries : delay = self . node . retry_backoff_s * ( 2 ** attempt ) time . sleep ( delay ) continue raise def _execute_with_timeout ( self , plugin : PluginInterface ) -> Dict [ str , Any ]: \"\"\"Execute plugin with timeout enforcement.\"\"\" timeout = self . node . timeout_s or 300 # Default 5 minutes with timeout_context ( timeout ): return plugin . run ( inputs = self . context . get_node_inputs ( self . node ), config = self . node . params , context = self . context ) def _persist_findings ( self , result : Dict [ str , Any ]) -> None : \"\"\"Persist findings via StoragePort.\"\"\" findings = result . get ( \"findings\" , []) for finding in findings : # Ensure schema compliance finding [ \"finding_schema_version\" ] = \"1.0.0\" self . storage . save_finding ( finding ) \ud83d\udd04 Concurrency Model (M1 vs M3) \u00b6 M1 Implementation : Sequential execution only - Nodes execute one after another in topological order - Dependencies are resolved before execution - Simple error handling and retry logic - Suitable for linear workflows and testing M3 Planned : Parallel execution where dependencies allow - Independent nodes run concurrently - Dependency resolution with parallel execution - Advanced error handling and recovery - Event-driven architecture for real-time monitoring Example Sequential Execution (M1) \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"ferox<br/>(discovery)\"] B[\"katana<br/>(subdomain)\"] C[\"nuclei<br/>(scan)\"] D[\"cve-enrich<br/>(enrichment)\"] A --> B B --> C A --> C C --> D Execution Order : ferox \u2192 katana \u2192 nuclei \u2192 cve-enrich \ud83e\udde9 Error Handling (M1) \u00b6 Error Type Handling Strategy Tool crash / non-zero exit Retry (max=3) then mark node failed Timeout Kill process, log event, continue DAG Missing input dataset Block downstream nodes, mark dependency unresolved Parser error Log raw output, fallback to generic findings schema Plugin load failure Skip node, log warning, continue execution Each failure is logged with context and stored via StoragePort for analysis. \ud83e\udde0 Event System (M1) \u00b6 The orchestration layer publishes events for monitoring and debugging: Example Event Contract \u00b6 { \"event\" : \"node_completed\" , \"workflow_id\" : \"abc123\" , \"node_id\" : \"scan\" , \"node_type\" : \"scan.nuclei\" , \"duration\" : 12.3 , \"findings_count\" : 124 , \"success\" : true , \"timestamp\" : \"2025-10-14T10:30:00Z\" } Events can be consumed by: - Logging systems for audit trails - Metrics collectors for performance monitoring - UI components for real-time progress (M3) \ud83e\udde9 Caching & Reuse (M1) \u00b6 Intermediate Data : Stored via StoragePort with schema validation Result Hashing : SHA256 of config + inputs for cache hits Warm Runs : Workflows can resume from cached intermediate outputs def get_cache_key ( node : NodeSpec , inputs : Dict [ str , Any ]) -> str : \"\"\"Generate cache key for node execution.\"\"\" config_hash = hashlib . sha256 ( json . dumps ( node . params ) . encode ()) . hexdigest () input_hash = hashlib . sha256 ( json . dumps ( inputs ) . encode ()) . hexdigest () return f \" { node . id } : { config_hash } : { input_hash } \" \ud83e\uddf1 Example DAG Execution Trace (M1) \u00b6 [2025-10-14 12:01:02] Workflow \"Linear Security Scan\" started [2025-10-14 12:01:05] Node discovery.ferox completed (urls=356) [2025-10-14 12:01:07] Node scan.nuclei completed (findings=112) [2025-10-14 12:01:10] Node enrich.cve completed (enriched_findings=112) [2025-10-14 12:01:10] Workflow completed successfully \ud83d\udd0c Integration with Other Components (M1) \u00b6 Component Interaction Findings Engine Receives raw output for normalization via StoragePort Plugin System Executes plugins through PluginLoader interface Storage Persists workflow runs, logs, results via StoragePort Security Validates plugin signatures and enforces sandboxing Observability Publishes events and metrics for monitoring \ud83e\udde9 Monitoring & Metrics (M1) \u00b6 Every node execution reports: - Duration (seconds) - Findings count - Exit status - Memory usage - Cache hits - Plugin execution details These metrics are stored via StoragePort and available for analysis. \ud83e\udde0 Future Enhancements (M3+) \u00b6 Parallel Execution : Concurrent node execution where dependencies allow Event System : Real-time event publishing and monitoring Advanced Caching : Intermediate result caching with persistence GraphQL API : Workflow builder UI integration Dynamic Scheduling : Priority and resource weighting Conditional Branching : if, switch nodes for complex workflows AI-assisted Suggestions : Workflow recommendations based on context Next: Plugin System","title":"Orchestration Engine"},{"location":"architecture/05-orchestration-and-workflow-engine/#05-orchestration-workflow-engine","text":"","title":"05 \u2014 Orchestration &amp; Workflow Engine"},{"location":"architecture/05-orchestration-and-workflow-engine/#overview","text":"The Orchestration Engine is the operational core of SecFlow. It executes multi-tool workflows composed of discovery , scanning , filtering , enrichment , and analysis stages. Workflows are defined declaratively using YAML recipes and executed as Directed Acyclic Graphs (DAGs) . Each node represents a tool invocation , and edges define data flow between outputs and inputs. M1 Implementation Status : \u2705 Complete - Sequential execution, validation, retry logic, and plugin integration delivered. Note : SecFlow executes workflow steps sequentially in M1 (parallel execution to be introduced in future versions).","title":"\ud83e\udded Overview"},{"location":"architecture/05-orchestration-and-workflow-engine/#conceptual-model","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Node A<br/>(Ferox)\"] B[\"Node B<br/>(Nuclei)\"] C[\"Node C<br/>(CVE Enr)\"] D[\"Inputs/Outputs\"] E[\"Config/Env\"] F[\"StoragePort\"] A --> B B --> C A --> D B --> E C --> F Each node produces one or more datasets that can be consumed by downstream nodes. The workflow engine guarantees: - Topological order of execution - Sequential execution (M1) with parallel execution planned for M3 - Automatic retries, timeouts, and logging per node - StoragePort integration for findings persistence","title":"\ud83e\uddf1 Conceptual Model"},{"location":"architecture/05-orchestration-and-workflow-engine/#workflow-specification-schema-m1","text":"version : \"1.0\" name : \"Linear Security Scan\" description : \"Simple linear workflow: discovery \u2192 scan \u2192 enrichment\" nodes : - id : \"discovery\" type : \"discovery.ferox\" config : wordlist : \"res://wordlists/dirb:latest\" threads : 50 timeout : 300 outputs : [ \"urls\" ] - id : \"scan\" type : \"scan.nuclei\" inputs : [ \"urls\" ] config : templates : \"res://templates/owasp-top10:latest\" rate_limit : 150 timeout : 600 outputs : [ \"findings\" ] - id : \"enrich\" type : \"enrich.cve\" inputs : [ \"findings\" ] config : sources : [ \"nvd\" , \"osv\" , \"exploitdb\" ] timeout : 120 outputs : [ \"enriched_findings\" ] # Retry configuration retry : max_attempts : 3 backoff_factor : 2.0 base_delay : 5.0 # State management state : checkpoint_interval : 30 resume_on_failure : true cache_intermediate : true","title":"\u2699\ufe0f Workflow Specification Schema (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#workflow-engine-architecture-m1","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"WorkflowExecutor\"] B[\"RecipeValidator\"] C[\"NodeExecutor\"] D[\"PluginLoader\"] E[\"StoragePort\"] F[\"Findings Schema\"] A --> B A --> C C --> D C --> E E --> F G[\"YAML Recipe\"] H[\"DAG Validation\"] I[\"Sequential Execution\"] G --> A H --> B I --> C M1 Components Delivered : - \u2705 WorkflowExecutor : Parses YAML recipes, builds DAG, executes nodes sequentially - \u2705 RecipeValidator : Comprehensive validation with Pydantic schemas - \u2705 NodeExecutor : Executes individual nodes with retry logic and timeouts - \u2705 PluginLoader : Integrates with plugin system for tool execution - \u2705 StoragePort Integration : Uses StoragePort for findings persistence","title":"\ud83e\udde9 Workflow Engine Architecture (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#python-model-dag-representation-m1","text":"# packages/workflow_engine/executor.py from typing import Dict , List , Any , Optional from pydantic import BaseModel from dataclasses import dataclass , field @dataclass class NodeSpec : \"\"\"Node specification for workflow execution.\"\"\" id : str type : str params : Dict [ str , Any ] = field ( default_factory = dict ) requires : List [ str ] = field ( default_factory = list ) outputs : List [ str ] = field ( default_factory = list ) timeout_s : Optional [ int ] = None retries : int = 0 retry_backoff_s : float = 0.5 @dataclass class WorkflowSpec : \"\"\"Workflow specification.\"\"\" id : str name : str description : str nodes : List [ NodeSpec ] retry_config : Dict [ str , Any ] = field ( default_factory = dict ) state_config : Dict [ str , Any ] = field ( default_factory = dict ) class WorkflowExecutor : \"\"\"M1 Workflow Executor with sequential execution.\"\"\" def __init__ ( self , storage : StoragePort = None ): self . storage = storage or InMemoryStorageAdapter () self . plugin_loader = PluginLoader () def execute_workflow ( self , workflow : WorkflowSpec ) -> Dict [ str , Any ]: \"\"\"Execute workflow with sequential node processing.\"\"\" # M1: Sequential execution # M3: Parallel execution where dependencies allow pass","title":"\u2699\ufe0f Python Model \u2014 DAG Representation (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#dag-validation-example-m1","text":"def validate_dag ( workflow : WorkflowSpec ): \"\"\"Validate workflow DAG structure.\"\"\" ids = [ n . id for n in workflow . nodes ] # Check for cycles if has_cycles ( workflow . nodes ): raise ValueError ( \"Workflow contains cycles\" ) # Validate input/output references for node in workflow . nodes : for inp in node . requires : if inp not in [ out for n in workflow . nodes for out in n . outputs ]: raise ValueError ( f \"Unresolved input ' { inp } ' in node { node . id } \" ) # Validate node types for node in workflow . nodes : if not is_valid_node_type ( node . type ): raise ValueError ( f \"Unknown node type: { node . type } \" )","title":"DAG Validation Example (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#execution-flow-m1-implementation","text":"Parse & Validate YAML workflow using Pydantic schema Build DAG with topological ordering and dependency resolution Register Workflow in execution context Execute Nodes Sequentially respecting topological order Plugin Integration via PluginLoader for tool execution Normalize Findings via StoragePort with schema validation Update Metrics and trigger downstream listeners M1 Execution Model : - Sequential : Nodes execute one after another in topological order - Retry Logic : Configurable retry with exponential backoff - Timeout Handling : Per-node timeout enforcement - Error Recovery : Graceful failure handling with partial execution support","title":"\ud83e\udde0 Execution Flow (M1 Implementation)"},{"location":"architecture/05-orchestration-and-workflow-engine/#node-executor-m1-implementation","text":"# packages/workflow_engine/executor.py class NodeExecutor : \"\"\"M1 Node Executor with retry logic and timeout handling.\"\"\" def __init__ ( self , node : NodeSpec , context : ExecutionContext ): self . node = node self . context = context self . storage = context . storage def execute ( self ) -> Dict [ str , Any ]: \"\"\"Execute node with retry logic and timeout handling.\"\"\" for attempt in range ( self . node . retries + 1 ): try : # Load plugin for node type plugin = self . context . plugin_loader . load ( self . node . type ) # Execute plugin with timeout result = self . _execute_with_timeout ( plugin ) # Persist findings via StoragePort self . _persist_findings ( result ) return result except TimeoutError : if attempt < self . node . retries : delay = self . node . retry_backoff_s * ( 2 ** attempt ) time . sleep ( delay ) continue raise except Exception as e : if attempt < self . node . retries : delay = self . node . retry_backoff_s * ( 2 ** attempt ) time . sleep ( delay ) continue raise def _execute_with_timeout ( self , plugin : PluginInterface ) -> Dict [ str , Any ]: \"\"\"Execute plugin with timeout enforcement.\"\"\" timeout = self . node . timeout_s or 300 # Default 5 minutes with timeout_context ( timeout ): return plugin . run ( inputs = self . context . get_node_inputs ( self . node ), config = self . node . params , context = self . context ) def _persist_findings ( self , result : Dict [ str , Any ]) -> None : \"\"\"Persist findings via StoragePort.\"\"\" findings = result . get ( \"findings\" , []) for finding in findings : # Ensure schema compliance finding [ \"finding_schema_version\" ] = \"1.0.0\" self . storage . save_finding ( finding )","title":"\u2699\ufe0f Node Executor (M1 Implementation)"},{"location":"architecture/05-orchestration-and-workflow-engine/#concurrency-model-m1-vs-m3","text":"M1 Implementation : Sequential execution only - Nodes execute one after another in topological order - Dependencies are resolved before execution - Simple error handling and retry logic - Suitable for linear workflows and testing M3 Planned : Parallel execution where dependencies allow - Independent nodes run concurrently - Dependency resolution with parallel execution - Advanced error handling and recovery - Event-driven architecture for real-time monitoring","title":"\ud83d\udd04 Concurrency Model (M1 vs M3)"},{"location":"architecture/05-orchestration-and-workflow-engine/#example-sequential-execution-m1","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"ferox<br/>(discovery)\"] B[\"katana<br/>(subdomain)\"] C[\"nuclei<br/>(scan)\"] D[\"cve-enrich<br/>(enrichment)\"] A --> B B --> C A --> C C --> D Execution Order : ferox \u2192 katana \u2192 nuclei \u2192 cve-enrich","title":"Example Sequential Execution (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#error-handling-m1","text":"Error Type Handling Strategy Tool crash / non-zero exit Retry (max=3) then mark node failed Timeout Kill process, log event, continue DAG Missing input dataset Block downstream nodes, mark dependency unresolved Parser error Log raw output, fallback to generic findings schema Plugin load failure Skip node, log warning, continue execution Each failure is logged with context and stored via StoragePort for analysis.","title":"\ud83e\udde9 Error Handling (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#event-system-m1","text":"The orchestration layer publishes events for monitoring and debugging:","title":"\ud83e\udde0 Event System (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#example-event-contract","text":"{ \"event\" : \"node_completed\" , \"workflow_id\" : \"abc123\" , \"node_id\" : \"scan\" , \"node_type\" : \"scan.nuclei\" , \"duration\" : 12.3 , \"findings_count\" : 124 , \"success\" : true , \"timestamp\" : \"2025-10-14T10:30:00Z\" } Events can be consumed by: - Logging systems for audit trails - Metrics collectors for performance monitoring - UI components for real-time progress (M3)","title":"Example Event Contract"},{"location":"architecture/05-orchestration-and-workflow-engine/#caching-reuse-m1","text":"Intermediate Data : Stored via StoragePort with schema validation Result Hashing : SHA256 of config + inputs for cache hits Warm Runs : Workflows can resume from cached intermediate outputs def get_cache_key ( node : NodeSpec , inputs : Dict [ str , Any ]) -> str : \"\"\"Generate cache key for node execution.\"\"\" config_hash = hashlib . sha256 ( json . dumps ( node . params ) . encode ()) . hexdigest () input_hash = hashlib . sha256 ( json . dumps ( inputs ) . encode ()) . hexdigest () return f \" { node . id } : { config_hash } : { input_hash } \"","title":"\ud83e\udde9 Caching &amp; Reuse (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#example-dag-execution-trace-m1","text":"[2025-10-14 12:01:02] Workflow \"Linear Security Scan\" started [2025-10-14 12:01:05] Node discovery.ferox completed (urls=356) [2025-10-14 12:01:07] Node scan.nuclei completed (findings=112) [2025-10-14 12:01:10] Node enrich.cve completed (enriched_findings=112) [2025-10-14 12:01:10] Workflow completed successfully","title":"\ud83e\uddf1 Example DAG Execution Trace (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#integration-with-other-components-m1","text":"Component Interaction Findings Engine Receives raw output for normalization via StoragePort Plugin System Executes plugins through PluginLoader interface Storage Persists workflow runs, logs, results via StoragePort Security Validates plugin signatures and enforces sandboxing Observability Publishes events and metrics for monitoring","title":"\ud83d\udd0c Integration with Other Components (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#monitoring-metrics-m1","text":"Every node execution reports: - Duration (seconds) - Findings count - Exit status - Memory usage - Cache hits - Plugin execution details These metrics are stored via StoragePort and available for analysis.","title":"\ud83e\udde9 Monitoring &amp; Metrics (M1)"},{"location":"architecture/05-orchestration-and-workflow-engine/#future-enhancements-m3","text":"Parallel Execution : Concurrent node execution where dependencies allow Event System : Real-time event publishing and monitoring Advanced Caching : Intermediate result caching with persistence GraphQL API : Workflow builder UI integration Dynamic Scheduling : Priority and resource weighting Conditional Branching : if, switch nodes for complex workflows AI-assisted Suggestions : Workflow recommendations based on context Next: Plugin System","title":"\ud83e\udde0 Future Enhancements (M3+)"},{"location":"architecture/06-plugin-system/","text":"06 \u2014 Plugin System \u00b6 \ud83e\udded Overview \u00b6 The Plugin System in SecFlow provides an extensible runtime for integrating new detection, enrichment, or analytics capabilities without modifying core code. Plugins operate under a controlled lifecycle and interact through strictly defined contracts (ports) . SecFlow supports three primary plugin categories: 1. Detectors \u2014 Introduce new scanning logic or post-processing steps. 2. Enrichers \u2014 Add vulnerability metadata (CVE, CWE, CVSS, etc.). 3. Analytics \u2014 Compute metrics, patterns, or visual insights over findings. M1 Implementation Status : \u2705 Complete - Plugin loader, security framework, and stub implementations delivered. \ud83e\udde9 Design Principles \u00b6 Principle Description Isolation Each plugin runs in its own context (thread/process sandbox). Declarative Registration Plugins register via manifests or decorators. Contract Enforcement Plugins implement strict base interfaces ( PluginInterface , PluginMetadata , etc.). Hot Reloading Plugin discovery occurs dynamically at startup. Security Each plugin is validated, signed, and sandboxed for safe execution. \ud83e\uddf1 Plugin Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Executor\"] B[\"Plugin Loader\"] C[\"Security Framework\"] D[\"Plugin Registry\"] E[\"StoragePort\"] F[\"Findings Schema\"] A --> B B --> C B --> D B --> E E --> F G[\"Plugin Implementation\"] H[\"Plugin Manifest\"] I[\"Signature Verification\"] G --> B H --> C I --> C M1 Components Delivered : - \u2705 PluginLoader : Dynamic plugin discovery and loading - \u2705 Security Framework : Signature verification and sandboxing - \u2705 PluginInterface : Standard contract for all plugins - \u2705 StoragePort Integration : Findings persistence via StoragePort - \u2705 Stub Implementations : CVEMapper, FeroxStub, NucleiStub plugins \u2699\ufe0f Plugin Interface (M1 Implementation) \u00b6 # packages/plugins/loader.py from abc import ABC , abstractmethod from typing import Dict , Any , Optional from dataclasses import dataclass @dataclass class PluginMetadata : \"\"\"Plugin metadata information.\"\"\" name : str version : str description : str author : str category : str # \"detector\" | \"enricher\" | \"analytics\" entrypoint : str dependencies : list = None config_schema : Optional [ Dict [ str , Any ]] = None class PluginInterface ( ABC ): \"\"\"Abstract base class that all plugins must implement.\"\"\" @abstractmethod def get_name ( self ) -> str : \"\"\"Get plugin name.\"\"\" pass @abstractmethod def get_version ( self ) -> str : \"\"\"Get plugin version.\"\"\" pass @abstractmethod def get_metadata ( self ) -> PluginMetadata : \"\"\"Get plugin metadata.\"\"\" pass @abstractmethod def run ( self , inputs : Dict [ str , Any ], config : Dict [ str , Any ], context : ExecutionContext ) -> Dict [ str , Any ]: \"\"\"Execute plugin logic on input data.\"\"\" pass def verify_signature ( self ) -> bool : \"\"\"Verify plugin signature if present.\"\"\" # M1: Basic signature verification # M2+: Full cryptographic verification pass All plugins must implement PluginInterface and provide proper metadata. \ud83d\udd10 Security Model (M1 Implementation) \u00b6 Plugin Signing & Verification \u00b6 M1 Security Features : - \u2705 Hash-based Verification : SHA256 checksums for plugin integrity - \u2705 Manifest Validation : JSON schema validation for plugin manifests - \u2705 Sandbox Execution : Controlled execution environment - \u2705 Audit Logging : Comprehensive security event logging # security/signing.py @dataclass class PluginManifest : \"\"\"Plugin manifest with signature information.\"\"\" name : str version : str description : str author : str entrypoint : str code_hash : str signature : Optional [ str ] = None signature_type : str = \"sha256\" # M1: hash-based, M2+: RSA/ECDSA created_at : str = None expires_at : Optional [ str ] = None class PluginSignatureVerifier : \"\"\"Plugin signature verification for M1.\"\"\" def verify_plugin_signature ( self , manifest : PluginManifest , plugin_path : str ) -> bool : \"\"\"Verify plugin signature using hash-based verification.\"\"\" # M1: SHA256 hash verification # M2+: Full cryptographic signature verification pass Sandbox Model \u00b6 Each plugin executes inside a restricted environment: Control Enforcement Filesystem Read-only mount or temp directory Network Denied by default, opt-in per manifest Memory / CPU Controlled via subprocess resource limits Timeouts Enforced via execution wrapper Audit Every plugin invocation logged with context \ud83e\udde9 Plugin Discovery & Loading (M1) \u00b6 Directory Layout \u00b6 packages/plugins/ \u251c\u2500\u2500 loader.py # PluginLoader implementation \u251c\u2500\u2500 registry.py # Plugin registry \u251c\u2500\u2500 security/ # Security framework \u2502 \u251c\u2500\u2500 signing.py # Signature verification \u2502 \u251c\u2500\u2500 sandbox.py # Sandbox execution \u2502 \u2514\u2500\u2500 audit_logging.py # Security audit logs \u2514\u2500\u2500 stubs/ # M1 stub implementations \u251c\u2500\u2500 cve_mapper.py # CVE enrichment plugin \u251c\u2500\u2500 ferox_stub.py # Feroxbuster discovery \u2514\u2500\u2500 nuclei_stub.py # Nuclei scanning Discovery Algorithm \u00b6 def discover_plugins (): \"\"\"Discover and load plugins dynamically.\"\"\" for manifest_file in Path ( \"packages/plugins/manifests\" ) . glob ( \"*.json\" ): manifest_data = json . loads ( manifest_file . read_text ()) manifest = PluginManifest ( ** manifest_data ) # Verify signature if not verify_plugin_signature ( manifest , plugin_path ): logger . warning ( f \"Plugin { manifest . name } signature verification failed\" ) continue # Load plugin plugin = load_plugin_class ( manifest . entrypoint ) PluginRegistry . register ( manifest . name , plugin ) \ud83e\udde0 Plugin Lifecycle (M1 Implementation) \u00b6 Discovery : Scan manifests directory for plugin definitions Validation : Verify signatures and validate manifests Registration : Register plugins in PluginRegistry Initialization : Create plugin instances with execution context Execution : Process data via run() method with StoragePort integration Teardown : Release resources, log metrics \ud83e\udde9 Plugin Manifest Specification (M1) \u00b6 { \"name\" : \"cve-mapper\" , \"version\" : \"1.0.0\" , \"description\" : \"CVE enrichment plugin for vulnerability findings\" , \"author\" : \"SecFlow Team\" , \"category\" : \"enricher\" , \"entrypoint\" : \"packages.plugins.stubs.cve_mapper:CVEMapperPlugin\" , \"dependencies\" : [ \"requests\" , \"cpe\" ], \"config_schema\" : { \"sources\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"default\" : [ \"nvd\" , \"osv\" ] }, \"timeout\" : { \"type\" : \"integer\" , \"default\" : 30 } }, \"code_hash\" : \"sha256:abc123...\" , \"signature\" : \"sha256:def456...\" , \"created_at\" : \"2025-10-14T10:00:00Z\" , \"expires_at\" : \"2026-10-14T10:00:00Z\" } Each manifest is stored under /packages/plugins/manifests/ and validated on startup. \u2699\ufe0f Example \u2014 CVE Mapper Plugin (M1 Stub) \u00b6 # packages/plugins/stubs/cve_mapper.py from packages.plugins.loader import PluginInterface , PluginMetadata from packages.runtime_core.storage.storage_port import StoragePort from typing import Dict , Any class CVEMapperPlugin ( PluginInterface ): \"\"\"CVE enrichment plugin - M1 stub implementation.\"\"\" def get_name ( self ) -> str : return \"cve-mapper\" def get_version ( self ) -> str : return \"1.0.0\" def get_metadata ( self ) -> PluginMetadata : return PluginMetadata ( name = \"cve-mapper\" , version = \"1.0.0\" , description = \"CVE enrichment plugin for vulnerability findings\" , author = \"SecFlow Team\" , category = \"enricher\" , entrypoint = \"packages.plugins.stubs.cve_mapper:CVEMapperPlugin\" ) def run ( self , inputs : Dict [ str , Any ], config : Dict [ str , Any ], context : ExecutionContext ) -> Dict [ str , Any ]: \"\"\"Enrich findings with CVE data.\"\"\" findings = inputs . get ( \"findings\" , []) sources = config . get ( \"sources\" , [ \"nvd\" , \"osv\" ]) enriched_findings = [] for finding in findings : # M1: Use golden sample data # M2+: Real CVE API calls enriched_finding = self . _enrich_with_cve_data ( finding , sources ) enriched_findings . append ( enriched_finding ) return { \"enriched_findings\" : enriched_findings } def _enrich_with_cve_data ( self , finding : Dict [ str , Any ], sources : list ) -> Dict [ str , Any ]: \"\"\"Enrich finding with CVE data from golden samples.\"\"\" # M1: Return golden sample data # M2+: Make real API calls to CVE databases return { ** finding , \"cve_details\" : { \"cve_id\" : \"CVE-2023-1234\" , \"cvss_score\" : 7.5 , \"description\" : \"Sample CVE description\" , \"sources\" : sources } } \ud83d\udd0c StoragePort Integration \u00b6 Plugins integrate with the StoragePort interface for findings persistence: # Plugin execution with StoragePort def execute_plugin_with_storage ( plugin : PluginInterface , inputs : Dict [ str , Any ], storage : StoragePort ) -> Dict [ str , Any ]: \"\"\"Execute plugin and persist findings via StoragePort.\"\"\" # Execute plugin results = plugin . run ( inputs , config , context ) # Persist findings via StoragePort findings = results . get ( \"findings\" , []) for finding in findings : # Ensure finding has schema version finding [ \"finding_schema_version\" ] = \"1.0.0\" storage . save_finding ( finding ) return results Finding Schema Integration : - All findings must include finding_schema_version: \"1.0.0\" - Findings are validated against schemas/finding.json - StoragePort enforces schema compliance \ud83e\udde0 Plugin Telemetry & Monitoring \u00b6 Each plugin emits lifecycle events for observability: { \"event\" : \"plugin_executed\" , \"plugin\" : \"cve-mapper\" , \"version\" : \"1.0.0\" , \"duration_ms\" : 342 , \"memory_mb\" : 42 , \"findings_processed\" : 15 , \"success\" : true , \"timestamp\" : \"2025-10-14T10:30:00Z\" } Telemetry is captured by the Observability subsystem and stored via StoragePort. \ud83e\udde9 Error Handling \u00b6 Error Strategy Invalid Manifest Skip plugin, log warning, continue execution Dependency ImportError Attempt isolated install if allowed Execution Timeout Abort plugin, mark node partial-success Sandbox Violation Terminate process, revoke plugin signature Signature Verification Failure Block plugin execution, log security event \ud83e\udde0 Example End-to-End Plugin Flow (M1) \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Executor\"] B[\"Plugin Loader\"] C[\"Security Verification\"] D[\"Plugin Execution\"] E[\"StoragePort\"] F[\"Findings Storage\"] A --> B B --> C C --> D D --> E E --> F G[\"Plugin Manifest\"] H[\"Signature Check\"] I[\"Sandbox Execution\"] G --> C H --> C I --> D Execution Steps : 1. Workflow Executor requests plugin execution 2. Plugin Loader discovers and loads plugin 3. Security Verification validates signature and manifest 4. Plugin Execution runs in sandboxed environment 5. StoragePort persists findings with schema validation 6. Findings Storage stores enriched data \ud83d\udd2e Future Enhancements (M2+) \u00b6 Full Cryptographic Signing : RSA/ECDSA signature verification Remote Plugin Repository : Index + version resolution In-UI Plugin Store : Validation and ratings Plugin Telemetry Dashboards : Aggregated metrics and insights Advanced Sandboxing : Container-based isolation Plugin Dependencies : Automatic dependency resolution Next: Tools Integration Model","title":"Plugin System"},{"location":"architecture/06-plugin-system/#06-plugin-system","text":"","title":"06 \u2014 Plugin System"},{"location":"architecture/06-plugin-system/#overview","text":"The Plugin System in SecFlow provides an extensible runtime for integrating new detection, enrichment, or analytics capabilities without modifying core code. Plugins operate under a controlled lifecycle and interact through strictly defined contracts (ports) . SecFlow supports three primary plugin categories: 1. Detectors \u2014 Introduce new scanning logic or post-processing steps. 2. Enrichers \u2014 Add vulnerability metadata (CVE, CWE, CVSS, etc.). 3. Analytics \u2014 Compute metrics, patterns, or visual insights over findings. M1 Implementation Status : \u2705 Complete - Plugin loader, security framework, and stub implementations delivered.","title":"\ud83e\udded Overview"},{"location":"architecture/06-plugin-system/#design-principles","text":"Principle Description Isolation Each plugin runs in its own context (thread/process sandbox). Declarative Registration Plugins register via manifests or decorators. Contract Enforcement Plugins implement strict base interfaces ( PluginInterface , PluginMetadata , etc.). Hot Reloading Plugin discovery occurs dynamically at startup. Security Each plugin is validated, signed, and sandboxed for safe execution.","title":"\ud83e\udde9 Design Principles"},{"location":"architecture/06-plugin-system/#plugin-architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Executor\"] B[\"Plugin Loader\"] C[\"Security Framework\"] D[\"Plugin Registry\"] E[\"StoragePort\"] F[\"Findings Schema\"] A --> B B --> C B --> D B --> E E --> F G[\"Plugin Implementation\"] H[\"Plugin Manifest\"] I[\"Signature Verification\"] G --> B H --> C I --> C M1 Components Delivered : - \u2705 PluginLoader : Dynamic plugin discovery and loading - \u2705 Security Framework : Signature verification and sandboxing - \u2705 PluginInterface : Standard contract for all plugins - \u2705 StoragePort Integration : Findings persistence via StoragePort - \u2705 Stub Implementations : CVEMapper, FeroxStub, NucleiStub plugins","title":"\ud83e\uddf1 Plugin Architecture Overview"},{"location":"architecture/06-plugin-system/#plugin-interface-m1-implementation","text":"# packages/plugins/loader.py from abc import ABC , abstractmethod from typing import Dict , Any , Optional from dataclasses import dataclass @dataclass class PluginMetadata : \"\"\"Plugin metadata information.\"\"\" name : str version : str description : str author : str category : str # \"detector\" | \"enricher\" | \"analytics\" entrypoint : str dependencies : list = None config_schema : Optional [ Dict [ str , Any ]] = None class PluginInterface ( ABC ): \"\"\"Abstract base class that all plugins must implement.\"\"\" @abstractmethod def get_name ( self ) -> str : \"\"\"Get plugin name.\"\"\" pass @abstractmethod def get_version ( self ) -> str : \"\"\"Get plugin version.\"\"\" pass @abstractmethod def get_metadata ( self ) -> PluginMetadata : \"\"\"Get plugin metadata.\"\"\" pass @abstractmethod def run ( self , inputs : Dict [ str , Any ], config : Dict [ str , Any ], context : ExecutionContext ) -> Dict [ str , Any ]: \"\"\"Execute plugin logic on input data.\"\"\" pass def verify_signature ( self ) -> bool : \"\"\"Verify plugin signature if present.\"\"\" # M1: Basic signature verification # M2+: Full cryptographic verification pass All plugins must implement PluginInterface and provide proper metadata.","title":"\u2699\ufe0f Plugin Interface (M1 Implementation)"},{"location":"architecture/06-plugin-system/#security-model-m1-implementation","text":"","title":"\ud83d\udd10 Security Model (M1 Implementation)"},{"location":"architecture/06-plugin-system/#plugin-signing-verification","text":"M1 Security Features : - \u2705 Hash-based Verification : SHA256 checksums for plugin integrity - \u2705 Manifest Validation : JSON schema validation for plugin manifests - \u2705 Sandbox Execution : Controlled execution environment - \u2705 Audit Logging : Comprehensive security event logging # security/signing.py @dataclass class PluginManifest : \"\"\"Plugin manifest with signature information.\"\"\" name : str version : str description : str author : str entrypoint : str code_hash : str signature : Optional [ str ] = None signature_type : str = \"sha256\" # M1: hash-based, M2+: RSA/ECDSA created_at : str = None expires_at : Optional [ str ] = None class PluginSignatureVerifier : \"\"\"Plugin signature verification for M1.\"\"\" def verify_plugin_signature ( self , manifest : PluginManifest , plugin_path : str ) -> bool : \"\"\"Verify plugin signature using hash-based verification.\"\"\" # M1: SHA256 hash verification # M2+: Full cryptographic signature verification pass","title":"Plugin Signing &amp; Verification"},{"location":"architecture/06-plugin-system/#sandbox-model","text":"Each plugin executes inside a restricted environment: Control Enforcement Filesystem Read-only mount or temp directory Network Denied by default, opt-in per manifest Memory / CPU Controlled via subprocess resource limits Timeouts Enforced via execution wrapper Audit Every plugin invocation logged with context","title":"Sandbox Model"},{"location":"architecture/06-plugin-system/#plugin-discovery-loading-m1","text":"","title":"\ud83e\udde9 Plugin Discovery &amp; Loading (M1)"},{"location":"architecture/06-plugin-system/#directory-layout","text":"packages/plugins/ \u251c\u2500\u2500 loader.py # PluginLoader implementation \u251c\u2500\u2500 registry.py # Plugin registry \u251c\u2500\u2500 security/ # Security framework \u2502 \u251c\u2500\u2500 signing.py # Signature verification \u2502 \u251c\u2500\u2500 sandbox.py # Sandbox execution \u2502 \u2514\u2500\u2500 audit_logging.py # Security audit logs \u2514\u2500\u2500 stubs/ # M1 stub implementations \u251c\u2500\u2500 cve_mapper.py # CVE enrichment plugin \u251c\u2500\u2500 ferox_stub.py # Feroxbuster discovery \u2514\u2500\u2500 nuclei_stub.py # Nuclei scanning","title":"Directory Layout"},{"location":"architecture/06-plugin-system/#discovery-algorithm","text":"def discover_plugins (): \"\"\"Discover and load plugins dynamically.\"\"\" for manifest_file in Path ( \"packages/plugins/manifests\" ) . glob ( \"*.json\" ): manifest_data = json . loads ( manifest_file . read_text ()) manifest = PluginManifest ( ** manifest_data ) # Verify signature if not verify_plugin_signature ( manifest , plugin_path ): logger . warning ( f \"Plugin { manifest . name } signature verification failed\" ) continue # Load plugin plugin = load_plugin_class ( manifest . entrypoint ) PluginRegistry . register ( manifest . name , plugin )","title":"Discovery Algorithm"},{"location":"architecture/06-plugin-system/#plugin-lifecycle-m1-implementation","text":"Discovery : Scan manifests directory for plugin definitions Validation : Verify signatures and validate manifests Registration : Register plugins in PluginRegistry Initialization : Create plugin instances with execution context Execution : Process data via run() method with StoragePort integration Teardown : Release resources, log metrics","title":"\ud83e\udde0 Plugin Lifecycle (M1 Implementation)"},{"location":"architecture/06-plugin-system/#plugin-manifest-specification-m1","text":"{ \"name\" : \"cve-mapper\" , \"version\" : \"1.0.0\" , \"description\" : \"CVE enrichment plugin for vulnerability findings\" , \"author\" : \"SecFlow Team\" , \"category\" : \"enricher\" , \"entrypoint\" : \"packages.plugins.stubs.cve_mapper:CVEMapperPlugin\" , \"dependencies\" : [ \"requests\" , \"cpe\" ], \"config_schema\" : { \"sources\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"default\" : [ \"nvd\" , \"osv\" ] }, \"timeout\" : { \"type\" : \"integer\" , \"default\" : 30 } }, \"code_hash\" : \"sha256:abc123...\" , \"signature\" : \"sha256:def456...\" , \"created_at\" : \"2025-10-14T10:00:00Z\" , \"expires_at\" : \"2026-10-14T10:00:00Z\" } Each manifest is stored under /packages/plugins/manifests/ and validated on startup.","title":"\ud83e\udde9 Plugin Manifest Specification (M1)"},{"location":"architecture/06-plugin-system/#example-cve-mapper-plugin-m1-stub","text":"# packages/plugins/stubs/cve_mapper.py from packages.plugins.loader import PluginInterface , PluginMetadata from packages.runtime_core.storage.storage_port import StoragePort from typing import Dict , Any class CVEMapperPlugin ( PluginInterface ): \"\"\"CVE enrichment plugin - M1 stub implementation.\"\"\" def get_name ( self ) -> str : return \"cve-mapper\" def get_version ( self ) -> str : return \"1.0.0\" def get_metadata ( self ) -> PluginMetadata : return PluginMetadata ( name = \"cve-mapper\" , version = \"1.0.0\" , description = \"CVE enrichment plugin for vulnerability findings\" , author = \"SecFlow Team\" , category = \"enricher\" , entrypoint = \"packages.plugins.stubs.cve_mapper:CVEMapperPlugin\" ) def run ( self , inputs : Dict [ str , Any ], config : Dict [ str , Any ], context : ExecutionContext ) -> Dict [ str , Any ]: \"\"\"Enrich findings with CVE data.\"\"\" findings = inputs . get ( \"findings\" , []) sources = config . get ( \"sources\" , [ \"nvd\" , \"osv\" ]) enriched_findings = [] for finding in findings : # M1: Use golden sample data # M2+: Real CVE API calls enriched_finding = self . _enrich_with_cve_data ( finding , sources ) enriched_findings . append ( enriched_finding ) return { \"enriched_findings\" : enriched_findings } def _enrich_with_cve_data ( self , finding : Dict [ str , Any ], sources : list ) -> Dict [ str , Any ]: \"\"\"Enrich finding with CVE data from golden samples.\"\"\" # M1: Return golden sample data # M2+: Make real API calls to CVE databases return { ** finding , \"cve_details\" : { \"cve_id\" : \"CVE-2023-1234\" , \"cvss_score\" : 7.5 , \"description\" : \"Sample CVE description\" , \"sources\" : sources } }","title":"\u2699\ufe0f Example \u2014 CVE Mapper Plugin (M1 Stub)"},{"location":"architecture/06-plugin-system/#storageport-integration","text":"Plugins integrate with the StoragePort interface for findings persistence: # Plugin execution with StoragePort def execute_plugin_with_storage ( plugin : PluginInterface , inputs : Dict [ str , Any ], storage : StoragePort ) -> Dict [ str , Any ]: \"\"\"Execute plugin and persist findings via StoragePort.\"\"\" # Execute plugin results = plugin . run ( inputs , config , context ) # Persist findings via StoragePort findings = results . get ( \"findings\" , []) for finding in findings : # Ensure finding has schema version finding [ \"finding_schema_version\" ] = \"1.0.0\" storage . save_finding ( finding ) return results Finding Schema Integration : - All findings must include finding_schema_version: \"1.0.0\" - Findings are validated against schemas/finding.json - StoragePort enforces schema compliance","title":"\ud83d\udd0c StoragePort Integration"},{"location":"architecture/06-plugin-system/#plugin-telemetry-monitoring","text":"Each plugin emits lifecycle events for observability: { \"event\" : \"plugin_executed\" , \"plugin\" : \"cve-mapper\" , \"version\" : \"1.0.0\" , \"duration_ms\" : 342 , \"memory_mb\" : 42 , \"findings_processed\" : 15 , \"success\" : true , \"timestamp\" : \"2025-10-14T10:30:00Z\" } Telemetry is captured by the Observability subsystem and stored via StoragePort.","title":"\ud83e\udde0 Plugin Telemetry &amp; Monitoring"},{"location":"architecture/06-plugin-system/#error-handling","text":"Error Strategy Invalid Manifest Skip plugin, log warning, continue execution Dependency ImportError Attempt isolated install if allowed Execution Timeout Abort plugin, mark node partial-success Sandbox Violation Terminate process, revoke plugin signature Signature Verification Failure Block plugin execution, log security event","title":"\ud83e\udde9 Error Handling"},{"location":"architecture/06-plugin-system/#example-end-to-end-plugin-flow-m1","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Executor\"] B[\"Plugin Loader\"] C[\"Security Verification\"] D[\"Plugin Execution\"] E[\"StoragePort\"] F[\"Findings Storage\"] A --> B B --> C C --> D D --> E E --> F G[\"Plugin Manifest\"] H[\"Signature Check\"] I[\"Sandbox Execution\"] G --> C H --> C I --> D Execution Steps : 1. Workflow Executor requests plugin execution 2. Plugin Loader discovers and loads plugin 3. Security Verification validates signature and manifest 4. Plugin Execution runs in sandboxed environment 5. StoragePort persists findings with schema validation 6. Findings Storage stores enriched data","title":"\ud83e\udde0 Example End-to-End Plugin Flow (M1)"},{"location":"architecture/06-plugin-system/#future-enhancements-m2","text":"Full Cryptographic Signing : RSA/ECDSA signature verification Remote Plugin Repository : Index + version resolution In-UI Plugin Store : Validation and ratings Plugin Telemetry Dashboards : Aggregated metrics and insights Advanced Sandboxing : Container-based isolation Plugin Dependencies : Automatic dependency resolution Next: Tools Integration Model","title":"\ud83d\udd2e Future Enhancements (M2+)"},{"location":"architecture/07-tools-integration-model/","text":"07 \u2014 Tools Integration Model \u00b6 \ud83e\udded Overview \u00b6 The Tools Integration Model defines how external security tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.) are integrated and orchestrated inside SecFlow . The system is manifest-driven , allowing tools to be defined declaratively, executed in isolation, and chained dynamically via the Workflow Engine. Each integrated tool is represented as: - A manifest (describes binary, configuration, security policies, I/O schema) - A wrapper (implements the standardized execution interface) - An optional adapter plugin (translates output to normalized Findings) \ud83e\uddf1 Architectural Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry<br/>- Manifest Validator<br/>- Sandbox Executor<br/>- Output Parser\"] B[\"Tool Wrapper<br/>(e.g., Nuclei)\"] C[\"Tool Output Parser\"] D[\"Findings Engine\"] A --> B B --> C C --> D \u2699\ufe0f Tool Manifest Structure \u00b6 Every tool integrated into SecFlow must define a manifest under /wrappers/manifests/<tool>.json . Example: nuclei.json \u00b6 { \"name\": \"nuclei\", \"version\": \"3.2.1\", \"binary\": \"nuclei\", \"capabilities\": [\"scan\"], \"outputs\": {\"dataset\": \"findings\"}, \"defaults\": { \"threads\": 25, \"rate_limit\": 150, \"templates\": \"res://templates/nuclei:latest\" }, \"configSchema\": \"schemas/nuclei-config.json\", \"resource_requirements\": { \"cpu_cores\": 2, \"memory_mb\": 512, \"timeout_seconds\": 300 }, \"security\": { \"sandbox\": true, \"network_access\": true, \"file_system_access\": \"read_only\" }, \"selftest\": { \"args\": [\"-version\"], \"expect\": \"Nuclei\" } } \ud83e\udde9 Wrapper Interface \u00b6 All tool wrappers implement the same contract to ensure consistent orchestration. # core-lib/ports/tool_port.py from typing import Any, Dict, Protocol, List from core_lib.models.finding import Finding class ToolPort(Protocol): def prepare(self, config: Dict[str, Any]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run(self) -> str: \"\"\"Execute tool and return raw output.\"\"\" pass def parse_output(self, raw_output: str) -> List[Finding]: \"\"\"Parse raw output into structured findings.\"\"\" pass \ud83e\udde0 Example Implementation: Nuclei Wrapper \u00b6 # wrappers/nuclei_wrapper.py import subprocess, json from core_lib.models.finding import Finding from core_lib.ports.tool_port import ToolPort class NucleiWrapper(ToolPort): def __init__(self, config): self.config = config def prepare(self, config): self.args = [ \"nuclei\", \"-json\", \"-t\", config.get(\"templates\", \"res://templates/nuclei:latest\"), \"-rl\", str(config.get(\"rate_limit\", 150)), \"-c\", str(config.get(\"threads\", 25)) ] def run(self): result = subprocess.run(self.args, capture_output=True, text=True, timeout=300) return result.stdout def parse_output(self, raw_output): findings = [] for line in raw_output.splitlines(): try: data = json.loads(line) findings.append( Finding( title=data[\"info\"][\"name\"], severity=data[\"info\"][\"severity\"], path=data[\"matched-at\"], detector_id=\"nuclei\", evidence=data ) ) except Exception: continue return findings \ud83d\udd10 Sandbox Execution \u00b6 Tools run through a Sandbox Executor, enforcing CPU, memory, and network constraints. # wrappers/executor.py import resource, subprocess, signal class SandboxExecutor: def __init__(self, cpu_limit=2, mem_limit_mb=512): self.cpu_limit = cpu_limit self.mem_limit_mb = mem_limit_mb def execute(self, args): def set_limits(): resource.setrlimit(resource.RLIMIT_CPU, (self.cpu_limit, self.cpu_limit)) resource.setrlimit(resource.RLIMIT_AS, (self.mem_limit_mb * 1024**2,) * 2) proc = subprocess.Popen(args, preexec_fn=set_limits, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = proc.communicate(timeout=300) return stdout.decode(), stderr.decode(), proc.returncode \ud83e\udde9 Tool Registry \u00b6 # wrappers/registry.py from typing import Dict, Type from core_lib.ports.tool_port import ToolPort class ToolRegistry: _registry: Dict[str, Type[ToolPort]] = {} @classmethod def register(cls, name: str, impl: Type[ToolPort]): cls._registry[name] = impl print(f\"[+] Registered tool: {name}\") @classmethod def get(cls, name: str) -> ToolPort: return cls._registry[name] Tools register via decorators or discovery: from wrappers.registry import ToolRegistry @ToolRegistry.register(\"feroxbuster\") class FeroxWrapper(ToolPort): def prepare(self, config: Dict[str, Any]) -> None: self.wordlist = config.get(\"wordlist\", \"res://wordlists/dirb:latest\") self.threads = config.get(\"threads\", 50) def run(self) -> str: cmd = [\"feroxbuster\", \"-u\", self.target, \"-w\", self.wordlist, \"-t\", str(self.threads)] result = subprocess.run(cmd, capture_output=True, text=True) return result.stdout def parse_output(self, raw_output: str) -> List[Finding]: findings = [] for line in raw_output.split('\\n'): if line.strip() and not line.startswith('#'): findings.append(Finding( id=f\"ferox_{hash(line)}\", url=line.strip(), tool=\"feroxbuster\", severity=\"info\" )) return findings \ud83e\udde0 Example Integration \u2014 Feroxbuster \u00b6 # wrappers/ferox_wrapper.py import subprocess from core_lib.models.finding import Finding class FeroxWrapper(ToolPort): def prepare(self, config): self.target = config[\"target\"] self.wordlist = config.get(\"wordlist\", \"/usr/share/wordlists/dirb/common.txt\") self.args = [\"feroxbuster\", \"-u\", self.target, \"-w\", self.wordlist, \"-o\", \"-\"] def run(self): result = subprocess.run(self.args, capture_output=True, text=True) return result.stdout def parse_output(self, raw): findings = [] for line in raw.splitlines(): if \"200\" in line or \"301\" in line: findings.append( Finding( title=\"Discovered Path\", severity=\"info\", path=line.split()[0], detector_id=\"feroxbuster\" ) ) return findings \ud83e\udde9 Tool Orchestration \u00b6 The Workflow Engine dynamically chains tool executions: - Each node defines a tool (by name), configuration, and expected outputs. - Outputs become inputs for subsequent nodes. Example: \u00b6 nodes: - id: discovery type: discovery.ferox config: wordlist: res://wordlists/dirb:latest outputs: [\"urls\"] - id: scan type: scan.nuclei inputs: [\"urls\"] outputs: [\"findings\"] \ud83e\udde0 Tool Output Normalization \u00b6 All tool outputs are normalized into the Finding schema before storage. Field Description detector_id Tool name (e.g., nuclei, zap) title Human-readable description path Target or endpoint severity Normalized (critical/high/medium/low/info) evidence Raw tool output created_at UTC timestamp provenance Manifest + execution context \ud83e\udde9 Error Handling \u00b6 Condition Behavior Tool binary missing Raise ToolNotFoundError Sandbox violation Terminate subprocess Timeout Abort execution, mark partial-success Parse failure Skip line, log warning Non-zero exit code Log and attach stderr to run context \ud83d\udd2e Future Extensions \u00b6 Interactive tool chaining (UI-based pipeline builder) Tool profiles for project-specific setups Remote execution agents (distributed scanning) Version-aware compatibility checks Auto-updating manifests (self-test validation) Next: Tool Manager & User Experience Design","title":"Tools Integration"},{"location":"architecture/07-tools-integration-model/#07-tools-integration-model","text":"","title":"07 \u2014 Tools Integration Model"},{"location":"architecture/07-tools-integration-model/#overview","text":"The Tools Integration Model defines how external security tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.) are integrated and orchestrated inside SecFlow . The system is manifest-driven , allowing tools to be defined declaratively, executed in isolation, and chained dynamically via the Workflow Engine. Each integrated tool is represented as: - A manifest (describes binary, configuration, security policies, I/O schema) - A wrapper (implements the standardized execution interface) - An optional adapter plugin (translates output to normalized Findings)","title":"\ud83e\udded Overview"},{"location":"architecture/07-tools-integration-model/#architectural-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry<br/>- Manifest Validator<br/>- Sandbox Executor<br/>- Output Parser\"] B[\"Tool Wrapper<br/>(e.g., Nuclei)\"] C[\"Tool Output Parser\"] D[\"Findings Engine\"] A --> B B --> C C --> D","title":"\ud83e\uddf1 Architectural Overview"},{"location":"architecture/07-tools-integration-model/#tool-manifest-structure","text":"Every tool integrated into SecFlow must define a manifest under /wrappers/manifests/<tool>.json .","title":"\u2699\ufe0f Tool Manifest Structure"},{"location":"architecture/07-tools-integration-model/#example-nucleijson","text":"{ \"name\": \"nuclei\", \"version\": \"3.2.1\", \"binary\": \"nuclei\", \"capabilities\": [\"scan\"], \"outputs\": {\"dataset\": \"findings\"}, \"defaults\": { \"threads\": 25, \"rate_limit\": 150, \"templates\": \"res://templates/nuclei:latest\" }, \"configSchema\": \"schemas/nuclei-config.json\", \"resource_requirements\": { \"cpu_cores\": 2, \"memory_mb\": 512, \"timeout_seconds\": 300 }, \"security\": { \"sandbox\": true, \"network_access\": true, \"file_system_access\": \"read_only\" }, \"selftest\": { \"args\": [\"-version\"], \"expect\": \"Nuclei\" } }","title":"Example: nuclei.json"},{"location":"architecture/07-tools-integration-model/#wrapper-interface","text":"All tool wrappers implement the same contract to ensure consistent orchestration. # core-lib/ports/tool_port.py from typing import Any, Dict, Protocol, List from core_lib.models.finding import Finding class ToolPort(Protocol): def prepare(self, config: Dict[str, Any]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run(self) -> str: \"\"\"Execute tool and return raw output.\"\"\" pass def parse_output(self, raw_output: str) -> List[Finding]: \"\"\"Parse raw output into structured findings.\"\"\" pass","title":"\ud83e\udde9 Wrapper Interface"},{"location":"architecture/07-tools-integration-model/#example-implementation-nuclei-wrapper","text":"# wrappers/nuclei_wrapper.py import subprocess, json from core_lib.models.finding import Finding from core_lib.ports.tool_port import ToolPort class NucleiWrapper(ToolPort): def __init__(self, config): self.config = config def prepare(self, config): self.args = [ \"nuclei\", \"-json\", \"-t\", config.get(\"templates\", \"res://templates/nuclei:latest\"), \"-rl\", str(config.get(\"rate_limit\", 150)), \"-c\", str(config.get(\"threads\", 25)) ] def run(self): result = subprocess.run(self.args, capture_output=True, text=True, timeout=300) return result.stdout def parse_output(self, raw_output): findings = [] for line in raw_output.splitlines(): try: data = json.loads(line) findings.append( Finding( title=data[\"info\"][\"name\"], severity=data[\"info\"][\"severity\"], path=data[\"matched-at\"], detector_id=\"nuclei\", evidence=data ) ) except Exception: continue return findings","title":"\ud83e\udde0 Example Implementation: Nuclei Wrapper"},{"location":"architecture/07-tools-integration-model/#sandbox-execution","text":"Tools run through a Sandbox Executor, enforcing CPU, memory, and network constraints. # wrappers/executor.py import resource, subprocess, signal class SandboxExecutor: def __init__(self, cpu_limit=2, mem_limit_mb=512): self.cpu_limit = cpu_limit self.mem_limit_mb = mem_limit_mb def execute(self, args): def set_limits(): resource.setrlimit(resource.RLIMIT_CPU, (self.cpu_limit, self.cpu_limit)) resource.setrlimit(resource.RLIMIT_AS, (self.mem_limit_mb * 1024**2,) * 2) proc = subprocess.Popen(args, preexec_fn=set_limits, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = proc.communicate(timeout=300) return stdout.decode(), stderr.decode(), proc.returncode","title":"\ud83d\udd10 Sandbox Execution"},{"location":"architecture/07-tools-integration-model/#tool-registry","text":"# wrappers/registry.py from typing import Dict, Type from core_lib.ports.tool_port import ToolPort class ToolRegistry: _registry: Dict[str, Type[ToolPort]] = {} @classmethod def register(cls, name: str, impl: Type[ToolPort]): cls._registry[name] = impl print(f\"[+] Registered tool: {name}\") @classmethod def get(cls, name: str) -> ToolPort: return cls._registry[name] Tools register via decorators or discovery: from wrappers.registry import ToolRegistry @ToolRegistry.register(\"feroxbuster\") class FeroxWrapper(ToolPort): def prepare(self, config: Dict[str, Any]) -> None: self.wordlist = config.get(\"wordlist\", \"res://wordlists/dirb:latest\") self.threads = config.get(\"threads\", 50) def run(self) -> str: cmd = [\"feroxbuster\", \"-u\", self.target, \"-w\", self.wordlist, \"-t\", str(self.threads)] result = subprocess.run(cmd, capture_output=True, text=True) return result.stdout def parse_output(self, raw_output: str) -> List[Finding]: findings = [] for line in raw_output.split('\\n'): if line.strip() and not line.startswith('#'): findings.append(Finding( id=f\"ferox_{hash(line)}\", url=line.strip(), tool=\"feroxbuster\", severity=\"info\" )) return findings","title":"\ud83e\udde9 Tool Registry"},{"location":"architecture/07-tools-integration-model/#example-integration-feroxbuster","text":"# wrappers/ferox_wrapper.py import subprocess from core_lib.models.finding import Finding class FeroxWrapper(ToolPort): def prepare(self, config): self.target = config[\"target\"] self.wordlist = config.get(\"wordlist\", \"/usr/share/wordlists/dirb/common.txt\") self.args = [\"feroxbuster\", \"-u\", self.target, \"-w\", self.wordlist, \"-o\", \"-\"] def run(self): result = subprocess.run(self.args, capture_output=True, text=True) return result.stdout def parse_output(self, raw): findings = [] for line in raw.splitlines(): if \"200\" in line or \"301\" in line: findings.append( Finding( title=\"Discovered Path\", severity=\"info\", path=line.split()[0], detector_id=\"feroxbuster\" ) ) return findings","title":"\ud83e\udde0 Example Integration \u2014 Feroxbuster"},{"location":"architecture/07-tools-integration-model/#tool-orchestration","text":"The Workflow Engine dynamically chains tool executions: - Each node defines a tool (by name), configuration, and expected outputs. - Outputs become inputs for subsequent nodes.","title":"\ud83e\udde9 Tool Orchestration"},{"location":"architecture/07-tools-integration-model/#example","text":"nodes: - id: discovery type: discovery.ferox config: wordlist: res://wordlists/dirb:latest outputs: [\"urls\"] - id: scan type: scan.nuclei inputs: [\"urls\"] outputs: [\"findings\"]","title":"Example:"},{"location":"architecture/07-tools-integration-model/#tool-output-normalization","text":"All tool outputs are normalized into the Finding schema before storage. Field Description detector_id Tool name (e.g., nuclei, zap) title Human-readable description path Target or endpoint severity Normalized (critical/high/medium/low/info) evidence Raw tool output created_at UTC timestamp provenance Manifest + execution context","title":"\ud83e\udde0 Tool Output Normalization"},{"location":"architecture/07-tools-integration-model/#error-handling","text":"Condition Behavior Tool binary missing Raise ToolNotFoundError Sandbox violation Terminate subprocess Timeout Abort execution, mark partial-success Parse failure Skip line, log warning Non-zero exit code Log and attach stderr to run context","title":"\ud83e\udde9 Error Handling"},{"location":"architecture/07-tools-integration-model/#future-extensions","text":"Interactive tool chaining (UI-based pipeline builder) Tool profiles for project-specific setups Remote execution agents (distributed scanning) Version-aware compatibility checks Auto-updating manifests (self-test validation) Next: Tool Manager & User Experience Design","title":"\ud83d\udd2e Future Extensions"},{"location":"architecture/08-tool-manager-and-ux-design/","text":"08 \u2014 Tool Manager & User Experience Design \u00b6 \ud83e\udded Overview \u00b6 The Tool Manager serves as the central orchestration and configuration interface for all integrated tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.). It acts as the human-facing control plane that bridges declarative configuration, project-level defaults, and the automated orchestration engine. The UX design ensures that both CLI users and UI users can: - Install or enable tools dynamically. - Configure tool parameters or use defaults. - Chain tools together (e.g., Ferox \u2192 Katana \u2192 Nuclei ). - Reuse shared wordlists, payloads, and templates. - View and modify execution pipelines visually or textually. \ud83e\udde9 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry (runtime manifest cache)<br/>- Config Editor (CLI + UI layer)<br/>- Execution Controller (worker interface)<br/>- Output Collector (findings normalization)\"] B[\"Tool Wrappers Layer\"] C[\"Findings Engine\"] A --> B B --> C \u2699\ufe0f Tool Manager Responsibilities \u00b6 Subsystem Responsibility Registry Keeps in-memory registry of available tools with metadata. Configurator Provides unified interface for tool configuration & validation. Executor Handles actual subprocess calls via SandboxExecutor . Cache Layer Stores tool state, results, and logs for replay. Renderer Generates UI/CLI views for configuration and results. \ud83e\uddf1 UX Goals \u00b6 Goal Description Low Friction User can run default scans without prior configuration. Transparency Full visibility into command-line arguments, logs, and results. Reusability Configurations and results can be reused across projects. Isolation Each project may have independent or shared tool setups. Extensibility Easy to add new tools through manifests. \ud83e\udde0 CLI Experience \u00b6 Example Commands \u00b6 # List available tools SecFlow tools list # Enable Feroxbuster & Nuclei SecFlow tools enable feroxbuster nuclei # Configure a tool SecFlow tools config nuclei --threads 50 --templates owasp-top10 # Run workflow interactively SecFlow run workflow.yaml # Chain tools manually SecFlow run feroxbuster --target https://example.com | SecFlow run nuclei CLI UX Principles \u00b6 YAML/JSON inputs mirror internal manifests. All commands are idempotent \u2014 re-runs use cached configurations. CLI supports both single-tool and multi-node workflow modes. \ud83e\udde0 Web UI Experience \u00b6 Visual Overview \u00b6 Project: Acme Web API %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project: Acme Web API\"] B[\"Tools Panel\"] C[\"Workflow Builder\"] D[\"Execution Log\"] E[\"Results\"] A --> B A --> C A --> D A --> E Key Panels \u00b6 Panel Description Tools Panel Shows installed tools, version, manifest, health check. Workflow Builder Drag-and-drop DAG composer (Ferox \u2192 Katana \u2192 Nuclei). Execution Log Real-time stdout/stderr stream with structured logging. Results Viewer Interactive findings dashboard, linked to triage view. Example Workflow Builder Nodes \u00b6 (Discovery) (Crawler) (Scanner) %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Katana\"] C[\"Nuclei\"] A --> B B --> C Users can: - Configure node properties via sidebar. - Save and export workflows as YAML ( workflow-recipe.yaml ). - Re-run saved workflows directly or modify them visually. \ud83e\udde9 Configuration Persistence \u00b6 Each tool has a persistent JSON/YAML config stored under: ~/.SecFlow/config/tools/<tool>.yaml Example \u00b6 version: \"1.0\" defaults: threads: 25 rate_limit: 150 templates: res://templates/nuclei:latest profiles: aggressive: rate_limit: 300 safe: rate_limit: 50 sandbox: true Profiles allow contextual configurations (e.g., \"aggressive\", \"safe\"). \ud83d\udd17 Tool Chaining (Runtime) \u00b6 Tool chaining uses the Workflow Engine, but users can also trigger ad-hoc sequences. Each output dataset is normalized as JSON Lines, allowing flexible piping. SecFlow run feroxbuster --target https://app.local \\ | SecFlow run katana --stdin \\ | SecFlow run nuclei --stdin Data interoperability: \u00b6 Each tool must produce output in a structured JSON format with required fields: { \"url\": \"https://example.com/login\", \"status\": 200, \"source\": \"feroxbuster\" } \ud83e\udde9 Default Configurations & Templates \u00b6 When the user installs SecFlow: - Default manifests are loaded from /resources/defaults/ - All core tools (Nuclei, Ferox, Katana) have pre-configured templates. - The user can immediately run scans without extra setup. Example command: \u00b6 SecFlow quickscan https://example.com Equivalent to: - Workflow: Ferox \u2192 Nuclei (OWASP templates) \ud83e\udde0 Advanced Features \u00b6 Feature Description Auto-Discovery Detects installed binaries and populates manifest registry automatically. Tool Self-Test Validates binary presence and functionality via manifest-defined tests. Runtime Profiles Switch between safe/aggressive scanning modes. Execution History Stores past runs and parameters for reproducibility. \ud83d\udd10 User Permissions \u00b6 Tool Manager respects project and role isolation: Role Capabilities Admin Install, configure, delete tools. Analyst Execute workflows, view logs, triage findings. Viewer Read-only access to results. All actions are logged to the Audit Log. \ud83e\uddf1 Integration with Resource Registry \u00b6 The Tool Manager automatically pulls shared wordlists, templates, or payloads from the Resource Registry (see 09-resource-registry.md ). Example: \u00b6 wordlist: res://wordlists/dirb:latest templates: res://templates/nuclei:latest \ud83e\udde9 Error Recovery UX \u00b6 If a tool fails: - The UI highlights failed nodes in red. - Logs show stderr output inline. - Users can retry individual nodes. - Failures generate a triage entry (finding type = \"tool error\"). \ud83e\udde0 Example CLI Session \u00b6 # Add Ferox with a project-local override wordlist secflow tools add ferox \\ --version 2.10.0 \\ --scope project \\ --config '{ \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64 }' # Run a two-node workflow directly from CLI (discovery \u2192 nuclei) secflow run \\ --project acme-web \\ --recipe workflows/discovery-to-nuclei.yaml \\ --var target=https://app.acme.com API Response Example \u00b6 { \"tool\": \"feroxbuster\", \"version\": \"2.10.0\", \"scope\": \"project\", \"effective_config\": { \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64, \"rate_limit\": 0 }, \"status\": \"installed\", \"selftest\": { \"ok\": true, \"elapsed_ms\": 143 } } \ud83d\udd2e Future Enhancements \u00b6 UI-based \"Workflow Marketplace\" for community templates. AI-assisted tool parameter tuning based on context. Live terminal dashboard with interactive progress visualization. Integration with Burp/OWASP ZAP APIs for direct import. Next: Resource Registry","title":"Tool Manager & UX"},{"location":"architecture/08-tool-manager-and-ux-design/#08-tool-manager-user-experience-design","text":"","title":"08 \u2014 Tool Manager &amp; User Experience Design"},{"location":"architecture/08-tool-manager-and-ux-design/#overview","text":"The Tool Manager serves as the central orchestration and configuration interface for all integrated tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.). It acts as the human-facing control plane that bridges declarative configuration, project-level defaults, and the automated orchestration engine. The UX design ensures that both CLI users and UI users can: - Install or enable tools dynamically. - Configure tool parameters or use defaults. - Chain tools together (e.g., Ferox \u2192 Katana \u2192 Nuclei ). - Reuse shared wordlists, payloads, and templates. - View and modify execution pipelines visually or textually.","title":"\ud83e\udded Overview"},{"location":"architecture/08-tool-manager-and-ux-design/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry (runtime manifest cache)<br/>- Config Editor (CLI + UI layer)<br/>- Execution Controller (worker interface)<br/>- Output Collector (findings normalization)\"] B[\"Tool Wrappers Layer\"] C[\"Findings Engine\"] A --> B B --> C","title":"\ud83e\udde9 Architecture Overview"},{"location":"architecture/08-tool-manager-and-ux-design/#tool-manager-responsibilities","text":"Subsystem Responsibility Registry Keeps in-memory registry of available tools with metadata. Configurator Provides unified interface for tool configuration & validation. Executor Handles actual subprocess calls via SandboxExecutor . Cache Layer Stores tool state, results, and logs for replay. Renderer Generates UI/CLI views for configuration and results.","title":"\u2699\ufe0f Tool Manager Responsibilities"},{"location":"architecture/08-tool-manager-and-ux-design/#ux-goals","text":"Goal Description Low Friction User can run default scans without prior configuration. Transparency Full visibility into command-line arguments, logs, and results. Reusability Configurations and results can be reused across projects. Isolation Each project may have independent or shared tool setups. Extensibility Easy to add new tools through manifests.","title":"\ud83e\uddf1 UX Goals"},{"location":"architecture/08-tool-manager-and-ux-design/#cli-experience","text":"","title":"\ud83e\udde0 CLI Experience"},{"location":"architecture/08-tool-manager-and-ux-design/#example-commands","text":"# List available tools SecFlow tools list # Enable Feroxbuster & Nuclei SecFlow tools enable feroxbuster nuclei # Configure a tool SecFlow tools config nuclei --threads 50 --templates owasp-top10 # Run workflow interactively SecFlow run workflow.yaml # Chain tools manually SecFlow run feroxbuster --target https://example.com | SecFlow run nuclei","title":"Example Commands"},{"location":"architecture/08-tool-manager-and-ux-design/#cli-ux-principles","text":"YAML/JSON inputs mirror internal manifests. All commands are idempotent \u2014 re-runs use cached configurations. CLI supports both single-tool and multi-node workflow modes.","title":"CLI UX Principles"},{"location":"architecture/08-tool-manager-and-ux-design/#web-ui-experience","text":"","title":"\ud83e\udde0 Web UI Experience"},{"location":"architecture/08-tool-manager-and-ux-design/#visual-overview","text":"Project: Acme Web API %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project: Acme Web API\"] B[\"Tools Panel\"] C[\"Workflow Builder\"] D[\"Execution Log\"] E[\"Results\"] A --> B A --> C A --> D A --> E","title":"Visual Overview"},{"location":"architecture/08-tool-manager-and-ux-design/#key-panels","text":"Panel Description Tools Panel Shows installed tools, version, manifest, health check. Workflow Builder Drag-and-drop DAG composer (Ferox \u2192 Katana \u2192 Nuclei). Execution Log Real-time stdout/stderr stream with structured logging. Results Viewer Interactive findings dashboard, linked to triage view.","title":"Key Panels"},{"location":"architecture/08-tool-manager-and-ux-design/#example-workflow-builder-nodes","text":"(Discovery) (Crawler) (Scanner) %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Katana\"] C[\"Nuclei\"] A --> B B --> C Users can: - Configure node properties via sidebar. - Save and export workflows as YAML ( workflow-recipe.yaml ). - Re-run saved workflows directly or modify them visually.","title":"Example Workflow Builder Nodes"},{"location":"architecture/08-tool-manager-and-ux-design/#configuration-persistence","text":"Each tool has a persistent JSON/YAML config stored under: ~/.SecFlow/config/tools/<tool>.yaml","title":"\ud83e\udde9 Configuration Persistence"},{"location":"architecture/08-tool-manager-and-ux-design/#example","text":"version: \"1.0\" defaults: threads: 25 rate_limit: 150 templates: res://templates/nuclei:latest profiles: aggressive: rate_limit: 300 safe: rate_limit: 50 sandbox: true Profiles allow contextual configurations (e.g., \"aggressive\", \"safe\").","title":"Example"},{"location":"architecture/08-tool-manager-and-ux-design/#tool-chaining-runtime","text":"Tool chaining uses the Workflow Engine, but users can also trigger ad-hoc sequences. Each output dataset is normalized as JSON Lines, allowing flexible piping. SecFlow run feroxbuster --target https://app.local \\ | SecFlow run katana --stdin \\ | SecFlow run nuclei --stdin","title":"\ud83d\udd17 Tool Chaining (Runtime)"},{"location":"architecture/08-tool-manager-and-ux-design/#data-interoperability","text":"Each tool must produce output in a structured JSON format with required fields: { \"url\": \"https://example.com/login\", \"status\": 200, \"source\": \"feroxbuster\" }","title":"Data interoperability:"},{"location":"architecture/08-tool-manager-and-ux-design/#default-configurations-templates","text":"When the user installs SecFlow: - Default manifests are loaded from /resources/defaults/ - All core tools (Nuclei, Ferox, Katana) have pre-configured templates. - The user can immediately run scans without extra setup.","title":"\ud83e\udde9 Default Configurations &amp; Templates"},{"location":"architecture/08-tool-manager-and-ux-design/#example-command","text":"SecFlow quickscan https://example.com Equivalent to: - Workflow: Ferox \u2192 Nuclei (OWASP templates)","title":"Example command:"},{"location":"architecture/08-tool-manager-and-ux-design/#advanced-features","text":"Feature Description Auto-Discovery Detects installed binaries and populates manifest registry automatically. Tool Self-Test Validates binary presence and functionality via manifest-defined tests. Runtime Profiles Switch between safe/aggressive scanning modes. Execution History Stores past runs and parameters for reproducibility.","title":"\ud83e\udde0 Advanced Features"},{"location":"architecture/08-tool-manager-and-ux-design/#user-permissions","text":"Tool Manager respects project and role isolation: Role Capabilities Admin Install, configure, delete tools. Analyst Execute workflows, view logs, triage findings. Viewer Read-only access to results. All actions are logged to the Audit Log.","title":"\ud83d\udd10 User Permissions"},{"location":"architecture/08-tool-manager-and-ux-design/#integration-with-resource-registry","text":"The Tool Manager automatically pulls shared wordlists, templates, or payloads from the Resource Registry (see 09-resource-registry.md ).","title":"\ud83e\uddf1 Integration with Resource Registry"},{"location":"architecture/08-tool-manager-and-ux-design/#example_1","text":"wordlist: res://wordlists/dirb:latest templates: res://templates/nuclei:latest","title":"Example:"},{"location":"architecture/08-tool-manager-and-ux-design/#error-recovery-ux","text":"If a tool fails: - The UI highlights failed nodes in red. - Logs show stderr output inline. - Users can retry individual nodes. - Failures generate a triage entry (finding type = \"tool error\").","title":"\ud83e\udde9 Error Recovery UX"},{"location":"architecture/08-tool-manager-and-ux-design/#example-cli-session","text":"# Add Ferox with a project-local override wordlist secflow tools add ferox \\ --version 2.10.0 \\ --scope project \\ --config '{ \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64 }' # Run a two-node workflow directly from CLI (discovery \u2192 nuclei) secflow run \\ --project acme-web \\ --recipe workflows/discovery-to-nuclei.yaml \\ --var target=https://app.acme.com","title":"\ud83e\udde0 Example CLI Session"},{"location":"architecture/08-tool-manager-and-ux-design/#api-response-example","text":"{ \"tool\": \"feroxbuster\", \"version\": \"2.10.0\", \"scope\": \"project\", \"effective_config\": { \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64, \"rate_limit\": 0 }, \"status\": \"installed\", \"selftest\": { \"ok\": true, \"elapsed_ms\": 143 } }","title":"API Response Example"},{"location":"architecture/08-tool-manager-and-ux-design/#future-enhancements","text":"UI-based \"Workflow Marketplace\" for community templates. AI-assisted tool parameter tuning based on context. Live terminal dashboard with interactive progress visualization. Integration with Burp/OWASP ZAP APIs for direct import. Next: Resource Registry","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/09-resource-registry/","text":"09 \u2014 Resource Registry \u00b6 \ud83e\udded Overview \u00b6 The Resource Registry is SecFlow's shared data layer for managing reusable assets such as: Wordlists Templates (Nuclei, ZAP, custom YAML) Headers and payload sets Configurations and schema files CVE/CWE enrichment databases This registry provides unified access to all resources, manages versioning and scope isolation, and acts as the single source of truth for both CLI and API workflows. \ud83e\uddf1 Design Goals \u00b6 Goal Description Centralized One consistent repository for all reusable data. Scoped Resources may be global, group-specific, or project-specific. Versioned Immutable artifacts with hash-based versioning. Queryable Searchable by tags, type, or metadata. Cacheable Local caching for fast re-use and offline execution. Auditable Every modification is logged in the audit trail. \ud83e\udde9 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource API<br/>- Resource Manager<br/>- Indexer / Search Engine<br/>- Version Resolver<br/>- Storage Adapter (File, DB, Remote)\"] B[\"Resource Blob\"] C[\"Local Cache\"] A --> B B --> C \u2699\ufe0f Resource Model \u00b6 # core-lib/models/resource.py from typing import Literal, Optional from datetime import datetime from pydantic import BaseModel class Resource(BaseModel): id: str name: str type: Literal[\"wordlist\", \"templates\", \"headers\", \"payloads\", \"configs\", \"schema\"] version: str hash: str scope: Literal[\"global\", \"group\", \"project\"] owner: Optional[str] blob_uri: str metadata: dict created_at: datetime updated_at: datetime usage_count: int last_used: Optional[datetime] \ud83e\udde9 Scope Hierarchy \u00b6 Level Description Example Path Global Available across all users and projects. res://wordlists/common.txt Group Shared among specific teams. res://group/redteam/headers.json Project Private to a specific pentest or client project. res://project/acme/templates/custom.yaml Precedence Rules \u00b6 Run-level override Node-level override Project default Group default Global default Example: \u00b6 If project-A has a custom wordlist res://project/acme/dirb.yaml , it overrides res://wordlists/dirb:latest . \ud83e\udde9 Example Registry Entry \u00b6 id: \"res://wordlists/dirb:1.2.0\" name: \"Dirbuster Common\" type: \"wordlist\" version: \"1.2.0\" hash: \"sha256:ea13f3b4c8d9e2f1a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" scope: \"global\" metadata: tags: [\"web\", \"discovery\"] size: 24312 license: \"GPL-3.0\" source: \"https://github.com/digination/dirbuster\" created_at: \"2025-08-01T12:32:00Z\" updated_at: \"2025-08-15T09:21:00Z\" \ud83e\uddf1 Storage Backends \u00b6 The Resource Registry supports multiple backends: Backend Usage Notes Local Filesystem Default for developer use. Fast, no dependencies. SQLite / Postgres Production database. Metadata stored in DB, blob on disk. S3 / MinIO Remote multi-tenant storage. Ideal for distributed environments. Git-backed Repository Version-controlled registry. Enables audit and rollback. \ud83e\udde9 Resource Manager Interface \u00b6 # core-lib/ports/resource_port.py from typing import List from .models import Resource class ResourcePort(Protocol): def register(self, resource: Resource) -> None: \"\"\"Register a new resource in the registry.\"\"\" pass def list(self, scope: str) -> List[Resource]: \"\"\"List resources within a scope.\"\"\" pass def get(self, id: str) -> Resource: \"\"\"Get a resource by ID.\"\"\" pass def resolve(self, name: str, version: str) -> Resource: \"\"\"Resolve a resource by name and version.\"\"\" pass def increment_usage(self, id: str) -> None: \"\"\"Increment usage counter for a resource.\"\"\" pass Example Adapter Implementation \u00b6 # storage/resource_repository.py import json, os from core_lib.models.resource import Resource class FileResourceRepo: def __init__(self, base_path=\"~/.SecFlow/resources\"): self.base = os.path.expanduser(base_path) def register(self, res: Resource): path = os.path.join(self.base, f\"{res.id.replace('res://','')}.json\") os.makedirs(os.path.dirname(path), exist_ok=True) with open(path, \"w\") as f: f.write(res.model_dump_json()) def get(self, id): path = os.path.join(self.base, f\"{id.replace('res://','')}.json\") return Resource.parse_file(path) \ud83e\udde9 Resource Fetching & Caching \u00b6 When a workflow references res://wordlists/dirb:latest : The registry resolves the resource (global/group/project). The manager checks the local cache. If missing or outdated, it downloads or loads the file blob. The reference is then injected into the tool configuration. resolved = ResourceManager.resolve(\"res://wordlists/dirb:latest\") path = CacheManager.fetch(resolved) \ud83e\udde0 Resource Versioning \u00b6 Resources are immutable once published; updates produce new versions. Operation Behavior publish Adds new resource version, preserves old one. retire Marks resource as deprecated (kept for history). promote Moves a project resource to group/global scope. Version Reference Syntax \u00b6 res://wordlists/dirb:1.2.0 res://templates/nuclei:latest \ud83e\udde9 Registry CLI Commands \u00b6 # List all resources SecFlow resources list # Show details for a resource SecFlow resources show res://wordlists/dirb:latest # Add new resource SecFlow resources add wordlist ./custom.txt --scope project # Promote resource SecFlow resources promote res://project/acme/dirb:latest --to global \ud83e\udde0 Integration with Tool Manager \u00b6 Tools reference resources via symbolic URIs. At runtime, the registry automatically resolves URIs into local paths. Example Nuclei config: \u00b6 templates: res://templates/nuclei:latest wordlist: res://wordlists/dirb:latest \ud83d\udd12 Security & Integrity \u00b6 Mechanism Description Hash Validation SHA-256 hashes validated before use. Signed Resources Optional GPG signing for remote sources. Access Control Scoped permissions (Admin / Analyst / Viewer). Audit Trail Every fetch and publish event logged. \ud83e\udde9 Garbage Collection Policy \u00b6 When a resource is deleted: - Orphaned blob files are queued for cleanup by the GarbageCollector . - The audit log retains metadata for traceability. - Cached copies remain valid until TTL expiration. \ud83e\udde0 Example Use Case \u00b6 Global Wordlist Shared by Tools \u00b6 All tools can reference res://wordlists/dirb:latest : - Feroxbuster uses it for directory brute-force. - Katana can reuse it for crawl hints. - Nuclei leverages it for parameter fuzzing. Each tool can still override with its own wordlist if required. \ud83d\udd2e Future Enhancements \u00b6 Remote Resource Index API (REST/GraphQL). Smart caching with usage-based prefetch. Resource metrics: \"most used\", \"last updated\", etc. Tag-based search and semantic discovery. Signed update feeds from verified community sources. Next: Wordlist & Output Sharing Rules","title":"Resource Registry"},{"location":"architecture/09-resource-registry/#09-resource-registry","text":"","title":"09 \u2014 Resource Registry"},{"location":"architecture/09-resource-registry/#overview","text":"The Resource Registry is SecFlow's shared data layer for managing reusable assets such as: Wordlists Templates (Nuclei, ZAP, custom YAML) Headers and payload sets Configurations and schema files CVE/CWE enrichment databases This registry provides unified access to all resources, manages versioning and scope isolation, and acts as the single source of truth for both CLI and API workflows.","title":"\ud83e\udded Overview"},{"location":"architecture/09-resource-registry/#design-goals","text":"Goal Description Centralized One consistent repository for all reusable data. Scoped Resources may be global, group-specific, or project-specific. Versioned Immutable artifacts with hash-based versioning. Queryable Searchable by tags, type, or metadata. Cacheable Local caching for fast re-use and offline execution. Auditable Every modification is logged in the audit trail.","title":"\ud83e\uddf1 Design Goals"},{"location":"architecture/09-resource-registry/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource API<br/>- Resource Manager<br/>- Indexer / Search Engine<br/>- Version Resolver<br/>- Storage Adapter (File, DB, Remote)\"] B[\"Resource Blob\"] C[\"Local Cache\"] A --> B B --> C","title":"\ud83e\udde9 Architecture Overview"},{"location":"architecture/09-resource-registry/#resource-model","text":"# core-lib/models/resource.py from typing import Literal, Optional from datetime import datetime from pydantic import BaseModel class Resource(BaseModel): id: str name: str type: Literal[\"wordlist\", \"templates\", \"headers\", \"payloads\", \"configs\", \"schema\"] version: str hash: str scope: Literal[\"global\", \"group\", \"project\"] owner: Optional[str] blob_uri: str metadata: dict created_at: datetime updated_at: datetime usage_count: int last_used: Optional[datetime]","title":"\u2699\ufe0f Resource Model"},{"location":"architecture/09-resource-registry/#scope-hierarchy","text":"Level Description Example Path Global Available across all users and projects. res://wordlists/common.txt Group Shared among specific teams. res://group/redteam/headers.json Project Private to a specific pentest or client project. res://project/acme/templates/custom.yaml","title":"\ud83e\udde9 Scope Hierarchy"},{"location":"architecture/09-resource-registry/#precedence-rules","text":"Run-level override Node-level override Project default Group default Global default","title":"Precedence Rules"},{"location":"architecture/09-resource-registry/#example","text":"If project-A has a custom wordlist res://project/acme/dirb.yaml , it overrides res://wordlists/dirb:latest .","title":"Example:"},{"location":"architecture/09-resource-registry/#example-registry-entry","text":"id: \"res://wordlists/dirb:1.2.0\" name: \"Dirbuster Common\" type: \"wordlist\" version: \"1.2.0\" hash: \"sha256:ea13f3b4c8d9e2f1a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" scope: \"global\" metadata: tags: [\"web\", \"discovery\"] size: 24312 license: \"GPL-3.0\" source: \"https://github.com/digination/dirbuster\" created_at: \"2025-08-01T12:32:00Z\" updated_at: \"2025-08-15T09:21:00Z\"","title":"\ud83e\udde9 Example Registry Entry"},{"location":"architecture/09-resource-registry/#storage-backends","text":"The Resource Registry supports multiple backends: Backend Usage Notes Local Filesystem Default for developer use. Fast, no dependencies. SQLite / Postgres Production database. Metadata stored in DB, blob on disk. S3 / MinIO Remote multi-tenant storage. Ideal for distributed environments. Git-backed Repository Version-controlled registry. Enables audit and rollback.","title":"\ud83e\uddf1 Storage Backends"},{"location":"architecture/09-resource-registry/#resource-manager-interface","text":"# core-lib/ports/resource_port.py from typing import List from .models import Resource class ResourcePort(Protocol): def register(self, resource: Resource) -> None: \"\"\"Register a new resource in the registry.\"\"\" pass def list(self, scope: str) -> List[Resource]: \"\"\"List resources within a scope.\"\"\" pass def get(self, id: str) -> Resource: \"\"\"Get a resource by ID.\"\"\" pass def resolve(self, name: str, version: str) -> Resource: \"\"\"Resolve a resource by name and version.\"\"\" pass def increment_usage(self, id: str) -> None: \"\"\"Increment usage counter for a resource.\"\"\" pass","title":"\ud83e\udde9 Resource Manager Interface"},{"location":"architecture/09-resource-registry/#example-adapter-implementation","text":"# storage/resource_repository.py import json, os from core_lib.models.resource import Resource class FileResourceRepo: def __init__(self, base_path=\"~/.SecFlow/resources\"): self.base = os.path.expanduser(base_path) def register(self, res: Resource): path = os.path.join(self.base, f\"{res.id.replace('res://','')}.json\") os.makedirs(os.path.dirname(path), exist_ok=True) with open(path, \"w\") as f: f.write(res.model_dump_json()) def get(self, id): path = os.path.join(self.base, f\"{id.replace('res://','')}.json\") return Resource.parse_file(path)","title":"Example Adapter Implementation"},{"location":"architecture/09-resource-registry/#resource-fetching-caching","text":"When a workflow references res://wordlists/dirb:latest : The registry resolves the resource (global/group/project). The manager checks the local cache. If missing or outdated, it downloads or loads the file blob. The reference is then injected into the tool configuration. resolved = ResourceManager.resolve(\"res://wordlists/dirb:latest\") path = CacheManager.fetch(resolved)","title":"\ud83e\udde9 Resource Fetching &amp; Caching"},{"location":"architecture/09-resource-registry/#resource-versioning","text":"Resources are immutable once published; updates produce new versions. Operation Behavior publish Adds new resource version, preserves old one. retire Marks resource as deprecated (kept for history). promote Moves a project resource to group/global scope.","title":"\ud83e\udde0 Resource Versioning"},{"location":"architecture/09-resource-registry/#version-reference-syntax","text":"res://wordlists/dirb:1.2.0 res://templates/nuclei:latest","title":"Version Reference Syntax"},{"location":"architecture/09-resource-registry/#registry-cli-commands","text":"# List all resources SecFlow resources list # Show details for a resource SecFlow resources show res://wordlists/dirb:latest # Add new resource SecFlow resources add wordlist ./custom.txt --scope project # Promote resource SecFlow resources promote res://project/acme/dirb:latest --to global","title":"\ud83e\udde9 Registry CLI Commands"},{"location":"architecture/09-resource-registry/#integration-with-tool-manager","text":"Tools reference resources via symbolic URIs. At runtime, the registry automatically resolves URIs into local paths.","title":"\ud83e\udde0 Integration with Tool Manager"},{"location":"architecture/09-resource-registry/#example-nuclei-config","text":"templates: res://templates/nuclei:latest wordlist: res://wordlists/dirb:latest","title":"Example Nuclei config:"},{"location":"architecture/09-resource-registry/#security-integrity","text":"Mechanism Description Hash Validation SHA-256 hashes validated before use. Signed Resources Optional GPG signing for remote sources. Access Control Scoped permissions (Admin / Analyst / Viewer). Audit Trail Every fetch and publish event logged.","title":"\ud83d\udd12 Security &amp; Integrity"},{"location":"architecture/09-resource-registry/#garbage-collection-policy","text":"When a resource is deleted: - Orphaned blob files are queued for cleanup by the GarbageCollector . - The audit log retains metadata for traceability. - Cached copies remain valid until TTL expiration.","title":"\ud83e\udde9 Garbage Collection Policy"},{"location":"architecture/09-resource-registry/#example-use-case","text":"","title":"\ud83e\udde0 Example Use Case"},{"location":"architecture/09-resource-registry/#global-wordlist-shared-by-tools","text":"All tools can reference res://wordlists/dirb:latest : - Feroxbuster uses it for directory brute-force. - Katana can reuse it for crawl hints. - Nuclei leverages it for parameter fuzzing. Each tool can still override with its own wordlist if required.","title":"Global Wordlist Shared by Tools"},{"location":"architecture/09-resource-registry/#future-enhancements","text":"Remote Resource Index API (REST/GraphQL). Smart caching with usage-based prefetch. Resource metrics: \"most used\", \"last updated\", etc. Tag-based search and semantic discovery. Signed update feeds from verified community sources. Next: Wordlist & Output Sharing Rules","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/10-wordlist-and-output-sharing/","text":"10 \u2014 Wordlist & Output Sharing Rules \u00b6 \ud83e\udded Overview \u00b6 One of SecFlow's key strengths is inter-tool resource sharing \u2014 allowing different scanners and discovery tools to reuse wordlists , templates , and output datasets through a unified and versioned Resource Registry. This enables workflows like: %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster<br/>(directory discovery)\"] B[\"Katana<br/>(crawler + parameter discovery)\"] C[\"Nuclei<br/>(template-driven vulnerability scan)\"] A --> B B --> C Each step consumes outputs and produces standardized findings that can be reused in subsequent stages \u2014 without manual conversions or reconfiguration. \ud83e\uddf1 Design Objectives \u00b6 Objective Description Shared Wordlists All tools can reference the same library of wordlists. Isolated Overrides Each tool may override global wordlists with project-specific ones. Consistent Output Contracts Every wrapper emits normalized JSON output. Dataset Interoperability Discovery results from one tool can feed another automatically. Performance Efficiency Cached resources and deduplication prevent redundant I/O. \u2699\ufe0f Wordlist Management Flow \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource Registry\"] B[\"Wordlist Resolver\"] C[\"Global Cache\"] D[\"Tool Wrapper Config\"] E[\"Execution Context\"] A --> B B --> C B --> D D --> E Each execution context resolves wordlists from the Resource Registry , fetching them locally if needed. \ud83e\udde9 Shared Wordlist Schema \u00b6 id: res://wordlists/dirb:1.2.0 type: wordlist scope: global metadata: description: \"Common web directory wordlist\" format: \"text\" size: 24312 Usage Example \u00b6 tools: feroxbuster: wordlist: res://wordlists/dirb:latest nuclei: templates: res://templates/owasp-top10:latest At runtime, SecFlow resolves these resources to absolute paths and injects them into each wrapper. \ud83e\udde0 Multi-Scope Selection Logic \u00b6 1. Run-Level Override \u00b6 Supplied directly in the workflow recipe or CLI argument. Example: SecFlow run feroxbuster --wordlist ./private.txt 2. Node-Level Configuration \u00b6 Declared inside workflow YAML. Example: nodes: - id: ferox type: discovery.ferox config: wordlist: res://wordlists/dirb:1.0 3. Project Default \u00b6 Stored under ~/.SecFlow/projects/<id>/config.yaml . 4. Group Default \u00b6 Shared among organizational units or red-team groups. 5. Global Default \u00b6 Fallback resource available for all users. \ud83e\udde0 Tool-Specific Overrides \u00b6 Each wrapper can define preferred wordlists and formats in its manifest. Example (feroxbuster.json): \u00b6 { \"defaults\": { \"wordlist\": \"res://wordlists/dirb:latest\" }, \"accepted_formats\": [\"txt\", \"lst\"] } Nuclei Example: \u00b6 { \"defaults\": { \"templates\": \"res://templates/nuclei:latest\" }, \"accepted_formats\": [\"yaml\"] } The Tool Manager dynamically validates that the provided wordlist matches the expected format before execution. \ud83e\udde9 Output Standardization \u00b6 All tools output to a shared JSON Lines dataset ( .jsonl ). Example \u2014 Ferox Output \u00b6 {\"url\": \"https://target.com/login\", \"status\": 200, \"source\": \"feroxbuster\"} Example \u2014 Nuclei Output \u00b6 {\"id\": \"CVE-2024-12345\", \"template\": \"sql-injection\", \"severity\": \"high\", \"matched-at\": \"https://target.com/login\"} These outputs are normalized into the Finding schema by the Findings Engine. \ud83e\udde0 Chained Data Exchange \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Output (urls.jsonl)\"] C[\"Katana\"] D[\"Nuclei\"] A --> B C --> B B --> D Each wrapper declares output channels (urls, parameters, endpoints, etc.). The Workflow Engine automatically maps compatible output types to input fields of the next node. Example Workflow Excerpt \u00b6 nodes: - id: ferox type: discovery.ferox outputs: [\"urls\"] - id: katana type: crawler.katana inputs: [\"urls\"] outputs: [\"urls\", \"params\"] - id: nuclei type: scanner.nuclei inputs: [\"urls\", \"params\"] outputs: [\"findings\"] \ud83e\udde9 Cross-Tool Resource Sharing Rules \u00b6 Rule Description Wordlists All tools can access any registered wordlist; wrappers define which formats they accept. Templates Shared globally between Nuclei, ZAP, or other scanners. Headers Reusable header sets (e.g., API tokens) can be applied per project. Payloads Payload libraries are versioned and accessible to all fuzzers. Findings Outputs Findings may be exported or reused as seed data for enrichment tools. \ud83e\udde0 Example \u2014 Shared Output Dataset \u00b6 Scenario: \u00b6 A user runs Ferox \u2192 Nuclei chain. Nuclei reuses the output from Ferox as its input dataset. SecFlow run feroxbuster --target https://api.company.com \\ | SecFlow run nuclei --stdin Result: \u00b6 Ferox writes discovered URLs to /runs/<uuid>/ferox/urls.jsonl The Workflow Engine pipes this dataset to the Nuclei node. Nuclei scans each URL using selected templates. Normalized findings are saved under /runs/<uuid>/nuclei/findings.jsonl \ud83e\udde9 Shared Dataset Metadata \u00b6 dataset: id: \"runs/2025-10-06T12:31Z-ferox-urls\" type: \"urls\" source: \"feroxbuster\" size: 243 created_at: \"2025-10-06T12:31Z\" This metadata is referenced by downstream nodes to ensure deterministic workflows. \ud83d\udd10 Data Isolation & Sharing Between Projects \u00b6 SecFlow supports granular sharing control for multi-project setups: Mode Description Isolated Each project keeps separate resources and findings. Shared Group Projects under the same group share wordlists and results. Selective User manually links resources or outputs between projects. Example: \u00b6 project: name: acme-api sharing: with: [\"internal-api\", \"dev-api\"] resources: [\"wordlists\", \"templates\"] outputs: [\"urls\", \"parameters\"] \ud83e\udde9 Cache & Deduplication \u00b6 Wordlists and tool outputs are hash-indexed and cached for reuse. Cache Key Formula \u00b6 cache_key = sha256(resource_id + version + scope) This guarantees consistent retrieval and avoids redundant downloads. \ud83e\udde0 Example End-to-End Flow \u00b6 Project: acme-api %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Shared wordlists (global)\"] B[\"Custom payloads (project)\"] C[\"Workflow: Ferox \u2192 Katana \u2192 Nuclei\"] D[\"URLs discovered \u2192 shared dataset\"] E[\"Parameters extracted \u2192 temp dataset\"] F[\"Findings enriched \u2192 persisted\"] G[\"Cached resources \u2192 reused in next run\"] A --> C B --> C C --> D C --> E C --> F C --> G \ud83e\udde9 Validation & Conflict Resolution \u00b6 Conflict Resolution Same resource name, different scope Higher precedence scope wins (project > group > global). Resource deleted mid-run Cached version remains until TTL expires. Output schema mismatch Adapter plugin transforms data. Duplicate URLs or findings De-duplication engine merges entries by hash. \ud83d\udd2e Future Enhancements \u00b6 Distributed cache synchronization (Redis or MinIO). Tag-based linking of related outputs (e.g., same target domain). Automatic scope inference based on project classification. Resource \"diff\" view between runs. AI suggestion for optimal wordlists or template selection. Next: Project Isolation & Data Sharing Controls","title":"Wordlists & Sharing"},{"location":"architecture/10-wordlist-and-output-sharing/#10-wordlist-output-sharing-rules","text":"","title":"10 \u2014 Wordlist &amp; Output Sharing Rules"},{"location":"architecture/10-wordlist-and-output-sharing/#overview","text":"One of SecFlow's key strengths is inter-tool resource sharing \u2014 allowing different scanners and discovery tools to reuse wordlists , templates , and output datasets through a unified and versioned Resource Registry. This enables workflows like: %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster<br/>(directory discovery)\"] B[\"Katana<br/>(crawler + parameter discovery)\"] C[\"Nuclei<br/>(template-driven vulnerability scan)\"] A --> B B --> C Each step consumes outputs and produces standardized findings that can be reused in subsequent stages \u2014 without manual conversions or reconfiguration.","title":"\ud83e\udded Overview"},{"location":"architecture/10-wordlist-and-output-sharing/#design-objectives","text":"Objective Description Shared Wordlists All tools can reference the same library of wordlists. Isolated Overrides Each tool may override global wordlists with project-specific ones. Consistent Output Contracts Every wrapper emits normalized JSON output. Dataset Interoperability Discovery results from one tool can feed another automatically. Performance Efficiency Cached resources and deduplication prevent redundant I/O.","title":"\ud83e\uddf1 Design Objectives"},{"location":"architecture/10-wordlist-and-output-sharing/#wordlist-management-flow","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource Registry\"] B[\"Wordlist Resolver\"] C[\"Global Cache\"] D[\"Tool Wrapper Config\"] E[\"Execution Context\"] A --> B B --> C B --> D D --> E Each execution context resolves wordlists from the Resource Registry , fetching them locally if needed.","title":"\u2699\ufe0f Wordlist Management Flow"},{"location":"architecture/10-wordlist-and-output-sharing/#shared-wordlist-schema","text":"id: res://wordlists/dirb:1.2.0 type: wordlist scope: global metadata: description: \"Common web directory wordlist\" format: \"text\" size: 24312","title":"\ud83e\udde9 Shared Wordlist Schema"},{"location":"architecture/10-wordlist-and-output-sharing/#usage-example","text":"tools: feroxbuster: wordlist: res://wordlists/dirb:latest nuclei: templates: res://templates/owasp-top10:latest At runtime, SecFlow resolves these resources to absolute paths and injects them into each wrapper.","title":"Usage Example"},{"location":"architecture/10-wordlist-and-output-sharing/#multi-scope-selection-logic","text":"","title":"\ud83e\udde0 Multi-Scope Selection Logic"},{"location":"architecture/10-wordlist-and-output-sharing/#1-run-level-override","text":"Supplied directly in the workflow recipe or CLI argument. Example: SecFlow run feroxbuster --wordlist ./private.txt","title":"1. Run-Level Override"},{"location":"architecture/10-wordlist-and-output-sharing/#2-node-level-configuration","text":"Declared inside workflow YAML. Example: nodes: - id: ferox type: discovery.ferox config: wordlist: res://wordlists/dirb:1.0","title":"2. Node-Level Configuration"},{"location":"architecture/10-wordlist-and-output-sharing/#3-project-default","text":"Stored under ~/.SecFlow/projects/<id>/config.yaml .","title":"3. Project Default"},{"location":"architecture/10-wordlist-and-output-sharing/#4-group-default","text":"Shared among organizational units or red-team groups.","title":"4. Group Default"},{"location":"architecture/10-wordlist-and-output-sharing/#5-global-default","text":"Fallback resource available for all users.","title":"5. Global Default"},{"location":"architecture/10-wordlist-and-output-sharing/#tool-specific-overrides","text":"Each wrapper can define preferred wordlists and formats in its manifest.","title":"\ud83e\udde0 Tool-Specific Overrides"},{"location":"architecture/10-wordlist-and-output-sharing/#example-feroxbusterjson","text":"{ \"defaults\": { \"wordlist\": \"res://wordlists/dirb:latest\" }, \"accepted_formats\": [\"txt\", \"lst\"] }","title":"Example (feroxbuster.json):"},{"location":"architecture/10-wordlist-and-output-sharing/#nuclei-example","text":"{ \"defaults\": { \"templates\": \"res://templates/nuclei:latest\" }, \"accepted_formats\": [\"yaml\"] } The Tool Manager dynamically validates that the provided wordlist matches the expected format before execution.","title":"Nuclei Example:"},{"location":"architecture/10-wordlist-and-output-sharing/#output-standardization","text":"All tools output to a shared JSON Lines dataset ( .jsonl ).","title":"\ud83e\udde9 Output Standardization"},{"location":"architecture/10-wordlist-and-output-sharing/#example-ferox-output","text":"{\"url\": \"https://target.com/login\", \"status\": 200, \"source\": \"feroxbuster\"}","title":"Example \u2014 Ferox Output"},{"location":"architecture/10-wordlist-and-output-sharing/#example-nuclei-output","text":"{\"id\": \"CVE-2024-12345\", \"template\": \"sql-injection\", \"severity\": \"high\", \"matched-at\": \"https://target.com/login\"} These outputs are normalized into the Finding schema by the Findings Engine.","title":"Example \u2014 Nuclei Output"},{"location":"architecture/10-wordlist-and-output-sharing/#chained-data-exchange","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Output (urls.jsonl)\"] C[\"Katana\"] D[\"Nuclei\"] A --> B C --> B B --> D Each wrapper declares output channels (urls, parameters, endpoints, etc.). The Workflow Engine automatically maps compatible output types to input fields of the next node.","title":"\ud83e\udde0 Chained Data Exchange"},{"location":"architecture/10-wordlist-and-output-sharing/#example-workflow-excerpt","text":"nodes: - id: ferox type: discovery.ferox outputs: [\"urls\"] - id: katana type: crawler.katana inputs: [\"urls\"] outputs: [\"urls\", \"params\"] - id: nuclei type: scanner.nuclei inputs: [\"urls\", \"params\"] outputs: [\"findings\"]","title":"Example Workflow Excerpt"},{"location":"architecture/10-wordlist-and-output-sharing/#cross-tool-resource-sharing-rules","text":"Rule Description Wordlists All tools can access any registered wordlist; wrappers define which formats they accept. Templates Shared globally between Nuclei, ZAP, or other scanners. Headers Reusable header sets (e.g., API tokens) can be applied per project. Payloads Payload libraries are versioned and accessible to all fuzzers. Findings Outputs Findings may be exported or reused as seed data for enrichment tools.","title":"\ud83e\udde9 Cross-Tool Resource Sharing Rules"},{"location":"architecture/10-wordlist-and-output-sharing/#example-shared-output-dataset","text":"","title":"\ud83e\udde0 Example \u2014 Shared Output Dataset"},{"location":"architecture/10-wordlist-and-output-sharing/#scenario","text":"A user runs Ferox \u2192 Nuclei chain. Nuclei reuses the output from Ferox as its input dataset. SecFlow run feroxbuster --target https://api.company.com \\ | SecFlow run nuclei --stdin","title":"Scenario:"},{"location":"architecture/10-wordlist-and-output-sharing/#result","text":"Ferox writes discovered URLs to /runs/<uuid>/ferox/urls.jsonl The Workflow Engine pipes this dataset to the Nuclei node. Nuclei scans each URL using selected templates. Normalized findings are saved under /runs/<uuid>/nuclei/findings.jsonl","title":"Result:"},{"location":"architecture/10-wordlist-and-output-sharing/#shared-dataset-metadata","text":"dataset: id: \"runs/2025-10-06T12:31Z-ferox-urls\" type: \"urls\" source: \"feroxbuster\" size: 243 created_at: \"2025-10-06T12:31Z\" This metadata is referenced by downstream nodes to ensure deterministic workflows.","title":"\ud83e\udde9 Shared Dataset Metadata"},{"location":"architecture/10-wordlist-and-output-sharing/#data-isolation-sharing-between-projects","text":"SecFlow supports granular sharing control for multi-project setups: Mode Description Isolated Each project keeps separate resources and findings. Shared Group Projects under the same group share wordlists and results. Selective User manually links resources or outputs between projects.","title":"\ud83d\udd10 Data Isolation &amp; Sharing Between Projects"},{"location":"architecture/10-wordlist-and-output-sharing/#example","text":"project: name: acme-api sharing: with: [\"internal-api\", \"dev-api\"] resources: [\"wordlists\", \"templates\"] outputs: [\"urls\", \"parameters\"]","title":"Example:"},{"location":"architecture/10-wordlist-and-output-sharing/#cache-deduplication","text":"Wordlists and tool outputs are hash-indexed and cached for reuse.","title":"\ud83e\udde9 Cache &amp; Deduplication"},{"location":"architecture/10-wordlist-and-output-sharing/#cache-key-formula","text":"cache_key = sha256(resource_id + version + scope) This guarantees consistent retrieval and avoids redundant downloads.","title":"Cache Key Formula"},{"location":"architecture/10-wordlist-and-output-sharing/#example-end-to-end-flow","text":"Project: acme-api %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Shared wordlists (global)\"] B[\"Custom payloads (project)\"] C[\"Workflow: Ferox \u2192 Katana \u2192 Nuclei\"] D[\"URLs discovered \u2192 shared dataset\"] E[\"Parameters extracted \u2192 temp dataset\"] F[\"Findings enriched \u2192 persisted\"] G[\"Cached resources \u2192 reused in next run\"] A --> C B --> C C --> D C --> E C --> F C --> G","title":"\ud83e\udde0 Example End-to-End Flow"},{"location":"architecture/10-wordlist-and-output-sharing/#validation-conflict-resolution","text":"Conflict Resolution Same resource name, different scope Higher precedence scope wins (project > group > global). Resource deleted mid-run Cached version remains until TTL expires. Output schema mismatch Adapter plugin transforms data. Duplicate URLs or findings De-duplication engine merges entries by hash.","title":"\ud83e\udde9 Validation &amp; Conflict Resolution"},{"location":"architecture/10-wordlist-and-output-sharing/#future-enhancements","text":"Distributed cache synchronization (Redis or MinIO). Tag-based linking of related outputs (e.g., same target domain). Automatic scope inference based on project classification. Resource \"diff\" view between runs. AI suggestion for optimal wordlists or template selection. Next: Project Isolation & Data Sharing Controls","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/11-project-isolation-and-data-sharing/","text":"11 \u2014 Project Isolation & Data Sharing Controls \u00b6 \ud83e\udded Overview \u00b6 The Project Isolation & Sharing subsystem defines how multiple SecFlow projects coexist securely within the same environment. Each project has its own namespace , storage domain , and execution context , but can optionally share data or resources under controlled policies. SecFlow's isolation model is designed to: - Protect sensitive customer or target data. - Enable reuse of discovery outputs between related engagements. - Support team-based collaboration while maintaining auditability. \ud83e\uddf1 Design Principles \u00b6 Principle Description Default Isolation Every project operates in its own workspace and database namespace. Explicit Sharing No cross-project access occurs unless explicitly declared. Scoped Resources Shared assets must be tagged and versioned. Least Privilege Access Users only see or manage projects they are assigned to. Full Traceability Every cross-project data access is logged and auditable. \u2699\ufe0f Architectural Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project Manager<br/>- Workspace Manager (namespacing, FS isolation)<br/>- Data Access Layer (scope resolution)<br/>- Sharing Policy Engine (group + project-level rules)<br/>- Audit Log (cross-project actions)\"] \ud83e\udde9 Project Data Model \u00b6 # core-lib/models/project.py from typing import List, Optional from datetime import datetime from pydantic import BaseModel class Project(BaseModel): id: str name: str owner: str group: Optional[str] description: Optional[str] created_at: datetime updated_at: datetime sharing: Optional[dict] = { \"enabled\": False, \"with\": [], \"resources\": [], \"outputs\": [] } \ud83e\udde9 Workspace Isolation \u00b6 Each project is backed by its own filesystem and database schema. Example Directory Layout \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/projects/\"] B[\"acme-api/\"] C[\"config.yaml\"] D[\"runs/\"] E[\"findings/\"] F[\"cache/\"] G[\"finance-portal/\"] H[\"config.yaml\"] I[\"runs/\"] J[\"findings/\"] K[\"cache/\"] A --> B A --> G B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K Database Schema Isolation \u00b6 Each project gets a dedicated schema: public.findings_acme_api public.findings_finance_portal This allows multiple concurrent engagements with strict data boundaries. \ud83e\udde0 Data Sharing Configuration \u00b6 Example: Controlled Cross-Project Sharing \u00b6 project: name: \"acme-api\" sharing: enabled: true with: - \"internal-api\" - \"qa-staging\" resources: - \"wordlists\" - \"templates\" outputs: - \"urls\" - \"parameters\" In this configuration: - The acme-api project shares wordlists and templates with two sibling projects. - The outputs (e.g., discovered URLs) are made available for enrichment or scanning reuse. \ud83e\udde9 Sharing Policy Engine \u00b6 Logic Flow \u00b6 User Request \u2193 Policy Check (Project A \u2192 Project B) \u2193 Scope Validation \u2193 Access Decision (allow | deny) Policy Structure \u00b6 Field Description resource_type What type of data is being shared (wordlist, output, finding). scope Allowed scope (project, group, global). mode Access type (read-only, read-write, clone). ttl Time-to-live for shared access. Example Policy Definition \u00b6 policies: - resource_type: \"outputs\" scope: \"group\" mode: \"read-only\" ttl: 30d \ud83e\udde9 Isolation Enforcement Mechanisms \u00b6 Layer Enforcement Filesystem Each project path is sandboxed under ~/.SecFlow/projects/<id> . Database Separate schema per project (namespaced tables). Cache Project-specific cache directories. Runtime Context Workers run with PROJECT_ID environment variable. Authorization API tokens include project_id scope claim. \ud83e\udde0 Access Token Scoping \u00b6 API tokens encode the project scope: { \"sub\": \"hernan.trajtemberg\", \"project_id\": \"acme-api\", \"roles\": [\"analyst\"], \"exp\": 1759870400 } Tokens can be project-scoped or group-scoped. The access control middleware rejects out-of-scope operations. \ud83e\udde9 Resource Linking Between Projects \u00b6 Projects can import shared assets from another project's registry. Example Command \u00b6 SecFlow projects link internal-api --resources wordlists templates Example Output \u00b6 Linked resources: \u2714 wordlists (3) \u2714 templates (5) Linked resources are marked in metadata: linked_from: \"project:internal-api\" mode: \"read-only\" \ud83e\uddf1 Output Sharing \u00b6 Outputs (datasets or findings) can also be shared for cross-project correlation or enrichment. Example Workflow: \u00b6 Project A \u2192 Discovery + Scan \u2193 Shared Outputs (URLs, endpoints) \u2193 Project B \u2192 Enrichment / Retesting Sharing Command \u00b6 SecFlow share outputs acme-api --with finance-portal --types urls parameters The receiving project's engine imports the shared dataset as a read-only reference. \ud83e\udde9 Audit Logging \u00b6 Every cross-project access event is logged. Example Log Entry \u00b6 { \"event\": \"resource_access\", \"actor\": \"hernan.trajtemberg\", \"source_project\": \"acme-api\", \"target_project\": \"internal-api\", \"resource\": \"wordlists\", \"timestamp\": \"2025-10-06T11:42:21Z\", \"action\": \"read\" } \ud83e\udde0 Isolation Example Scenarios \u00b6 1. Strict Isolation (Default) \u00b6 Each project operates completely independently. Useful for sensitive pentests or regulated environments. 2. Group-Level Sharing \u00b6 Multiple analysts share discovery data across projects within the same team. 3. Selective Sharing \u00b6 A red-team lead selectively shares Nuclei findings between \"internal-api\" and \"staging-api\". \ud83d\udd10 Security Considerations \u00b6 Risk Mitigation Unauthorized access to shared data Token-scoped enforcement and audit logging Resource version drift Immutable hashes + version pinning Data leakage across clients No implicit sharing; explicit only Lateral movement between project schemas Database role isolation Policy misconfiguration Policy validation + test harness \ud83e\udde9 Example Policy Validation Script \u00b6 def validate_policy(policy): assert policy[\"mode\"] in (\"read-only\", \"read-write\", \"clone\") assert policy[\"scope\"] in (\"project\", \"group\", \"global\") if policy[\"ttl\"]: assert re.match(r\"^\\d+[dh]$\", policy[\"ttl\"]) \ud83d\udd2e Future Enhancements \u00b6 Group-level \"Shared Intelligence Pool\" for recurring findings. Automatic synchronization of enrichment data across related projects. AI-based data deduplication and anomaly detection. Visual dependency graph of shared resources in UI. Temporal sharing policies (\"auto-expire after 30 days\"). Next: Findings Model & Schema Normalization","title":"Project Isolation & Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#11-project-isolation-data-sharing-controls","text":"","title":"11 \u2014 Project Isolation &amp; Data Sharing Controls"},{"location":"architecture/11-project-isolation-and-data-sharing/#overview","text":"The Project Isolation & Sharing subsystem defines how multiple SecFlow projects coexist securely within the same environment. Each project has its own namespace , storage domain , and execution context , but can optionally share data or resources under controlled policies. SecFlow's isolation model is designed to: - Protect sensitive customer or target data. - Enable reuse of discovery outputs between related engagements. - Support team-based collaboration while maintaining auditability.","title":"\ud83e\udded Overview"},{"location":"architecture/11-project-isolation-and-data-sharing/#design-principles","text":"Principle Description Default Isolation Every project operates in its own workspace and database namespace. Explicit Sharing No cross-project access occurs unless explicitly declared. Scoped Resources Shared assets must be tagged and versioned. Least Privilege Access Users only see or manage projects they are assigned to. Full Traceability Every cross-project data access is logged and auditable.","title":"\ud83e\uddf1 Design Principles"},{"location":"architecture/11-project-isolation-and-data-sharing/#architectural-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project Manager<br/>- Workspace Manager (namespacing, FS isolation)<br/>- Data Access Layer (scope resolution)<br/>- Sharing Policy Engine (group + project-level rules)<br/>- Audit Log (cross-project actions)\"]","title":"\u2699\ufe0f Architectural Overview"},{"location":"architecture/11-project-isolation-and-data-sharing/#project-data-model","text":"# core-lib/models/project.py from typing import List, Optional from datetime import datetime from pydantic import BaseModel class Project(BaseModel): id: str name: str owner: str group: Optional[str] description: Optional[str] created_at: datetime updated_at: datetime sharing: Optional[dict] = { \"enabled\": False, \"with\": [], \"resources\": [], \"outputs\": [] }","title":"\ud83e\udde9 Project Data Model"},{"location":"architecture/11-project-isolation-and-data-sharing/#workspace-isolation","text":"Each project is backed by its own filesystem and database schema.","title":"\ud83e\udde9 Workspace Isolation"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-directory-layout","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/projects/\"] B[\"acme-api/\"] C[\"config.yaml\"] D[\"runs/\"] E[\"findings/\"] F[\"cache/\"] G[\"finance-portal/\"] H[\"config.yaml\"] I[\"runs/\"] J[\"findings/\"] K[\"cache/\"] A --> B A --> G B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K","title":"Example Directory Layout"},{"location":"architecture/11-project-isolation-and-data-sharing/#database-schema-isolation","text":"Each project gets a dedicated schema: public.findings_acme_api public.findings_finance_portal This allows multiple concurrent engagements with strict data boundaries.","title":"Database Schema Isolation"},{"location":"architecture/11-project-isolation-and-data-sharing/#data-sharing-configuration","text":"","title":"\ud83e\udde0 Data Sharing Configuration"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-controlled-cross-project-sharing","text":"project: name: \"acme-api\" sharing: enabled: true with: - \"internal-api\" - \"qa-staging\" resources: - \"wordlists\" - \"templates\" outputs: - \"urls\" - \"parameters\" In this configuration: - The acme-api project shares wordlists and templates with two sibling projects. - The outputs (e.g., discovered URLs) are made available for enrichment or scanning reuse.","title":"Example: Controlled Cross-Project Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#sharing-policy-engine","text":"","title":"\ud83e\udde9 Sharing Policy Engine"},{"location":"architecture/11-project-isolation-and-data-sharing/#logic-flow","text":"User Request \u2193 Policy Check (Project A \u2192 Project B) \u2193 Scope Validation \u2193 Access Decision (allow | deny)","title":"Logic Flow"},{"location":"architecture/11-project-isolation-and-data-sharing/#policy-structure","text":"Field Description resource_type What type of data is being shared (wordlist, output, finding). scope Allowed scope (project, group, global). mode Access type (read-only, read-write, clone). ttl Time-to-live for shared access.","title":"Policy Structure"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-policy-definition","text":"policies: - resource_type: \"outputs\" scope: \"group\" mode: \"read-only\" ttl: 30d","title":"Example Policy Definition"},{"location":"architecture/11-project-isolation-and-data-sharing/#isolation-enforcement-mechanisms","text":"Layer Enforcement Filesystem Each project path is sandboxed under ~/.SecFlow/projects/<id> . Database Separate schema per project (namespaced tables). Cache Project-specific cache directories. Runtime Context Workers run with PROJECT_ID environment variable. Authorization API tokens include project_id scope claim.","title":"\ud83e\udde9 Isolation Enforcement Mechanisms"},{"location":"architecture/11-project-isolation-and-data-sharing/#access-token-scoping","text":"API tokens encode the project scope: { \"sub\": \"hernan.trajtemberg\", \"project_id\": \"acme-api\", \"roles\": [\"analyst\"], \"exp\": 1759870400 } Tokens can be project-scoped or group-scoped. The access control middleware rejects out-of-scope operations.","title":"\ud83e\udde0 Access Token Scoping"},{"location":"architecture/11-project-isolation-and-data-sharing/#resource-linking-between-projects","text":"Projects can import shared assets from another project's registry.","title":"\ud83e\udde9 Resource Linking Between Projects"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-command","text":"SecFlow projects link internal-api --resources wordlists templates","title":"Example Command"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-output","text":"Linked resources: \u2714 wordlists (3) \u2714 templates (5) Linked resources are marked in metadata: linked_from: \"project:internal-api\" mode: \"read-only\"","title":"Example Output"},{"location":"architecture/11-project-isolation-and-data-sharing/#output-sharing","text":"Outputs (datasets or findings) can also be shared for cross-project correlation or enrichment.","title":"\ud83e\uddf1 Output Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-workflow","text":"Project A \u2192 Discovery + Scan \u2193 Shared Outputs (URLs, endpoints) \u2193 Project B \u2192 Enrichment / Retesting","title":"Example Workflow:"},{"location":"architecture/11-project-isolation-and-data-sharing/#sharing-command","text":"SecFlow share outputs acme-api --with finance-portal --types urls parameters The receiving project's engine imports the shared dataset as a read-only reference.","title":"Sharing Command"},{"location":"architecture/11-project-isolation-and-data-sharing/#audit-logging","text":"Every cross-project access event is logged.","title":"\ud83e\udde9 Audit Logging"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-log-entry","text":"{ \"event\": \"resource_access\", \"actor\": \"hernan.trajtemberg\", \"source_project\": \"acme-api\", \"target_project\": \"internal-api\", \"resource\": \"wordlists\", \"timestamp\": \"2025-10-06T11:42:21Z\", \"action\": \"read\" }","title":"Example Log Entry"},{"location":"architecture/11-project-isolation-and-data-sharing/#isolation-example-scenarios","text":"","title":"\ud83e\udde0 Isolation Example Scenarios"},{"location":"architecture/11-project-isolation-and-data-sharing/#1-strict-isolation-default","text":"Each project operates completely independently. Useful for sensitive pentests or regulated environments.","title":"1. Strict Isolation (Default)"},{"location":"architecture/11-project-isolation-and-data-sharing/#2-group-level-sharing","text":"Multiple analysts share discovery data across projects within the same team.","title":"2. Group-Level Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#3-selective-sharing","text":"A red-team lead selectively shares Nuclei findings between \"internal-api\" and \"staging-api\".","title":"3. Selective Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#security-considerations","text":"Risk Mitigation Unauthorized access to shared data Token-scoped enforcement and audit logging Resource version drift Immutable hashes + version pinning Data leakage across clients No implicit sharing; explicit only Lateral movement between project schemas Database role isolation Policy misconfiguration Policy validation + test harness","title":"\ud83d\udd10 Security Considerations"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-policy-validation-script","text":"def validate_policy(policy): assert policy[\"mode\"] in (\"read-only\", \"read-write\", \"clone\") assert policy[\"scope\"] in (\"project\", \"group\", \"global\") if policy[\"ttl\"]: assert re.match(r\"^\\d+[dh]$\", policy[\"ttl\"])","title":"\ud83e\udde9 Example Policy Validation Script"},{"location":"architecture/11-project-isolation-and-data-sharing/#future-enhancements","text":"Group-level \"Shared Intelligence Pool\" for recurring findings. Automatic synchronization of enrichment data across related projects. AI-based data deduplication and anomaly detection. Visual dependency graph of shared resources in UI. Temporal sharing policies (\"auto-expire after 30 days\"). Next: Findings Model & Schema Normalization","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/12-findings-model-and-schema/","text":"12 \u2014 Findings Model & Schema Normalization \u00b6 \ud83e\udded Overview \u00b6 The Findings Schema is the canonical data contract for SecFlow \u2014 it defines how vulnerabilities, anomalies, and tool outputs are normalized, enriched, and persisted. Every discovery, scan, or enrichment step in the workflow produces Findings , which are standardized across tools (Nuclei, Ferox, ZAP, Caido, custom detectors). The schema ensures: - Cross-tool consistency - Automated CVE/CWE/OWASP correlation - Risk scoring alignment (CVSS/NIST) - Deterministic enrichment for analytics and triage \ud83e\uddf1 Data Flow Summary \u00b6 [ Tool Output ] \u2193 [ Findings Engine ] \u2193 [ Normalization Layer ] \u2193 [ Enrichment Layer (CVE/CWE/OWASP) ] \u2193 [ Persistent Store + Cache + Analytics ] \u2699\ufe0f Findings Core Model \u00b6 from pydantic import BaseModel, Field from typing import Optional, List, Dict from uuid import UUID from datetime import datetime class Finding(BaseModel): id: UUID project_id: UUID run_id: Optional[UUID] detector_id: str = Field(description=\"canonical tool or plugin id, e.g. scan.nuclei\") title: str severity: str # enum: info, low, medium, high, critical resource: str # canonical URL or identifier evidence: Dict[str, object] = {} cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] cvss: Optional[float] mitre_attack: List[str] = [] poc_links: List[str] = [] created_at: datetime provenance: Dict[str, object] = {} \ud83e\udde9 Normalization Process \u00b6 Each wrapper or plugin output goes through the Findings Normalizer, which performs: Schema validation Severity normalization Field mapping Evidence compression Deduplication Example Normalized Finding \u00b6 { \"id\": \"7a3d9f8e-1f83-4a55-9a6c-5ea9e1a3b8d2\", \"project_id\": \"b0b8b6e0-7c6a-43e8-8b2f-8a0d0e8b0f3c\", \"run_id\": \"2a2b3c4d-5e6f-7081-92a3-b4c5d6e7f809\", \"detector_id\": \"scan.nuclei\", \"title\": \"X-Frame-Options header missing\", \"severity\": \"low\", \"resource\": \"https://app.acme.com/\", \"evidence\": { \"request_id\": \"r-01HXXQ5AKP0NWS\", \"template\": \"misconfig/headers/xfo-missing.yaml\" }, \"cwe\": 693, \"owasp\": \"A05\", \"cve_ids\": [], \"cvss\": 3.7, \"mitre_attack\": [\"T1190\"], \"poc_links\": [], \"created_at\": \"2025-10-07T08:22:31Z\", \"provenance\": { \"tool_version\": \"nuclei 3.1.0\", \"host\": \"runner-12\" } } def normalize(raw: dict, source: str) -> Finding: severity_map = {\"info\": \"info\", \"low\": \"low\", \"medium\": \"medium\", \"high\": \"high\", \"critical\": \"critical\"} return Finding( id=str(uuid4()), project_id=raw.get(\"project_id\"), detector_id=source, title=raw.get(\"info\", {}).get(\"name\") or raw.get(\"title\", \"Unnamed Finding\"), severity=severity_map.get(raw.get(\"severity\", \"info\")), path=raw.get(\"matched-at\") or raw.get(\"url\"), evidence=raw, created_at=datetime.utcnow(), provenance={\"tool\": source, \"version\": raw.get(\"version\", \"unknown\")} ) \ud83e\udde9 Normalization Rules by Source \u00b6 Tool Input Format Mapping Nuclei JSON lines info.name \u2192 title , info.severity \u2192 severity , matched-at \u2192 path Feroxbuster Text URL \u2192 path , status \u2192 evidence.status ZAP/Burp XML/JSON PluginId \u2192 cwe , RiskDesc \u2192 severity Caido SQLite Vulnerability.name \u2192 title , score \u2192 cvss_score Custom Detectors Python dict Arbitrary fields normalized via schema mapping Normalization is performed by findings-engine using source-specific adapters. \ud83e\udde0 Severity Mapping \u00b6 Raw Severity Normalized CVSS Equivalent informational info 0.0\u20133.9 low low 4.0\u20136.9 medium medium 6.0\u20137.4 high high 7.5\u20138.9 critical critical 9.0\u201310.0 \ud83e\udde0 Deduplication Strategy \u00b6 Findings are hashed on a deterministic fingerprint: hash_input = f\"{finding.path}:{finding.title}:{finding.detector_id}\" finding_hash = hashlib.sha256(hash_input.encode()).hexdigest() If the hash already exists in the same project and run scope, the finding is merged rather than duplicated. \ud83e\udde9 Enrichment Metadata Structure \u00b6 finding.enrichment = { \"cwe\": {\"id\": 89, \"name\": \"SQL Injection\"}, \"owasp\": {\"category\": \"A03\", \"name\": \"Injection\"}, \"CVSS\": {\"score\": 9.1, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\"}, \"mitre\": {\"technique\": \"T1505\", \"tactic\": \"Persistence\"}, \"poc\": {\"source\": \"exploitdb\", \"link\": \"https://www.exploit-db.com/exploits/52341\"} } This metadata is populated during the Enrichment Phase (CVE/CWE/CVSS sync). \ud83e\udde9 CVE / CWE / OWASP Mapping \u00b6 CVE \u2192 CWE \u00b6 NVD API provides cve.affects.vendor.vendor_data \u2192 cwe.id Mapping stored in local SQLite cache. CWE \u2192 OWASP \u00b6 CWE OWASP Category CWE-79 A03: Injection CWE-89 A03: Injection CWE-287 A07: Identification and Authentication Failures CWE-601 A10: Server-Side Request Forgery (SSRF) Example Mapping Resolver \u00b6 def resolve_owasp(cwe_id): mapping = {\"79\": \"A03\", \"89\": \"A03\", \"287\": \"A07\", \"601\": \"A10\"} return mapping.get(str(cwe_id)) \ud83e\udde9 Confidence & Risk Scoring \u00b6 Confidence combines tool reliability, correlation consistency, and enrichment coverage. Formula \u00b6 confidence = (tool_weight * 0.5) + (enrichment_score * 0.3) + (cross_source_count * 0.2) Risk Score Calculation \u00b6 risk_score = CVSS * confidence This allows probabilistic triage prioritization. \ud83e\udde0 Evidence Normalization \u00b6 Evidence is stored in compact, structured form for indexing: { \"request\": { \"method\": \"POST\", \"url\": \"https://target.com/login\", \"headers\": {\"Content-Type\": \"application/json\"}, \"body\": \"{\\\"username\\\":\\\"admin\\\"}\" }, \"response\": { \"status\": 500, \"headers\": {\"Server\": \"Apache\"}, \"body_snippet\": \"SQL syntax error\" } } Large payloads are truncated or compressed to avoid storage overhead. \ud83e\udde9 Finding Status Lifecycle \u00b6 Status Meaning Managed By open Newly discovered issue Scanner triaged Analyst reviewed Analyst resolved Fixed or confirmed Analyst false_positive Invalid finding Analyst archived Expired or obsolete System (GC) Each status change triggers an audit log event and optional webhook notification. \ud83e\uddf1 Storage Layer Integration \u00b6 Findings are persisted via the StoragePort interface: class FindingsRepository(Protocol): def save(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list(self, project_id: str) -> List[Finding]: \"\"\"List findings for a project.\"\"\" pass def get(self, id: str) -> Finding: \"\"\"Get a finding by ID.\"\"\" pass Supported backends: \u00b6 SQLite (default local mode) PostgreSQL (production multi-project) JSON (testing or demo mode) \ud83e\udde9 Findings Export Schema \u00b6 SecFlow exports findings in structured formats for interoperability: Format Command JSON SecFlow export findings --format json CSV SecFlow export findings --format csv HTML SecFlow report findings --template summary.html SARIF SecFlow export findings --format sarif Example JSON export: \u00b6 { \"project\": \"acme-api\", \"findings\": [ { \"id\": \"a2b4f3c8-d9e2-4f1a-5b6c-7d8e9f0a1b2c\", \"title\": \"SQL Injection\", \"severity\": \"critical\", \"cwe\": 89, \"cvss_score\": 9.1, \"path\": \"/login\", \"created_at\": \"2025-10-06T10:15:00Z\" } ] } \ud83e\udde0 Indexing & Analytics \u00b6 Each finding is indexed in the analytics database: Primary Index: project_id + severity + cwe Full-Text Search: title , path , evidence.body_snippet Aggregations: count by severity, top affected endpoints, common CWE classes. The metrics system (see 06-plugin-system.md & 17-observability-logging-and-metrics.md ) uses these indexes to generate dashboards. \ud83d\udd2e Future Enhancements \u00b6 Graph-based finding correlation (attack paths). AI-driven risk clustering (\"find similar vulnerabilities\"). Contextual auto-triage (OWASP/NIST mapping feedback loops). Delta reports between runs ( run_id diff). Live sync to vulnerability management platforms (DefectDojo, VulnDB). Next: CVE/CWE/POC Enrichment Layer","title":"Findings Schema"},{"location":"architecture/12-findings-model-and-schema/#12-findings-model-schema-normalization","text":"","title":"12 \u2014 Findings Model &amp; Schema Normalization"},{"location":"architecture/12-findings-model-and-schema/#overview","text":"The Findings Schema is the canonical data contract for SecFlow \u2014 it defines how vulnerabilities, anomalies, and tool outputs are normalized, enriched, and persisted. Every discovery, scan, or enrichment step in the workflow produces Findings , which are standardized across tools (Nuclei, Ferox, ZAP, Caido, custom detectors). The schema ensures: - Cross-tool consistency - Automated CVE/CWE/OWASP correlation - Risk scoring alignment (CVSS/NIST) - Deterministic enrichment for analytics and triage","title":"\ud83e\udded Overview"},{"location":"architecture/12-findings-model-and-schema/#data-flow-summary","text":"[ Tool Output ] \u2193 [ Findings Engine ] \u2193 [ Normalization Layer ] \u2193 [ Enrichment Layer (CVE/CWE/OWASP) ] \u2193 [ Persistent Store + Cache + Analytics ]","title":"\ud83e\uddf1 Data Flow Summary"},{"location":"architecture/12-findings-model-and-schema/#findings-core-model","text":"from pydantic import BaseModel, Field from typing import Optional, List, Dict from uuid import UUID from datetime import datetime class Finding(BaseModel): id: UUID project_id: UUID run_id: Optional[UUID] detector_id: str = Field(description=\"canonical tool or plugin id, e.g. scan.nuclei\") title: str severity: str # enum: info, low, medium, high, critical resource: str # canonical URL or identifier evidence: Dict[str, object] = {} cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] cvss: Optional[float] mitre_attack: List[str] = [] poc_links: List[str] = [] created_at: datetime provenance: Dict[str, object] = {}","title":"\u2699\ufe0f Findings Core Model"},{"location":"architecture/12-findings-model-and-schema/#normalization-process","text":"Each wrapper or plugin output goes through the Findings Normalizer, which performs: Schema validation Severity normalization Field mapping Evidence compression Deduplication","title":"\ud83e\udde9 Normalization Process"},{"location":"architecture/12-findings-model-and-schema/#example-normalized-finding","text":"{ \"id\": \"7a3d9f8e-1f83-4a55-9a6c-5ea9e1a3b8d2\", \"project_id\": \"b0b8b6e0-7c6a-43e8-8b2f-8a0d0e8b0f3c\", \"run_id\": \"2a2b3c4d-5e6f-7081-92a3-b4c5d6e7f809\", \"detector_id\": \"scan.nuclei\", \"title\": \"X-Frame-Options header missing\", \"severity\": \"low\", \"resource\": \"https://app.acme.com/\", \"evidence\": { \"request_id\": \"r-01HXXQ5AKP0NWS\", \"template\": \"misconfig/headers/xfo-missing.yaml\" }, \"cwe\": 693, \"owasp\": \"A05\", \"cve_ids\": [], \"cvss\": 3.7, \"mitre_attack\": [\"T1190\"], \"poc_links\": [], \"created_at\": \"2025-10-07T08:22:31Z\", \"provenance\": { \"tool_version\": \"nuclei 3.1.0\", \"host\": \"runner-12\" } } def normalize(raw: dict, source: str) -> Finding: severity_map = {\"info\": \"info\", \"low\": \"low\", \"medium\": \"medium\", \"high\": \"high\", \"critical\": \"critical\"} return Finding( id=str(uuid4()), project_id=raw.get(\"project_id\"), detector_id=source, title=raw.get(\"info\", {}).get(\"name\") or raw.get(\"title\", \"Unnamed Finding\"), severity=severity_map.get(raw.get(\"severity\", \"info\")), path=raw.get(\"matched-at\") or raw.get(\"url\"), evidence=raw, created_at=datetime.utcnow(), provenance={\"tool\": source, \"version\": raw.get(\"version\", \"unknown\")} )","title":"Example Normalized Finding"},{"location":"architecture/12-findings-model-and-schema/#normalization-rules-by-source","text":"Tool Input Format Mapping Nuclei JSON lines info.name \u2192 title , info.severity \u2192 severity , matched-at \u2192 path Feroxbuster Text URL \u2192 path , status \u2192 evidence.status ZAP/Burp XML/JSON PluginId \u2192 cwe , RiskDesc \u2192 severity Caido SQLite Vulnerability.name \u2192 title , score \u2192 cvss_score Custom Detectors Python dict Arbitrary fields normalized via schema mapping Normalization is performed by findings-engine using source-specific adapters.","title":"\ud83e\udde9 Normalization Rules by Source"},{"location":"architecture/12-findings-model-and-schema/#severity-mapping","text":"Raw Severity Normalized CVSS Equivalent informational info 0.0\u20133.9 low low 4.0\u20136.9 medium medium 6.0\u20137.4 high high 7.5\u20138.9 critical critical 9.0\u201310.0","title":"\ud83e\udde0 Severity Mapping"},{"location":"architecture/12-findings-model-and-schema/#deduplication-strategy","text":"Findings are hashed on a deterministic fingerprint: hash_input = f\"{finding.path}:{finding.title}:{finding.detector_id}\" finding_hash = hashlib.sha256(hash_input.encode()).hexdigest() If the hash already exists in the same project and run scope, the finding is merged rather than duplicated.","title":"\ud83e\udde0 Deduplication Strategy"},{"location":"architecture/12-findings-model-and-schema/#enrichment-metadata-structure","text":"finding.enrichment = { \"cwe\": {\"id\": 89, \"name\": \"SQL Injection\"}, \"owasp\": {\"category\": \"A03\", \"name\": \"Injection\"}, \"CVSS\": {\"score\": 9.1, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\"}, \"mitre\": {\"technique\": \"T1505\", \"tactic\": \"Persistence\"}, \"poc\": {\"source\": \"exploitdb\", \"link\": \"https://www.exploit-db.com/exploits/52341\"} } This metadata is populated during the Enrichment Phase (CVE/CWE/CVSS sync).","title":"\ud83e\udde9 Enrichment Metadata Structure"},{"location":"architecture/12-findings-model-and-schema/#cve-cwe-owasp-mapping","text":"","title":"\ud83e\udde9 CVE / CWE / OWASP Mapping"},{"location":"architecture/12-findings-model-and-schema/#cve-cwe","text":"NVD API provides cve.affects.vendor.vendor_data \u2192 cwe.id Mapping stored in local SQLite cache.","title":"CVE \u2192 CWE"},{"location":"architecture/12-findings-model-and-schema/#cwe-owasp","text":"CWE OWASP Category CWE-79 A03: Injection CWE-89 A03: Injection CWE-287 A07: Identification and Authentication Failures CWE-601 A10: Server-Side Request Forgery (SSRF)","title":"CWE \u2192 OWASP"},{"location":"architecture/12-findings-model-and-schema/#example-mapping-resolver","text":"def resolve_owasp(cwe_id): mapping = {\"79\": \"A03\", \"89\": \"A03\", \"287\": \"A07\", \"601\": \"A10\"} return mapping.get(str(cwe_id))","title":"Example Mapping Resolver"},{"location":"architecture/12-findings-model-and-schema/#confidence-risk-scoring","text":"Confidence combines tool reliability, correlation consistency, and enrichment coverage.","title":"\ud83e\udde9 Confidence &amp; Risk Scoring"},{"location":"architecture/12-findings-model-and-schema/#formula","text":"confidence = (tool_weight * 0.5) + (enrichment_score * 0.3) + (cross_source_count * 0.2)","title":"Formula"},{"location":"architecture/12-findings-model-and-schema/#risk-score-calculation","text":"risk_score = CVSS * confidence This allows probabilistic triage prioritization.","title":"Risk Score Calculation"},{"location":"architecture/12-findings-model-and-schema/#evidence-normalization","text":"Evidence is stored in compact, structured form for indexing: { \"request\": { \"method\": \"POST\", \"url\": \"https://target.com/login\", \"headers\": {\"Content-Type\": \"application/json\"}, \"body\": \"{\\\"username\\\":\\\"admin\\\"}\" }, \"response\": { \"status\": 500, \"headers\": {\"Server\": \"Apache\"}, \"body_snippet\": \"SQL syntax error\" } } Large payloads are truncated or compressed to avoid storage overhead.","title":"\ud83e\udde0 Evidence Normalization"},{"location":"architecture/12-findings-model-and-schema/#finding-status-lifecycle","text":"Status Meaning Managed By open Newly discovered issue Scanner triaged Analyst reviewed Analyst resolved Fixed or confirmed Analyst false_positive Invalid finding Analyst archived Expired or obsolete System (GC) Each status change triggers an audit log event and optional webhook notification.","title":"\ud83e\udde9 Finding Status Lifecycle"},{"location":"architecture/12-findings-model-and-schema/#storage-layer-integration","text":"Findings are persisted via the StoragePort interface: class FindingsRepository(Protocol): def save(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list(self, project_id: str) -> List[Finding]: \"\"\"List findings for a project.\"\"\" pass def get(self, id: str) -> Finding: \"\"\"Get a finding by ID.\"\"\" pass","title":"\ud83e\uddf1 Storage Layer Integration"},{"location":"architecture/12-findings-model-and-schema/#supported-backends","text":"SQLite (default local mode) PostgreSQL (production multi-project) JSON (testing or demo mode)","title":"Supported backends:"},{"location":"architecture/12-findings-model-and-schema/#findings-export-schema","text":"SecFlow exports findings in structured formats for interoperability: Format Command JSON SecFlow export findings --format json CSV SecFlow export findings --format csv HTML SecFlow report findings --template summary.html SARIF SecFlow export findings --format sarif","title":"\ud83e\udde9 Findings Export Schema"},{"location":"architecture/12-findings-model-and-schema/#example-json-export","text":"{ \"project\": \"acme-api\", \"findings\": [ { \"id\": \"a2b4f3c8-d9e2-4f1a-5b6c-7d8e9f0a1b2c\", \"title\": \"SQL Injection\", \"severity\": \"critical\", \"cwe\": 89, \"cvss_score\": 9.1, \"path\": \"/login\", \"created_at\": \"2025-10-06T10:15:00Z\" } ] }","title":"Example JSON export:"},{"location":"architecture/12-findings-model-and-schema/#indexing-analytics","text":"Each finding is indexed in the analytics database: Primary Index: project_id + severity + cwe Full-Text Search: title , path , evidence.body_snippet Aggregations: count by severity, top affected endpoints, common CWE classes. The metrics system (see 06-plugin-system.md & 17-observability-logging-and-metrics.md ) uses these indexes to generate dashboards.","title":"\ud83e\udde0 Indexing &amp; Analytics"},{"location":"architecture/12-findings-model-and-schema/#future-enhancements","text":"Graph-based finding correlation (attack paths). AI-driven risk clustering (\"find similar vulnerabilities\"). Contextual auto-triage (OWASP/NIST mapping feedback loops). Delta reports between runs ( run_id diff). Live sync to vulnerability management platforms (DefectDojo, VulnDB). Next: CVE/CWE/POC Enrichment Layer","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/","text":"13 \u2014 CVE/CWE/POC Enrichment Layer \u00b6 \ud83e\udded Overview \u00b6 The Enrichment Layer bridges the gap between raw scanner results and full vulnerability intelligence. It performs automated correlation between findings and vulnerability databases such as: CVE/NVD \u2014 canonical vulnerability records CWE \u2014 weakness categorization CVSS \u2014 scoring and severity models Exploit-DB , Vulners , Metasploit , OSV \u2014 PoC and exploit references MITRE ATT&CK \u2014 behavioral mapping SecFlow treats enrichment as a pluggable knowledge graph layer that can be extended via simple JSON/YAML definitions or API connectors. \u2699\ufe0f Enrichment Pipeline \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Normalized Finding\"] B[\"CPE Identifier Extraction\"] C[\"CVE Resolver (NVD, OSV, Vulners)\"] D[\"CWE / OWASP Mapping\"] E[\"CVSS Calculation\"] F[\"POC + Exploit Lookup\"] G[\"MITRE ATT&CK Correlation\"] H[\"Final Enriched Finding\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H \ud83e\udde9 CPE & Component Extraction \u00b6 The enrichment process begins by fingerprinting software components. def extract_cpe(finding: Finding) -> Optional[str]: headers = finding.evidence.get(\"response\", {}).get(\"headers\", {}) banner = headers.get(\"Server\") or headers.get(\"X-Powered-By\") return cpe_guess(banner) if banner else None Example results: \u00b6 Server: Apache/2.4.54 \u2192 cpe:/a:apache:http_server:2.4.54 X-Powered-By: Express \u2192 cpe:/a:npmjs:express:4.18.2 \ud83e\udde9 CVE Resolution Engine \u00b6 The CVE Resolver queries multiple backends in a failover chain: Source Endpoint Rate Limit Cache TTL NVD https://services.nvd.nist.gov/rest/json/cves/2.0 1000/day 24h OSV.dev https://api.osv.dev/v1/query Unlimited 12h Vulners API https://vulners.com/api/v3/search/lucene/ 2000/day 24h class CVEResolver: def resolve(self, cpe: str) -> List[dict]: cached = self.cache.get(cpe) if cached: return cached results = [] for backend in self.backends: try: results.extend(backend.query(cpe)) except Exception: continue self.cache.set(cpe, results) return results Results are merged and normalized into a unified CVE format. \ud83e\udde0 CVE Normalized Model \u00b6 class CVEEntry(BaseModel): cve_id: str description: str published: datetime cvss_score: float cvss_vector: str cwe_ids: List[int] references: List[str] exploit_refs: List[str] source: str Each finding may be associated with multiple CVE entries. \ud83e\udde9 CWE / OWASP / MITRE Mapping \u00b6 Once CVEs are linked, weaknesses and behavioral context are resolved. Source Purpose Mapping Strategy CWE Weakness classification CVE \u2192 CWE via NVD JSON OWASP Application risk class CWE \u2192 OWASP Top 10 map MITRE ATT&CK Adversary tactics/techniques CWE \u2192 ATT&CK TID correlation def map_cwe_to_owasp(cwe_id: int) -> str: mapping = { 79: \"A03: Injection\", 89: \"A03: Injection\", 787: \"A05: Buffer Overflow\", 601: \"A10: SSRF\" } return mapping.get(cwe_id, \"N/A\") MITRE correlation example: \u00b6 mitre_map = { \"CWE-79\": \"T1059.007 (Cross-Site Scripting)\", \"CWE-89\": \"T1505.003 (SQL Injection)\" } \ud83e\udde9 CVSS Calculation \u00b6 If a finding lacks explicit CVSS scoring, SecFlow derives one via heuristics: def derive_cvss(cwe_id: int, context: dict) -> float: # basic fallback estimation if cwe_id in (79, 89): return 9.0 elif cwe_id in (200, 201): return 7.5 return 5.0 Final score combines: \u00b6 base = CVSS temporal = exploit_availability * 0.2 environmental = exposure_factor * 0.3 final = (base + temporal + environmental) / 1.5 \ud83e\udde0 PoC & Exploit Correlation \u00b6 Data Sources \u00b6 Source Access Notes Exploit-DB Public dump Weekly sync Vulners API Indexed by CVE Metasploit Local metadata Optional GitHub PoC OSV + GitHub GraphQL Filter by repo tags SecurityFocus (legacy) Offline mirror Static references Example Resolver \u00b6 def resolve_poc(cve_id: str) -> list: sources = [exploitdb, vulners, githubpoc] results = [] for s in sources: results.extend(s.search(cve_id)) return list(set(results)) \ud83e\udde9 PoC Safety Governance \u00b6 Because PoCs can contain malicious payloads, SecFlow enforces strict isolation. Policy Enforcement Read-only storage PoCs stored as text blobs, no exec permission Sandbox validation Hash-check before use Legal disclaimer Must be accepted before PoC download Runtime restriction Execution allowed only in --sandbox mode Example governance guard: \u00b6 def safe_open_poc(poc_path: Path): if not user.accepted_disclaimer: raise PermissionError(\"PoC execution disabled until disclaimer accepted.\") subprocess.run([\"sandbox\", \"python3\", poc_path]) \ud83e\udde9 Caching & Synchronization \u00b6 Each enrichment source maintains a versioned local cache: Component Backend Format TTL CVE NVD JSON SQLite 24h CWE MITRE XML JSON 7d PoC Exploit-DB FS/JSON 14d Example cache adapter: \u00b6 class LocalCache: def get(self, key: str) -> Optional[Any]: \"\"\"Get value from cache by key.\"\"\" pass def set(self, key: str, value: Any, ttl: int) -> None: \"\"\"Set value in cache with TTL.\"\"\" pass def purge_expired(self) -> None: \"\"\"Remove expired entries from cache.\"\"\" pass \ud83e\udde9 API Exposure \u00b6 The enrichment system provides a unified query interface: Endpoint Method Description /api/v1/enrich/cve POST Enrich a finding by CPE/CVE /api/v1/enrich/cwe POST Map CWE to OWASP /api/v1/enrich/poc GET Retrieve PoC links /api/v1/enrich/status GET Show cache health Example response: \u00b6 { \"finding_id\": \"1234\", \"cve\": [\"CVE-2024-12345\"], \"CVSS\": 9.8, \"cwe\": 89, \"owasp\": \"A03: Injection\", \"poc_links\": [\"https://exploit-db.com/exploits/52341\"] } \ud83e\udde9 Enrichment Rules & Priority \u00b6 Local cache first API sources (NVD/OSV) second Third-party mirrors (Vulners, Exploit-DB) last Each backend includes retry and circuit-breaker logic via Tenacity. \ud83e\udde9 Parallel Enrichment \u00b6 The enrichment worker uses async pipelines for batch enrichment: async def enrich_findings_batch(findings: list): async with aiohttp.ClientSession() as session: tasks = [enrich_one(f, session) for f in findings] return await asyncio.gather(*tasks) Each finding may take 0.1\u20131.5 seconds depending on CVE count; concurrency keeps throughput high. \ud83e\udde0 Example Enrichment Output \u00b6 { \"finding_id\": \"abcd-1234\", \"cpe\": \"cpe:/a:apache:http_server:2.4.54\", \"cve_ids\": [\"CVE-2023-25690\"], \"cwe\": 89, \"owasp\": \"A03: Injection\", \"cvss_score\": 9.8, \"poc_links\": [\"https://exploit-db.com/exploits/52341\"], \"mitre_tid\": \"T1505.003\", \"last_enriched\": \"2025-10-06T09:43:00Z\" } \ud83d\udd12 Security & Compliance \u00b6 All enrichment data is public-source only. No proprietary or restricted databases. Caching system automatically purges expired CVE entries. Full audit trail for every enrichment event. \ud83d\udd2e Future Enhancements \u00b6 Integrate VulnCheck, CISA KEV, and EPSS for exploit prediction. Introduce AI-based CVE clustering for semantic matching. Enrichment correlation graph for cross-project analysis. Automated risk re-scoring based on KEV updates. Next: POC Sources, Safety & Legal Guidelines","title":"Enrichment Layer"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#13-cvecwepoc-enrichment-layer","text":"","title":"13 \u2014 CVE/CWE/POC Enrichment Layer"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#overview","text":"The Enrichment Layer bridges the gap between raw scanner results and full vulnerability intelligence. It performs automated correlation between findings and vulnerability databases such as: CVE/NVD \u2014 canonical vulnerability records CWE \u2014 weakness categorization CVSS \u2014 scoring and severity models Exploit-DB , Vulners , Metasploit , OSV \u2014 PoC and exploit references MITRE ATT&CK \u2014 behavioral mapping SecFlow treats enrichment as a pluggable knowledge graph layer that can be extended via simple JSON/YAML definitions or API connectors.","title":"\ud83e\udded Overview"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#enrichment-pipeline","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Normalized Finding\"] B[\"CPE Identifier Extraction\"] C[\"CVE Resolver (NVD, OSV, Vulners)\"] D[\"CWE / OWASP Mapping\"] E[\"CVSS Calculation\"] F[\"POC + Exploit Lookup\"] G[\"MITRE ATT&CK Correlation\"] H[\"Final Enriched Finding\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H","title":"\u2699\ufe0f Enrichment Pipeline"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cpe-component-extraction","text":"The enrichment process begins by fingerprinting software components. def extract_cpe(finding: Finding) -> Optional[str]: headers = finding.evidence.get(\"response\", {}).get(\"headers\", {}) banner = headers.get(\"Server\") or headers.get(\"X-Powered-By\") return cpe_guess(banner) if banner else None","title":"\ud83e\udde9 CPE &amp; Component Extraction"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-results","text":"Server: Apache/2.4.54 \u2192 cpe:/a:apache:http_server:2.4.54 X-Powered-By: Express \u2192 cpe:/a:npmjs:express:4.18.2","title":"Example results:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cve-resolution-engine","text":"The CVE Resolver queries multiple backends in a failover chain: Source Endpoint Rate Limit Cache TTL NVD https://services.nvd.nist.gov/rest/json/cves/2.0 1000/day 24h OSV.dev https://api.osv.dev/v1/query Unlimited 12h Vulners API https://vulners.com/api/v3/search/lucene/ 2000/day 24h class CVEResolver: def resolve(self, cpe: str) -> List[dict]: cached = self.cache.get(cpe) if cached: return cached results = [] for backend in self.backends: try: results.extend(backend.query(cpe)) except Exception: continue self.cache.set(cpe, results) return results Results are merged and normalized into a unified CVE format.","title":"\ud83e\udde9 CVE Resolution Engine"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cve-normalized-model","text":"class CVEEntry(BaseModel): cve_id: str description: str published: datetime cvss_score: float cvss_vector: str cwe_ids: List[int] references: List[str] exploit_refs: List[str] source: str Each finding may be associated with multiple CVE entries.","title":"\ud83e\udde0 CVE Normalized Model"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cwe-owasp-mitre-mapping","text":"Once CVEs are linked, weaknesses and behavioral context are resolved. Source Purpose Mapping Strategy CWE Weakness classification CVE \u2192 CWE via NVD JSON OWASP Application risk class CWE \u2192 OWASP Top 10 map MITRE ATT&CK Adversary tactics/techniques CWE \u2192 ATT&CK TID correlation def map_cwe_to_owasp(cwe_id: int) -> str: mapping = { 79: \"A03: Injection\", 89: \"A03: Injection\", 787: \"A05: Buffer Overflow\", 601: \"A10: SSRF\" } return mapping.get(cwe_id, \"N/A\")","title":"\ud83e\udde9 CWE / OWASP / MITRE Mapping"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#mitre-correlation-example","text":"mitre_map = { \"CWE-79\": \"T1059.007 (Cross-Site Scripting)\", \"CWE-89\": \"T1505.003 (SQL Injection)\" }","title":"MITRE correlation example:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cvss-calculation","text":"If a finding lacks explicit CVSS scoring, SecFlow derives one via heuristics: def derive_cvss(cwe_id: int, context: dict) -> float: # basic fallback estimation if cwe_id in (79, 89): return 9.0 elif cwe_id in (200, 201): return 7.5 return 5.0","title":"\ud83e\udde9 CVSS Calculation"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#final-score-combines","text":"base = CVSS temporal = exploit_availability * 0.2 environmental = exposure_factor * 0.3 final = (base + temporal + environmental) / 1.5","title":"Final score combines:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#poc-exploit-correlation","text":"","title":"\ud83e\udde0 PoC &amp; Exploit Correlation"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#data-sources","text":"Source Access Notes Exploit-DB Public dump Weekly sync Vulners API Indexed by CVE Metasploit Local metadata Optional GitHub PoC OSV + GitHub GraphQL Filter by repo tags SecurityFocus (legacy) Offline mirror Static references","title":"Data Sources"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-resolver","text":"def resolve_poc(cve_id: str) -> list: sources = [exploitdb, vulners, githubpoc] results = [] for s in sources: results.extend(s.search(cve_id)) return list(set(results))","title":"Example Resolver"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#poc-safety-governance","text":"Because PoCs can contain malicious payloads, SecFlow enforces strict isolation. Policy Enforcement Read-only storage PoCs stored as text blobs, no exec permission Sandbox validation Hash-check before use Legal disclaimer Must be accepted before PoC download Runtime restriction Execution allowed only in --sandbox mode","title":"\ud83e\udde9 PoC Safety Governance"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-governance-guard","text":"def safe_open_poc(poc_path: Path): if not user.accepted_disclaimer: raise PermissionError(\"PoC execution disabled until disclaimer accepted.\") subprocess.run([\"sandbox\", \"python3\", poc_path])","title":"Example governance guard:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#caching-synchronization","text":"Each enrichment source maintains a versioned local cache: Component Backend Format TTL CVE NVD JSON SQLite 24h CWE MITRE XML JSON 7d PoC Exploit-DB FS/JSON 14d","title":"\ud83e\udde9 Caching &amp; Synchronization"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-cache-adapter","text":"class LocalCache: def get(self, key: str) -> Optional[Any]: \"\"\"Get value from cache by key.\"\"\" pass def set(self, key: str, value: Any, ttl: int) -> None: \"\"\"Set value in cache with TTL.\"\"\" pass def purge_expired(self) -> None: \"\"\"Remove expired entries from cache.\"\"\" pass","title":"Example cache adapter:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#api-exposure","text":"The enrichment system provides a unified query interface: Endpoint Method Description /api/v1/enrich/cve POST Enrich a finding by CPE/CVE /api/v1/enrich/cwe POST Map CWE to OWASP /api/v1/enrich/poc GET Retrieve PoC links /api/v1/enrich/status GET Show cache health","title":"\ud83e\udde9 API Exposure"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-response","text":"{ \"finding_id\": \"1234\", \"cve\": [\"CVE-2024-12345\"], \"CVSS\": 9.8, \"cwe\": 89, \"owasp\": \"A03: Injection\", \"poc_links\": [\"https://exploit-db.com/exploits/52341\"] }","title":"Example response:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#enrichment-rules-priority","text":"Local cache first API sources (NVD/OSV) second Third-party mirrors (Vulners, Exploit-DB) last Each backend includes retry and circuit-breaker logic via Tenacity.","title":"\ud83e\udde9 Enrichment Rules &amp; Priority"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#parallel-enrichment","text":"The enrichment worker uses async pipelines for batch enrichment: async def enrich_findings_batch(findings: list): async with aiohttp.ClientSession() as session: tasks = [enrich_one(f, session) for f in findings] return await asyncio.gather(*tasks) Each finding may take 0.1\u20131.5 seconds depending on CVE count; concurrency keeps throughput high.","title":"\ud83e\udde9 Parallel Enrichment"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-enrichment-output","text":"{ \"finding_id\": \"abcd-1234\", \"cpe\": \"cpe:/a:apache:http_server:2.4.54\", \"cve_ids\": [\"CVE-2023-25690\"], \"cwe\": 89, \"owasp\": \"A03: Injection\", \"cvss_score\": 9.8, \"poc_links\": [\"https://exploit-db.com/exploits/52341\"], \"mitre_tid\": \"T1505.003\", \"last_enriched\": \"2025-10-06T09:43:00Z\" }","title":"\ud83e\udde0 Example Enrichment Output"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#security-compliance","text":"All enrichment data is public-source only. No proprietary or restricted databases. Caching system automatically purges expired CVE entries. Full audit trail for every enrichment event.","title":"\ud83d\udd12 Security &amp; Compliance"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#future-enhancements","text":"Integrate VulnCheck, CISA KEV, and EPSS for exploit prediction. Introduce AI-based CVE clustering for semantic matching. Enrichment correlation graph for cross-project analysis. Automated risk re-scoring based on KEV updates. Next: POC Sources, Safety & Legal Guidelines","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/14-poc-sources-and-legal-guidelines/","text":"14 \u2014 PoC Governance, Safety, and Legal Framework \u00b6 \ud83e\udded Overview \u00b6 Proof-of-Concept (PoC) execution is the most sensitive aspect of the SecFlow platform. While enrichment and discovery involve data collection, PoC modules may trigger actual exploits \u2014 often on controlled test targets or lab environments. This section defines: - The security model for PoC management and execution. - Legal and ethical boundaries for researchers. - Technical enforcement of sandbox isolation. - Policy hierarchy for PoC lifecycle, provenance, and deletion. \u2696\ufe0f Legal & Ethical Framework \u00b6 1. Authorized Testing Only \u00b6 PoC execution is strictly limited to: - Assets owned or explicitly authorized by the user. - Lab or offline sandbox targets. - Non-production systems unless a signed authorization exists. 2. Compliance with International Norms \u00b6 SecFlow aligns with: - ISO/IEC 29147 \u2014 Vulnerability Disclosure. - ISO/IEC 30111 \u2014 Vulnerability Handling. - NIST SP 800-115 \u2014 Technical Guide to Information Security Testing. - CISA KEV and CVE\u00ae Program standards. 3. Researcher Agreement \u00b6 Before using any exploit-related features: - Users must accept the Responsible Use Agreement (RUA) . - Every PoC download or execution is logged. - No redistribution of PoC code outside permitted licensing (e.g., GPL, BSD). \ud83e\uddf1 PoC Lifecycle Management \u00b6 Stage Description Acquisition PoC fetched from trusted sources (Exploit-DB, Vulners, GitHub PoC, OSV). Verification Signature/hash validated; metadata normalized. Classification Marked as local, remote, DoS, or privilege escalation. Sandbox Execution Run only in isolated containers or VMs. Audit Logging Execution parameters, user, and timestamp recorded. Retention & Purge Expired PoCs automatically archived or deleted. \ud83e\udde9 Storage & Metadata \u00b6 PoCs are stored as immutable artifacts under the internal resource store: %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/resources/poc/\"] B[\"CVE-2024-12345/\"] C[\"exploit.py\"] D[\"metadata.json\"] E[\"README.md\"] F[\"CVE-2023-98765/\"] A --> B A --> F B --> C B --> D B --> E Example metadata.json \u00b6 { \"cve\": \"CVE-2024-12345\", \"source\": \"exploit-db\", \"url\": \"https://www.exploit-db.com/exploits/52341\", \"verified\": true, \"language\": \"python\", \"type\": \"rce\", \"hash\": \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\", \"sandbox_only\": true, \"license\": \"GPLv2\", \"date_fetched\": \"2025-10-06T10:20:00Z\" } \ud83d\udd12 Sandbox Execution Architecture \u00b6 1. Isolation Model \u00b6 All PoC executions occur within the SecFlow Sandbox Runtime, implemented as: Docker container with restricted capabilities ( --cap-drop=ALL ). Read-only mount of PoC files. No network egress unless explicitly allowed. Time & memory quotas enforced by cgroups. 2. Runtime Diagram \u00b6 +-------------------------------------------------------+ | SecFlow Sandbox Runtime | |-------------------------------------------------------| | - Namespace Isolation (PID, NET, MNT) | | - Read-only FS for /poc | | - IPC + SYS_ADMIN disabled | | - AppArmor / SELinux profiles | | - Seccomp filters (deny dangerous syscalls) | +-------------------------------------------------------+ \u2191 | PoC artifact + parameters | [SecFlow Worker] \u2192 [Sandbox Orchestrator] \u2192 [Container Runtime] 3. Sample Sandbox Invocation \u00b6 SecFlow sandbox run poc CVE-2024-12345 --target https://staging.example.com Under the hood: subprocess.run([ \"docker\", \"run\", \"--rm\", \"--network\", \"none\", \"--memory\", \"512m\", \"--cpus\", \"1\", \"-v\", \"/pocstore/CVE-2024-12345:/poc:ro\", \"SecFlow-sandbox:latest\", \"python3\", \"/poc/exploit.py\", \"--target\", \"https://staging.example.com\" ]) \u2699\ufe0f PoC Execution Policy \u00b6 Policy Enforcement Sandbox Only No PoC runs on host system. Read-only Filesystem Prevents code modification or persistence. No Network by Default All outbound connections blocked. User Authorization Each run signed with user identity & timestamp. Logging & Replay Stdout/stderr captured in audit logs. Time-Bound Execution Hard kill if runtime exceeds timeout_seconds . \ud83e\udde0 Policy Configuration Example \u00b6 # ~/.SecFlow/policies/poc.yaml sandbox: image: SecFlow-sandbox:latest max_cpu: 1 max_memory_mb: 512 timeout_seconds: 300 allow_network: false allow_filesystem_write: false compliance: require_disclaimer: true require_project_authorization: true auto_verify_hashes: true \ud83e\udde9 Governance Logging \u00b6 Every PoC-related event is appended to a tamper-resistant audit log: Field Description event_id UUID of the audit entry user_id Executing user timestamp UTC ISO-8601 time action e.g., \"sandbox_run\", \"download\", \"verify\" cve_id Related CVE tool Source or wrapper (e.g., \"exploitdb\") sandbox_id Container identifier hash SHA256 of PoC code Example Log Entry \u00b6 { \"event_id\": \"a1f3b7c2-9f11-45d0-bc97-6c9472cbdcb2\", \"user_id\": \"hernan\", \"action\": \"sandbox_run\", \"cve_id\": \"CVE-2024-12345\", \"timestamp\": \"2025-10-06T10:43:00Z\", \"sandbox_id\": \"sandbox-83214\", \"hash\": \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\", \"tool\": \"exploitdb\" } \ud83e\udde0 Legal Notice Enforcement \u00b6 Before any PoC interaction, users must sign a legal disclaimer: Responsible Use Notice: You acknowledge that PoC exploitation is to be performed exclusively on systems you own or are explicitly authorized to test. SecFlow is not liable for any misuse or damages resulting from unauthorized use. The system stores an acceptance hash: ~/.SecFlow/.disclaimer_accepted \ud83e\udde9 Cross-Project PoC Access \u00b6 To prevent accidental disclosure: - PoC artifacts are scoped per project by default. - Shared PoCs require explicit admin approval. - Access controlled via role-based permissions (RBAC). Role Access Admin Global & project PoCs Analyst Project PoCs only Viewer Read-only (metadata only) \ud83e\uddf1 PoC Verification Workflow \u00b6 graph TD A[Download PoC] --> B[Verify Hash & Signature] B --> C{Trusted Source?} C -->|Yes| D[Mark Verified] C -->|No| E[Quarantine] E --> F[Manual Review Required] D --> G[Available for Sandbox Run] \ud83d\udd12 Quarantine Mechanism \u00b6 Unknown or tampered PoCs are moved to: ~/.SecFlow/resources/poc/quarantine/ Each is tagged with a quarantine reason. Admins can review and promote back to verified status. def quarantine_poc(poc_id: str, reason: str): shutil.move(f\"/pocstore/{poc_id}\", \"/pocstore/quarantine/\") write_log(f\"PoC {poc_id} quarantined: {reason}\") \ud83e\udde0 Example Execution Trace \u00b6 [PoC: CVE-2024-12345] Verified source: ExploitDB [Sandbox] Starting container SecFlow-sandbox:latest [Sandbox] CPU quota: 1 core, Memory: 512MB [Sandbox] Network: disabled [Output] [+] Exploit successful: remote command executed [Cleanup] PoC container destroyed after 120s \ud83d\udd2e Future Enhancements \u00b6 AppArmor enforcement policies with dynamic runtime profiling. PoC provenance blockchain for cryptographic integrity. AI-assisted PoC safety classification (RCE, DoS, PrivEsc). Multi-tenant isolation for collaborative workspaces. Live replay of PoC runs for training and documentation. Next: Garbage Collection & Data Retention Policy","title":"PoC Sources & Legal"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#14-poc-governance-safety-and-legal-framework","text":"","title":"14 \u2014 PoC Governance, Safety, and Legal Framework"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#overview","text":"Proof-of-Concept (PoC) execution is the most sensitive aspect of the SecFlow platform. While enrichment and discovery involve data collection, PoC modules may trigger actual exploits \u2014 often on controlled test targets or lab environments. This section defines: - The security model for PoC management and execution. - Legal and ethical boundaries for researchers. - Technical enforcement of sandbox isolation. - Policy hierarchy for PoC lifecycle, provenance, and deletion.","title":"\ud83e\udded Overview"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#legal-ethical-framework","text":"","title":"\u2696\ufe0f Legal &amp; Ethical Framework"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#1-authorized-testing-only","text":"PoC execution is strictly limited to: - Assets owned or explicitly authorized by the user. - Lab or offline sandbox targets. - Non-production systems unless a signed authorization exists.","title":"1. Authorized Testing Only"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#2-compliance-with-international-norms","text":"SecFlow aligns with: - ISO/IEC 29147 \u2014 Vulnerability Disclosure. - ISO/IEC 30111 \u2014 Vulnerability Handling. - NIST SP 800-115 \u2014 Technical Guide to Information Security Testing. - CISA KEV and CVE\u00ae Program standards.","title":"2. Compliance with International Norms"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#3-researcher-agreement","text":"Before using any exploit-related features: - Users must accept the Responsible Use Agreement (RUA) . - Every PoC download or execution is logged. - No redistribution of PoC code outside permitted licensing (e.g., GPL, BSD).","title":"3. Researcher Agreement"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#poc-lifecycle-management","text":"Stage Description Acquisition PoC fetched from trusted sources (Exploit-DB, Vulners, GitHub PoC, OSV). Verification Signature/hash validated; metadata normalized. Classification Marked as local, remote, DoS, or privilege escalation. Sandbox Execution Run only in isolated containers or VMs. Audit Logging Execution parameters, user, and timestamp recorded. Retention & Purge Expired PoCs automatically archived or deleted.","title":"\ud83e\uddf1 PoC Lifecycle Management"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#storage-metadata","text":"PoCs are stored as immutable artifacts under the internal resource store: %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/resources/poc/\"] B[\"CVE-2024-12345/\"] C[\"exploit.py\"] D[\"metadata.json\"] E[\"README.md\"] F[\"CVE-2023-98765/\"] A --> B A --> F B --> C B --> D B --> E","title":"\ud83e\udde9 Storage &amp; Metadata"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#example-metadatajson","text":"{ \"cve\": \"CVE-2024-12345\", \"source\": \"exploit-db\", \"url\": \"https://www.exploit-db.com/exploits/52341\", \"verified\": true, \"language\": \"python\", \"type\": \"rce\", \"hash\": \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\", \"sandbox_only\": true, \"license\": \"GPLv2\", \"date_fetched\": \"2025-10-06T10:20:00Z\" }","title":"Example metadata.json"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#sandbox-execution-architecture","text":"","title":"\ud83d\udd12 Sandbox Execution Architecture"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#1-isolation-model","text":"All PoC executions occur within the SecFlow Sandbox Runtime, implemented as: Docker container with restricted capabilities ( --cap-drop=ALL ). Read-only mount of PoC files. No network egress unless explicitly allowed. Time & memory quotas enforced by cgroups.","title":"1. Isolation Model"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#2-runtime-diagram","text":"+-------------------------------------------------------+ | SecFlow Sandbox Runtime | |-------------------------------------------------------| | - Namespace Isolation (PID, NET, MNT) | | - Read-only FS for /poc | | - IPC + SYS_ADMIN disabled | | - AppArmor / SELinux profiles | | - Seccomp filters (deny dangerous syscalls) | +-------------------------------------------------------+ \u2191 | PoC artifact + parameters | [SecFlow Worker] \u2192 [Sandbox Orchestrator] \u2192 [Container Runtime]","title":"2. Runtime Diagram"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#3-sample-sandbox-invocation","text":"SecFlow sandbox run poc CVE-2024-12345 --target https://staging.example.com Under the hood: subprocess.run([ \"docker\", \"run\", \"--rm\", \"--network\", \"none\", \"--memory\", \"512m\", \"--cpus\", \"1\", \"-v\", \"/pocstore/CVE-2024-12345:/poc:ro\", \"SecFlow-sandbox:latest\", \"python3\", \"/poc/exploit.py\", \"--target\", \"https://staging.example.com\" ])","title":"3. Sample Sandbox Invocation"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#poc-execution-policy","text":"Policy Enforcement Sandbox Only No PoC runs on host system. Read-only Filesystem Prevents code modification or persistence. No Network by Default All outbound connections blocked. User Authorization Each run signed with user identity & timestamp. Logging & Replay Stdout/stderr captured in audit logs. Time-Bound Execution Hard kill if runtime exceeds timeout_seconds .","title":"\u2699\ufe0f PoC Execution Policy"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#policy-configuration-example","text":"# ~/.SecFlow/policies/poc.yaml sandbox: image: SecFlow-sandbox:latest max_cpu: 1 max_memory_mb: 512 timeout_seconds: 300 allow_network: false allow_filesystem_write: false compliance: require_disclaimer: true require_project_authorization: true auto_verify_hashes: true","title":"\ud83e\udde0 Policy Configuration Example"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#governance-logging","text":"Every PoC-related event is appended to a tamper-resistant audit log: Field Description event_id UUID of the audit entry user_id Executing user timestamp UTC ISO-8601 time action e.g., \"sandbox_run\", \"download\", \"verify\" cve_id Related CVE tool Source or wrapper (e.g., \"exploitdb\") sandbox_id Container identifier hash SHA256 of PoC code","title":"\ud83e\udde9 Governance Logging"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#example-log-entry","text":"{ \"event_id\": \"a1f3b7c2-9f11-45d0-bc97-6c9472cbdcb2\", \"user_id\": \"hernan\", \"action\": \"sandbox_run\", \"cve_id\": \"CVE-2024-12345\", \"timestamp\": \"2025-10-06T10:43:00Z\", \"sandbox_id\": \"sandbox-83214\", \"hash\": \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\", \"tool\": \"exploitdb\" }","title":"Example Log Entry"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#legal-notice-enforcement","text":"Before any PoC interaction, users must sign a legal disclaimer: Responsible Use Notice: You acknowledge that PoC exploitation is to be performed exclusively on systems you own or are explicitly authorized to test. SecFlow is not liable for any misuse or damages resulting from unauthorized use. The system stores an acceptance hash: ~/.SecFlow/.disclaimer_accepted","title":"\ud83e\udde0 Legal Notice Enforcement"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#cross-project-poc-access","text":"To prevent accidental disclosure: - PoC artifacts are scoped per project by default. - Shared PoCs require explicit admin approval. - Access controlled via role-based permissions (RBAC). Role Access Admin Global & project PoCs Analyst Project PoCs only Viewer Read-only (metadata only)","title":"\ud83e\udde9 Cross-Project PoC Access"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#poc-verification-workflow","text":"graph TD A[Download PoC] --> B[Verify Hash & Signature] B --> C{Trusted Source?} C -->|Yes| D[Mark Verified] C -->|No| E[Quarantine] E --> F[Manual Review Required] D --> G[Available for Sandbox Run]","title":"\ud83e\uddf1 PoC Verification Workflow"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#quarantine-mechanism","text":"Unknown or tampered PoCs are moved to: ~/.SecFlow/resources/poc/quarantine/ Each is tagged with a quarantine reason. Admins can review and promote back to verified status. def quarantine_poc(poc_id: str, reason: str): shutil.move(f\"/pocstore/{poc_id}\", \"/pocstore/quarantine/\") write_log(f\"PoC {poc_id} quarantined: {reason}\")","title":"\ud83d\udd12 Quarantine Mechanism"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#example-execution-trace","text":"[PoC: CVE-2024-12345] Verified source: ExploitDB [Sandbox] Starting container SecFlow-sandbox:latest [Sandbox] CPU quota: 1 core, Memory: 512MB [Sandbox] Network: disabled [Output] [+] Exploit successful: remote command executed [Cleanup] PoC container destroyed after 120s","title":"\ud83e\udde0 Example Execution Trace"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#future-enhancements","text":"AppArmor enforcement policies with dynamic runtime profiling. PoC provenance blockchain for cryptographic integrity. AI-assisted PoC safety classification (RCE, DoS, PrivEsc). Multi-tenant isolation for collaborative workspaces. Live replay of PoC runs for training and documentation. Next: Garbage Collection & Data Retention Policy","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/15-garbage-collection-and-retention/","text":"15 \u2014 Garbage Collection & Data Retention Policy \u00b6 \ud83e\udded Overview \u00b6 The Garbage Collection (GC) subsystem ensures long-term maintainability and performance of the SecFlow platform. It automates the cleanup of: - Expired runs and temporary results - Deleted project data (findings, resources, logs) - Old cache entries and transient files - Obsolete resources (e.g., old templates or wordlists) GC operates as an asynchronous background service within the worker app and supports both soft-delete and hard-delete modes. \u2699\ufe0f Core Objectives \u00b6 Objective Description Consistency Prevent orphaned or dangling records (runs, findings). Recoverability Allow temporary undo (via soft delete + TTL). Auditability Maintain logs of every GC event for compliance. Efficiency Keep database and disk footprint minimal. Isolation Per-project retention scopes (independent cleanup). \ud83e\udde9 Architecture Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Worker App<br/>- GC Scheduler (Celery Beat)<br/>- GC Worker (Async Cleanup Tasks)<br/>- Retention Policy Manager<br/>- Audit Trail Logger\"] B[\"Persistent Storage Layer<br/>- DB Tables: projects, runs, findings, logs<br/>- FS Paths: /resources, /cache, /artifacts\"] A --> B \ud83e\udde9 Retention Model \u00b6 Each project defines its retention profile in ~/.SecFlow/projects/<id>/config.yaml : retention: findings_ttl_days: 180 runs_ttl_days: 90 cache_ttl_days: 30 artifacts_ttl_days: 180 soft_delete_ttl_days: 14 auto_cleanup: true \ud83e\uddf1 Data Lifecycle \u00b6 Stage Description Active Data used by ongoing projects or workflows. Soft Deleted Marked for deletion but restorable ( flag: deleted=true ). Expired TTL exceeded; scheduled for cleanup. Hard Deleted Permanently removed after grace period. \ud83e\udde0 Database-Level Soft Delete \u00b6 class BaseModel(SQLModel): id: UUID created_at: datetime updated_at: datetime deleted: bool = False deleted_at: Optional[datetime] = None When a record is soft-deleted: def soft_delete(obj): obj.deleted = True obj.deleted_at = datetime.utcnow() session.add(obj) session.commit() Recovery: def restore(obj): obj.deleted = False obj.deleted_at = None session.commit() \ud83e\udde9 File System Garbage Collector \u00b6 Directory Structure \u00b6 /data/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/\"] B[\"projects/\"] C[\"<project_id>/\"] D[\"findings/\"] E[\"runs/\"] F[\"logs/\"] G[\"artifacts/\"] H[\"cache/\"] I[\"tmp/\"] A --> B A --> H A --> I B --> C C --> D C --> E C --> F C --> G The GC worker traverses these trees periodically: def sweep_directory(base_path: Path, older_than: timedelta): now = datetime.utcnow() for p in base_path.rglob(\"*\"): if p.is_file() and (now - datetime.fromtimestamp(p.stat().st_mtime)) > older_than: p.unlink() \ud83e\udde0 GC Task Scheduling \u00b6 Celery Task Definition \u00b6 @app.task(name=\"gc.cleanup_expired\") def cleanup_expired(): sweep_projects() sweep_cache() sweep_artifacts() Scheduler Configuration \u00b6 CELERY_BEAT_SCHEDULE = { \"cleanup-every-6h\": { \"task\": \"gc.cleanup_expired\", \"schedule\": crontab(hour=\"*/6\"), }, } GC tasks can be triggered manually: SecFlow gc run --project acme-api \ud83e\udde9 Retention Policy Evaluation \u00b6 Example Policy Rules \u00b6 Rule Condition Action Inactive Runs Run ended > 90 days ago Delete run logs Soft-Deleted Findings Deleted > 14 days ago Purge permanently Cache Expired Cache entry older than 30 days Remove Unused Artifacts Artifact not accessed for 180 days Archive or delete Policy Engine Snippet \u00b6 def evaluate_retention(entity, policy): if entity.deleted and expired(entity.deleted_at, policy.soft_delete_ttl_days): hard_delete(entity) elif expired(entity.updated_at, policy.findings_ttl_days): soft_delete(entity) \ud83e\udde9 Audit Logging for GC \u00b6 Each GC operation generates an audit record: { \"event\": \"gc_delete\", \"type\": \"finding\", \"target_id\": \"f123-45ac\", \"project_id\": \"p001\", \"timestamp\": \"2025-10-06T09:30:00Z\", \"user\": \"system\", \"ttl_rule\": \"soft_delete_ttl_days=14\" } Stored in: ~/.SecFlow/audit/gc.log \ud83e\uddf1 Orphan Detection \u00b6 SQL Example \u00b6 SELECT f.id FROM findings f LEFT JOIN runs r ON f.run_id = r.id WHERE r.id IS NULL; Any orphaned findings or artifacts (without associated runs/projects) are purged automatically. \ud83e\udde9 Cache Lifecycle \u00b6 Caches (e.g., CVE data, scan results, tool logs) use a standardized interface: class CacheEntry(BaseModel): key: str value: bytes expires_at: datetime def purge_expired(): session.query(CacheEntry).filter(CacheEntry.expires_at < datetime.utcnow()).delete() \ud83e\udde0 Manual Cleanup Command \u00b6 Users can trigger GC manually via CLI: # Run full cleanup (all projects) SecFlow gc run # Run cleanup for one project SecFlow gc run --project acme-api # Preview what will be deleted SecFlow gc dry-run Example output: \u00b6 [GC] Found 12 expired runs, 4 orphaned findings, 6 stale cache entries [GC] Total reclaimed: 1.2 GB \ud83d\udd10 Security Considerations \u00b6 All deletions (soft or hard) are logged. Data is never removed without audit trace. System prevents GC while a project is locked or running. Manual GC requires Admin role. \ud83d\udd04 GC Metrics & Observability \u00b6 Metric Description gc_runs_total Number of GC cycles executed gc_files_removed_total Number of files deleted gc_bytes_reclaimed_total Storage reclaimed in bytes gc_duration_seconds Time per GC cycle gc_errors_total Failed cleanup operations Exposed via Prometheus at /metrics . \ud83e\udde0 Example GC Cycle Log \u00b6 [GC] Cycle started at 2025-10-06T09:00:00Z [GC] Processed 3 projects [GC] Deleted 15 findings (soft) [GC] Purged 10 runs (hard) [GC] Reclaimed 1.8GB disk space [GC] Cycle completed in 42.3s \ud83d\udd2e Future Enhancements \u00b6 Incremental snapshot pruning \u2014 keep only latest versions of runs. Policy-as-Code \u2014 customizable YAML rulesets. AI-based cleanup predictions \u2014 identify stale datasets dynamically. Storage quota enforcement per user/project. UI dashboard for retention and GC insights. Next: Security Model (RBAC, Authentication, Sandboxing)","title":"Garbage Collection"},{"location":"architecture/15-garbage-collection-and-retention/#15-garbage-collection-data-retention-policy","text":"","title":"15 \u2014 Garbage Collection &amp; Data Retention Policy"},{"location":"architecture/15-garbage-collection-and-retention/#overview","text":"The Garbage Collection (GC) subsystem ensures long-term maintainability and performance of the SecFlow platform. It automates the cleanup of: - Expired runs and temporary results - Deleted project data (findings, resources, logs) - Old cache entries and transient files - Obsolete resources (e.g., old templates or wordlists) GC operates as an asynchronous background service within the worker app and supports both soft-delete and hard-delete modes.","title":"\ud83e\udded Overview"},{"location":"architecture/15-garbage-collection-and-retention/#core-objectives","text":"Objective Description Consistency Prevent orphaned or dangling records (runs, findings). Recoverability Allow temporary undo (via soft delete + TTL). Auditability Maintain logs of every GC event for compliance. Efficiency Keep database and disk footprint minimal. Isolation Per-project retention scopes (independent cleanup).","title":"\u2699\ufe0f Core Objectives"},{"location":"architecture/15-garbage-collection-and-retention/#architecture-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Worker App<br/>- GC Scheduler (Celery Beat)<br/>- GC Worker (Async Cleanup Tasks)<br/>- Retention Policy Manager<br/>- Audit Trail Logger\"] B[\"Persistent Storage Layer<br/>- DB Tables: projects, runs, findings, logs<br/>- FS Paths: /resources, /cache, /artifacts\"] A --> B","title":"\ud83e\udde9 Architecture Diagram"},{"location":"architecture/15-garbage-collection-and-retention/#retention-model","text":"Each project defines its retention profile in ~/.SecFlow/projects/<id>/config.yaml : retention: findings_ttl_days: 180 runs_ttl_days: 90 cache_ttl_days: 30 artifacts_ttl_days: 180 soft_delete_ttl_days: 14 auto_cleanup: true","title":"\ud83e\udde9 Retention Model"},{"location":"architecture/15-garbage-collection-and-retention/#data-lifecycle","text":"Stage Description Active Data used by ongoing projects or workflows. Soft Deleted Marked for deletion but restorable ( flag: deleted=true ). Expired TTL exceeded; scheduled for cleanup. Hard Deleted Permanently removed after grace period.","title":"\ud83e\uddf1 Data Lifecycle"},{"location":"architecture/15-garbage-collection-and-retention/#database-level-soft-delete","text":"class BaseModel(SQLModel): id: UUID created_at: datetime updated_at: datetime deleted: bool = False deleted_at: Optional[datetime] = None When a record is soft-deleted: def soft_delete(obj): obj.deleted = True obj.deleted_at = datetime.utcnow() session.add(obj) session.commit() Recovery: def restore(obj): obj.deleted = False obj.deleted_at = None session.commit()","title":"\ud83e\udde0 Database-Level Soft Delete"},{"location":"architecture/15-garbage-collection-and-retention/#file-system-garbage-collector","text":"","title":"\ud83e\udde9 File System Garbage Collector"},{"location":"architecture/15-garbage-collection-and-retention/#directory-structure","text":"/data/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/\"] B[\"projects/\"] C[\"<project_id>/\"] D[\"findings/\"] E[\"runs/\"] F[\"logs/\"] G[\"artifacts/\"] H[\"cache/\"] I[\"tmp/\"] A --> B A --> H A --> I B --> C C --> D C --> E C --> F C --> G The GC worker traverses these trees periodically: def sweep_directory(base_path: Path, older_than: timedelta): now = datetime.utcnow() for p in base_path.rglob(\"*\"): if p.is_file() and (now - datetime.fromtimestamp(p.stat().st_mtime)) > older_than: p.unlink()","title":"Directory Structure"},{"location":"architecture/15-garbage-collection-and-retention/#gc-task-scheduling","text":"","title":"\ud83e\udde0 GC Task Scheduling"},{"location":"architecture/15-garbage-collection-and-retention/#celery-task-definition","text":"@app.task(name=\"gc.cleanup_expired\") def cleanup_expired(): sweep_projects() sweep_cache() sweep_artifacts()","title":"Celery Task Definition"},{"location":"architecture/15-garbage-collection-and-retention/#scheduler-configuration","text":"CELERY_BEAT_SCHEDULE = { \"cleanup-every-6h\": { \"task\": \"gc.cleanup_expired\", \"schedule\": crontab(hour=\"*/6\"), }, } GC tasks can be triggered manually: SecFlow gc run --project acme-api","title":"Scheduler Configuration"},{"location":"architecture/15-garbage-collection-and-retention/#retention-policy-evaluation","text":"","title":"\ud83e\udde9 Retention Policy Evaluation"},{"location":"architecture/15-garbage-collection-and-retention/#example-policy-rules","text":"Rule Condition Action Inactive Runs Run ended > 90 days ago Delete run logs Soft-Deleted Findings Deleted > 14 days ago Purge permanently Cache Expired Cache entry older than 30 days Remove Unused Artifacts Artifact not accessed for 180 days Archive or delete","title":"Example Policy Rules"},{"location":"architecture/15-garbage-collection-and-retention/#policy-engine-snippet","text":"def evaluate_retention(entity, policy): if entity.deleted and expired(entity.deleted_at, policy.soft_delete_ttl_days): hard_delete(entity) elif expired(entity.updated_at, policy.findings_ttl_days): soft_delete(entity)","title":"Policy Engine Snippet"},{"location":"architecture/15-garbage-collection-and-retention/#audit-logging-for-gc","text":"Each GC operation generates an audit record: { \"event\": \"gc_delete\", \"type\": \"finding\", \"target_id\": \"f123-45ac\", \"project_id\": \"p001\", \"timestamp\": \"2025-10-06T09:30:00Z\", \"user\": \"system\", \"ttl_rule\": \"soft_delete_ttl_days=14\" } Stored in: ~/.SecFlow/audit/gc.log","title":"\ud83e\udde9 Audit Logging for GC"},{"location":"architecture/15-garbage-collection-and-retention/#orphan-detection","text":"","title":"\ud83e\uddf1 Orphan Detection"},{"location":"architecture/15-garbage-collection-and-retention/#sql-example","text":"SELECT f.id FROM findings f LEFT JOIN runs r ON f.run_id = r.id WHERE r.id IS NULL; Any orphaned findings or artifacts (without associated runs/projects) are purged automatically.","title":"SQL Example"},{"location":"architecture/15-garbage-collection-and-retention/#cache-lifecycle","text":"Caches (e.g., CVE data, scan results, tool logs) use a standardized interface: class CacheEntry(BaseModel): key: str value: bytes expires_at: datetime def purge_expired(): session.query(CacheEntry).filter(CacheEntry.expires_at < datetime.utcnow()).delete()","title":"\ud83e\udde9 Cache Lifecycle"},{"location":"architecture/15-garbage-collection-and-retention/#manual-cleanup-command","text":"Users can trigger GC manually via CLI: # Run full cleanup (all projects) SecFlow gc run # Run cleanup for one project SecFlow gc run --project acme-api # Preview what will be deleted SecFlow gc dry-run","title":"\ud83e\udde0 Manual Cleanup Command"},{"location":"architecture/15-garbage-collection-and-retention/#example-output","text":"[GC] Found 12 expired runs, 4 orphaned findings, 6 stale cache entries [GC] Total reclaimed: 1.2 GB","title":"Example output:"},{"location":"architecture/15-garbage-collection-and-retention/#security-considerations","text":"All deletions (soft or hard) are logged. Data is never removed without audit trace. System prevents GC while a project is locked or running. Manual GC requires Admin role.","title":"\ud83d\udd10 Security Considerations"},{"location":"architecture/15-garbage-collection-and-retention/#gc-metrics-observability","text":"Metric Description gc_runs_total Number of GC cycles executed gc_files_removed_total Number of files deleted gc_bytes_reclaimed_total Storage reclaimed in bytes gc_duration_seconds Time per GC cycle gc_errors_total Failed cleanup operations Exposed via Prometheus at /metrics .","title":"\ud83d\udd04 GC Metrics &amp; Observability"},{"location":"architecture/15-garbage-collection-and-retention/#example-gc-cycle-log","text":"[GC] Cycle started at 2025-10-06T09:00:00Z [GC] Processed 3 projects [GC] Deleted 15 findings (soft) [GC] Purged 10 runs (hard) [GC] Reclaimed 1.8GB disk space [GC] Cycle completed in 42.3s","title":"\ud83e\udde0 Example GC Cycle Log"},{"location":"architecture/15-garbage-collection-and-retention/#future-enhancements","text":"Incremental snapshot pruning \u2014 keep only latest versions of runs. Policy-as-Code \u2014 customizable YAML rulesets. AI-based cleanup predictions \u2014 identify stale datasets dynamically. Storage quota enforcement per user/project. UI dashboard for retention and GC insights. Next: Security Model (RBAC, Authentication, Sandboxing)","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/16-security-model/","text":"16 \u2014 Security Model (RBAC, Authentication, and Sandboxing) \u00b6 \ud83e\udded Overview \u00b6 Security is a first-class design goal in the SecFlow architecture. Every user action, process execution, and data mutation is subject to authentication, authorization, and sandboxing. This model ensures: - Strict access separation between users, projects, and data. - Controlled execution of potentially dangerous tools or PoCs. - Immutable auditability of all operations. - Zero-trust posture inside the orchestration environment. \ud83e\udde9 Layers of the Security Model \u00b6 Layer Scope Mechanism Authentication User identity verification JWT / OAuth2 tokens Authorization (RBAC) Access control Role-based & scope-aware Execution Sandboxing Process isolation Containerized runners Secrets Management API keys, credentials Encrypted vault Audit Logging Accountability Immutable append-only logs \ud83e\udde0 Authentication Architecture \u00b6 SecFlow supports two auth models: 1. Local Auth (default): For standalone or on-premise deployments. 2. Federated Auth (optional): OAuth2/OpenID Connect integration (GitHub, Google, Azure AD). Token Model \u00b6 { \"sub\": \"hernan\", \"role\": \"admin\", \"exp\": 1738783200, \"projects\": [\"proj-01\", \"proj-02\"] } Token Flow Diagram \u00b6 [User Login] \u2192 [Auth Provider] \u2192 [JWT Issued] \u2192 [API Gateway] \u2192 [SecFlow Web/API] Each request to /api/* must include: Authorization: Bearer <token> Tokens are verified by the API middleware using RS256 signature validation. \ud83e\udde9 Role-Based Access Control (RBAC) \u00b6 Roles define the scope of capabilities across the platform. Role Permissions Admin Full control \u2014 manage tools, users, projects, retention, policies. Analyst Execute workflows, triage findings, view reports, limited editing. Viewer Read-only access to results and dashboards. Automation (Service) Used by background tasks (limited scoped tokens). Example permission matrix: \u00b6 Action Admin Analyst Viewer Service Run workflow \u2705 \u2705 \u274c \u2705 Modify tool config \u2705 \u274c \u274c \u274c View findings \u2705 \u2705 \u2705 \u2705 Delete project \u2705 \u274c \u274c \u274c Access PoCs \u2705 \u2705 \u274c \u274c Run GC tasks \u2705 \u274c \u274c \u2705 \u2699\ufe0f Policy Enforcement \u00b6 Every endpoint and command passes through an Access Policy Filter: def authorize(action: str, user: User, project: Optional[str] = None): if not user.has_permission(action, project): raise HTTPException(403, detail=f\"Forbidden: {action}\") Example route decorator: \u00b6 @app.get(\"/api/v1/findings\") @require_role([\"admin\", \"analyst\", \"viewer\"]) def list_findings(): def authorize(self, action: str, user: User, project: Optional[str] = None) -> bool: \"\"\"Check if user is authorized for action.\"\"\" return user.has_permission(action, project) \ud83e\uddf1 Secrets Management \u00b6 Secret Types \u00b6 API tokens for external tools (e.g., Shodan, Vulners) Private SSH keys for remote scans Encrypted credentials for authenticated targets Storage Backend \u00b6 All secrets are stored in the SecFlow Vault, an encrypted JSON database backed by Fernet (AES-256). class SecretVault: def __init__(self, keyfile: Path): self.key = load_key(keyfile) def store(self, id: str, data: dict): enc = fernet_encrypt(json.dumps(data), self.key) write_file(f\"/vault/{id}.enc\", enc) Secrets CLI \u00b6 SecFlow secrets add nuclei_api --value \"TOKEN123\" SecFlow secrets list SecFlow secrets remove nuclei_api All access is scoped by user and project context. \ud83d\udd12 Execution Sandboxing \u00b6 All scanner and PoC executions run inside restricted containers or subprocess jails. Isolation Techniques \u00b6 Mechanism Purpose Namespaces (PID, NET, MNT) Process isolation Seccomp Filters Syscall restriction cgroups v2 CPU/memory limits No-root UID mapping Drops privileges AppArmor profiles File access control Read-only FS Prevents persistence Example \u00b6 docker run --rm --cap-drop=ALL \\ --security-opt=no-new-privileges \\ --read-only -m 512m --cpus=1 \\ SecFlow-runner:latest nuclei -t /templates \ud83e\udde9 Network & Data Security \u00b6 Channel Encryption Notes API <-> UI HTTPS (TLS 1.3) Strict transport enforced Worker <-> API Mutual TLS Each worker has its own cert File Sync AES-256 encrypted Optional compression Database At-rest encryption SQLite: SEE, Postgres: TDE Audit Logs Signed + timestamped Prevents tampering \ud83e\udde0 Secure Inter-Process Communication \u00b6 Internal apps communicate via ZeroMQ or Redis queues over TLS. Each message includes a signed envelope: { \"msg_id\": \"uuid\", \"issuer\": \"worker-1\", \"signature\": \"HMAC-SHA256\", \"payload\": {\"data\": \"encrypted_content\"} } This ensures authenticity and non-repudiation. \ud83e\udde9 Security Hooks & Middleware \u00b6 SecFlow injects middleware for: - Request validation (pydantic schemas) - JWT token expiry verification - Role validation (fast-path lookup) - Audit log writing after each mutating operation Example: \u00b6 @app.middleware(\"http\") async def audit_request(request, call_next): response = await call_next(request) if request.method in (\"POST\", \"DELETE\", \"PATCH\"): log_audit(request, response) return response \ud83d\udd10 Audit Trail & Tamper Resistance \u00b6 Log Format \u00b6 { \"timestamp\": \"2025-10-06T09:40:00Z\", \"user\": \"hernan\", \"action\": \"workflow_run\", \"project\": \"api-audit\", \"status\": \"success\", \"hash\": \"sha256:0a3b1c2d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2\" } Storage & Verification \u00b6 Logs stored as JSON lines under /audit/ Each file signed with an HMAC chain: H_i = HMAC(H_prev + log_i, key) Immutable and verifiable chain-of-trust. \ud83e\udde0 Security Monitoring \u00b6 Metric Description auth_failures_total Failed login attempts sandbox_executions_total Containers spawned policy_violations_total Unauthorized actions vault_accesses_total Secret retrievals audit_events_total Log entries recorded \u2699\ufe0f Compliance Framework Alignment \u00b6 SecFlow's security architecture aligns with: Framework Compliance Area NIST SP 800-53 Access control, auditing, system protection ISO/IEC 27001 Information security management OWASP SAMM Secure software development lifecycle MITRE ATT&CK Mapping detection behaviors GDPR Art. 32 Data confidentiality and integrity \ud83d\udd12 Key Rotation & Secrets Expiry \u00b6 Secrets have explicit TTLs (default: 180 days). Vault rotation command: SecFlow vault rotate Rotation regenerates the encryption key and re-encrypts all entries. \ud83e\udde9 Example Access Workflow \u00b6 [Analyst] \u2192 [Login via OIDC] \u2192 [JWT Issued] \u2192 [UI/API Gateway] \u2192 [RBAC Policy Check] \u2192 [Worker Executes Workflow in Sandbox] \u2192 [Findings Logged + Audit Entry Created] \ud83d\udd2e Future Enhancements \u00b6 Hardware-backed encryption (YubiKey / TPM) for Vault keys. FIPS-compliant container sandbox. Behavior-based anomaly detection on audit logs. Single Sign-On (SSO) with just-in-time role provisioning. Runtime policy engine (OPA / Rego integration). Next: Observability, Logging & Metrics","title":"Security Model"},{"location":"architecture/16-security-model/#16-security-model-rbac-authentication-and-sandboxing","text":"","title":"16 \u2014 Security Model (RBAC, Authentication, and Sandboxing)"},{"location":"architecture/16-security-model/#overview","text":"Security is a first-class design goal in the SecFlow architecture. Every user action, process execution, and data mutation is subject to authentication, authorization, and sandboxing. This model ensures: - Strict access separation between users, projects, and data. - Controlled execution of potentially dangerous tools or PoCs. - Immutable auditability of all operations. - Zero-trust posture inside the orchestration environment.","title":"\ud83e\udded Overview"},{"location":"architecture/16-security-model/#layers-of-the-security-model","text":"Layer Scope Mechanism Authentication User identity verification JWT / OAuth2 tokens Authorization (RBAC) Access control Role-based & scope-aware Execution Sandboxing Process isolation Containerized runners Secrets Management API keys, credentials Encrypted vault Audit Logging Accountability Immutable append-only logs","title":"\ud83e\udde9 Layers of the Security Model"},{"location":"architecture/16-security-model/#authentication-architecture","text":"SecFlow supports two auth models: 1. Local Auth (default): For standalone or on-premise deployments. 2. Federated Auth (optional): OAuth2/OpenID Connect integration (GitHub, Google, Azure AD).","title":"\ud83e\udde0 Authentication Architecture"},{"location":"architecture/16-security-model/#token-model","text":"{ \"sub\": \"hernan\", \"role\": \"admin\", \"exp\": 1738783200, \"projects\": [\"proj-01\", \"proj-02\"] }","title":"Token Model"},{"location":"architecture/16-security-model/#token-flow-diagram","text":"[User Login] \u2192 [Auth Provider] \u2192 [JWT Issued] \u2192 [API Gateway] \u2192 [SecFlow Web/API] Each request to /api/* must include: Authorization: Bearer <token> Tokens are verified by the API middleware using RS256 signature validation.","title":"Token Flow Diagram"},{"location":"architecture/16-security-model/#role-based-access-control-rbac","text":"Roles define the scope of capabilities across the platform. Role Permissions Admin Full control \u2014 manage tools, users, projects, retention, policies. Analyst Execute workflows, triage findings, view reports, limited editing. Viewer Read-only access to results and dashboards. Automation (Service) Used by background tasks (limited scoped tokens).","title":"\ud83e\udde9 Role-Based Access Control (RBAC)"},{"location":"architecture/16-security-model/#example-permission-matrix","text":"Action Admin Analyst Viewer Service Run workflow \u2705 \u2705 \u274c \u2705 Modify tool config \u2705 \u274c \u274c \u274c View findings \u2705 \u2705 \u2705 \u2705 Delete project \u2705 \u274c \u274c \u274c Access PoCs \u2705 \u2705 \u274c \u274c Run GC tasks \u2705 \u274c \u274c \u2705","title":"Example permission matrix:"},{"location":"architecture/16-security-model/#policy-enforcement","text":"Every endpoint and command passes through an Access Policy Filter: def authorize(action: str, user: User, project: Optional[str] = None): if not user.has_permission(action, project): raise HTTPException(403, detail=f\"Forbidden: {action}\")","title":"\u2699\ufe0f Policy Enforcement"},{"location":"architecture/16-security-model/#example-route-decorator","text":"@app.get(\"/api/v1/findings\") @require_role([\"admin\", \"analyst\", \"viewer\"]) def list_findings(): def authorize(self, action: str, user: User, project: Optional[str] = None) -> bool: \"\"\"Check if user is authorized for action.\"\"\" return user.has_permission(action, project)","title":"Example route decorator:"},{"location":"architecture/16-security-model/#secrets-management","text":"","title":"\ud83e\uddf1 Secrets Management"},{"location":"architecture/16-security-model/#secret-types","text":"API tokens for external tools (e.g., Shodan, Vulners) Private SSH keys for remote scans Encrypted credentials for authenticated targets","title":"Secret Types"},{"location":"architecture/16-security-model/#storage-backend","text":"All secrets are stored in the SecFlow Vault, an encrypted JSON database backed by Fernet (AES-256). class SecretVault: def __init__(self, keyfile: Path): self.key = load_key(keyfile) def store(self, id: str, data: dict): enc = fernet_encrypt(json.dumps(data), self.key) write_file(f\"/vault/{id}.enc\", enc)","title":"Storage Backend"},{"location":"architecture/16-security-model/#secrets-cli","text":"SecFlow secrets add nuclei_api --value \"TOKEN123\" SecFlow secrets list SecFlow secrets remove nuclei_api All access is scoped by user and project context.","title":"Secrets CLI"},{"location":"architecture/16-security-model/#execution-sandboxing","text":"All scanner and PoC executions run inside restricted containers or subprocess jails.","title":"\ud83d\udd12 Execution Sandboxing"},{"location":"architecture/16-security-model/#isolation-techniques","text":"Mechanism Purpose Namespaces (PID, NET, MNT) Process isolation Seccomp Filters Syscall restriction cgroups v2 CPU/memory limits No-root UID mapping Drops privileges AppArmor profiles File access control Read-only FS Prevents persistence","title":"Isolation Techniques"},{"location":"architecture/16-security-model/#example","text":"docker run --rm --cap-drop=ALL \\ --security-opt=no-new-privileges \\ --read-only -m 512m --cpus=1 \\ SecFlow-runner:latest nuclei -t /templates","title":"Example"},{"location":"architecture/16-security-model/#network-data-security","text":"Channel Encryption Notes API <-> UI HTTPS (TLS 1.3) Strict transport enforced Worker <-> API Mutual TLS Each worker has its own cert File Sync AES-256 encrypted Optional compression Database At-rest encryption SQLite: SEE, Postgres: TDE Audit Logs Signed + timestamped Prevents tampering","title":"\ud83e\udde9 Network &amp; Data Security"},{"location":"architecture/16-security-model/#secure-inter-process-communication","text":"Internal apps communicate via ZeroMQ or Redis queues over TLS. Each message includes a signed envelope: { \"msg_id\": \"uuid\", \"issuer\": \"worker-1\", \"signature\": \"HMAC-SHA256\", \"payload\": {\"data\": \"encrypted_content\"} } This ensures authenticity and non-repudiation.","title":"\ud83e\udde0 Secure Inter-Process Communication"},{"location":"architecture/16-security-model/#security-hooks-middleware","text":"SecFlow injects middleware for: - Request validation (pydantic schemas) - JWT token expiry verification - Role validation (fast-path lookup) - Audit log writing after each mutating operation","title":"\ud83e\udde9 Security Hooks &amp; Middleware"},{"location":"architecture/16-security-model/#example_1","text":"@app.middleware(\"http\") async def audit_request(request, call_next): response = await call_next(request) if request.method in (\"POST\", \"DELETE\", \"PATCH\"): log_audit(request, response) return response","title":"Example:"},{"location":"architecture/16-security-model/#audit-trail-tamper-resistance","text":"","title":"\ud83d\udd10 Audit Trail &amp; Tamper Resistance"},{"location":"architecture/16-security-model/#log-format","text":"{ \"timestamp\": \"2025-10-06T09:40:00Z\", \"user\": \"hernan\", \"action\": \"workflow_run\", \"project\": \"api-audit\", \"status\": \"success\", \"hash\": \"sha256:0a3b1c2d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2\" }","title":"Log Format"},{"location":"architecture/16-security-model/#storage-verification","text":"Logs stored as JSON lines under /audit/ Each file signed with an HMAC chain: H_i = HMAC(H_prev + log_i, key) Immutable and verifiable chain-of-trust.","title":"Storage &amp; Verification"},{"location":"architecture/16-security-model/#security-monitoring","text":"Metric Description auth_failures_total Failed login attempts sandbox_executions_total Containers spawned policy_violations_total Unauthorized actions vault_accesses_total Secret retrievals audit_events_total Log entries recorded","title":"\ud83e\udde0 Security Monitoring"},{"location":"architecture/16-security-model/#compliance-framework-alignment","text":"SecFlow's security architecture aligns with: Framework Compliance Area NIST SP 800-53 Access control, auditing, system protection ISO/IEC 27001 Information security management OWASP SAMM Secure software development lifecycle MITRE ATT&CK Mapping detection behaviors GDPR Art. 32 Data confidentiality and integrity","title":"\u2699\ufe0f Compliance Framework Alignment"},{"location":"architecture/16-security-model/#key-rotation-secrets-expiry","text":"Secrets have explicit TTLs (default: 180 days). Vault rotation command: SecFlow vault rotate Rotation regenerates the encryption key and re-encrypts all entries.","title":"\ud83d\udd12 Key Rotation &amp; Secrets Expiry"},{"location":"architecture/16-security-model/#example-access-workflow","text":"[Analyst] \u2192 [Login via OIDC] \u2192 [JWT Issued] \u2192 [UI/API Gateway] \u2192 [RBAC Policy Check] \u2192 [Worker Executes Workflow in Sandbox] \u2192 [Findings Logged + Audit Entry Created]","title":"\ud83e\udde9 Example Access Workflow"},{"location":"architecture/16-security-model/#future-enhancements","text":"Hardware-backed encryption (YubiKey / TPM) for Vault keys. FIPS-compliant container sandbox. Behavior-based anomaly detection on audit logs. Single Sign-On (SSO) with just-in-time role provisioning. Runtime policy engine (OPA / Rego integration). Next: Observability, Logging & Metrics","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/17-observability-logging-and-metrics/","text":"17 \u2014 Observability, Logging, Metrics & Tracing \u00b6 \ud83e\udded Overview \u00b6 The Observability Layer provides full transparency into the runtime behavior of SecFlow. It enables developers, analysts, and operators to: - Track system health and performance metrics. - Trace workflow execution across tools and workers. - Monitor logs and alerts from all layers (API, Worker, Sandbox). - Correlate findings, runs, and audit trails with unique identifiers. M1 Implementation Status : \u2705 Complete - In-memory metrics collection, structured logging, and audit trails delivered. Note : In M1, metrics are collected in-memory and shown in logs; a future release will expose them in Prometheus format. The observability system is standards-based , built upon: - Structured Logging for event correlation (M1) - In-Memory Metrics for performance monitoring (M1) - Audit Trails for security and compliance (M1) - Prometheus for metrics export (M5 planned) - OpenTelemetry for tracing and context propagation (M5 planned) \ud83e\uddf1 Observability Architecture Overview (M1) \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Executor\"] B[\"Plugin Loader\"] C[\"StoragePort\"] D[\"Security Framework\"] A --> E[\"Structured Logging\"] B --> E C --> E D --> E A --> F[\"In-Memory Metrics\"] B --> F C --> F D --> F D --> G[\"Audit Logging\"] E --> H[\"Log Files\"] F --> I[\"Metrics Storage\"] G --> J[\"Security Events\"] K[\"M5: Prometheus Export\"] L[\"M5: OpenTelemetry\"] F -.-> K E -.-> L M1 Components Delivered : - \u2705 Structured Logging : JSON format with correlation IDs - \u2705 In-Memory Metrics : Performance and execution metrics - \u2705 Audit Logging : Security events and compliance trails - \u2705 Event Correlation : Trace IDs for workflow execution - \u2705 StoragePort Integration : Metrics persistence via StoragePort \ud83e\udde9 Logging Subsystem (M1) \u00b6 Logging Design Goals \u00b6 Goal M1 Implementation Machine-readable JSON structured format with standard fields Correlated across systems trace_id and span_id included Searchable Standardized field names and values Audit-compliant Immutable logs with timestamps Performance-aware Async logging to avoid blocking Structured Log Format (M1) \u00b6 { \"timestamp\" : \"2025-10-14T10:30:00.123Z\" , \"level\" : \"INFO\" , \"logger\" : \"secflow.workflow.executor\" , \"message\" : \"Node execution completed\" , \"trace_id\" : \"abc123def456\" , \"span_id\" : \"span789\" , \"workflow_id\" : \"workflow-123\" , \"node_id\" : \"scan.nuclei\" , \"plugin_name\" : \"nuclei-stub\" , \"duration_ms\" : 1250 , \"findings_count\" : 45 , \"success\" : true , \"metadata\" : { \"project_id\" : \"project-456\" , \"user_id\" : \"user-789\" , \"execution_context\" : \"production\" } } Logging Configuration (M1) \u00b6 # app/logging_conf.py import logging import json from datetime import datetime class StructuredFormatter ( logging . Formatter ): \"\"\"Structured JSON formatter for M1.\"\"\" def format ( self , record ): log_entry = { \"timestamp\" : datetime . utcnow () . isoformat () + \"Z\" , \"level\" : record . levelname , \"logger\" : record . name , \"message\" : record . getMessage (), \"trace_id\" : getattr ( record , 'trace_id' , None ), \"span_id\" : getattr ( record , 'span_id' , None ), \"workflow_id\" : getattr ( record , 'workflow_id' , None ), \"node_id\" : getattr ( record , 'node_id' , None ), \"plugin_name\" : getattr ( record , 'plugin_name' , None ), \"duration_ms\" : getattr ( record , 'duration_ms' , None ), \"findings_count\" : getattr ( record , 'findings_count' , None ), \"success\" : getattr ( record , 'success' , None ), \"metadata\" : getattr ( record , 'metadata' , {}) } return json . dumps ( log_entry ) # Configure logging def setup_logging (): \"\"\"Setup structured logging for M1.\"\"\" handler = logging . StreamHandler () handler . setFormatter ( StructuredFormatter ()) logger = logging . getLogger ( 'secflow' ) logger . addHandler ( handler ) logger . setLevel ( logging . INFO ) return logger Workflow Execution Logging \u00b6 # packages/workflow_engine/executor.py import logging import time from contextlib import contextmanager logger = logging . getLogger ( 'secflow.workflow.executor' ) @contextmanager def log_workflow_execution ( workflow_id : str , node_id : str , plugin_name : str ): \"\"\"Context manager for workflow execution logging.\"\"\" start_time = time . time () # Log start logger . info ( \"Node execution started\" , extra = { 'workflow_id' : workflow_id , 'node_id' : node_id , 'plugin_name' : plugin_name , 'trace_id' : generate_trace_id (), 'span_id' : generate_span_id () } ) try : yield success = True except Exception as e : logger . error ( f \"Node execution failed: { e } \" , extra = { 'workflow_id' : workflow_id , 'node_id' : node_id , 'plugin_name' : plugin_name , 'error' : str ( e ), 'success' : False } ) raise finally : # Log completion duration_ms = ( time . time () - start_time ) * 1000 logger . info ( \"Node execution completed\" , extra = { 'workflow_id' : workflow_id , 'node_id' : node_id , 'plugin_name' : plugin_name , 'duration_ms' : duration_ms , 'success' : success } ) \ud83d\udcca Metrics Subsystem (M1) \u00b6 In-Memory Metrics Collection \u00b6 M1 Implementation : Metrics collected in-memory and stored via StoragePort. # analytics_core/metrics_telemetry.py from typing import Dict , Any , List from dataclasses import dataclass , field from datetime import datetime import threading @dataclass class MetricPoint : \"\"\"Individual metric data point.\"\"\" name : str value : float labels : Dict [ str , str ] = field ( default_factory = dict ) timestamp : str = field ( default_factory = lambda : datetime . utcnow () . isoformat () + \"Z\" ) @dataclass class MetricsSnapshot : \"\"\"Snapshot of metrics at a point in time.\"\"\" timestamp : str metrics : List [ MetricPoint ] = field ( default_factory = list ) class InMemoryMetricsCollector : \"\"\"In-memory metrics collector for M1.\"\"\" def __init__ ( self ): self . _metrics : List [ MetricPoint ] = [] self . _lock = threading . Lock () def record_counter ( self , name : str , value : float = 1.0 , labels : Dict [ str , str ] = None ): \"\"\"Record a counter metric.\"\"\" with self . _lock : self . _metrics . append ( MetricPoint ( name = name , value = value , labels = labels or {} )) def record_gauge ( self , name : str , value : float , labels : Dict [ str , str ] = None ): \"\"\"Record a gauge metric.\"\"\" with self . _lock : self . _metrics . append ( MetricPoint ( name = name , value = value , labels = labels or {} )) def record_histogram ( self , name : str , value : float , labels : Dict [ str , str ] = None ): \"\"\"Record a histogram metric.\"\"\" with self . _lock : self . _metrics . append ( MetricPoint ( name = name , value = value , labels = labels or {} )) def get_metrics_snapshot ( self ) -> MetricsSnapshot : \"\"\"Get current metrics snapshot.\"\"\" with self . _lock : return MetricsSnapshot ( timestamp = datetime . utcnow () . isoformat () + \"Z\" , metrics = self . _metrics . copy () ) def clear_metrics ( self ): \"\"\"Clear collected metrics.\"\"\" with self . _lock : self . _metrics . clear () Workflow Metrics \u00b6 # packages/workflow_engine/metrics.py from analytics_core.metrics_telemetry import InMemoryMetricsCollector class WorkflowMetrics : \"\"\"Workflow-specific metrics for M1.\"\"\" def __init__ ( self , collector : InMemoryMetricsCollector ): self . collector = collector def record_workflow_start ( self , workflow_id : str , workflow_name : str ): \"\"\"Record workflow start metric.\"\"\" self . collector . record_counter ( \"secflow_workflows_started_total\" , labels = { \"workflow_id\" : workflow_id , \"workflow_name\" : workflow_name } ) def record_workflow_completion ( self , workflow_id : str , workflow_name : str , duration_ms : float , success : bool ): \"\"\"Record workflow completion metric.\"\"\" self . collector . record_counter ( \"secflow_workflows_completed_total\" , labels = { \"workflow_id\" : workflow_id , \"workflow_name\" : workflow_name , \"status\" : \"success\" if success else \"failure\" } ) self . collector . record_histogram ( \"secflow_workflow_duration_ms\" , duration_ms , labels = { \"workflow_id\" : workflow_id , \"workflow_name\" : workflow_name } ) def record_node_execution ( self , node_id : str , plugin_name : str , duration_ms : float , findings_count : int ): \"\"\"Record node execution metric.\"\"\" self . collector . record_counter ( \"secflow_nodes_executed_total\" , labels = { \"node_id\" : node_id , \"plugin_name\" : plugin_name } ) self . collector . record_histogram ( \"secflow_node_duration_ms\" , duration_ms , labels = { \"node_id\" : node_id , \"plugin_name\" : plugin_name } ) self . collector . record_gauge ( \"secflow_findings_generated_total\" , findings_count , labels = { \"node_id\" : node_id , \"plugin_name\" : plugin_name } ) Plugin Metrics \u00b6 # packages/plugins/metrics.py class PluginMetrics : \"\"\"Plugin-specific metrics for M1.\"\"\" def __init__ ( self , collector : InMemoryMetricsCollector ): self . collector = collector def record_plugin_load ( self , plugin_name : str , success : bool ): \"\"\"Record plugin loading metric.\"\"\" self . collector . record_counter ( \"secflow_plugins_loaded_total\" , labels = { \"plugin_name\" : plugin_name , \"status\" : \"success\" if success else \"failure\" } ) def record_plugin_execution ( self , plugin_name : str , duration_ms : float , memory_mb : float , success : bool ): \"\"\"Record plugin execution metric.\"\"\" self . collector . record_counter ( \"secflow_plugin_executions_total\" , labels = { \"plugin_name\" : plugin_name , \"status\" : \"success\" if success else \"failure\" } ) self . collector . record_histogram ( \"secflow_plugin_duration_ms\" , duration_ms , labels = { \"plugin_name\" : plugin_name } ) self . collector . record_gauge ( \"secflow_plugin_memory_mb\" , memory_mb , labels = { \"plugin_name\" : plugin_name } ) \ud83d\udd0d Debugging & Troubleshooting (M1) \u00b6 Debug Mode \u00b6 # Enable debug logging export SECFLOW_LOG_LEVEL = \"DEBUG\" python tools/run_workflow.py workflows/sample-linear.yaml --execute --verbose # Debug plugin loading python tools/validate_recipe.py workflows/sample-linear.yaml --debug # Debug storage operations python tools/test_storage.py --adapter memory --debug Debug Configuration \u00b6 # app/logging_conf.py def setup_debug_logging (): \"\"\"Setup debug logging for troubleshooting.\"\"\" handler = logging . StreamHandler () handler . setFormatter ( StructuredFormatter ()) # Enable debug for all SecFlow loggers for logger_name in [ 'secflow' , 'packages' , 'tools' ]: logger = logging . getLogger ( logger_name ) logger . addHandler ( handler ) logger . setLevel ( logging . DEBUG ) return logger Common Debug Scenarios \u00b6 1. Plugin Loading Issues \u00b6 # Debug plugin loading def debug_plugin_loading ( plugin_name : str ): \"\"\"Debug plugin loading issues.\"\"\" logger = logging . getLogger ( 'secflow.plugins.loader' ) try : logger . debug ( f \"Attempting to load plugin: { plugin_name } \" ) plugin = plugin_loader . load ( plugin_name ) logger . debug ( f \"Plugin loaded successfully: { plugin . get_name () } \" ) return plugin except Exception as e : logger . error ( f \"Plugin loading failed: { e } \" ) logger . debug ( f \"Plugin path: { plugin_path } \" ) logger . debug ( f \"Manifest: { manifest } \" ) raise 2. Workflow Execution Issues \u00b6 # Debug workflow execution def debug_workflow_execution ( workflow_id : str ): \"\"\"Debug workflow execution issues.\"\"\" logger = logging . getLogger ( 'secflow.workflow.executor' ) logger . debug ( f \"Starting workflow execution: { workflow_id } \" ) for node in workflow . nodes : logger . debug ( f \"Executing node: { node . id } ( { node . type } )\" ) try : result = execute_node ( node ) logger . debug ( f \"Node completed: { node . id } \" ) except Exception as e : logger . error ( f \"Node failed: { node . id } - { e } \" ) logger . debug ( f \"Node config: { node . config } \" ) logger . debug ( f \"Node inputs: { node . inputs } \" ) raise 3. Storage Issues \u00b6 # Debug storage operations def debug_storage_operations ( storage : StoragePort ): \"\"\"Debug storage operation issues.\"\"\" logger = logging . getLogger ( 'secflow.storage' ) try : # Test basic operations logger . debug ( \"Testing storage operations\" ) # Test finding save test_finding = create_test_finding () storage . save_finding ( test_finding ) logger . debug ( \"Finding saved successfully\" ) # Test finding list findings = storage . list_findings ( test_finding [ \"project_id\" ]) logger . debug ( f \"Found { len ( findings ) } findings\" ) except Exception as e : logger . error ( f \"Storage operation failed: { e } \" ) logger . debug ( f \"Storage type: { type ( storage ) } \" ) raise \ud83d\udcc8 Performance Monitoring (M1) \u00b6 Performance Metrics \u00b6 # analytics_core/performance_monitor.py class PerformanceMonitor : \"\"\"Performance monitoring for M1.\"\"\" def __init__ ( self , collector : InMemoryMetricsCollector ): self . collector = collector def record_system_metrics ( self ): \"\"\"Record system performance metrics.\"\"\" import psutil # CPU usage cpu_percent = psutil . cpu_percent () self . collector . record_gauge ( \"secflow_system_cpu_percent\" , cpu_percent ) # Memory usage memory = psutil . virtual_memory () self . collector . record_gauge ( \"secflow_system_memory_percent\" , memory . percent ) self . collector . record_gauge ( \"secflow_system_memory_mb\" , memory . used / 1024 / 1024 ) # Disk usage disk = psutil . disk_usage ( '/' ) self . collector . record_gauge ( \"secflow_system_disk_percent\" , disk . percent ) def record_workflow_performance ( self , workflow_id : str , total_duration_ms : float , node_count : int , findings_count : int ): \"\"\"Record workflow performance metrics.\"\"\" # Throughput metrics findings_per_second = findings_count / ( total_duration_ms / 1000 ) if total_duration_ms > 0 else 0 self . collector . record_gauge ( \"secflow_workflow_findings_per_second\" , findings_per_second , labels = { \"workflow_id\" : workflow_id } ) # Efficiency metrics avg_node_duration = total_duration_ms / node_count if node_count > 0 else 0 self . collector . record_gauge ( \"secflow_workflow_avg_node_duration_ms\" , avg_node_duration , labels = { \"workflow_id\" : workflow_id } ) Performance Analysis \u00b6 # tools/performance_analysis.py def analyze_workflow_performance ( workflow_id : str ) -> Dict [ str , Any ]: \"\"\"Analyze workflow performance metrics.\"\"\" metrics = get_metrics_snapshot () workflow_metrics = [ m for m in metrics . metrics if m . labels . get ( \"workflow_id\" ) == workflow_id ] analysis = { \"workflow_id\" : workflow_id , \"total_duration_ms\" : 0 , \"node_count\" : 0 , \"findings_count\" : 0 , \"avg_node_duration_ms\" : 0 , \"findings_per_second\" : 0 } # Calculate metrics for metric in workflow_metrics : if metric . name == \"secflow_workflow_duration_ms\" : analysis [ \"total_duration_ms\" ] = metric . value elif metric . name == \"secflow_nodes_executed_total\" : analysis [ \"node_count\" ] += metric . value elif metric . name == \"secflow_findings_generated_total\" : analysis [ \"findings_count\" ] += metric . value # Calculate derived metrics if analysis [ \"node_count\" ] > 0 : analysis [ \"avg_node_duration_ms\" ] = analysis [ \"total_duration_ms\" ] / analysis [ \"node_count\" ] if analysis [ \"total_duration_ms\" ] > 0 : analysis [ \"findings_per_second\" ] = analysis [ \"findings_count\" ] / ( analysis [ \"total_duration_ms\" ] / 1000 ) return analysis \ud83d\udd2e Future Enhancements (M5+) \u00b6 Prometheus Integration (M5) \u00b6 # M5: Prometheus metrics export (planned) from prometheus_client import Counter , Histogram , Gauge , start_http_server # Prometheus metrics workflow_counter = Counter ( 'secflow_workflows_total' , 'Total workflows executed' , [ 'status' ]) workflow_duration = Histogram ( 'secflow_workflow_duration_seconds' , 'Workflow execution duration' ) node_duration = Histogram ( 'secflow_node_duration_seconds' , 'Node execution duration' , [ 'node_type' ]) findings_gauge = Gauge ( 'secflow_findings_total' , 'Total findings generated' ) def start_prometheus_server ( port : int = 8000 ): \"\"\"Start Prometheus metrics server.\"\"\" start_http_server ( port ) logger . info ( f \"Prometheus metrics server started on port { port } \" ) OpenTelemetry Integration (M5) \u00b6 # M5: OpenTelemetry tracing (planned) from opentelemetry import trace from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter from opentelemetry.sdk.trace import TracerProvider from opentelemetry.sdk.trace.export import BatchSpanProcessor def setup_tracing (): \"\"\"Setup OpenTelemetry tracing.\"\"\" trace . set_tracer_provider ( TracerProvider ()) tracer = trace . get_tracer ( __name__ ) otlp_exporter = OTLPSpanExporter ( endpoint = \"http://localhost:4317\" ) span_processor = BatchSpanProcessor ( otlp_exporter ) trace . get_tracer_provider () . add_span_processor ( span_processor ) return tracer Grafana Dashboards (M5) \u00b6 # M5: Grafana dashboard configuration (planned) dashboard : title : \"SecFlow Observability Dashboard\" panels : - title : \"Workflow Execution Rate\" type : \"graph\" targets : - expr : \"rate(secflow_workflows_total[5m])\" - title : \"Average Workflow Duration\" type : \"graph\" targets : - expr : \"histogram_quantile(0.95, secflow_workflow_duration_seconds)\" - title : \"Findings Generated\" type : \"graph\" targets : - expr : \"secflow_findings_total\" \ud83d\udcda Observability Best Practices \u00b6 For Developers \u00b6 Use Structured Logging : Always use JSON format with correlation IDs Include Context : Add relevant metadata to log entries Record Metrics : Instrument key operations with metrics Handle Errors Gracefully : Log errors with sufficient context Use Debug Mode : Enable debug logging for troubleshooting For Operators \u00b6 Monitor Key Metrics : Track workflow execution rates and durations Set Up Alerts : Configure alerts for critical failures Regular Log Review : Periodically review logs for issues Performance Analysis : Analyze performance trends over time Capacity Planning : Use metrics for capacity planning For Security Teams \u00b6 Audit Log Review : Regularly review security audit logs Anomaly Detection : Monitor for unusual patterns Compliance Monitoring : Ensure logging meets compliance requirements Incident Response : Use logs for security incident investigation Access Monitoring : Track user access and actions Next: Error Handling & Recovery SecFlow uses OpenTelemetry (OTel) for distributed tracing. Every API request, task dispatch, and plugin call generates spans linked under one root trace. Example Trace Structure \u00b6 TraceID: 5b2e4f21c9a344f9 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"api.request (GET /api/v1/workflows)\"] B[\"worker.dispatch\"] C[\"tool.nuclei.run\"] D[\"sandbox.spawn\"] E[\"sandbox.cleanup\"] F[\"tool.ferox.run\"] G[\"findings.persist\"] H[\"api.response\"] A --> B A --> H B --> C B --> F B --> G C --> D C --> E Code Example \u00b6 from opentelemetry import trace tracer = trace.get_tracer(\"SecFlow.worker\") with tracer.start_as_current_span(\"workflow.execute\") as span: span.set_attribute(\"workflow.id\", workflow.id) run_workflow(workflow) All traces are exported through OTLP gRPC to the observability backend (e.g., Tempo, Jaeger). \ud83d\udcca Metrics System \u00b6 SecFlow exposes runtime metrics through Prometheus-compatible endpoints. Default Endpoint \u00b6 /metrics Example Metrics \u00b6 Metric Type Description secflow_requests_total Counter Total API requests handled secflow_active_workflows Gauge Currently running workflows secflow_findings_generated_total Counter Findings created secflow_task_duration_seconds Histogram Time taken by async tasks secflow_gc_bytes_reclaimed_total Counter GC reclaimed bytes secflow_sandbox_executions_total Counter Number of sandbox runs secflow_tool_failures_total Counter Failed tool executions secflow_worker_queue_depth Gauge Pending Celery tasks secflow_cve_enrichment_latency_seconds Histogram Time per CVE query Prometheus Export Example \u00b6 from prometheus_client import Counter, Gauge findings_total = Counter(\"secflow_findings_generated_total\", \"Number of findings created\") active_workflows = Gauge(\"secflow_active_workflows\", \"Currently running workflows\") \ud83d\udd0d Example Grafana Dashboard Panels \u00b6 Panel Visualization Query Workflow Throughput Time series rate(secflow_requests_total[5m]) Average Scan Duration Histogram histogram_quantile(0.9, rate(secflow_task_duration_seconds_bucket[5m])) Findings per Project Bar chart sum by (project)(secflow_findings_generated_total) GC Efficiency SingleStat rate(secflow_gc_bytes_reclaimed_total[1h]) Sandbox Failures Table secflow_tool_failures_total \ud83e\udde9 Error Correlation & Incident Debugging \u00b6 Every finding, workflow, and audit entry includes a trace ID. Errors can be traced back to exact processes and spans. Example correlation: \u00b6 Finding \u2192 Workflow ID: wf-abc123 \u2192 Trace ID: cbd82b67a1e4f9d2c8b5e6f7a3d4c9e2b1f8a5c6d7e9f2a3b4c5d6e7f8a9b2c3d4e5f6 \u2192 Logs: worker.log \u2192 Span: tool.nuclei.run This guarantees reproducibility and fast RCA (root cause analysis). \ud83e\udde0 Alerting & Health Checks \u00b6 Health Endpoints \u00b6 Endpoint Description /healthz Liveness probe (basic app status) /readyz Readiness probe (DB + cache + queue connectivity) Example Output \u00b6 { \"status\": \"ok\", \"services\": { \"database\": \"up\", \"cache\": \"up\", \"worker\": \"idle\" } } Alerts (Prometheus Rules) \u00b6 groups: - name: secflow_alerts rules: - alert: HighErrorRate expr: rate(secflow_tool_failures_total[5m]) > 5 for: 10m labels: { severity: warning } annotations: summary: \"Tool failure rate too high\" \ud83d\udd12 Security of Observability Data \u00b6 Concern Mitigation Sensitive logs Field redaction (password, token, secret) Trace integrity HMAC signing of exported spans Log tampering Append-only JSONL + rotation Metrics abuse Authenticated /metrics endpoint (basic token or mutual TLS) Example redaction middleware: \u00b6 def sanitize(data: dict) -> dict: for key in data.keys(): if \"token\" in key.lower() or \"password\" in key.lower(): data[key] = \"[REDACTED]\" return data \ud83e\uddf1 Correlation Example: End-to-End Trace \u00b6 [TRACE 5b2e4f21c9a344f9] %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"/api/v1/workflows/start\"] B[\"worker.queue.dispatch (duration=12ms)\"] C[\"tool.ferox.run (duration=4.1s)\"] D[\"tool.nuclei.run (duration=9.3s)\"] E[\"enrich.cve (duration=1.4s)\"] F[\"findings.persist (duration=320ms)\"] G[\"audit.log (duration=7ms)\"] A --> B A --> C A --> D A --> E A --> F A --> G \ud83e\udde9 Integration with CI/CD and Testing \u00b6 During CI runs: - Unit and integration tests export OTel traces for regression analysis. - Performance tests measure task durations and error rates. Example CI configuration: \u00b6 env: OTEL_EXPORTER_OTLP_ENDPOINT: \"http://otel-collector:4317\" PROMETHEUS_MULTIPROC_DIR: \"/tmp/metrics\" \ud83d\udd2e Future Enhancements \u00b6 Distributed tracing for multi-cluster deployments. Real-time log streaming to the web UI. AI-assisted anomaly detection for workflow performance. Adaptive sampling for trace volume reduction. On-demand debug mode via CLI flag ( --trace verbose ). Next: Error Handling & Recovery","title":"Observability"},{"location":"architecture/17-observability-logging-and-metrics/#17-observability-logging-metrics-tracing","text":"","title":"17 \u2014 Observability, Logging, Metrics &amp; Tracing"},{"location":"architecture/17-observability-logging-and-metrics/#overview","text":"The Observability Layer provides full transparency into the runtime behavior of SecFlow. It enables developers, analysts, and operators to: - Track system health and performance metrics. - Trace workflow execution across tools and workers. - Monitor logs and alerts from all layers (API, Worker, Sandbox). - Correlate findings, runs, and audit trails with unique identifiers. M1 Implementation Status : \u2705 Complete - In-memory metrics collection, structured logging, and audit trails delivered. Note : In M1, metrics are collected in-memory and shown in logs; a future release will expose them in Prometheus format. The observability system is standards-based , built upon: - Structured Logging for event correlation (M1) - In-Memory Metrics for performance monitoring (M1) - Audit Trails for security and compliance (M1) - Prometheus for metrics export (M5 planned) - OpenTelemetry for tracing and context propagation (M5 planned)","title":"\ud83e\udded Overview"},{"location":"architecture/17-observability-logging-and-metrics/#observability-architecture-overview-m1","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Executor\"] B[\"Plugin Loader\"] C[\"StoragePort\"] D[\"Security Framework\"] A --> E[\"Structured Logging\"] B --> E C --> E D --> E A --> F[\"In-Memory Metrics\"] B --> F C --> F D --> F D --> G[\"Audit Logging\"] E --> H[\"Log Files\"] F --> I[\"Metrics Storage\"] G --> J[\"Security Events\"] K[\"M5: Prometheus Export\"] L[\"M5: OpenTelemetry\"] F -.-> K E -.-> L M1 Components Delivered : - \u2705 Structured Logging : JSON format with correlation IDs - \u2705 In-Memory Metrics : Performance and execution metrics - \u2705 Audit Logging : Security events and compliance trails - \u2705 Event Correlation : Trace IDs for workflow execution - \u2705 StoragePort Integration : Metrics persistence via StoragePort","title":"\ud83e\uddf1 Observability Architecture Overview (M1)"},{"location":"architecture/17-observability-logging-and-metrics/#logging-subsystem-m1","text":"","title":"\ud83e\udde9 Logging Subsystem (M1)"},{"location":"architecture/17-observability-logging-and-metrics/#logging-design-goals","text":"Goal M1 Implementation Machine-readable JSON structured format with standard fields Correlated across systems trace_id and span_id included Searchable Standardized field names and values Audit-compliant Immutable logs with timestamps Performance-aware Async logging to avoid blocking","title":"Logging Design Goals"},{"location":"architecture/17-observability-logging-and-metrics/#structured-log-format-m1","text":"{ \"timestamp\" : \"2025-10-14T10:30:00.123Z\" , \"level\" : \"INFO\" , \"logger\" : \"secflow.workflow.executor\" , \"message\" : \"Node execution completed\" , \"trace_id\" : \"abc123def456\" , \"span_id\" : \"span789\" , \"workflow_id\" : \"workflow-123\" , \"node_id\" : \"scan.nuclei\" , \"plugin_name\" : \"nuclei-stub\" , \"duration_ms\" : 1250 , \"findings_count\" : 45 , \"success\" : true , \"metadata\" : { \"project_id\" : \"project-456\" , \"user_id\" : \"user-789\" , \"execution_context\" : \"production\" } }","title":"Structured Log Format (M1)"},{"location":"architecture/17-observability-logging-and-metrics/#logging-configuration-m1","text":"# app/logging_conf.py import logging import json from datetime import datetime class StructuredFormatter ( logging . Formatter ): \"\"\"Structured JSON formatter for M1.\"\"\" def format ( self , record ): log_entry = { \"timestamp\" : datetime . utcnow () . isoformat () + \"Z\" , \"level\" : record . levelname , \"logger\" : record . name , \"message\" : record . getMessage (), \"trace_id\" : getattr ( record , 'trace_id' , None ), \"span_id\" : getattr ( record , 'span_id' , None ), \"workflow_id\" : getattr ( record , 'workflow_id' , None ), \"node_id\" : getattr ( record , 'node_id' , None ), \"plugin_name\" : getattr ( record , 'plugin_name' , None ), \"duration_ms\" : getattr ( record , 'duration_ms' , None ), \"findings_count\" : getattr ( record , 'findings_count' , None ), \"success\" : getattr ( record , 'success' , None ), \"metadata\" : getattr ( record , 'metadata' , {}) } return json . dumps ( log_entry ) # Configure logging def setup_logging (): \"\"\"Setup structured logging for M1.\"\"\" handler = logging . StreamHandler () handler . setFormatter ( StructuredFormatter ()) logger = logging . getLogger ( 'secflow' ) logger . addHandler ( handler ) logger . setLevel ( logging . INFO ) return logger","title":"Logging Configuration (M1)"},{"location":"architecture/17-observability-logging-and-metrics/#workflow-execution-logging","text":"# packages/workflow_engine/executor.py import logging import time from contextlib import contextmanager logger = logging . getLogger ( 'secflow.workflow.executor' ) @contextmanager def log_workflow_execution ( workflow_id : str , node_id : str , plugin_name : str ): \"\"\"Context manager for workflow execution logging.\"\"\" start_time = time . time () # Log start logger . info ( \"Node execution started\" , extra = { 'workflow_id' : workflow_id , 'node_id' : node_id , 'plugin_name' : plugin_name , 'trace_id' : generate_trace_id (), 'span_id' : generate_span_id () } ) try : yield success = True except Exception as e : logger . error ( f \"Node execution failed: { e } \" , extra = { 'workflow_id' : workflow_id , 'node_id' : node_id , 'plugin_name' : plugin_name , 'error' : str ( e ), 'success' : False } ) raise finally : # Log completion duration_ms = ( time . time () - start_time ) * 1000 logger . info ( \"Node execution completed\" , extra = { 'workflow_id' : workflow_id , 'node_id' : node_id , 'plugin_name' : plugin_name , 'duration_ms' : duration_ms , 'success' : success } )","title":"Workflow Execution Logging"},{"location":"architecture/17-observability-logging-and-metrics/#metrics-subsystem-m1","text":"","title":"\ud83d\udcca Metrics Subsystem (M1)"},{"location":"architecture/17-observability-logging-and-metrics/#in-memory-metrics-collection","text":"M1 Implementation : Metrics collected in-memory and stored via StoragePort. # analytics_core/metrics_telemetry.py from typing import Dict , Any , List from dataclasses import dataclass , field from datetime import datetime import threading @dataclass class MetricPoint : \"\"\"Individual metric data point.\"\"\" name : str value : float labels : Dict [ str , str ] = field ( default_factory = dict ) timestamp : str = field ( default_factory = lambda : datetime . utcnow () . isoformat () + \"Z\" ) @dataclass class MetricsSnapshot : \"\"\"Snapshot of metrics at a point in time.\"\"\" timestamp : str metrics : List [ MetricPoint ] = field ( default_factory = list ) class InMemoryMetricsCollector : \"\"\"In-memory metrics collector for M1.\"\"\" def __init__ ( self ): self . _metrics : List [ MetricPoint ] = [] self . _lock = threading . Lock () def record_counter ( self , name : str , value : float = 1.0 , labels : Dict [ str , str ] = None ): \"\"\"Record a counter metric.\"\"\" with self . _lock : self . _metrics . append ( MetricPoint ( name = name , value = value , labels = labels or {} )) def record_gauge ( self , name : str , value : float , labels : Dict [ str , str ] = None ): \"\"\"Record a gauge metric.\"\"\" with self . _lock : self . _metrics . append ( MetricPoint ( name = name , value = value , labels = labels or {} )) def record_histogram ( self , name : str , value : float , labels : Dict [ str , str ] = None ): \"\"\"Record a histogram metric.\"\"\" with self . _lock : self . _metrics . append ( MetricPoint ( name = name , value = value , labels = labels or {} )) def get_metrics_snapshot ( self ) -> MetricsSnapshot : \"\"\"Get current metrics snapshot.\"\"\" with self . _lock : return MetricsSnapshot ( timestamp = datetime . utcnow () . isoformat () + \"Z\" , metrics = self . _metrics . copy () ) def clear_metrics ( self ): \"\"\"Clear collected metrics.\"\"\" with self . _lock : self . _metrics . clear ()","title":"In-Memory Metrics Collection"},{"location":"architecture/17-observability-logging-and-metrics/#workflow-metrics","text":"# packages/workflow_engine/metrics.py from analytics_core.metrics_telemetry import InMemoryMetricsCollector class WorkflowMetrics : \"\"\"Workflow-specific metrics for M1.\"\"\" def __init__ ( self , collector : InMemoryMetricsCollector ): self . collector = collector def record_workflow_start ( self , workflow_id : str , workflow_name : str ): \"\"\"Record workflow start metric.\"\"\" self . collector . record_counter ( \"secflow_workflows_started_total\" , labels = { \"workflow_id\" : workflow_id , \"workflow_name\" : workflow_name } ) def record_workflow_completion ( self , workflow_id : str , workflow_name : str , duration_ms : float , success : bool ): \"\"\"Record workflow completion metric.\"\"\" self . collector . record_counter ( \"secflow_workflows_completed_total\" , labels = { \"workflow_id\" : workflow_id , \"workflow_name\" : workflow_name , \"status\" : \"success\" if success else \"failure\" } ) self . collector . record_histogram ( \"secflow_workflow_duration_ms\" , duration_ms , labels = { \"workflow_id\" : workflow_id , \"workflow_name\" : workflow_name } ) def record_node_execution ( self , node_id : str , plugin_name : str , duration_ms : float , findings_count : int ): \"\"\"Record node execution metric.\"\"\" self . collector . record_counter ( \"secflow_nodes_executed_total\" , labels = { \"node_id\" : node_id , \"plugin_name\" : plugin_name } ) self . collector . record_histogram ( \"secflow_node_duration_ms\" , duration_ms , labels = { \"node_id\" : node_id , \"plugin_name\" : plugin_name } ) self . collector . record_gauge ( \"secflow_findings_generated_total\" , findings_count , labels = { \"node_id\" : node_id , \"plugin_name\" : plugin_name } )","title":"Workflow Metrics"},{"location":"architecture/17-observability-logging-and-metrics/#plugin-metrics","text":"# packages/plugins/metrics.py class PluginMetrics : \"\"\"Plugin-specific metrics for M1.\"\"\" def __init__ ( self , collector : InMemoryMetricsCollector ): self . collector = collector def record_plugin_load ( self , plugin_name : str , success : bool ): \"\"\"Record plugin loading metric.\"\"\" self . collector . record_counter ( \"secflow_plugins_loaded_total\" , labels = { \"plugin_name\" : plugin_name , \"status\" : \"success\" if success else \"failure\" } ) def record_plugin_execution ( self , plugin_name : str , duration_ms : float , memory_mb : float , success : bool ): \"\"\"Record plugin execution metric.\"\"\" self . collector . record_counter ( \"secflow_plugin_executions_total\" , labels = { \"plugin_name\" : plugin_name , \"status\" : \"success\" if success else \"failure\" } ) self . collector . record_histogram ( \"secflow_plugin_duration_ms\" , duration_ms , labels = { \"plugin_name\" : plugin_name } ) self . collector . record_gauge ( \"secflow_plugin_memory_mb\" , memory_mb , labels = { \"plugin_name\" : plugin_name } )","title":"Plugin Metrics"},{"location":"architecture/17-observability-logging-and-metrics/#debugging-troubleshooting-m1","text":"","title":"\ud83d\udd0d Debugging &amp; Troubleshooting (M1)"},{"location":"architecture/17-observability-logging-and-metrics/#debug-mode","text":"# Enable debug logging export SECFLOW_LOG_LEVEL = \"DEBUG\" python tools/run_workflow.py workflows/sample-linear.yaml --execute --verbose # Debug plugin loading python tools/validate_recipe.py workflows/sample-linear.yaml --debug # Debug storage operations python tools/test_storage.py --adapter memory --debug","title":"Debug Mode"},{"location":"architecture/17-observability-logging-and-metrics/#debug-configuration","text":"# app/logging_conf.py def setup_debug_logging (): \"\"\"Setup debug logging for troubleshooting.\"\"\" handler = logging . StreamHandler () handler . setFormatter ( StructuredFormatter ()) # Enable debug for all SecFlow loggers for logger_name in [ 'secflow' , 'packages' , 'tools' ]: logger = logging . getLogger ( logger_name ) logger . addHandler ( handler ) logger . setLevel ( logging . DEBUG ) return logger","title":"Debug Configuration"},{"location":"architecture/17-observability-logging-and-metrics/#common-debug-scenarios","text":"","title":"Common Debug Scenarios"},{"location":"architecture/17-observability-logging-and-metrics/#1-plugin-loading-issues","text":"# Debug plugin loading def debug_plugin_loading ( plugin_name : str ): \"\"\"Debug plugin loading issues.\"\"\" logger = logging . getLogger ( 'secflow.plugins.loader' ) try : logger . debug ( f \"Attempting to load plugin: { plugin_name } \" ) plugin = plugin_loader . load ( plugin_name ) logger . debug ( f \"Plugin loaded successfully: { plugin . get_name () } \" ) return plugin except Exception as e : logger . error ( f \"Plugin loading failed: { e } \" ) logger . debug ( f \"Plugin path: { plugin_path } \" ) logger . debug ( f \"Manifest: { manifest } \" ) raise","title":"1. Plugin Loading Issues"},{"location":"architecture/17-observability-logging-and-metrics/#2-workflow-execution-issues","text":"# Debug workflow execution def debug_workflow_execution ( workflow_id : str ): \"\"\"Debug workflow execution issues.\"\"\" logger = logging . getLogger ( 'secflow.workflow.executor' ) logger . debug ( f \"Starting workflow execution: { workflow_id } \" ) for node in workflow . nodes : logger . debug ( f \"Executing node: { node . id } ( { node . type } )\" ) try : result = execute_node ( node ) logger . debug ( f \"Node completed: { node . id } \" ) except Exception as e : logger . error ( f \"Node failed: { node . id } - { e } \" ) logger . debug ( f \"Node config: { node . config } \" ) logger . debug ( f \"Node inputs: { node . inputs } \" ) raise","title":"2. Workflow Execution Issues"},{"location":"architecture/17-observability-logging-and-metrics/#3-storage-issues","text":"# Debug storage operations def debug_storage_operations ( storage : StoragePort ): \"\"\"Debug storage operation issues.\"\"\" logger = logging . getLogger ( 'secflow.storage' ) try : # Test basic operations logger . debug ( \"Testing storage operations\" ) # Test finding save test_finding = create_test_finding () storage . save_finding ( test_finding ) logger . debug ( \"Finding saved successfully\" ) # Test finding list findings = storage . list_findings ( test_finding [ \"project_id\" ]) logger . debug ( f \"Found { len ( findings ) } findings\" ) except Exception as e : logger . error ( f \"Storage operation failed: { e } \" ) logger . debug ( f \"Storage type: { type ( storage ) } \" ) raise","title":"3. Storage Issues"},{"location":"architecture/17-observability-logging-and-metrics/#performance-monitoring-m1","text":"","title":"\ud83d\udcc8 Performance Monitoring (M1)"},{"location":"architecture/17-observability-logging-and-metrics/#performance-metrics","text":"# analytics_core/performance_monitor.py class PerformanceMonitor : \"\"\"Performance monitoring for M1.\"\"\" def __init__ ( self , collector : InMemoryMetricsCollector ): self . collector = collector def record_system_metrics ( self ): \"\"\"Record system performance metrics.\"\"\" import psutil # CPU usage cpu_percent = psutil . cpu_percent () self . collector . record_gauge ( \"secflow_system_cpu_percent\" , cpu_percent ) # Memory usage memory = psutil . virtual_memory () self . collector . record_gauge ( \"secflow_system_memory_percent\" , memory . percent ) self . collector . record_gauge ( \"secflow_system_memory_mb\" , memory . used / 1024 / 1024 ) # Disk usage disk = psutil . disk_usage ( '/' ) self . collector . record_gauge ( \"secflow_system_disk_percent\" , disk . percent ) def record_workflow_performance ( self , workflow_id : str , total_duration_ms : float , node_count : int , findings_count : int ): \"\"\"Record workflow performance metrics.\"\"\" # Throughput metrics findings_per_second = findings_count / ( total_duration_ms / 1000 ) if total_duration_ms > 0 else 0 self . collector . record_gauge ( \"secflow_workflow_findings_per_second\" , findings_per_second , labels = { \"workflow_id\" : workflow_id } ) # Efficiency metrics avg_node_duration = total_duration_ms / node_count if node_count > 0 else 0 self . collector . record_gauge ( \"secflow_workflow_avg_node_duration_ms\" , avg_node_duration , labels = { \"workflow_id\" : workflow_id } )","title":"Performance Metrics"},{"location":"architecture/17-observability-logging-and-metrics/#performance-analysis","text":"# tools/performance_analysis.py def analyze_workflow_performance ( workflow_id : str ) -> Dict [ str , Any ]: \"\"\"Analyze workflow performance metrics.\"\"\" metrics = get_metrics_snapshot () workflow_metrics = [ m for m in metrics . metrics if m . labels . get ( \"workflow_id\" ) == workflow_id ] analysis = { \"workflow_id\" : workflow_id , \"total_duration_ms\" : 0 , \"node_count\" : 0 , \"findings_count\" : 0 , \"avg_node_duration_ms\" : 0 , \"findings_per_second\" : 0 } # Calculate metrics for metric in workflow_metrics : if metric . name == \"secflow_workflow_duration_ms\" : analysis [ \"total_duration_ms\" ] = metric . value elif metric . name == \"secflow_nodes_executed_total\" : analysis [ \"node_count\" ] += metric . value elif metric . name == \"secflow_findings_generated_total\" : analysis [ \"findings_count\" ] += metric . value # Calculate derived metrics if analysis [ \"node_count\" ] > 0 : analysis [ \"avg_node_duration_ms\" ] = analysis [ \"total_duration_ms\" ] / analysis [ \"node_count\" ] if analysis [ \"total_duration_ms\" ] > 0 : analysis [ \"findings_per_second\" ] = analysis [ \"findings_count\" ] / ( analysis [ \"total_duration_ms\" ] / 1000 ) return analysis","title":"Performance Analysis"},{"location":"architecture/17-observability-logging-and-metrics/#future-enhancements-m5","text":"","title":"\ud83d\udd2e Future Enhancements (M5+)"},{"location":"architecture/17-observability-logging-and-metrics/#prometheus-integration-m5","text":"# M5: Prometheus metrics export (planned) from prometheus_client import Counter , Histogram , Gauge , start_http_server # Prometheus metrics workflow_counter = Counter ( 'secflow_workflows_total' , 'Total workflows executed' , [ 'status' ]) workflow_duration = Histogram ( 'secflow_workflow_duration_seconds' , 'Workflow execution duration' ) node_duration = Histogram ( 'secflow_node_duration_seconds' , 'Node execution duration' , [ 'node_type' ]) findings_gauge = Gauge ( 'secflow_findings_total' , 'Total findings generated' ) def start_prometheus_server ( port : int = 8000 ): \"\"\"Start Prometheus metrics server.\"\"\" start_http_server ( port ) logger . info ( f \"Prometheus metrics server started on port { port } \" )","title":"Prometheus Integration (M5)"},{"location":"architecture/17-observability-logging-and-metrics/#opentelemetry-integration-m5","text":"# M5: OpenTelemetry tracing (planned) from opentelemetry import trace from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter from opentelemetry.sdk.trace import TracerProvider from opentelemetry.sdk.trace.export import BatchSpanProcessor def setup_tracing (): \"\"\"Setup OpenTelemetry tracing.\"\"\" trace . set_tracer_provider ( TracerProvider ()) tracer = trace . get_tracer ( __name__ ) otlp_exporter = OTLPSpanExporter ( endpoint = \"http://localhost:4317\" ) span_processor = BatchSpanProcessor ( otlp_exporter ) trace . get_tracer_provider () . add_span_processor ( span_processor ) return tracer","title":"OpenTelemetry Integration (M5)"},{"location":"architecture/17-observability-logging-and-metrics/#grafana-dashboards-m5","text":"# M5: Grafana dashboard configuration (planned) dashboard : title : \"SecFlow Observability Dashboard\" panels : - title : \"Workflow Execution Rate\" type : \"graph\" targets : - expr : \"rate(secflow_workflows_total[5m])\" - title : \"Average Workflow Duration\" type : \"graph\" targets : - expr : \"histogram_quantile(0.95, secflow_workflow_duration_seconds)\" - title : \"Findings Generated\" type : \"graph\" targets : - expr : \"secflow_findings_total\"","title":"Grafana Dashboards (M5)"},{"location":"architecture/17-observability-logging-and-metrics/#observability-best-practices","text":"","title":"\ud83d\udcda Observability Best Practices"},{"location":"architecture/17-observability-logging-and-metrics/#for-developers","text":"Use Structured Logging : Always use JSON format with correlation IDs Include Context : Add relevant metadata to log entries Record Metrics : Instrument key operations with metrics Handle Errors Gracefully : Log errors with sufficient context Use Debug Mode : Enable debug logging for troubleshooting","title":"For Developers"},{"location":"architecture/17-observability-logging-and-metrics/#for-operators","text":"Monitor Key Metrics : Track workflow execution rates and durations Set Up Alerts : Configure alerts for critical failures Regular Log Review : Periodically review logs for issues Performance Analysis : Analyze performance trends over time Capacity Planning : Use metrics for capacity planning","title":"For Operators"},{"location":"architecture/17-observability-logging-and-metrics/#for-security-teams","text":"Audit Log Review : Regularly review security audit logs Anomaly Detection : Monitor for unusual patterns Compliance Monitoring : Ensure logging meets compliance requirements Incident Response : Use logs for security incident investigation Access Monitoring : Track user access and actions Next: Error Handling & Recovery SecFlow uses OpenTelemetry (OTel) for distributed tracing. Every API request, task dispatch, and plugin call generates spans linked under one root trace.","title":"For Security Teams"},{"location":"architecture/17-observability-logging-and-metrics/#example-trace-structure","text":"TraceID: 5b2e4f21c9a344f9 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"api.request (GET /api/v1/workflows)\"] B[\"worker.dispatch\"] C[\"tool.nuclei.run\"] D[\"sandbox.spawn\"] E[\"sandbox.cleanup\"] F[\"tool.ferox.run\"] G[\"findings.persist\"] H[\"api.response\"] A --> B A --> H B --> C B --> F B --> G C --> D C --> E","title":"Example Trace Structure"},{"location":"architecture/17-observability-logging-and-metrics/#code-example","text":"from opentelemetry import trace tracer = trace.get_tracer(\"SecFlow.worker\") with tracer.start_as_current_span(\"workflow.execute\") as span: span.set_attribute(\"workflow.id\", workflow.id) run_workflow(workflow) All traces are exported through OTLP gRPC to the observability backend (e.g., Tempo, Jaeger).","title":"Code Example"},{"location":"architecture/17-observability-logging-and-metrics/#metrics-system","text":"SecFlow exposes runtime metrics through Prometheus-compatible endpoints.","title":"\ud83d\udcca Metrics System"},{"location":"architecture/17-observability-logging-and-metrics/#default-endpoint","text":"/metrics","title":"Default Endpoint"},{"location":"architecture/17-observability-logging-and-metrics/#example-metrics","text":"Metric Type Description secflow_requests_total Counter Total API requests handled secflow_active_workflows Gauge Currently running workflows secflow_findings_generated_total Counter Findings created secflow_task_duration_seconds Histogram Time taken by async tasks secflow_gc_bytes_reclaimed_total Counter GC reclaimed bytes secflow_sandbox_executions_total Counter Number of sandbox runs secflow_tool_failures_total Counter Failed tool executions secflow_worker_queue_depth Gauge Pending Celery tasks secflow_cve_enrichment_latency_seconds Histogram Time per CVE query","title":"Example Metrics"},{"location":"architecture/17-observability-logging-and-metrics/#prometheus-export-example","text":"from prometheus_client import Counter, Gauge findings_total = Counter(\"secflow_findings_generated_total\", \"Number of findings created\") active_workflows = Gauge(\"secflow_active_workflows\", \"Currently running workflows\")","title":"Prometheus Export Example"},{"location":"architecture/17-observability-logging-and-metrics/#example-grafana-dashboard-panels","text":"Panel Visualization Query Workflow Throughput Time series rate(secflow_requests_total[5m]) Average Scan Duration Histogram histogram_quantile(0.9, rate(secflow_task_duration_seconds_bucket[5m])) Findings per Project Bar chart sum by (project)(secflow_findings_generated_total) GC Efficiency SingleStat rate(secflow_gc_bytes_reclaimed_total[1h]) Sandbox Failures Table secflow_tool_failures_total","title":"\ud83d\udd0d Example Grafana Dashboard Panels"},{"location":"architecture/17-observability-logging-and-metrics/#error-correlation-incident-debugging","text":"Every finding, workflow, and audit entry includes a trace ID. Errors can be traced back to exact processes and spans.","title":"\ud83e\udde9 Error Correlation &amp; Incident Debugging"},{"location":"architecture/17-observability-logging-and-metrics/#example-correlation","text":"Finding \u2192 Workflow ID: wf-abc123 \u2192 Trace ID: cbd82b67a1e4f9d2c8b5e6f7a3d4c9e2b1f8a5c6d7e9f2a3b4c5d6e7f8a9b2c3d4e5f6 \u2192 Logs: worker.log \u2192 Span: tool.nuclei.run This guarantees reproducibility and fast RCA (root cause analysis).","title":"Example correlation:"},{"location":"architecture/17-observability-logging-and-metrics/#alerting-health-checks","text":"","title":"\ud83e\udde0 Alerting &amp; Health Checks"},{"location":"architecture/17-observability-logging-and-metrics/#health-endpoints","text":"Endpoint Description /healthz Liveness probe (basic app status) /readyz Readiness probe (DB + cache + queue connectivity)","title":"Health Endpoints"},{"location":"architecture/17-observability-logging-and-metrics/#example-output","text":"{ \"status\": \"ok\", \"services\": { \"database\": \"up\", \"cache\": \"up\", \"worker\": \"idle\" } }","title":"Example Output"},{"location":"architecture/17-observability-logging-and-metrics/#alerts-prometheus-rules","text":"groups: - name: secflow_alerts rules: - alert: HighErrorRate expr: rate(secflow_tool_failures_total[5m]) > 5 for: 10m labels: { severity: warning } annotations: summary: \"Tool failure rate too high\"","title":"Alerts (Prometheus Rules)"},{"location":"architecture/17-observability-logging-and-metrics/#security-of-observability-data","text":"Concern Mitigation Sensitive logs Field redaction (password, token, secret) Trace integrity HMAC signing of exported spans Log tampering Append-only JSONL + rotation Metrics abuse Authenticated /metrics endpoint (basic token or mutual TLS)","title":"\ud83d\udd12 Security of Observability Data"},{"location":"architecture/17-observability-logging-and-metrics/#example-redaction-middleware","text":"def sanitize(data: dict) -> dict: for key in data.keys(): if \"token\" in key.lower() or \"password\" in key.lower(): data[key] = \"[REDACTED]\" return data","title":"Example redaction middleware:"},{"location":"architecture/17-observability-logging-and-metrics/#correlation-example-end-to-end-trace","text":"[TRACE 5b2e4f21c9a344f9] %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"/api/v1/workflows/start\"] B[\"worker.queue.dispatch (duration=12ms)\"] C[\"tool.ferox.run (duration=4.1s)\"] D[\"tool.nuclei.run (duration=9.3s)\"] E[\"enrich.cve (duration=1.4s)\"] F[\"findings.persist (duration=320ms)\"] G[\"audit.log (duration=7ms)\"] A --> B A --> C A --> D A --> E A --> F A --> G","title":"\ud83e\uddf1 Correlation Example: End-to-End Trace"},{"location":"architecture/17-observability-logging-and-metrics/#integration-with-cicd-and-testing","text":"During CI runs: - Unit and integration tests export OTel traces for regression analysis. - Performance tests measure task durations and error rates.","title":"\ud83e\udde9 Integration with CI/CD and Testing"},{"location":"architecture/17-observability-logging-and-metrics/#example-ci-configuration","text":"env: OTEL_EXPORTER_OTLP_ENDPOINT: \"http://otel-collector:4317\" PROMETHEUS_MULTIPROC_DIR: \"/tmp/metrics\"","title":"Example CI configuration:"},{"location":"architecture/17-observability-logging-and-metrics/#future-enhancements","text":"Distributed tracing for multi-cluster deployments. Real-time log streaming to the web UI. AI-assisted anomaly detection for workflow performance. Adaptive sampling for trace volume reduction. On-demand debug mode via CLI flag ( --trace verbose ). Next: Error Handling & Recovery","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/18-error-handling-and-recovery/","text":"18 \u2014 Error Handling, Fault Tolerance & Recovery Architecture \u00b6 \ud83e\udded Overview \u00b6 SecFlow is designed to operate in unpredictable environments \u2014 remote APIs, external security tools, local sandboxes, and asynchronous task queues. This chapter defines the resilience layer of the platform, ensuring that the system can detect, contain, recover, and self-heal from errors without compromising data integrity or user experience. \ud83e\uddf1 Core Resilience Principles \u00b6 Principle Description Isolation Failures in one tool or workflow should not cascade. Retryability Transient failures should automatically reattempt. Idempotency Repeated executions must produce consistent outcomes. Observability Every error must be traceable with full context. Graceful Degradation System continues partially even under partial failure. \u2699\ufe0f Error Taxonomy \u00b6 Errors are classified to determine handling strategy: Type Example Recovery Strategy Transient Network timeout, rate-limit, temporary unavailability Retry with exponential backoff Permanent Invalid config, missing file, malformed schema Fail fast, log, require user fix External Tool Nuclei crash, Feroxbuster exit code \u2260 0 Capture stdout/stderr, mark node failed Internal Logic Python exception, schema mismatch Rollback transaction, log critical Security Violation Sandbox breakout, unauthorized access Immediate isolation + alert User Abort Manual workflow stop Graceful cancellation, cleanup resources \ud83e\udde9 Error Handling Architecture \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Engine<br/>- Retry Controller<br/>- Error Propagation Manager<br/>- Compensation Handlers<br/>- Dead Letter Queue\"] B[\"Observability & Audit Log\"] A --> B \ud83e\udde0 Exception Handling Model \u00b6 All SecFlow components use a unified exception hierarchy: class SecFlowError(Exception): \"\"\"Base class for all SecFlow exceptions.\"\"\" class TransientError(SecFlowError): \"\"\"Recoverable error, eligible for retry.\"\"\" class PermanentError(SecFlowError): \"\"\"Non-recoverable error, must be logged and halted.\"\"\" class SecurityError(SecFlowError): \"\"\"Unauthorized or unsafe action detected.\"\"\" Every operation that might fail is wrapped in a retry-safe decorator. \ud83d\udd01 Retry Logic & Tenacity Integration \u00b6 SecFlow uses the Tenacity library for intelligent retries. from tenacity import retry, wait_exponential, stop_after_attempt @retry( wait=wait_exponential(multiplier=1, min=2, max=30), stop=stop_after_attempt(5), retry_error_callback=lambda r: log_error(r) ) def run_tool(tool_name, args): return subprocess.run(args, check=True) Retry Rules \u00b6 Context Max Retries Delay Type API HTTP Requests 5 Exponential CVE Enrichment Queries 3 Linear Worker Tasks 3 Exponential File System Operations 2 Immediate PoC Sandbox Launch 1 No retry (for safety) \ud83e\uddf1 Circuit Breaker Pattern \u00b6 SecFlow prevents repeated failures from overloading systems via circuit breakers. Implementation Example \u00b6 class CircuitBreaker: def __init__(self, threshold=5, timeout=60): self.failures = 0 self.opened_at = None self.threshold = threshold self.timeout = timeout def record_failure(self): self.failures += 1 if self.failures >= self.threshold: self.opened_at = datetime.utcnow() def can_execute(self): if not self.opened_at: return True return (datetime.utcnow() - self.opened_at).seconds > self.timeout Used for: - Remote API (NVD, OSV, Exploit-DB) - File I/O saturation - Tool wrappers under repeated crashes \ud83e\udde9 Dead Letter Queue (DLQ) \u00b6 Failed tasks that exceed retry limits are pushed into the DLQ for manual review. @app.task(bind=True, max_retries=3) def run_scan(self, task_id): try: run_workflow(task_id) except Exception as e: if self.request.retries == self.max_retries: enqueue_dlq(task_id, str(e)) raise self.retry(exc=e) DLQ entries include: \u00b6 Task ID Workflow ID Exception message Retry count Timestamp Example DLQ record: \u00b6 { \"task\": \"wf-1234-node-nuclei\", \"error\": \"Connection timeout to target\", \"retries\": 3, \"timestamp\": \"2025-10-06T10:22:00Z\" } \ud83e\udde0 Self-Healing Workflows \u00b6 SecFlow supports automatic node rehydration \u2014 failed steps in a workflow can be restarted independently without restarting the entire pipeline. Rehydration Process \u00b6 Detect failed node. Mark upstream outputs as valid. Restart failed node only. Merge results into workflow graph. CLI example: \u00b6 SecFlow workflow resume --node nuclei --project acme-api \ud83e\udde9 Transactional Integrity \u00b6 Database operations are wrapped in ACID transactions using SQLModel context managers: from sqlmodel import Session def save_finding(finding): with Session(engine) as session: try: session.add(finding) session.commit() except Exception: session.rollback() raise All cross-project mutations (findings, triage, cache) are transactional. \ud83e\udde0 Error Event Logging & Correlation \u00b6 Each exception generates an audit entry: { \"event\": \"error\", \"component\": \"worker\", \"type\": \"TransientError\", \"workflow_id\": \"wf-abc123\", \"trace_id\": \"b73f0b7c-47f9-4bb3-9b9c-ffacbd1d6a67\", \"message\": \"Feroxbuster timeout\", \"retries\": 3 } Errors are correlated with: - Workflow Trace ID - Finding UUID (if relevant) - User and project context This allows full replay and debugging via observability dashboards. \u2699\ufe0f Graceful Degradation \u00b6 If a subsystem fails (e.g., enrichment API offline): - Workflows continue with reduced functionality. - Missing data marked as \"partial\": true . - Users notified in the triage panel: \u26a0 CVE enrichment service temporarily unavailable \u2014 retry later. \ud83e\udde9 Alerting & Notification Hooks \u00b6 Integration with Prometheus Alertmanager for system errors. Optional Slack / Email webhook for high-severity failures. Rate-limited notifications to avoid alert fatigue. Example alert webhook payload: \u00b6 { \"severity\": \"critical\", \"component\": \"sandbox\", \"message\": \"PoC execution timeout\", \"project\": \"api-audit\", \"trace_id\": \"a2c134b5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" } \ud83e\uddf1 Recovery Strategies \u00b6 Context Recovery Action Sandbox crash Auto-restart with clean container API outage Retry with backoff + circuit breaker Tool misconfiguration Disable tool temporarily, notify user Cache corruption Rebuild from source Disk full Trigger GC and alert Worker crash Celery task re-queued DB lock contention Exponential backoff retry \ud83e\udde0 Example Error Lifecycle \u00b6 [Error Detected] \u2192 [Retry 1/3] \u2192 [Retry 2/3] \u2192 [DLQ] \u2192 [Alert sent to Slack] \u2192 [Analyst re-runs workflow node] \u2192 [Recovered] \ud83d\udd12 Security Implications \u00b6 Sensitive stack traces are redacted before exposure. Error details logged internally only. External responses use generic safe messages: {\"error\": \"Internal processing issue, please retry later.\"} \ud83d\udd2e Future Enhancements \u00b6 Adaptive retry policies based on machine learning. AI-powered incident summarization for triage. Transactional outbox pattern for guaranteed task delivery. Fine-grained chaos testing integrated in CI/CD. Next: Risk Assessment & Scoring Framework","title":"Error Handling"},{"location":"architecture/18-error-handling-and-recovery/#18-error-handling-fault-tolerance-recovery-architecture","text":"","title":"18 \u2014 Error Handling, Fault Tolerance &amp; Recovery Architecture"},{"location":"architecture/18-error-handling-and-recovery/#overview","text":"SecFlow is designed to operate in unpredictable environments \u2014 remote APIs, external security tools, local sandboxes, and asynchronous task queues. This chapter defines the resilience layer of the platform, ensuring that the system can detect, contain, recover, and self-heal from errors without compromising data integrity or user experience.","title":"\ud83e\udded Overview"},{"location":"architecture/18-error-handling-and-recovery/#core-resilience-principles","text":"Principle Description Isolation Failures in one tool or workflow should not cascade. Retryability Transient failures should automatically reattempt. Idempotency Repeated executions must produce consistent outcomes. Observability Every error must be traceable with full context. Graceful Degradation System continues partially even under partial failure.","title":"\ud83e\uddf1 Core Resilience Principles"},{"location":"architecture/18-error-handling-and-recovery/#error-taxonomy","text":"Errors are classified to determine handling strategy: Type Example Recovery Strategy Transient Network timeout, rate-limit, temporary unavailability Retry with exponential backoff Permanent Invalid config, missing file, malformed schema Fail fast, log, require user fix External Tool Nuclei crash, Feroxbuster exit code \u2260 0 Capture stdout/stderr, mark node failed Internal Logic Python exception, schema mismatch Rollback transaction, log critical Security Violation Sandbox breakout, unauthorized access Immediate isolation + alert User Abort Manual workflow stop Graceful cancellation, cleanup resources","title":"\u2699\ufe0f Error Taxonomy"},{"location":"architecture/18-error-handling-and-recovery/#error-handling-architecture","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Engine<br/>- Retry Controller<br/>- Error Propagation Manager<br/>- Compensation Handlers<br/>- Dead Letter Queue\"] B[\"Observability & Audit Log\"] A --> B","title":"\ud83e\udde9 Error Handling Architecture"},{"location":"architecture/18-error-handling-and-recovery/#exception-handling-model","text":"All SecFlow components use a unified exception hierarchy: class SecFlowError(Exception): \"\"\"Base class for all SecFlow exceptions.\"\"\" class TransientError(SecFlowError): \"\"\"Recoverable error, eligible for retry.\"\"\" class PermanentError(SecFlowError): \"\"\"Non-recoverable error, must be logged and halted.\"\"\" class SecurityError(SecFlowError): \"\"\"Unauthorized or unsafe action detected.\"\"\" Every operation that might fail is wrapped in a retry-safe decorator.","title":"\ud83e\udde0 Exception Handling Model"},{"location":"architecture/18-error-handling-and-recovery/#retry-logic-tenacity-integration","text":"SecFlow uses the Tenacity library for intelligent retries. from tenacity import retry, wait_exponential, stop_after_attempt @retry( wait=wait_exponential(multiplier=1, min=2, max=30), stop=stop_after_attempt(5), retry_error_callback=lambda r: log_error(r) ) def run_tool(tool_name, args): return subprocess.run(args, check=True)","title":"\ud83d\udd01 Retry Logic &amp; Tenacity Integration"},{"location":"architecture/18-error-handling-and-recovery/#retry-rules","text":"Context Max Retries Delay Type API HTTP Requests 5 Exponential CVE Enrichment Queries 3 Linear Worker Tasks 3 Exponential File System Operations 2 Immediate PoC Sandbox Launch 1 No retry (for safety)","title":"Retry Rules"},{"location":"architecture/18-error-handling-and-recovery/#circuit-breaker-pattern","text":"SecFlow prevents repeated failures from overloading systems via circuit breakers.","title":"\ud83e\uddf1 Circuit Breaker Pattern"},{"location":"architecture/18-error-handling-and-recovery/#implementation-example","text":"class CircuitBreaker: def __init__(self, threshold=5, timeout=60): self.failures = 0 self.opened_at = None self.threshold = threshold self.timeout = timeout def record_failure(self): self.failures += 1 if self.failures >= self.threshold: self.opened_at = datetime.utcnow() def can_execute(self): if not self.opened_at: return True return (datetime.utcnow() - self.opened_at).seconds > self.timeout Used for: - Remote API (NVD, OSV, Exploit-DB) - File I/O saturation - Tool wrappers under repeated crashes","title":"Implementation Example"},{"location":"architecture/18-error-handling-and-recovery/#dead-letter-queue-dlq","text":"Failed tasks that exceed retry limits are pushed into the DLQ for manual review. @app.task(bind=True, max_retries=3) def run_scan(self, task_id): try: run_workflow(task_id) except Exception as e: if self.request.retries == self.max_retries: enqueue_dlq(task_id, str(e)) raise self.retry(exc=e)","title":"\ud83e\udde9 Dead Letter Queue (DLQ)"},{"location":"architecture/18-error-handling-and-recovery/#dlq-entries-include","text":"Task ID Workflow ID Exception message Retry count Timestamp","title":"DLQ entries include:"},{"location":"architecture/18-error-handling-and-recovery/#example-dlq-record","text":"{ \"task\": \"wf-1234-node-nuclei\", \"error\": \"Connection timeout to target\", \"retries\": 3, \"timestamp\": \"2025-10-06T10:22:00Z\" }","title":"Example DLQ record:"},{"location":"architecture/18-error-handling-and-recovery/#self-healing-workflows","text":"SecFlow supports automatic node rehydration \u2014 failed steps in a workflow can be restarted independently without restarting the entire pipeline.","title":"\ud83e\udde0 Self-Healing Workflows"},{"location":"architecture/18-error-handling-and-recovery/#rehydration-process","text":"Detect failed node. Mark upstream outputs as valid. Restart failed node only. Merge results into workflow graph.","title":"Rehydration Process"},{"location":"architecture/18-error-handling-and-recovery/#cli-example","text":"SecFlow workflow resume --node nuclei --project acme-api","title":"CLI example:"},{"location":"architecture/18-error-handling-and-recovery/#transactional-integrity","text":"Database operations are wrapped in ACID transactions using SQLModel context managers: from sqlmodel import Session def save_finding(finding): with Session(engine) as session: try: session.add(finding) session.commit() except Exception: session.rollback() raise All cross-project mutations (findings, triage, cache) are transactional.","title":"\ud83e\udde9 Transactional Integrity"},{"location":"architecture/18-error-handling-and-recovery/#error-event-logging-correlation","text":"Each exception generates an audit entry: { \"event\": \"error\", \"component\": \"worker\", \"type\": \"TransientError\", \"workflow_id\": \"wf-abc123\", \"trace_id\": \"b73f0b7c-47f9-4bb3-9b9c-ffacbd1d6a67\", \"message\": \"Feroxbuster timeout\", \"retries\": 3 } Errors are correlated with: - Workflow Trace ID - Finding UUID (if relevant) - User and project context This allows full replay and debugging via observability dashboards.","title":"\ud83e\udde0 Error Event Logging &amp; Correlation"},{"location":"architecture/18-error-handling-and-recovery/#graceful-degradation","text":"If a subsystem fails (e.g., enrichment API offline): - Workflows continue with reduced functionality. - Missing data marked as \"partial\": true . - Users notified in the triage panel: \u26a0 CVE enrichment service temporarily unavailable \u2014 retry later.","title":"\u2699\ufe0f Graceful Degradation"},{"location":"architecture/18-error-handling-and-recovery/#alerting-notification-hooks","text":"Integration with Prometheus Alertmanager for system errors. Optional Slack / Email webhook for high-severity failures. Rate-limited notifications to avoid alert fatigue.","title":"\ud83e\udde9 Alerting &amp; Notification Hooks"},{"location":"architecture/18-error-handling-and-recovery/#example-alert-webhook-payload","text":"{ \"severity\": \"critical\", \"component\": \"sandbox\", \"message\": \"PoC execution timeout\", \"project\": \"api-audit\", \"trace_id\": \"a2c134b5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" }","title":"Example alert webhook payload:"},{"location":"architecture/18-error-handling-and-recovery/#recovery-strategies","text":"Context Recovery Action Sandbox crash Auto-restart with clean container API outage Retry with backoff + circuit breaker Tool misconfiguration Disable tool temporarily, notify user Cache corruption Rebuild from source Disk full Trigger GC and alert Worker crash Celery task re-queued DB lock contention Exponential backoff retry","title":"\ud83e\uddf1 Recovery Strategies"},{"location":"architecture/18-error-handling-and-recovery/#example-error-lifecycle","text":"[Error Detected] \u2192 [Retry 1/3] \u2192 [Retry 2/3] \u2192 [DLQ] \u2192 [Alert sent to Slack] \u2192 [Analyst re-runs workflow node] \u2192 [Recovered]","title":"\ud83e\udde0 Example Error Lifecycle"},{"location":"architecture/18-error-handling-and-recovery/#security-implications","text":"Sensitive stack traces are redacted before exposure. Error details logged internally only. External responses use generic safe messages: {\"error\": \"Internal processing issue, please retry later.\"}","title":"\ud83d\udd12 Security Implications"},{"location":"architecture/18-error-handling-and-recovery/#future-enhancements","text":"Adaptive retry policies based on machine learning. AI-powered incident summarization for triage. Transactional outbox pattern for guaranteed task delivery. Fine-grained chaos testing integrated in CI/CD. Next: Risk Assessment & Scoring Framework","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/19-risk-assessment-framework/","text":"19 \u2014 Risk Assessment, Scoring & Prioritization Framework \u00b6 \ud83e\udded Overview \u00b6 The Risk Assessment Framework in SecFlow provides a consistent and transparent way to evaluate and prioritize vulnerabilities across multiple projects and tools. It merges four complementary standards: NIST 5\u00d75 Matrix \u2014 contextual risk evaluation (Impact \u00d7 Likelihood). CVSS v3.1 \u2014 standardized vulnerability severity scoring. CWE / OWASP Mapping \u2014 weakness classification. MITRE ATT&CK Correlation \u2014 adversarial behavior context. This layered model supports both quantitative (numeric scores) and qualitative (High/Critical/etc.) analysis for triage and reporting. \ud83e\uddf1 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"SecFlow Risk Engine<br/>- CVSS Normalizer (from findings or enrichment)<br/>- NIST 5\u00d75 Contextual Matrix<br/>- MITRE ATT&CK Mapper<br/>- Risk Aggregator & Scorer<br/>- Project Risk Dashboard\"] \u2699\ufe0f Core Objectives \u00b6 Goal Description Standardization Consistent risk scoring across tools and projects. Transparency Traceable logic behind every score. Extensibility Pluggable scoring policies (per organization). Automation Auto-scoring during enrichment and triage. Context-Awareness Includes exploitability, exposure, and asset criticality. \ud83e\uddf1 Scoring Pipeline \u00b6 Finding \u2193 CVSS Vector Parsing \u2193 CWE / OWASP Classification \u2193 Exploit & Exposure Context \u2193 NIST Risk Matrix Evaluation \u2193 Final Risk Score (0\u2013100) + Risk Tier \ud83e\udde0 CVSS Normalization \u00b6 SecFlow ingests CVSS vectors either from NVD (via enrichment) or tool metadata. Example: \u00b6 CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H Converted into internal representation: { \"base_score\": 9.8, \"impact_subscore\": 5.9, \"exploitability_subscore\": 3.9, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\" } If missing, heuristic fallback is applied based on CWE ID or OWASP category (see 13-cve-cwe-poc-enrichment-layer.md ). \ud83e\udde9 CWE / OWASP Mapping \u00b6 CWE ID OWASP Category Default Impact Default Likelihood 79 A03: Injection High High 89 A03: Injection Very High High 200 A01: Broken Access Control High Medium 601 A10: SSRF Medium Medium 787 A05: Buffer Overflow Critical Medium 352 A08: CSRF Medium High Mappings are maintained in /resources/mappings/cwe_owasp.json . \ud83e\udde0 MITRE ATT&CK Mapping \u00b6 Findings enriched with ATT&CK technique IDs (TIDs) during correlation (see 13-cve-cwe-poc-enrichment-layer.md ) are leveraged to infer attack chain context. MITRE Technique ID Tactic Effect T1059.007 Execution Cross-Site Scripting T1505.003 Persistence SQL Injection T1071.001 Command & Control Web Protocols T1190 Initial Access Exploit Public-Facing App This mapping helps classify whether the finding affects initial compromise, lateral movement, or data exfiltration. \u2699\ufe0f NIST 5\u00d75 Risk Matrix \u00b6 Definition \u00b6 Impact \u2193 / Likelihood \u2192 Very Low Low Medium High Very High Very High Medium High High Critical Critical High Low Medium High High Critical Medium Low Low Medium High High Low Low Low Low Medium High Very Low Low Low Low Low Medium Mapping to Severity \u00b6 Result Score Range Label Critical 90\u2013100 \ud83d\udd25 High 70\u201389 \u26a0\ufe0f Medium 40\u201369 \u2696\ufe0f Low 20\u201339 \ud83e\udde9 Informational 0\u201319 \u2139\ufe0f \ud83e\udde9 Likelihood Factors \u00b6 Likelihood is dynamically computed using multiple context sources: Factor Description Weight Exploit Availability Known PoC, KEV presence +0.3 Network Exposure Publicly reachable target +0.25 Authentication Required Lowers likelihood if true -0.15 Complexity Tool-derived complexity \u00b10.1 Detection Confidence Based on finding engine \u00b10.2 Pseudo-code: \u00b6 def likelihood_score(finding): score = 0.3 if finding.poc_available else 0 if finding.exposure == \"internet\": score += 0.25 if finding.auth_required: score -= 0.15 if finding.complexity == \"low\": score += 0.1 return min(max(score, 0), 1) \ud83e\udde0 Impact Factors \u00b6 Impact combines technical and business context: Factor Example Weight Confidentiality Data exposure +0.3 Integrity Tampering possible +0.3 Availability Service crash, DoS +0.2 Privilege Escalation Root/system access +0.2 Asset Criticality System importance +0.4 Final impact = weighted sum normalized to 1.0. \u2699\ufe0f Combined Risk Formula \u00b6 Final quantitative risk score (0\u2013100): risk_score = ((CVSS_base / 10) * 0.6 + impact_factor * 0.25 + likelihood_factor * 0.15) * 100 Rounded to nearest integer. Example \u00b6 Metric Value CVSS Base 9.8 Impact Factor 0.8 Likelihood Factor 0.7 Final Score ((0.98*0.6)+(0.8*0.25)+(0.7*0.15))*100 = 89.7 \u2192 High \ud83e\udde0 Contextual Adjustments \u00b6 Certain contexts modify final risk score: Context Adjustment Active exploit in wild (CISA KEV) +10 Proof-of-concept verified +5 Patched version available -5 Internal-only system -10 Compensating controls present -15 Scores are capped at 100 and floored at 0. \ud83e\udde9 Aggregated Risk Dashboard \u00b6 Each project's analytics tab visualizes: Metric Description Average CVSS per project Top 10 findings by risk score Risk evolution over time Distribution by OWASP category ATT&CK tactics heatmap Example chart: \u00b6 %%{init: {\"theme\":\"neutral\"}}%% xychart-beta title \"Risk Trend (Score over Time)\" x-axis [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"] y-axis \"Risk Score\" 0 --> 100 bar [85, 70, 45, 60, 80] \u2699\ufe0f Risk Normalization Across Tools \u00b6 Different tools report different severity models (e.g., \"Critical\", \"High\", \"Medium\"). SecFlow converts all to internal numeric ranges: Tool Critical High Medium Low Nuclei 90\u2013100 70\u201389 40\u201369 20\u201339 ZAP 85\u2013100 65\u201384 35\u201364 15\u201334 Burp 90\u2013100 75\u201389 45\u201374 25\u201344 All are harmonized via the risk formula to produce consistent prioritization. \ud83e\udde0 Risk Aggregation & Reporting \u00b6 Project-level risk is computed as weighted mean: def project_risk(findings): weights = [f.cvss_score * f.impact_weight for f in findings] return sum(weights) / len(weights) Analytics engine stores snapshots in /analytics/risk_snapshots/ . \ud83e\udde9 Risk API Endpoints \u00b6 Endpoint Method Description /api/v1/risk/score/{finding_id} GET Returns risk vector and classification /api/v1/risk/project/{id} GET Aggregated project risk summary /api/v1/risk/export POST Export risk data to JSON/CSV /api/v1/risk/heatmap GET Generates OWASP \u00d7 ATT&CK matrix Example Response: \u00b6 { \"finding_id\": \"abcd-123\", \"score\": 89.7, \"severity\": \"High\", \"CVSS\": 9.8, \"impact_factor\": 0.8, \"likelihood_factor\": 0.7, \"nist_matrix\": \"High/High \u2192 Critical\", \"owasp\": \"A03: Injection\", \"mitre_tid\": \"T1505.003\" } \ud83d\udd12 Auditability & Traceability \u00b6 Every risk computation is versioned and auditable: - Stored with enrichment metadata hash. - Recomputed automatically if CVSS source data updates. Log entry example: \u00b6 { \"event\": \"risk_recalc\", \"finding_id\": \"abcd-123\", \"old_score\": 78, \"new_score\": 89.7, \"reason\": \"CISA KEV inclusion\" } \ud83d\udd2e Future Enhancements \u00b6 Integration with EPSS (Exploit Prediction Scoring System). ML-based contextual risk forecasting. Auto-adjustment based on exploit telemetry feeds. Risk-driven workflow prioritization for automated scanning. AI-assistant suggestions for mitigations. Next: Migration & Implementation Phases","title":"Risk Assessment"},{"location":"architecture/19-risk-assessment-framework/#19-risk-assessment-scoring-prioritization-framework","text":"","title":"19 \u2014 Risk Assessment, Scoring &amp; Prioritization Framework"},{"location":"architecture/19-risk-assessment-framework/#overview","text":"The Risk Assessment Framework in SecFlow provides a consistent and transparent way to evaluate and prioritize vulnerabilities across multiple projects and tools. It merges four complementary standards: NIST 5\u00d75 Matrix \u2014 contextual risk evaluation (Impact \u00d7 Likelihood). CVSS v3.1 \u2014 standardized vulnerability severity scoring. CWE / OWASP Mapping \u2014 weakness classification. MITRE ATT&CK Correlation \u2014 adversarial behavior context. This layered model supports both quantitative (numeric scores) and qualitative (High/Critical/etc.) analysis for triage and reporting.","title":"\ud83e\udded Overview"},{"location":"architecture/19-risk-assessment-framework/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"SecFlow Risk Engine<br/>- CVSS Normalizer (from findings or enrichment)<br/>- NIST 5\u00d75 Contextual Matrix<br/>- MITRE ATT&CK Mapper<br/>- Risk Aggregator & Scorer<br/>- Project Risk Dashboard\"]","title":"\ud83e\uddf1 Architecture Overview"},{"location":"architecture/19-risk-assessment-framework/#core-objectives","text":"Goal Description Standardization Consistent risk scoring across tools and projects. Transparency Traceable logic behind every score. Extensibility Pluggable scoring policies (per organization). Automation Auto-scoring during enrichment and triage. Context-Awareness Includes exploitability, exposure, and asset criticality.","title":"\u2699\ufe0f Core Objectives"},{"location":"architecture/19-risk-assessment-framework/#scoring-pipeline","text":"Finding \u2193 CVSS Vector Parsing \u2193 CWE / OWASP Classification \u2193 Exploit & Exposure Context \u2193 NIST Risk Matrix Evaluation \u2193 Final Risk Score (0\u2013100) + Risk Tier","title":"\ud83e\uddf1 Scoring Pipeline"},{"location":"architecture/19-risk-assessment-framework/#cvss-normalization","text":"SecFlow ingests CVSS vectors either from NVD (via enrichment) or tool metadata.","title":"\ud83e\udde0 CVSS Normalization"},{"location":"architecture/19-risk-assessment-framework/#example","text":"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H Converted into internal representation: { \"base_score\": 9.8, \"impact_subscore\": 5.9, \"exploitability_subscore\": 3.9, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\" } If missing, heuristic fallback is applied based on CWE ID or OWASP category (see 13-cve-cwe-poc-enrichment-layer.md ).","title":"Example:"},{"location":"architecture/19-risk-assessment-framework/#cwe-owasp-mapping","text":"CWE ID OWASP Category Default Impact Default Likelihood 79 A03: Injection High High 89 A03: Injection Very High High 200 A01: Broken Access Control High Medium 601 A10: SSRF Medium Medium 787 A05: Buffer Overflow Critical Medium 352 A08: CSRF Medium High Mappings are maintained in /resources/mappings/cwe_owasp.json .","title":"\ud83e\udde9 CWE / OWASP Mapping"},{"location":"architecture/19-risk-assessment-framework/#mitre-attck-mapping","text":"Findings enriched with ATT&CK technique IDs (TIDs) during correlation (see 13-cve-cwe-poc-enrichment-layer.md ) are leveraged to infer attack chain context. MITRE Technique ID Tactic Effect T1059.007 Execution Cross-Site Scripting T1505.003 Persistence SQL Injection T1071.001 Command & Control Web Protocols T1190 Initial Access Exploit Public-Facing App This mapping helps classify whether the finding affects initial compromise, lateral movement, or data exfiltration.","title":"\ud83e\udde0 MITRE ATT&amp;CK Mapping"},{"location":"architecture/19-risk-assessment-framework/#nist-55-risk-matrix","text":"","title":"\u2699\ufe0f NIST 5\u00d75 Risk Matrix"},{"location":"architecture/19-risk-assessment-framework/#definition","text":"Impact \u2193 / Likelihood \u2192 Very Low Low Medium High Very High Very High Medium High High Critical Critical High Low Medium High High Critical Medium Low Low Medium High High Low Low Low Low Medium High Very Low Low Low Low Low Medium","title":"Definition"},{"location":"architecture/19-risk-assessment-framework/#mapping-to-severity","text":"Result Score Range Label Critical 90\u2013100 \ud83d\udd25 High 70\u201389 \u26a0\ufe0f Medium 40\u201369 \u2696\ufe0f Low 20\u201339 \ud83e\udde9 Informational 0\u201319 \u2139\ufe0f","title":"Mapping to Severity"},{"location":"architecture/19-risk-assessment-framework/#likelihood-factors","text":"Likelihood is dynamically computed using multiple context sources: Factor Description Weight Exploit Availability Known PoC, KEV presence +0.3 Network Exposure Publicly reachable target +0.25 Authentication Required Lowers likelihood if true -0.15 Complexity Tool-derived complexity \u00b10.1 Detection Confidence Based on finding engine \u00b10.2","title":"\ud83e\udde9 Likelihood Factors"},{"location":"architecture/19-risk-assessment-framework/#pseudo-code","text":"def likelihood_score(finding): score = 0.3 if finding.poc_available else 0 if finding.exposure == \"internet\": score += 0.25 if finding.auth_required: score -= 0.15 if finding.complexity == \"low\": score += 0.1 return min(max(score, 0), 1)","title":"Pseudo-code:"},{"location":"architecture/19-risk-assessment-framework/#impact-factors","text":"Impact combines technical and business context: Factor Example Weight Confidentiality Data exposure +0.3 Integrity Tampering possible +0.3 Availability Service crash, DoS +0.2 Privilege Escalation Root/system access +0.2 Asset Criticality System importance +0.4 Final impact = weighted sum normalized to 1.0.","title":"\ud83e\udde0 Impact Factors"},{"location":"architecture/19-risk-assessment-framework/#combined-risk-formula","text":"Final quantitative risk score (0\u2013100): risk_score = ((CVSS_base / 10) * 0.6 + impact_factor * 0.25 + likelihood_factor * 0.15) * 100 Rounded to nearest integer.","title":"\u2699\ufe0f Combined Risk Formula"},{"location":"architecture/19-risk-assessment-framework/#example_1","text":"Metric Value CVSS Base 9.8 Impact Factor 0.8 Likelihood Factor 0.7 Final Score ((0.98*0.6)+(0.8*0.25)+(0.7*0.15))*100 = 89.7 \u2192 High","title":"Example"},{"location":"architecture/19-risk-assessment-framework/#contextual-adjustments","text":"Certain contexts modify final risk score: Context Adjustment Active exploit in wild (CISA KEV) +10 Proof-of-concept verified +5 Patched version available -5 Internal-only system -10 Compensating controls present -15 Scores are capped at 100 and floored at 0.","title":"\ud83e\udde0 Contextual Adjustments"},{"location":"architecture/19-risk-assessment-framework/#aggregated-risk-dashboard","text":"Each project's analytics tab visualizes: Metric Description Average CVSS per project Top 10 findings by risk score Risk evolution over time Distribution by OWASP category ATT&CK tactics heatmap","title":"\ud83e\udde9 Aggregated Risk Dashboard"},{"location":"architecture/19-risk-assessment-framework/#example-chart","text":"%%{init: {\"theme\":\"neutral\"}}%% xychart-beta title \"Risk Trend (Score over Time)\" x-axis [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"] y-axis \"Risk Score\" 0 --> 100 bar [85, 70, 45, 60, 80]","title":"Example chart:"},{"location":"architecture/19-risk-assessment-framework/#risk-normalization-across-tools","text":"Different tools report different severity models (e.g., \"Critical\", \"High\", \"Medium\"). SecFlow converts all to internal numeric ranges: Tool Critical High Medium Low Nuclei 90\u2013100 70\u201389 40\u201369 20\u201339 ZAP 85\u2013100 65\u201384 35\u201364 15\u201334 Burp 90\u2013100 75\u201389 45\u201374 25\u201344 All are harmonized via the risk formula to produce consistent prioritization.","title":"\u2699\ufe0f Risk Normalization Across Tools"},{"location":"architecture/19-risk-assessment-framework/#risk-aggregation-reporting","text":"Project-level risk is computed as weighted mean: def project_risk(findings): weights = [f.cvss_score * f.impact_weight for f in findings] return sum(weights) / len(weights) Analytics engine stores snapshots in /analytics/risk_snapshots/ .","title":"\ud83e\udde0 Risk Aggregation &amp; Reporting"},{"location":"architecture/19-risk-assessment-framework/#risk-api-endpoints","text":"Endpoint Method Description /api/v1/risk/score/{finding_id} GET Returns risk vector and classification /api/v1/risk/project/{id} GET Aggregated project risk summary /api/v1/risk/export POST Export risk data to JSON/CSV /api/v1/risk/heatmap GET Generates OWASP \u00d7 ATT&CK matrix","title":"\ud83e\udde9 Risk API Endpoints"},{"location":"architecture/19-risk-assessment-framework/#example-response","text":"{ \"finding_id\": \"abcd-123\", \"score\": 89.7, \"severity\": \"High\", \"CVSS\": 9.8, \"impact_factor\": 0.8, \"likelihood_factor\": 0.7, \"nist_matrix\": \"High/High \u2192 Critical\", \"owasp\": \"A03: Injection\", \"mitre_tid\": \"T1505.003\" }","title":"Example Response:"},{"location":"architecture/19-risk-assessment-framework/#auditability-traceability","text":"Every risk computation is versioned and auditable: - Stored with enrichment metadata hash. - Recomputed automatically if CVSS source data updates.","title":"\ud83d\udd12 Auditability &amp; Traceability"},{"location":"architecture/19-risk-assessment-framework/#log-entry-example","text":"{ \"event\": \"risk_recalc\", \"finding_id\": \"abcd-123\", \"old_score\": 78, \"new_score\": 89.7, \"reason\": \"CISA KEV inclusion\" }","title":"Log entry example:"},{"location":"architecture/19-risk-assessment-framework/#future-enhancements","text":"Integration with EPSS (Exploit Prediction Scoring System). ML-based contextual risk forecasting. Auto-adjustment based on exploit telemetry feeds. Risk-driven workflow prioritization for automated scanning. AI-assistant suggestions for mitigations. Next: Migration & Implementation Phases","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/20-migration-and-implementation-phases/","text":"20 \u2014 Migration & Implementation Phases \u00b6 \ud83e\udded Overview \u00b6 The migration plan defines a controlled, four-phase rollout of SecFlow's refactored architecture \u2014 from scaffolding and core model setup to the full orchestration of tools, UI, and enrichment. This plan emphasizes: - Incremental refactoring (no big-bang rewrite) - Zero architectural drift (guardrails first) - Progressive integration (tools, plugins, UI) - Data isolation and cleanup (garbage collector enabled) \ud83d\udce6 Phase 0 \u2014 Foundation & Guardrails (Week 1) \u00b6 Objective \u00b6 Establish the new repository structure and enforce architectural discipline before any migration work. Tasks \u00b6 Create mono-repo scaffold under /src/SecFlow/ Add dev environment setup via Makefile , pyproject.toml , and Poetry Enable static analysis tooling : ruff for linting pyright for typing import-linter for import boundaries Setup unit testing scaffold: /tests/core , /tests/wrappers , /tests/plugins Define .github/workflows/ci.yml for matrix builds (Python 3.10\u20133.12) Establish base docs/architecture/ folder for ongoing documentation Expected Deliverables \u00b6 Working mono-repo with guardrails CI passing with lint/type/test checks Developer guide for environment setup ( docs/dev-setup.md ) Example Command \u00b6 make init make lint make test \ud83e\uddf1 Phase 1 \u2014 Core Models & Data Persistence (Week 2) \u00b6 Objective \u00b6 Move fundamental entities (Projects, Findings, Resources, Runs) into modular core-lib and storage packages. Tasks \u00b6 Create core-lib/ package: Models for Project, Finding, Resource, Run Pydantic schemas for DTOs Create storage/ package: Database adapters for SQLite (local) and PostgreSQL (production) Repository interfaces (IProjectRepo, IFindingsRepo, etc.) Alembic or SQLModel migrations Implement CRUD API endpoints: /api/v1/projects /api/v1/findings /api/v1/resources Add test fixtures for sample data Expected Deliverables \u00b6 Persistent data layer Core models validated by schema Functional CRUD endpoints 80%+ test coverage on models and repos Example Model \u00b6 class Project(BaseModel): id: UUID name: str owner: str description: Optional[str] created_at: datetime updated_at: datetime \u2699\ufe0f Phase 2 \u2014 Findings Engine, Wrappers & Workflow (Week 3) \u00b6 Objective \u00b6 Integrate scanning tools (Nuclei, Feroxbuster, Katana, etc.) into the new workflow engine and plugin registry. Tasks \u00b6 Implement findings-engine/ : Normalization layer for all scanners Parser adapters for each tool Implement wrappers/ : NucleiWrapper, FeroxWrapper, ZAPWrapper Each using standardized manifest + sandbox Create plugins/ package: Detection and enrichment plugins (e.g., CVEMapper, RiskScorer) Build workflow engine with DAG executor: YAML recipe parsing Input/output data mapping Caching and persistence Integrate tool registry UI in web frontend Expected Deliverables \u00b6 Tool registry and manifest system Workflow DAG execution engine Normalized findings output (JSON schema compliant) Risk engine integration (Phase 1 of enrichment) Example Wrapper Interface \u00b6 class ToolWrapper(Protocol): def prepare(self, config: Dict[str, Any]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run(self) -> ToolOutput: \"\"\"Execute tool and return output.\"\"\" pass def parse_output(self, raw: str) -> List[Finding]: \"\"\"Parse raw output into findings.\"\"\" pass \ud83c\udf10 Phase 3 \u2014 API, Worker, and Triage UI (Week 4) \u00b6 Objective \u00b6 Deliver full orchestration capability through APIs, background workers, and a lightweight HTMX/React UI. Tasks \u00b6 Create web-api/ : REST endpoints for workflows, findings, triage WebSocket for live updates Create worker/ : Celery/asyncio-based job processor Queues for workflow nodes and enrichment Create triage-ui/ : Interactive HTMX dashboard for findings triage Tabs: \"Projects\", \"Findings\", \"Tools\", \"Metrics\" Implement user auth & RBAC JWT + role middleware Add audit logging for all changes Integrate observability stack (Prometheus, OpenTelemetry) Expected Deliverables \u00b6 Full end-to-end scan \u2192 finding \u2192 triage pipeline Live progress dashboard Role-based access and logging Metrics export for dashboards Example Endpoint \u00b6 @app.post(\"/api/v1/workflows/run\") async def run_workflow(workflow_id: str): job_id = await worker.enqueue(workflow_id) return {\"status\": \"queued\", \"job_id\": job_id} \ud83e\uddf9 Phase 4 \u2014 Garbage Collection, AI, and Advanced Analytics (Week 5+) \u00b6 Objective \u00b6 Introduce garbage collection, retention policy enforcement, and AI-assisted triage. Tasks \u00b6 Implement GarbageCollector service: Sweep orphaned runs and findings Archive logs >30 days Introduce CVE/CWE/PoC Enrichment Integration with NVD, OSV, ExploitDB Deploy AI assistant for: Finding summaries Risk triage automation Workflow suggestions Add cross-project analytics dashboard Implement export formats (PDF, CSV, JSON) Expected Deliverables \u00b6 Fully production-ready orchestration platform Retention-safe data lifecycle AI triage beta enabled Analytics module complete \ud83d\udcc8 Migration Timeline Overview \u00b6 Week Phase Key Deliverables 1 Phase 0 \u2014 Scaffold Repo, linting, CI/CD, guardrails 2 Phase 1 \u2014 Core Models, DB, CRUD API 3 Phase 2 \u2014 Engine Wrappers, Plugins, Workflow 4 Phase 3 \u2014 API/UI Worker, Triage UI, Auth 5+ Phase 4 \u2014 AI/GC Retention, Enrichment, Analytics \ud83d\ude80 Deployment Strategy \u00b6 Branch-per-phase workflow ( feature/phase-1-core , etc.) Pre-merge CI enforcement for all phases Feature flags for new modules Nightly build for cross-validation Docker-compose dev stack for quick testing Example Command \u00b6 docker compose up -d pytest --maxfail=1 --disable-warnings \ud83e\udde0 Key Success Metrics \u00b6 Metric Target Test Coverage >90% for core-lib & storage CI Lint Pass Rate 100% Workflow Execution Latency <300ms/node avg Risk Score Accuracy \u00b15% of reference Mean Time to Deploy <10 min via CI/CD \ud83d\udd2e Next Steps \u00b6 After migration completion: - Freeze legacy JSON-store modules. - Enable CVE/PoC enrichment. - Integrate AI triage assistant. - Conduct internal red-team testing. - Prepare open-source release structure. Next: CI/CD & Testing Strategy","title":"Migration Plan"},{"location":"architecture/20-migration-and-implementation-phases/#20-migration-implementation-phases","text":"","title":"20 \u2014 Migration &amp; Implementation Phases"},{"location":"architecture/20-migration-and-implementation-phases/#overview","text":"The migration plan defines a controlled, four-phase rollout of SecFlow's refactored architecture \u2014 from scaffolding and core model setup to the full orchestration of tools, UI, and enrichment. This plan emphasizes: - Incremental refactoring (no big-bang rewrite) - Zero architectural drift (guardrails first) - Progressive integration (tools, plugins, UI) - Data isolation and cleanup (garbage collector enabled)","title":"\ud83e\udded Overview"},{"location":"architecture/20-migration-and-implementation-phases/#phase-0-foundation-guardrails-week-1","text":"","title":"\ud83d\udce6 Phase 0 \u2014 Foundation &amp; Guardrails (Week 1)"},{"location":"architecture/20-migration-and-implementation-phases/#objective","text":"Establish the new repository structure and enforce architectural discipline before any migration work.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks","text":"Create mono-repo scaffold under /src/SecFlow/ Add dev environment setup via Makefile , pyproject.toml , and Poetry Enable static analysis tooling : ruff for linting pyright for typing import-linter for import boundaries Setup unit testing scaffold: /tests/core , /tests/wrappers , /tests/plugins Define .github/workflows/ci.yml for matrix builds (Python 3.10\u20133.12) Establish base docs/architecture/ folder for ongoing documentation","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables","text":"Working mono-repo with guardrails CI passing with lint/type/test checks Developer guide for environment setup ( docs/dev-setup.md )","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-command","text":"make init make lint make test","title":"Example Command"},{"location":"architecture/20-migration-and-implementation-phases/#phase-1-core-models-data-persistence-week-2","text":"","title":"\ud83e\uddf1 Phase 1 \u2014 Core Models &amp; Data Persistence (Week 2)"},{"location":"architecture/20-migration-and-implementation-phases/#objective_1","text":"Move fundamental entities (Projects, Findings, Resources, Runs) into modular core-lib and storage packages.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks_1","text":"Create core-lib/ package: Models for Project, Finding, Resource, Run Pydantic schemas for DTOs Create storage/ package: Database adapters for SQLite (local) and PostgreSQL (production) Repository interfaces (IProjectRepo, IFindingsRepo, etc.) Alembic or SQLModel migrations Implement CRUD API endpoints: /api/v1/projects /api/v1/findings /api/v1/resources Add test fixtures for sample data","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables_1","text":"Persistent data layer Core models validated by schema Functional CRUD endpoints 80%+ test coverage on models and repos","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-model","text":"class Project(BaseModel): id: UUID name: str owner: str description: Optional[str] created_at: datetime updated_at: datetime","title":"Example Model"},{"location":"architecture/20-migration-and-implementation-phases/#phase-2-findings-engine-wrappers-workflow-week-3","text":"","title":"\u2699\ufe0f Phase 2 \u2014 Findings Engine, Wrappers &amp; Workflow (Week 3)"},{"location":"architecture/20-migration-and-implementation-phases/#objective_2","text":"Integrate scanning tools (Nuclei, Feroxbuster, Katana, etc.) into the new workflow engine and plugin registry.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks_2","text":"Implement findings-engine/ : Normalization layer for all scanners Parser adapters for each tool Implement wrappers/ : NucleiWrapper, FeroxWrapper, ZAPWrapper Each using standardized manifest + sandbox Create plugins/ package: Detection and enrichment plugins (e.g., CVEMapper, RiskScorer) Build workflow engine with DAG executor: YAML recipe parsing Input/output data mapping Caching and persistence Integrate tool registry UI in web frontend","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables_2","text":"Tool registry and manifest system Workflow DAG execution engine Normalized findings output (JSON schema compliant) Risk engine integration (Phase 1 of enrichment)","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-wrapper-interface","text":"class ToolWrapper(Protocol): def prepare(self, config: Dict[str, Any]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run(self) -> ToolOutput: \"\"\"Execute tool and return output.\"\"\" pass def parse_output(self, raw: str) -> List[Finding]: \"\"\"Parse raw output into findings.\"\"\" pass","title":"Example Wrapper Interface"},{"location":"architecture/20-migration-and-implementation-phases/#phase-3-api-worker-and-triage-ui-week-4","text":"","title":"\ud83c\udf10 Phase 3 \u2014 API, Worker, and Triage UI (Week 4)"},{"location":"architecture/20-migration-and-implementation-phases/#objective_3","text":"Deliver full orchestration capability through APIs, background workers, and a lightweight HTMX/React UI.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks_3","text":"Create web-api/ : REST endpoints for workflows, findings, triage WebSocket for live updates Create worker/ : Celery/asyncio-based job processor Queues for workflow nodes and enrichment Create triage-ui/ : Interactive HTMX dashboard for findings triage Tabs: \"Projects\", \"Findings\", \"Tools\", \"Metrics\" Implement user auth & RBAC JWT + role middleware Add audit logging for all changes Integrate observability stack (Prometheus, OpenTelemetry)","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables_3","text":"Full end-to-end scan \u2192 finding \u2192 triage pipeline Live progress dashboard Role-based access and logging Metrics export for dashboards","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-endpoint","text":"@app.post(\"/api/v1/workflows/run\") async def run_workflow(workflow_id: str): job_id = await worker.enqueue(workflow_id) return {\"status\": \"queued\", \"job_id\": job_id}","title":"Example Endpoint"},{"location":"architecture/20-migration-and-implementation-phases/#phase-4-garbage-collection-ai-and-advanced-analytics-week-5","text":"","title":"\ud83e\uddf9 Phase 4 \u2014 Garbage Collection, AI, and Advanced Analytics (Week 5+)"},{"location":"architecture/20-migration-and-implementation-phases/#objective_4","text":"Introduce garbage collection, retention policy enforcement, and AI-assisted triage.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks_4","text":"Implement GarbageCollector service: Sweep orphaned runs and findings Archive logs >30 days Introduce CVE/CWE/PoC Enrichment Integration with NVD, OSV, ExploitDB Deploy AI assistant for: Finding summaries Risk triage automation Workflow suggestions Add cross-project analytics dashboard Implement export formats (PDF, CSV, JSON)","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables_4","text":"Fully production-ready orchestration platform Retention-safe data lifecycle AI triage beta enabled Analytics module complete","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#migration-timeline-overview","text":"Week Phase Key Deliverables 1 Phase 0 \u2014 Scaffold Repo, linting, CI/CD, guardrails 2 Phase 1 \u2014 Core Models, DB, CRUD API 3 Phase 2 \u2014 Engine Wrappers, Plugins, Workflow 4 Phase 3 \u2014 API/UI Worker, Triage UI, Auth 5+ Phase 4 \u2014 AI/GC Retention, Enrichment, Analytics","title":"\ud83d\udcc8 Migration Timeline Overview"},{"location":"architecture/20-migration-and-implementation-phases/#deployment-strategy","text":"Branch-per-phase workflow ( feature/phase-1-core , etc.) Pre-merge CI enforcement for all phases Feature flags for new modules Nightly build for cross-validation Docker-compose dev stack for quick testing","title":"\ud83d\ude80 Deployment Strategy"},{"location":"architecture/20-migration-and-implementation-phases/#example-command_1","text":"docker compose up -d pytest --maxfail=1 --disable-warnings","title":"Example Command"},{"location":"architecture/20-migration-and-implementation-phases/#key-success-metrics","text":"Metric Target Test Coverage >90% for core-lib & storage CI Lint Pass Rate 100% Workflow Execution Latency <300ms/node avg Risk Score Accuracy \u00b15% of reference Mean Time to Deploy <10 min via CI/CD","title":"\ud83e\udde0 Key Success Metrics"},{"location":"architecture/20-migration-and-implementation-phases/#next-steps","text":"After migration completion: - Freeze legacy JSON-store modules. - Enable CVE/PoC enrichment. - Integrate AI triage assistant. - Conduct internal red-team testing. - Prepare open-source release structure. Next: CI/CD & Testing Strategy","title":"\ud83d\udd2e Next Steps"},{"location":"architecture/21-ci-cd-and-testing-strategy/","text":"21 \u2014 CI/CD and Testing Strategy \u00b6 \ud83e\udded Overview \u00b6 The CI/CD and testing strategy ensures SecFlow maintains high reliability, reproducibility, and security compliance across all modules. It defines automated pipelines for build validation, testing, linting, packaging, and deployment to staging and production. SecFlow's architecture enables modular CI pipelines for packages (core-lib, storage, wrappers, plugins) and apps (web-api, worker, triage-ui). \u2699\ufe0f CI/CD Architecture Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Developer\"] B[\"Commit / PR \u2192 GitHub Repository\"] C[\"GitHub Actions CI Workflow<br/>- Lint (Ruff)<br/>- Type Check (Pyright)<br/>- Test (Pytest Matrix)<br/>- Build (Poetry / Docker)\"] D[\"Artifacts Published \u2192 Container Registry\"] E[\"CD Pipeline (Staging \u2192 Production)\"] A --> B B --> C C --> D D --> E \ud83e\uddf1 CI Pipeline Structure \u00b6 Files \u00b6 .github/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"workflows/\"] B[\"ci.yml<br/>(Main build & test pipeline)\"] C[\"lint.yml<br/>(Fast linting PR checks)\"] D[\"deploy.yml<br/>(CD pipeline to staging/prod)\"] E[\"nightly.yml<br/>(Nightly validation builds)\"] F[\"security-scan.yml<br/>(Dependency & container scanning)\"] A --> B A --> C A --> D A --> E A --> F Environments \u00b6 dev \u2192 local or containerized build staging \u2192 auto-deployed for QA validation production \u2192 manual approval required \ud83e\uddea Test Taxonomy \u00b6 Level Scope Example Unit Tests Function-level logic validation Testing CVSS normalization, config parsing Integration Tests Module interoperability NucleiWrapper + FindingsEngine Functional Tests End-to-end system behavior Workflow execution pipeline Regression Tests Legacy feature coverage Old project import/export Performance Tests Latency and scalability Parallel scan runs Security Tests Dependency & vulnerability checks Pip-audit, Trivy \ud83e\udde9 Test Framework Stack \u00b6 Tool Purpose pytest Primary test runner pytest-asyncio Async tests for API and worker pytest-cov Coverage reports tox Matrix execution (Python 3.10\u20133.12) httpx HTTP API test client sqlite-memory Fast ephemeral DB backend for testing faker Generate synthetic test data pytest-docker Integration tests for containerized tools \ud83e\uddf1 Test Folder Structure \u00b6 tests/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"tests/\"] B[\"core/\"] C[\"test_models.py\"] D[\"test_utils.py\"] E[\"test_ports.py\"] F[\"wrappers/\"] G[\"test_nuclei_wrapper.py\"] H[\"test_ferox_wrapper.py\"] I[\"test_zap_wrapper.py\"] J[\"plugins/\"] K[\"test_cve_enrichment.py\"] L[\"test_risk_scoring.py\"] M[\"api/\"] N[\"test_projects_api.py\"] O[\"test_findings_api.py\"] P[\"test_workflow_api.py\"] Q[\"e2e/\"] R[\"test_workflow_dag_execution.py\"] A --> B A --> F A --> J A --> M A --> Q B --> C B --> D B --> E F --> G F --> H F --> I J --> K J --> L M --> N M --> O M --> P Q --> R \ud83e\uddee CI Matrix Configuration Example \u00b6 # .github/workflows/ci.yml name: CI on: push: branches: [ main ] pull_request: branches: [ main ] jobs: build-test: runs-on: ubuntu-latest strategy: matrix: python-version: [ \"3.10\", \"3.11\", \"3.12\" ] database: [ \"sqlite\", \"postgres\" ] steps: - uses: actions/checkout@v4 - uses: actions/setup-python@v5 with: python-version: ${{ matrix.python-version }} - run: pip install poetry - run: poetry install - run: make lint - run: make test DB=${{ matrix.database }} - run: pytest --cov=src --cov-report=xml \ud83e\udde0 Deployment Pipeline \u00b6 Staging Pipeline (Continuous Deployment) \u00b6 Triggered on merge to main Deploys to staging environment automatically Runs post-deploy smoke tests: /healthz endpoint Workflow execution sanity test Production Pipeline \u00b6 Requires manual approval ( workflow_dispatch ) Signs Docker images before publishing Deploys to Kubernetes or Docker Swarm cluster Monitors deployment via Prometheus metrics Example job snippet: \u00b6 - name: Deploy to Staging run: | docker-compose -f docker-compose.staging.yml up -d pytest tests/e2e/ --maxfail=1 \ud83e\uddf0 Build Artifacts & Packages \u00b6 Type Output Destination Python Wheels dist/*.whl PyPI private index Docker Images SecFlow-api , SecFlow-worker Container registry Reports coverage.xml , lint.txt , typecheck.json GitHub artifacts Documentation mkdocs site/ GitHub Pages \ud83e\udde0 Quality Gates \u00b6 Check Tool Threshold Linting Ruff No errors Type Checking Pyright 100% coverage Test Coverage Pytest + Coverage > 90% Dependency Scan Pip-audit / Trivy 0 Critical Build Size Docker < 400 MB per image Failed gates block merges automatically. \ud83e\uddea Continuous Security Testing \u00b6 Dependency Auditing: via pip-audit and Safety Container Scanning: via Trivy in CI Secrets Detection: via gitleaks pre-commit hook Infrastructure Scan: via tfsec (for IaC configs) pip install pip-audit safety gitleaks trivy make security-scan \ud83d\udd04 Regression & Replay Testing \u00b6 Each workflow run can be recorded and replayed for regression tests. This ensures stability across version upgrades. Example: \u00b6 pytest tests/e2e/test_workflow_dag_execution.py --record pytest --replay last-run Replay data is stored under /tests/artifacts/replays/ . \ud83e\uddf0 Local Developer Testing \u00b6 Developers can run lightweight tests locally: make test pytest -k \"not e2e\" With Docker-enabled integration tests: make test-docker \ud83d\udcca Metrics & Reporting \u00b6 After each CI build: - Coverage report published to Codecov - Lint/type results annotated in GitHub PR - Performance metrics logged to Prometheus Example coverage badge: \u00b6 [![Coverage](https://img.shields.io/badge/coverage-93%25-brightgreen)](https://codecov.io/gh/SecFlow) \ud83e\uddf1 Disaster Recovery & Rollback \u00b6 Every deployment is versioned: - Docker image tags = vX.Y.Z-buildhash Rollback command: \u00b6 docker pull SecFlow-api:v1.3.2 docker compose up -d --no-build Database snapshots every 6h during staging deployment \ud83d\udd12 Contract Testing \u00b6 Contract tests ensure data integrity and API compatibility across system boundaries. They validate invariants that must hold true regardless of implementation changes. Contract Test Categories \u00b6 Category Purpose Location Example Finding Invariants Validate finding data structure tests/contracts/test_finding_invariants.py Detector ID regex, UTC timestamps Storage Layout Validate data persistence format tests/contracts/test_storage_layout.py Schema version, file structure Finding Invariants Contract \u00b6 Purpose: Ensure findings conform to expected data structure and format requirements. Key Validations: - Detector IDs match pattern: ^[A-Za-z0-9_.-]+$ - Timestamps are ISO 8601 UTC format ending with 'Z' - Required fields are present and properly typed Test Structure: # Positive cases - valid data should pass def test_detector_id_regex_valid (): assert DETECTOR_RE . match ( \"EXAMPLE_1\" ) # Negative cases - invalid data should fail def test_detector_id_regex_invalid (): assert not DETECTOR_RE . match ( \"invalid:id\" ) # Future implementation stubs @pytest . mark . xfail ( reason = \"Implementation pending\" ) def test_finding_schema_validation (): # Will pass when schema validation is implemented Storage Layout Contract \u00b6 Purpose: Ensure data persistence maintains expected structure and versioning. Key Validations: - Schema version field present and valid format - File structure supports expected operations - Backward compatibility for data migration Test Structure: # Happy path - valid storage layout def test_store_layout_has_schema_version ( tmp_path ): data = { \"store_schema_version\" : \"1.0.0\" } assert \"store_schema_version\" in data # Edge cases - missing or invalid versions def test_store_layout_missing_schema_version ( tmp_path ): data = { \"detector_id\" : \"test\" } assert \"store_schema_version\" not in data Contract Test Execution \u00b6 Run contract tests independently: # Run all contract tests pytest tests/contracts/ -q # Run specific contract category pytest tests/contracts/test_finding_invariants.py -q pytest tests/contracts/test_storage_layout.py -q Contract Test Principles \u00b6 Fail Fast: Contract tests should fail immediately when invariants are violated Implementation Agnostic: Tests validate contracts, not implementation details Future-Proof: Use @pytest.mark.xfail for pending implementations Comprehensive Coverage: Include both positive and negative test cases Clear Documentation: Each test explains the contract being validated Integration with CI/CD \u00b6 Contract tests run in the main CI pipeline: - name : Run Contract Tests run : pytest tests/contracts/ --tb=short They provide early warning when: - Data format changes break compatibility - Schema evolution violates existing contracts - New implementations don't meet contract requirements \ud83d\udd2e Future Enhancements \u00b6 Integration with GitHub Advanced Security (code scanning) Dynamic Test Selection (test impacted code only) Chaos Testing on worker queue reliability Parallelized build matrix using GitHub Actions caching Next: Developer Experience & Documentation Plan","title":"CI/CD & Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#21-cicd-and-testing-strategy","text":"","title":"21 \u2014 CI/CD and Testing Strategy"},{"location":"architecture/21-ci-cd-and-testing-strategy/#overview","text":"The CI/CD and testing strategy ensures SecFlow maintains high reliability, reproducibility, and security compliance across all modules. It defines automated pipelines for build validation, testing, linting, packaging, and deployment to staging and production. SecFlow's architecture enables modular CI pipelines for packages (core-lib, storage, wrappers, plugins) and apps (web-api, worker, triage-ui).","title":"\ud83e\udded Overview"},{"location":"architecture/21-ci-cd-and-testing-strategy/#cicd-architecture-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Developer\"] B[\"Commit / PR \u2192 GitHub Repository\"] C[\"GitHub Actions CI Workflow<br/>- Lint (Ruff)<br/>- Type Check (Pyright)<br/>- Test (Pytest Matrix)<br/>- Build (Poetry / Docker)\"] D[\"Artifacts Published \u2192 Container Registry\"] E[\"CD Pipeline (Staging \u2192 Production)\"] A --> B B --> C C --> D D --> E","title":"\u2699\ufe0f CI/CD Architecture Diagram"},{"location":"architecture/21-ci-cd-and-testing-strategy/#ci-pipeline-structure","text":"","title":"\ud83e\uddf1 CI Pipeline Structure"},{"location":"architecture/21-ci-cd-and-testing-strategy/#files","text":".github/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"workflows/\"] B[\"ci.yml<br/>(Main build & test pipeline)\"] C[\"lint.yml<br/>(Fast linting PR checks)\"] D[\"deploy.yml<br/>(CD pipeline to staging/prod)\"] E[\"nightly.yml<br/>(Nightly validation builds)\"] F[\"security-scan.yml<br/>(Dependency & container scanning)\"] A --> B A --> C A --> D A --> E A --> F","title":"Files"},{"location":"architecture/21-ci-cd-and-testing-strategy/#environments","text":"dev \u2192 local or containerized build staging \u2192 auto-deployed for QA validation production \u2192 manual approval required","title":"Environments"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-taxonomy","text":"Level Scope Example Unit Tests Function-level logic validation Testing CVSS normalization, config parsing Integration Tests Module interoperability NucleiWrapper + FindingsEngine Functional Tests End-to-end system behavior Workflow execution pipeline Regression Tests Legacy feature coverage Old project import/export Performance Tests Latency and scalability Parallel scan runs Security Tests Dependency & vulnerability checks Pip-audit, Trivy","title":"\ud83e\uddea Test Taxonomy"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-framework-stack","text":"Tool Purpose pytest Primary test runner pytest-asyncio Async tests for API and worker pytest-cov Coverage reports tox Matrix execution (Python 3.10\u20133.12) httpx HTTP API test client sqlite-memory Fast ephemeral DB backend for testing faker Generate synthetic test data pytest-docker Integration tests for containerized tools","title":"\ud83e\udde9 Test Framework Stack"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-folder-structure","text":"tests/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"tests/\"] B[\"core/\"] C[\"test_models.py\"] D[\"test_utils.py\"] E[\"test_ports.py\"] F[\"wrappers/\"] G[\"test_nuclei_wrapper.py\"] H[\"test_ferox_wrapper.py\"] I[\"test_zap_wrapper.py\"] J[\"plugins/\"] K[\"test_cve_enrichment.py\"] L[\"test_risk_scoring.py\"] M[\"api/\"] N[\"test_projects_api.py\"] O[\"test_findings_api.py\"] P[\"test_workflow_api.py\"] Q[\"e2e/\"] R[\"test_workflow_dag_execution.py\"] A --> B A --> F A --> J A --> M A --> Q B --> C B --> D B --> E F --> G F --> H F --> I J --> K J --> L M --> N M --> O M --> P Q --> R","title":"\ud83e\uddf1 Test Folder Structure"},{"location":"architecture/21-ci-cd-and-testing-strategy/#ci-matrix-configuration-example","text":"# .github/workflows/ci.yml name: CI on: push: branches: [ main ] pull_request: branches: [ main ] jobs: build-test: runs-on: ubuntu-latest strategy: matrix: python-version: [ \"3.10\", \"3.11\", \"3.12\" ] database: [ \"sqlite\", \"postgres\" ] steps: - uses: actions/checkout@v4 - uses: actions/setup-python@v5 with: python-version: ${{ matrix.python-version }} - run: pip install poetry - run: poetry install - run: make lint - run: make test DB=${{ matrix.database }} - run: pytest --cov=src --cov-report=xml","title":"\ud83e\uddee CI Matrix Configuration Example"},{"location":"architecture/21-ci-cd-and-testing-strategy/#deployment-pipeline","text":"","title":"\ud83e\udde0 Deployment Pipeline"},{"location":"architecture/21-ci-cd-and-testing-strategy/#staging-pipeline-continuous-deployment","text":"Triggered on merge to main Deploys to staging environment automatically Runs post-deploy smoke tests: /healthz endpoint Workflow execution sanity test","title":"Staging Pipeline (Continuous Deployment)"},{"location":"architecture/21-ci-cd-and-testing-strategy/#production-pipeline","text":"Requires manual approval ( workflow_dispatch ) Signs Docker images before publishing Deploys to Kubernetes or Docker Swarm cluster Monitors deployment via Prometheus metrics","title":"Production Pipeline"},{"location":"architecture/21-ci-cd-and-testing-strategy/#example-job-snippet","text":"- name: Deploy to Staging run: | docker-compose -f docker-compose.staging.yml up -d pytest tests/e2e/ --maxfail=1","title":"Example job snippet:"},{"location":"architecture/21-ci-cd-and-testing-strategy/#build-artifacts-packages","text":"Type Output Destination Python Wheels dist/*.whl PyPI private index Docker Images SecFlow-api , SecFlow-worker Container registry Reports coverage.xml , lint.txt , typecheck.json GitHub artifacts Documentation mkdocs site/ GitHub Pages","title":"\ud83e\uddf0 Build Artifacts &amp; Packages"},{"location":"architecture/21-ci-cd-and-testing-strategy/#quality-gates","text":"Check Tool Threshold Linting Ruff No errors Type Checking Pyright 100% coverage Test Coverage Pytest + Coverage > 90% Dependency Scan Pip-audit / Trivy 0 Critical Build Size Docker < 400 MB per image Failed gates block merges automatically.","title":"\ud83e\udde0 Quality Gates"},{"location":"architecture/21-ci-cd-and-testing-strategy/#continuous-security-testing","text":"Dependency Auditing: via pip-audit and Safety Container Scanning: via Trivy in CI Secrets Detection: via gitleaks pre-commit hook Infrastructure Scan: via tfsec (for IaC configs) pip install pip-audit safety gitleaks trivy make security-scan","title":"\ud83e\uddea Continuous Security Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#regression-replay-testing","text":"Each workflow run can be recorded and replayed for regression tests. This ensures stability across version upgrades.","title":"\ud83d\udd04 Regression &amp; Replay Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#example","text":"pytest tests/e2e/test_workflow_dag_execution.py --record pytest --replay last-run Replay data is stored under /tests/artifacts/replays/ .","title":"Example:"},{"location":"architecture/21-ci-cd-and-testing-strategy/#local-developer-testing","text":"Developers can run lightweight tests locally: make test pytest -k \"not e2e\" With Docker-enabled integration tests: make test-docker","title":"\ud83e\uddf0 Local Developer Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#metrics-reporting","text":"After each CI build: - Coverage report published to Codecov - Lint/type results annotated in GitHub PR - Performance metrics logged to Prometheus","title":"\ud83d\udcca Metrics &amp; Reporting"},{"location":"architecture/21-ci-cd-and-testing-strategy/#example-coverage-badge","text":"[![Coverage](https://img.shields.io/badge/coverage-93%25-brightgreen)](https://codecov.io/gh/SecFlow)","title":"Example coverage badge:"},{"location":"architecture/21-ci-cd-and-testing-strategy/#disaster-recovery-rollback","text":"Every deployment is versioned: - Docker image tags = vX.Y.Z-buildhash","title":"\ud83e\uddf1 Disaster Recovery &amp; Rollback"},{"location":"architecture/21-ci-cd-and-testing-strategy/#rollback-command","text":"docker pull SecFlow-api:v1.3.2 docker compose up -d --no-build Database snapshots every 6h during staging deployment","title":"Rollback command:"},{"location":"architecture/21-ci-cd-and-testing-strategy/#contract-testing","text":"Contract tests ensure data integrity and API compatibility across system boundaries. They validate invariants that must hold true regardless of implementation changes.","title":"\ud83d\udd12 Contract Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#contract-test-categories","text":"Category Purpose Location Example Finding Invariants Validate finding data structure tests/contracts/test_finding_invariants.py Detector ID regex, UTC timestamps Storage Layout Validate data persistence format tests/contracts/test_storage_layout.py Schema version, file structure","title":"Contract Test Categories"},{"location":"architecture/21-ci-cd-and-testing-strategy/#finding-invariants-contract","text":"Purpose: Ensure findings conform to expected data structure and format requirements. Key Validations: - Detector IDs match pattern: ^[A-Za-z0-9_.-]+$ - Timestamps are ISO 8601 UTC format ending with 'Z' - Required fields are present and properly typed Test Structure: # Positive cases - valid data should pass def test_detector_id_regex_valid (): assert DETECTOR_RE . match ( \"EXAMPLE_1\" ) # Negative cases - invalid data should fail def test_detector_id_regex_invalid (): assert not DETECTOR_RE . match ( \"invalid:id\" ) # Future implementation stubs @pytest . mark . xfail ( reason = \"Implementation pending\" ) def test_finding_schema_validation (): # Will pass when schema validation is implemented","title":"Finding Invariants Contract"},{"location":"architecture/21-ci-cd-and-testing-strategy/#storage-layout-contract","text":"Purpose: Ensure data persistence maintains expected structure and versioning. Key Validations: - Schema version field present and valid format - File structure supports expected operations - Backward compatibility for data migration Test Structure: # Happy path - valid storage layout def test_store_layout_has_schema_version ( tmp_path ): data = { \"store_schema_version\" : \"1.0.0\" } assert \"store_schema_version\" in data # Edge cases - missing or invalid versions def test_store_layout_missing_schema_version ( tmp_path ): data = { \"detector_id\" : \"test\" } assert \"store_schema_version\" not in data","title":"Storage Layout Contract"},{"location":"architecture/21-ci-cd-and-testing-strategy/#contract-test-execution","text":"Run contract tests independently: # Run all contract tests pytest tests/contracts/ -q # Run specific contract category pytest tests/contracts/test_finding_invariants.py -q pytest tests/contracts/test_storage_layout.py -q","title":"Contract Test Execution"},{"location":"architecture/21-ci-cd-and-testing-strategy/#contract-test-principles","text":"Fail Fast: Contract tests should fail immediately when invariants are violated Implementation Agnostic: Tests validate contracts, not implementation details Future-Proof: Use @pytest.mark.xfail for pending implementations Comprehensive Coverage: Include both positive and negative test cases Clear Documentation: Each test explains the contract being validated","title":"Contract Test Principles"},{"location":"architecture/21-ci-cd-and-testing-strategy/#integration-with-cicd","text":"Contract tests run in the main CI pipeline: - name : Run Contract Tests run : pytest tests/contracts/ --tb=short They provide early warning when: - Data format changes break compatibility - Schema evolution violates existing contracts - New implementations don't meet contract requirements","title":"Integration with CI/CD"},{"location":"architecture/21-ci-cd-and-testing-strategy/#future-enhancements","text":"Integration with GitHub Advanced Security (code scanning) Dynamic Test Selection (test impacted code only) Chaos Testing on worker queue reliability Parallelized build matrix using GitHub Actions caching Next: Developer Experience & Documentation Plan","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/22-developer-experience-and-docs/","text":"22 \u2014 Developer Experience & Documentation Plan \u00b6 \ud83e\udded Overview \u00b6 SecFlow prioritizes an efficient, reliable, and enjoyable developer experience (DX) that promotes rapid iteration without compromising architectural integrity. This section outlines the local environment setup, development workflow, CLI tools, documentation strategy, and contribution guidelines. \ud83e\udde9 Core DX Principles \u00b6 Principle Description Fast Feedback Every command ( make test , make dev ) provides results in < 5s. Safe by Default Guardrails enforce architectural discipline (ruff, pyright, import-linter). Visible Architecture Auto-generated MkDocs diagrams and interlinked specs. One-Command Onboarding New developers can start coding in minutes. DevOps Parity Local, CI, and production environments behave identically. \u2699\ufe0f Local Environment Setup \u00b6 Prerequisites \u00b6 Python \u22653.11 Poetry Docker + Docker Compose Node.js \u226518 (for triage-ui builds) Setup Commands \u00b6 git clone https://github.com/SecFlow/security-toolkit.git cd security-toolkit make init make up make init performs: - Poetry virtualenv setup - Dependency installation - Database migration (SQLite dev DB) - Git pre-commit hooks (ruff, pyright, pytest) - Environment validation ( make check ) \ud83e\uddf1 Developer Makefile Commands \u00b6 Command Description make up Start local stack (API, worker, UI) make down Stop containers and cleanup make dev Launch dev server with autoreload make test Run all tests make lint Run lint + type check make docs Build MkDocs documentation make check Validate dependencies and environment make clean Remove caches and build artifacts Example: \u00b6 make dev # http://localhost:8080 \ud83e\uddf0 Developer CLI \u2014 secflowctl \u00b6 SecFlow provides an integrated command-line interface for developers and operators. Example Commands \u00b6 secflowctl project list secflowctl scan start nuclei --project mytest secflowctl workflow run owasp-top10.yaml secflowctl plugin list secflowctl risk report --format table CLI Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"secflowctl/\"] B[\"__main__.py\"] C[\"commands/\"] D[\"project.py\"] E[\"scan.py\"] F[\"workflow.py\"] G[\"plugin.py\"] H[\"risk.py\"] I[\"utils/\"] J[\"formatting.py\"] A --> B A --> C A --> I C --> D C --> E C --> F C --> G C --> H I --> J CLI Design Features \u00b6 Rich TUI (Textual) output for interactive sessions Tab autocompletion JSON/YAML output modes Direct API calls or local orchestration \ud83e\udded Development Workflow \u00b6 Branching Model \u00b6 main \u2192 stable production branch develop \u2192 integration branch feature/* \u2192 new features or refactors fix/* \u2192 bug fixes release/* \u2192 versioned release candidates Pull Request Requirements \u00b6 1 approving review All CI checks passed (lint, test, type, security scan) Linked issue ID Updated changelog entry Commit Style (Conventional Commits) \u00b6 feat(workflow): add nuclei plugin support fix(storage): handle null resource hash docs(readme): update setup instructions \ud83d\udcd8 Documentation System (MkDocs) \u00b6 MkDocs Project Layout \u00b6 docs/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/<br/>(Deep technical docs)\"] C[\"api/<br/>(OpenAPI spec & usage)\"] D[\"dev/<br/>(Developer onboarding)\"] E[\"operations/<br/>(Deployment & monitoring)\"] F[\"plugins/<br/>(Plugin development guide)\"] G[\"index.md<br/>(Landing page)\"] A --> B A --> C A --> D A --> E A --> F A --> G Build Command \u00b6 make docs # Builds into site/ Features \u00b6 Material for MkDocs theme Auto-generated architecture diagrams via Mermaid Built-in search and code highlighting Versioned docs (via mike) for each release Plugin-based navigation for \"core\", \"apps\", \"plugins\", \"API\" Example mkdocs.yml: \u00b6 site_name: \"SecFlow Developer Docs\" theme: name: material features: - navigation.sections - navigation.instant markdown_extensions: - toc: permalink: true - admonition - pymdownx.superfences plugins: - search - mermaid2 - awesome-pages \ud83e\udde0 Architecture Visualization \u00b6 Architecture diagrams are auto-generated from the codebase using diagrams + pydeps. Example script: \u00b6 make diagram Output: /docs/architecture/assets/architecture.svg Example generated image (ASCII simplified): \u00b6 +-------------+ | web-api | +------+------+----+ | | +-------v--+ +----v--------+ | worker | | triage-ui | +----------+ +-------------+ | | +----v---------v----+ | core-lib / engine | +--------------------+ \ud83e\udde9 Developer Onboarding Flow \u00b6 Step Description 1. Clone Repository git clone and make init 2. Run Local Stack make up \u2192 visit localhost:8080 3. Explore CLI secflowctl help 4. Read Docs make docs \u2192 open site/index.html 5. Add Feature Create feature/my-feature branch 6. Submit PR Push to GitHub, run CI, get review 7. Merge & Deploy Auto-deployed to staging \ud83e\uddf0 Tooling Summary \u00b6 Category Tool Purpose Package Management Poetry Dependency control Linting Ruff Code style & hygiene Typing Pyright Static type enforcement Testing Pytest Unit & integration tests Docs MkDocs Documentation Visualization Diagrams Auto-generate architecture maps Security Gitleaks, Safety Prevent secrets & vulns Formatting Black Consistent code format \ud83e\udde9 Developer Guidelines \u00b6 Code Style \u00b6 Follow PEP8 + Ruff config Enforce docstrings for public classes/functions Avoid circular imports (use ports) Use dependency injection where possible Commit Rules \u00b6 Keep commits atomic (1 logical change) Use descriptive messages Reference related issue (#123) Code Review Expectations \u00b6 Small PRs (<500 LOC preferred) Include before/after screenshots for UI changes Add unit tests for every new feature \ud83e\udde0 Local Testing Shortcuts \u00b6 Scenario Command Run single test pytest tests/core/test_models.py::test_project_model Run tests with coverage pytest --cov=src --cov-report=html Run async API tests pytest tests/api -k \"async\" Skip slow tests pytest -m \"not slow\" Lint before commit pre-commit run --all-files \ud83d\udcd8 Developer Documentation Contributions \u00b6 Docs are written in Markdown under docs/ Always include: \u00b6 Code examples Usage samples Config references Build locally via: \u00b6 mkdocs serve For architecture updates: \u00b6 make diagram && make docs \ud83d\udd2e Future DX Enhancements \u00b6 VS Code Dev Containers for instant onboarding CLI Autoupdate System for secflowctl MkDocs AI Search Plugin (semantic search) Interactive Architecture Map (Mermaid + Live API) Unified Dev Dashboard combining logs, metrics, and CI build state Next: Future Roadmap","title":"Dev Experience & Docs"},{"location":"architecture/22-developer-experience-and-docs/#22-developer-experience-documentation-plan","text":"","title":"22 \u2014 Developer Experience &amp; Documentation Plan"},{"location":"architecture/22-developer-experience-and-docs/#overview","text":"SecFlow prioritizes an efficient, reliable, and enjoyable developer experience (DX) that promotes rapid iteration without compromising architectural integrity. This section outlines the local environment setup, development workflow, CLI tools, documentation strategy, and contribution guidelines.","title":"\ud83e\udded Overview"},{"location":"architecture/22-developer-experience-and-docs/#core-dx-principles","text":"Principle Description Fast Feedback Every command ( make test , make dev ) provides results in < 5s. Safe by Default Guardrails enforce architectural discipline (ruff, pyright, import-linter). Visible Architecture Auto-generated MkDocs diagrams and interlinked specs. One-Command Onboarding New developers can start coding in minutes. DevOps Parity Local, CI, and production environments behave identically.","title":"\ud83e\udde9 Core DX Principles"},{"location":"architecture/22-developer-experience-and-docs/#local-environment-setup","text":"","title":"\u2699\ufe0f Local Environment Setup"},{"location":"architecture/22-developer-experience-and-docs/#prerequisites","text":"Python \u22653.11 Poetry Docker + Docker Compose Node.js \u226518 (for triage-ui builds)","title":"Prerequisites"},{"location":"architecture/22-developer-experience-and-docs/#setup-commands","text":"git clone https://github.com/SecFlow/security-toolkit.git cd security-toolkit make init make up make init performs: - Poetry virtualenv setup - Dependency installation - Database migration (SQLite dev DB) - Git pre-commit hooks (ruff, pyright, pytest) - Environment validation ( make check )","title":"Setup Commands"},{"location":"architecture/22-developer-experience-and-docs/#developer-makefile-commands","text":"Command Description make up Start local stack (API, worker, UI) make down Stop containers and cleanup make dev Launch dev server with autoreload make test Run all tests make lint Run lint + type check make docs Build MkDocs documentation make check Validate dependencies and environment make clean Remove caches and build artifacts","title":"\ud83e\uddf1 Developer Makefile Commands"},{"location":"architecture/22-developer-experience-and-docs/#example","text":"make dev # http://localhost:8080","title":"Example:"},{"location":"architecture/22-developer-experience-and-docs/#developer-cli-secflowctl","text":"SecFlow provides an integrated command-line interface for developers and operators.","title":"\ud83e\uddf0 Developer CLI \u2014 secflowctl"},{"location":"architecture/22-developer-experience-and-docs/#example-commands","text":"secflowctl project list secflowctl scan start nuclei --project mytest secflowctl workflow run owasp-top10.yaml secflowctl plugin list secflowctl risk report --format table","title":"Example Commands"},{"location":"architecture/22-developer-experience-and-docs/#cli-structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"secflowctl/\"] B[\"__main__.py\"] C[\"commands/\"] D[\"project.py\"] E[\"scan.py\"] F[\"workflow.py\"] G[\"plugin.py\"] H[\"risk.py\"] I[\"utils/\"] J[\"formatting.py\"] A --> B A --> C A --> I C --> D C --> E C --> F C --> G C --> H I --> J","title":"CLI Structure"},{"location":"architecture/22-developer-experience-and-docs/#cli-design-features","text":"Rich TUI (Textual) output for interactive sessions Tab autocompletion JSON/YAML output modes Direct API calls or local orchestration","title":"CLI Design Features"},{"location":"architecture/22-developer-experience-and-docs/#development-workflow","text":"","title":"\ud83e\udded Development Workflow"},{"location":"architecture/22-developer-experience-and-docs/#branching-model","text":"main \u2192 stable production branch develop \u2192 integration branch feature/* \u2192 new features or refactors fix/* \u2192 bug fixes release/* \u2192 versioned release candidates","title":"Branching Model"},{"location":"architecture/22-developer-experience-and-docs/#pull-request-requirements","text":"1 approving review All CI checks passed (lint, test, type, security scan) Linked issue ID Updated changelog entry","title":"Pull Request Requirements"},{"location":"architecture/22-developer-experience-and-docs/#commit-style-conventional-commits","text":"feat(workflow): add nuclei plugin support fix(storage): handle null resource hash docs(readme): update setup instructions","title":"Commit Style (Conventional Commits)"},{"location":"architecture/22-developer-experience-and-docs/#documentation-system-mkdocs","text":"","title":"\ud83d\udcd8 Documentation System (MkDocs)"},{"location":"architecture/22-developer-experience-and-docs/#mkdocs-project-layout","text":"docs/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/<br/>(Deep technical docs)\"] C[\"api/<br/>(OpenAPI spec & usage)\"] D[\"dev/<br/>(Developer onboarding)\"] E[\"operations/<br/>(Deployment & monitoring)\"] F[\"plugins/<br/>(Plugin development guide)\"] G[\"index.md<br/>(Landing page)\"] A --> B A --> C A --> D A --> E A --> F A --> G","title":"MkDocs Project Layout"},{"location":"architecture/22-developer-experience-and-docs/#build-command","text":"make docs # Builds into site/","title":"Build Command"},{"location":"architecture/22-developer-experience-and-docs/#features","text":"Material for MkDocs theme Auto-generated architecture diagrams via Mermaid Built-in search and code highlighting Versioned docs (via mike) for each release Plugin-based navigation for \"core\", \"apps\", \"plugins\", \"API\"","title":"Features"},{"location":"architecture/22-developer-experience-and-docs/#example-mkdocsyml","text":"site_name: \"SecFlow Developer Docs\" theme: name: material features: - navigation.sections - navigation.instant markdown_extensions: - toc: permalink: true - admonition - pymdownx.superfences plugins: - search - mermaid2 - awesome-pages","title":"Example mkdocs.yml:"},{"location":"architecture/22-developer-experience-and-docs/#architecture-visualization","text":"Architecture diagrams are auto-generated from the codebase using diagrams + pydeps.","title":"\ud83e\udde0 Architecture Visualization"},{"location":"architecture/22-developer-experience-and-docs/#example-script","text":"make diagram Output: /docs/architecture/assets/architecture.svg","title":"Example script:"},{"location":"architecture/22-developer-experience-and-docs/#example-generated-image-ascii-simplified","text":"+-------------+ | web-api | +------+------+----+ | | +-------v--+ +----v--------+ | worker | | triage-ui | +----------+ +-------------+ | | +----v---------v----+ | core-lib / engine | +--------------------+","title":"Example generated image (ASCII simplified):"},{"location":"architecture/22-developer-experience-and-docs/#developer-onboarding-flow","text":"Step Description 1. Clone Repository git clone and make init 2. Run Local Stack make up \u2192 visit localhost:8080 3. Explore CLI secflowctl help 4. Read Docs make docs \u2192 open site/index.html 5. Add Feature Create feature/my-feature branch 6. Submit PR Push to GitHub, run CI, get review 7. Merge & Deploy Auto-deployed to staging","title":"\ud83e\udde9 Developer Onboarding Flow"},{"location":"architecture/22-developer-experience-and-docs/#tooling-summary","text":"Category Tool Purpose Package Management Poetry Dependency control Linting Ruff Code style & hygiene Typing Pyright Static type enforcement Testing Pytest Unit & integration tests Docs MkDocs Documentation Visualization Diagrams Auto-generate architecture maps Security Gitleaks, Safety Prevent secrets & vulns Formatting Black Consistent code format","title":"\ud83e\uddf0 Tooling Summary"},{"location":"architecture/22-developer-experience-and-docs/#developer-guidelines","text":"","title":"\ud83e\udde9 Developer Guidelines"},{"location":"architecture/22-developer-experience-and-docs/#code-style","text":"Follow PEP8 + Ruff config Enforce docstrings for public classes/functions Avoid circular imports (use ports) Use dependency injection where possible","title":"Code Style"},{"location":"architecture/22-developer-experience-and-docs/#commit-rules","text":"Keep commits atomic (1 logical change) Use descriptive messages Reference related issue (#123)","title":"Commit Rules"},{"location":"architecture/22-developer-experience-and-docs/#code-review-expectations","text":"Small PRs (<500 LOC preferred) Include before/after screenshots for UI changes Add unit tests for every new feature","title":"Code Review Expectations"},{"location":"architecture/22-developer-experience-and-docs/#local-testing-shortcuts","text":"Scenario Command Run single test pytest tests/core/test_models.py::test_project_model Run tests with coverage pytest --cov=src --cov-report=html Run async API tests pytest tests/api -k \"async\" Skip slow tests pytest -m \"not slow\" Lint before commit pre-commit run --all-files","title":"\ud83e\udde0 Local Testing Shortcuts"},{"location":"architecture/22-developer-experience-and-docs/#developer-documentation-contributions","text":"Docs are written in Markdown under docs/","title":"\ud83d\udcd8 Developer Documentation Contributions"},{"location":"architecture/22-developer-experience-and-docs/#always-include","text":"Code examples Usage samples Config references","title":"Always include:"},{"location":"architecture/22-developer-experience-and-docs/#build-locally-via","text":"mkdocs serve","title":"Build locally via:"},{"location":"architecture/22-developer-experience-and-docs/#for-architecture-updates","text":"make diagram && make docs","title":"For architecture updates:"},{"location":"architecture/22-developer-experience-and-docs/#future-dx-enhancements","text":"VS Code Dev Containers for instant onboarding CLI Autoupdate System for secflowctl MkDocs AI Search Plugin (semantic search) Interactive Architecture Map (Mermaid + Live API) Unified Dev Dashboard combining logs, metrics, and CI build state Next: Future Roadmap","title":"\ud83d\udd2e Future DX Enhancements"},{"location":"architecture/23-future-roadmap/","text":"23 \u2014 Future Roadmap & Evolution Strategy \u00b6 \ud83e\udded Overview \u00b6 This roadmap defines the strategic evolution of SecFlow from a modular pentesting orchestration toolkit to a next-generation autonomous security platform . It describes the planned capabilities, milestones, and technical direction for the next 12\u201324 months \u2014 integrating AI, multi-tenant architectures, collaborative triage, and intelligent automation. \ud83e\uddf1 Phase Overview (Long-Term Vision) \u00b6 Phase Focus Area Description Phase 1 (Current) Foundation Mono-repo, hexagonal core, plugin registry, workflow orchestration Phase 2 (6\u201312 months) Intelligence & AI AI triage, enrichment automation, anomaly detection Phase 3 (12\u201318 months) Collaboration Multi-tenant architecture, role-based analytics, red/blue team integration Phase 4 (18\u201324 months) Autonomy Self-optimizing workflows, adaptive scanners, predictive risk modeling \u2699\ufe0f Phase 2 \u2014 Intelligence & AI Integration \u00b6 The AI Integration Phase introduces cognitive automation into all major subsystems of SecFlow. 1. AI-Assisted Triage \u00b6 LLM-based summarization of findings Context inference (vulnerability relevance, duplication detection) Recommendation engine for next actions and prioritization 2. AI-Driven Risk Enrichment \u00b6 Predict missing CVSS scores and exploit likelihood (ML regression models) Ingest external threat feeds (CISA KEV, ExploitDB updates) Leverage EPSS (Exploit Prediction Scoring System) for dynamic risk adjustments 3. Automated Pattern Discovery \u00b6 Cluster recurring issues across multiple projects Identify \"weak controls\" using unsupervised learning Integrate MITRE ATT&CK correlation heatmaps Example architecture: \u00b6 +--------------------------------------------------+ | AI Engine | | - LLM-based Triage Assistant | | - ML Risk Predictor (EPSS + CVSS Hybrid) | | - Anomaly Detector (Outlier Findings) | | - Natural Language Query Interface | +--------------------------------------------------+ 4. Conversational Analysis Interface \u00b6 A secure chat layer connected to core-lib enabling queries like: show me all high-risk CVEs in projects using nginx summarize findings related to broken authentication predict which components will likely fail next pentest \ud83c\udf10 Phase 3 \u2014 Collaboration & Multi-Tenant Platform \u00b6 This phase focuses on scaling SecFlow for enterprise teams, MSSPs, and collaborative pentest environments. 1. Multi-Tenant Architecture \u00b6 Isolated tenants with shared infrastructure RBAC-based access and audit segregation Namespace-aware project storage ( tenant_id/project_id pattern) 2. Centralized Insights Hub \u00b6 Aggregate findings and metrics across tenants Role-based dashboards (CISO, Pentester, Developer) Global intelligence layer \u2014 vulnerability trends, exploit timelines 3. Cross-Project Correlation \u00b6 Federated search across tenants Shared resource registry (optional per policy) Vulnerability fingerprinting and reuse detection 4. Collaboration Tools \u00b6 Comment threads and annotations on findings Assignments and status tracking Shared remediation checklists \ud83e\udd16 Phase 4 \u2014 Autonomous Security Orchestration \u00b6 The final stage evolves SecFlow into an autonomous orchestration and reasoning engine . 1. Adaptive Scanning \u00b6 Automatically adjusts scanning parameters based on previous findings Integrates feedback loop from risk trends Uses Reinforcement Learning (RL) to optimize coverage vs time 2. Self-Healing Workflows \u00b6 Automatically retries failed nodes Re-schedules scans based on impact or SLA breach Learns ideal tool chaining patterns 3. Predictive Risk Forecasting \u00b6 Time-series models predicting vulnerability resurgence Integration with incident response systems for proactive alerts 4. Security Knowledge Graph \u00b6 Unifies all findings, CVEs, CWEs, ATT&CK tactics, and tool metadata Graph queries (Cypher/SPARQL) for advanced relationships Enables semantic enrichment and AI reasoning \ud83e\uddf0 Supporting Infrastructure Evolution \u00b6 Subsystem Upgrade Path Description Database Postgres \u2192 TimescaleDB Time-series analytics and trend computation Cache Layer Redis \u2192 Redis Cluster High-throughput async workflows Task Queue Celery \u2192 Prefect 3 Enhanced DAG orchestration and observability API Gateway FastAPI \u2192 APIStar (optional microgateway) Modular service boundaries UI Stack HTMX \u2192 React + GraphQL Hybrid Real-time triage dashboards \ud83e\udde0 Advanced Integrations \u00b6 Integration Purpose Caido & ZAP Direct import of active scan results GitHub Advanced Security Repo-based vulnerability correlation ExploitDB / Vulners API Continuous POC enrichment CISA KEV Sync Real-time critical vulnerability flagging AI Plugin System Extensible via OpenAI function-style agents \ud83d\udcca Planned Metrics & Analytics Expansion \u00b6 Area Metric Use Case Risk Intelligence Mean CVSS delta over time Track improvement rate Workflow Analytics Avg. scan node latency Optimize orchestration Tool Effectiveness Finding yield per tool ROI assessment Anomaly Detection Outlier frequency Identify misbehaving scans Team Collaboration Comment-to-remediation ratio Operational efficiency \ud83d\udd12 Future Security Enhancements \u00b6 Hardware attestation for sensitive tool execution (YubiHSM/TPM) Encrypted local storage using Fernet or AWS KMS Multi-factor API access tokens Zero-trust plugin sandboxing (seccomp profiles) Supply chain integrity checks (sigstore/cosign) \ud83c\udf0d Open Source & Community Roadmap \u00b6 SecFlow will release a Community Edition (CE) focusing on: - Lightweight orchestration - Built-in nuclei/ferox wrappers - Project & finding dashboards - Local SQLite backend Planned for mid-2026 under an Apache 2.0 license with optional enterprise modules. Community contributions roadmap: \u00b6 Plugin SDK Workflow Recipe Gallery Integration templates (for Burp, ZAP, Caido) AI-Triage contribution guide \ud83d\udcc5 Timeline Summary \u00b6 Quarter Milestone Highlights Q4 2025 Phase 1 \u2013 Finalize core platform API + Worker stable release Q1 2026 Phase 2 \u2013 AI & enrichment rollout LLM triage + CVE prediction Q2 2026 Phase 3 \u2013 Multi-tenant & collaboration Team dashboards, tenant isolation Q3 2026 Phase 4 \u2013 Autonomous orchestration Adaptive scans, AI reasoning graph Q4 2026 Community Edition release OSS launch, plugin marketplace \ud83e\udde9 Success Metrics & KPIs \u00b6 KPI Target Workflow execution success rate \u2265 98% Mean enrichment latency < 500 ms AI triage accuracy \u2265 90% vs manual MTTR (Mean Time to Risk Report) < 10 min Plugin SDK adoption 50+ community plugins Platform uptime (SLA) 99.9% \ud83d\udd2e Long-Term Vision \u00b6 \"From manual scanning to autonomous, continuous, and intelligent security operations.\" SecFlow's long-term mission is to become the AI-native, self-orchestrating red-team assistant , capable of: - Running its own scan campaigns, - Adjusting priorities dynamically, - Explaining vulnerabilities in human language, - Suggesting fixes, - And predicting attack surfaces before exploitation occurs. Next: Final Consensus Summary","title":"Future Roadmap"},{"location":"architecture/23-future-roadmap/#23-future-roadmap-evolution-strategy","text":"","title":"23 \u2014 Future Roadmap &amp; Evolution Strategy"},{"location":"architecture/23-future-roadmap/#overview","text":"This roadmap defines the strategic evolution of SecFlow from a modular pentesting orchestration toolkit to a next-generation autonomous security platform . It describes the planned capabilities, milestones, and technical direction for the next 12\u201324 months \u2014 integrating AI, multi-tenant architectures, collaborative triage, and intelligent automation.","title":"\ud83e\udded Overview"},{"location":"architecture/23-future-roadmap/#phase-overview-long-term-vision","text":"Phase Focus Area Description Phase 1 (Current) Foundation Mono-repo, hexagonal core, plugin registry, workflow orchestration Phase 2 (6\u201312 months) Intelligence & AI AI triage, enrichment automation, anomaly detection Phase 3 (12\u201318 months) Collaboration Multi-tenant architecture, role-based analytics, red/blue team integration Phase 4 (18\u201324 months) Autonomy Self-optimizing workflows, adaptive scanners, predictive risk modeling","title":"\ud83e\uddf1 Phase Overview (Long-Term Vision)"},{"location":"architecture/23-future-roadmap/#phase-2-intelligence-ai-integration","text":"The AI Integration Phase introduces cognitive automation into all major subsystems of SecFlow.","title":"\u2699\ufe0f Phase 2 \u2014 Intelligence &amp; AI Integration"},{"location":"architecture/23-future-roadmap/#1-ai-assisted-triage","text":"LLM-based summarization of findings Context inference (vulnerability relevance, duplication detection) Recommendation engine for next actions and prioritization","title":"1. AI-Assisted Triage"},{"location":"architecture/23-future-roadmap/#2-ai-driven-risk-enrichment","text":"Predict missing CVSS scores and exploit likelihood (ML regression models) Ingest external threat feeds (CISA KEV, ExploitDB updates) Leverage EPSS (Exploit Prediction Scoring System) for dynamic risk adjustments","title":"2. AI-Driven Risk Enrichment"},{"location":"architecture/23-future-roadmap/#3-automated-pattern-discovery","text":"Cluster recurring issues across multiple projects Identify \"weak controls\" using unsupervised learning Integrate MITRE ATT&CK correlation heatmaps","title":"3. Automated Pattern Discovery"},{"location":"architecture/23-future-roadmap/#example-architecture","text":"+--------------------------------------------------+ | AI Engine | | - LLM-based Triage Assistant | | - ML Risk Predictor (EPSS + CVSS Hybrid) | | - Anomaly Detector (Outlier Findings) | | - Natural Language Query Interface | +--------------------------------------------------+","title":"Example architecture:"},{"location":"architecture/23-future-roadmap/#4-conversational-analysis-interface","text":"A secure chat layer connected to core-lib enabling queries like: show me all high-risk CVEs in projects using nginx summarize findings related to broken authentication predict which components will likely fail next pentest","title":"4. Conversational Analysis Interface"},{"location":"architecture/23-future-roadmap/#phase-3-collaboration-multi-tenant-platform","text":"This phase focuses on scaling SecFlow for enterprise teams, MSSPs, and collaborative pentest environments.","title":"\ud83c\udf10 Phase 3 \u2014 Collaboration &amp; Multi-Tenant Platform"},{"location":"architecture/23-future-roadmap/#1-multi-tenant-architecture","text":"Isolated tenants with shared infrastructure RBAC-based access and audit segregation Namespace-aware project storage ( tenant_id/project_id pattern)","title":"1. Multi-Tenant Architecture"},{"location":"architecture/23-future-roadmap/#2-centralized-insights-hub","text":"Aggregate findings and metrics across tenants Role-based dashboards (CISO, Pentester, Developer) Global intelligence layer \u2014 vulnerability trends, exploit timelines","title":"2. Centralized Insights Hub"},{"location":"architecture/23-future-roadmap/#3-cross-project-correlation","text":"Federated search across tenants Shared resource registry (optional per policy) Vulnerability fingerprinting and reuse detection","title":"3. Cross-Project Correlation"},{"location":"architecture/23-future-roadmap/#4-collaboration-tools","text":"Comment threads and annotations on findings Assignments and status tracking Shared remediation checklists","title":"4. Collaboration Tools"},{"location":"architecture/23-future-roadmap/#phase-4-autonomous-security-orchestration","text":"The final stage evolves SecFlow into an autonomous orchestration and reasoning engine .","title":"\ud83e\udd16 Phase 4 \u2014 Autonomous Security Orchestration"},{"location":"architecture/23-future-roadmap/#1-adaptive-scanning","text":"Automatically adjusts scanning parameters based on previous findings Integrates feedback loop from risk trends Uses Reinforcement Learning (RL) to optimize coverage vs time","title":"1. Adaptive Scanning"},{"location":"architecture/23-future-roadmap/#2-self-healing-workflows","text":"Automatically retries failed nodes Re-schedules scans based on impact or SLA breach Learns ideal tool chaining patterns","title":"2. Self-Healing Workflows"},{"location":"architecture/23-future-roadmap/#3-predictive-risk-forecasting","text":"Time-series models predicting vulnerability resurgence Integration with incident response systems for proactive alerts","title":"3. Predictive Risk Forecasting"},{"location":"architecture/23-future-roadmap/#4-security-knowledge-graph","text":"Unifies all findings, CVEs, CWEs, ATT&CK tactics, and tool metadata Graph queries (Cypher/SPARQL) for advanced relationships Enables semantic enrichment and AI reasoning","title":"4. Security Knowledge Graph"},{"location":"architecture/23-future-roadmap/#supporting-infrastructure-evolution","text":"Subsystem Upgrade Path Description Database Postgres \u2192 TimescaleDB Time-series analytics and trend computation Cache Layer Redis \u2192 Redis Cluster High-throughput async workflows Task Queue Celery \u2192 Prefect 3 Enhanced DAG orchestration and observability API Gateway FastAPI \u2192 APIStar (optional microgateway) Modular service boundaries UI Stack HTMX \u2192 React + GraphQL Hybrid Real-time triage dashboards","title":"\ud83e\uddf0 Supporting Infrastructure Evolution"},{"location":"architecture/23-future-roadmap/#advanced-integrations","text":"Integration Purpose Caido & ZAP Direct import of active scan results GitHub Advanced Security Repo-based vulnerability correlation ExploitDB / Vulners API Continuous POC enrichment CISA KEV Sync Real-time critical vulnerability flagging AI Plugin System Extensible via OpenAI function-style agents","title":"\ud83e\udde0 Advanced Integrations"},{"location":"architecture/23-future-roadmap/#planned-metrics-analytics-expansion","text":"Area Metric Use Case Risk Intelligence Mean CVSS delta over time Track improvement rate Workflow Analytics Avg. scan node latency Optimize orchestration Tool Effectiveness Finding yield per tool ROI assessment Anomaly Detection Outlier frequency Identify misbehaving scans Team Collaboration Comment-to-remediation ratio Operational efficiency","title":"\ud83d\udcca Planned Metrics &amp; Analytics Expansion"},{"location":"architecture/23-future-roadmap/#future-security-enhancements","text":"Hardware attestation for sensitive tool execution (YubiHSM/TPM) Encrypted local storage using Fernet or AWS KMS Multi-factor API access tokens Zero-trust plugin sandboxing (seccomp profiles) Supply chain integrity checks (sigstore/cosign)","title":"\ud83d\udd12 Future Security Enhancements"},{"location":"architecture/23-future-roadmap/#open-source-community-roadmap","text":"SecFlow will release a Community Edition (CE) focusing on: - Lightweight orchestration - Built-in nuclei/ferox wrappers - Project & finding dashboards - Local SQLite backend Planned for mid-2026 under an Apache 2.0 license with optional enterprise modules.","title":"\ud83c\udf0d Open Source &amp; Community Roadmap"},{"location":"architecture/23-future-roadmap/#community-contributions-roadmap","text":"Plugin SDK Workflow Recipe Gallery Integration templates (for Burp, ZAP, Caido) AI-Triage contribution guide","title":"Community contributions roadmap:"},{"location":"architecture/23-future-roadmap/#timeline-summary","text":"Quarter Milestone Highlights Q4 2025 Phase 1 \u2013 Finalize core platform API + Worker stable release Q1 2026 Phase 2 \u2013 AI & enrichment rollout LLM triage + CVE prediction Q2 2026 Phase 3 \u2013 Multi-tenant & collaboration Team dashboards, tenant isolation Q3 2026 Phase 4 \u2013 Autonomous orchestration Adaptive scans, AI reasoning graph Q4 2026 Community Edition release OSS launch, plugin marketplace","title":"\ud83d\udcc5 Timeline Summary"},{"location":"architecture/23-future-roadmap/#success-metrics-kpis","text":"KPI Target Workflow execution success rate \u2265 98% Mean enrichment latency < 500 ms AI triage accuracy \u2265 90% vs manual MTTR (Mean Time to Risk Report) < 10 min Plugin SDK adoption 50+ community plugins Platform uptime (SLA) 99.9%","title":"\ud83e\udde9 Success Metrics &amp; KPIs"},{"location":"architecture/23-future-roadmap/#long-term-vision","text":"\"From manual scanning to autonomous, continuous, and intelligent security operations.\" SecFlow's long-term mission is to become the AI-native, self-orchestrating red-team assistant , capable of: - Running its own scan campaigns, - Adjusting priorities dynamically, - Explaining vulnerabilities in human language, - Suggesting fixes, - And predicting attack surfaces before exploitation occurs. Next: Final Consensus Summary","title":"\ud83d\udd2e Long-Term Vision"},{"location":"architecture/24-final-consensus-summary/","text":"24 \u2014 Final Consensus Summary \u00b6 \ud83e\udded Overview \u00b6 This document consolidates the final architectural consensus between ChatGPT and Cursor regarding the design, structure, and roadmap of the SecFlow platform. It serves as the canonical closure of the initial R&D phase and provides a clear blueprint for implementation. \ud83e\uddf1 Core Consensus Highlights \u00b6 Aspect Consensus Decision Rationale Architecture Pattern Hexagonal (Ports & Adapters) Enforces separation of concerns, testability, and modularity Repository Structure Mono-repo with Poetry Simplifies dependency management and cross-package development Workflow Engine DAG-based orchestration Enables complex tool chaining and parallel execution Plugin System Decorator-based registration Provides clean extensibility without complex frameworks Security Model RBAC + Sandboxing Balances usability with security requirements Data Model Pydantic + SQLModel Ensures type safety and database integration \ud83e\uddf1 Foundational Architecture Principles \u00b6 1. Hexagonal Architecture (Ports & Adapters) \u00b6 Core business logic isolated from I/O concerns Ports define interfaces, Adapters implement them Dependency inversion ensures testability and modularity 2. Event-Driven Design \u00b6 Asynchronous workflows using Celery/RQ Event sourcing for audit trails and replay capabilities Pub/Sub patterns for loose coupling 3. Immutable Data Flow \u00b6 Findings are immutable once created Versioned resources for reproducibility Audit trails for all data modifications \ud83e\uddf1 Key Technical Agreements \u00b6 1. Core Packages Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"packages/\"] B[\"core-lib/<br/>(Business logic, ports, DTOs)\"] C[\"findings-engine/<br/>(Normalization, enrichment, deduplication)\"] D[\"wrappers/<br/>(Tool integration adapters)\"] E[\"resources/<br/>(Resource registry and management)\"] F[\"storage/<br/>(Database and file system adapters)\"] G[\"plugins/<br/>(Plugin system and registry)\"] H[\"utils/<br/>(Shared utilities and helpers)\"] A --> B A --> C A --> D A --> E A --> F A --> G A --> H 2. Applications Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"apps/\"] B[\"cli/<br/>(Command-line interface)\"] C[\"web/<br/>(Web dashboard and API)\"] D[\"worker/<br/>(Background task processor)\"] E[\"admin/<br/>(Administrative tools)\"] A --> B A --> C A --> D A --> E 3. Workflow Orchestration \u00b6 YAML-based DAG definitions for tool chaining Node-based execution with parallel processing Error handling with retry logic and circuit breakers Caching for intermediate results and resource reuse 4. Security Model \u00b6 JWT-based authentication with role-based access control Sandboxed execution for external tools and PoCs Fernet encryption for sensitive data storage Audit logging for all security-relevant operations \ud83e\uddf1 Risk & Compliance Integration \u00b6 Frameworks Adopted \u00b6 NIST 5x5 Risk Matrix for business risk assessment CVSS v3.1 for technical severity scoring CWE/OWASP for vulnerability classification MITRE ATT&CK for attack pattern mapping Risk Formula \u00b6 Risk Score = (Likelihood \u00d7 Impact) \u00d7 Contextual_Factor Where: - Likelihood = CVSS_Exploitability + EPSS_Score - Impact = CVSS_Impact + Business_Criticality - Contextual_Factor = Asset_Value + Exposure_Level \ud83e\uddf1 Migration Path Summary \u00b6 Phase Duration Focus Deliverables Phase 0 2 weeks Foundation & Guardrails Poetry setup, import boundaries, CI/CD Phase 1 4 weeks Core Models & Data Persistence Pydantic models, SQLModel, basic CRUD Phase 2 6 weeks Findings Engine & Workflow Normalization, DAG execution, tool wrappers Phase 3 8 weeks API, Worker & Triage UI REST API, background tasks, web dashboard Phase 4 4 weeks GC, AI & Advanced Analytics Cleanup, enrichment, risk scoring \ud83e\uddf1 Validation Metrics \u00b6 Metric Target Measurement Test Coverage \u2265 90% pytest + coverage.py Type Safety 100% pyright + mypy Import Boundaries 0 violations import-linter Security Scan 0 high/critical bandit + safety Performance < 2s API response locust load testing \ud83e\uddf1 Strategic Extensions Agreed \u00b6 1. AI Integration (Phase 2) \u00b6 LLM-based triage for finding summarization ML risk prediction using EPSS and CVSS correlation Anomaly detection for outlier findings Natural language queries for analysis 2. Multi-Tenant Architecture (Phase 3) \u00b6 Tenant isolation with shared infrastructure Role-based analytics and dashboards Cross-project correlation and federated search Collaboration tools for team workflows 3. Autonomous Orchestration (Phase 4) \u00b6 Adaptive scanning with reinforcement learning Self-healing workflows with automatic retry Predictive risk forecasting using time-series models Security knowledge graph for semantic reasoning \ud83e\uddf1 Documentation Structure \u00b6 The architecture documentation follows a MkDocs-based structure with: - Material theme for professional presentation - Internal linking between related concepts - Code examples in Python, YAML, and JSON - ASCII diagrams for visual understanding - Version control with semantic versioning \ud83e\uddf1 Consensus from Cursor Review \u00b6 Strengths Identified \u00b6 Clear separation of concerns through hexagonal architecture Comprehensive security model with multiple layers Scalable plugin system for extensibility Robust error handling and recovery mechanisms Areas for Improvement \u00b6 Performance optimization for large-scale deployments Monitoring and observability enhancements Developer experience improvements Community contribution guidelines \ud83e\uddf1 Execution Path Forward \u00b6 Immediate Next Steps \u00b6 Set up Poetry workspace with proper dependency management Implement import boundaries using import-linter Create core Pydantic models for findings and projects Establish CI/CD pipeline with quality gates Success Criteria \u00b6 All 24 architecture documents reviewed and approved Core packages implemented with 90%+ test coverage Basic workflow orchestration functional Security model validated through penetration testing \ud83e\uddf1 Final Architectural Mantra \u00b6 \"Hexagonal by design, event-driven by nature, secure by default, extensible by plugin.\" This mantra encapsulates the core principles that will guide all future development of SecFlow. \ud83e\uddf1 Next Deliverables \u00b6 Deliverable Timeline Owner Poetry Workspace Setup Week 1 Lead Engineer Core Models Implementation Week 2-3 Backend Team Import Boundaries Enforcement Week 1 DevOps Team CI/CD Pipeline Week 2 DevOps Team Basic Workflow Engine Week 4-6 Backend Team \ud83e\uddf1 Closure Statement \u00b6 This document represents the official closure of the SecFlow architecture design phase. All stakeholders have reached consensus on: Technical architecture and implementation approach Security model and compliance requirements Migration strategy and timeline Future roadmap and evolution path The project is now ready to proceed to Phase 0: Foundation & Guardrails implementation. Previous: Future Roadmap Back to: Architecture Index","title":"Final Consensus"},{"location":"architecture/24-final-consensus-summary/#24-final-consensus-summary","text":"","title":"24 \u2014 Final Consensus Summary"},{"location":"architecture/24-final-consensus-summary/#overview","text":"This document consolidates the final architectural consensus between ChatGPT and Cursor regarding the design, structure, and roadmap of the SecFlow platform. It serves as the canonical closure of the initial R&D phase and provides a clear blueprint for implementation.","title":"\ud83e\udded Overview"},{"location":"architecture/24-final-consensus-summary/#core-consensus-highlights","text":"Aspect Consensus Decision Rationale Architecture Pattern Hexagonal (Ports & Adapters) Enforces separation of concerns, testability, and modularity Repository Structure Mono-repo with Poetry Simplifies dependency management and cross-package development Workflow Engine DAG-based orchestration Enables complex tool chaining and parallel execution Plugin System Decorator-based registration Provides clean extensibility without complex frameworks Security Model RBAC + Sandboxing Balances usability with security requirements Data Model Pydantic + SQLModel Ensures type safety and database integration","title":"\ud83e\uddf1 Core Consensus Highlights"},{"location":"architecture/24-final-consensus-summary/#foundational-architecture-principles","text":"","title":"\ud83e\uddf1 Foundational Architecture Principles"},{"location":"architecture/24-final-consensus-summary/#1-hexagonal-architecture-ports-adapters","text":"Core business logic isolated from I/O concerns Ports define interfaces, Adapters implement them Dependency inversion ensures testability and modularity","title":"1. Hexagonal Architecture (Ports &amp; Adapters)"},{"location":"architecture/24-final-consensus-summary/#2-event-driven-design","text":"Asynchronous workflows using Celery/RQ Event sourcing for audit trails and replay capabilities Pub/Sub patterns for loose coupling","title":"2. Event-Driven Design"},{"location":"architecture/24-final-consensus-summary/#3-immutable-data-flow","text":"Findings are immutable once created Versioned resources for reproducibility Audit trails for all data modifications","title":"3. Immutable Data Flow"},{"location":"architecture/24-final-consensus-summary/#key-technical-agreements","text":"","title":"\ud83e\uddf1 Key Technical Agreements"},{"location":"architecture/24-final-consensus-summary/#1-core-packages-structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"packages/\"] B[\"core-lib/<br/>(Business logic, ports, DTOs)\"] C[\"findings-engine/<br/>(Normalization, enrichment, deduplication)\"] D[\"wrappers/<br/>(Tool integration adapters)\"] E[\"resources/<br/>(Resource registry and management)\"] F[\"storage/<br/>(Database and file system adapters)\"] G[\"plugins/<br/>(Plugin system and registry)\"] H[\"utils/<br/>(Shared utilities and helpers)\"] A --> B A --> C A --> D A --> E A --> F A --> G A --> H","title":"1. Core Packages Structure"},{"location":"architecture/24-final-consensus-summary/#2-applications-structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"apps/\"] B[\"cli/<br/>(Command-line interface)\"] C[\"web/<br/>(Web dashboard and API)\"] D[\"worker/<br/>(Background task processor)\"] E[\"admin/<br/>(Administrative tools)\"] A --> B A --> C A --> D A --> E","title":"2. Applications Structure"},{"location":"architecture/24-final-consensus-summary/#3-workflow-orchestration","text":"YAML-based DAG definitions for tool chaining Node-based execution with parallel processing Error handling with retry logic and circuit breakers Caching for intermediate results and resource reuse","title":"3. Workflow Orchestration"},{"location":"architecture/24-final-consensus-summary/#4-security-model","text":"JWT-based authentication with role-based access control Sandboxed execution for external tools and PoCs Fernet encryption for sensitive data storage Audit logging for all security-relevant operations","title":"4. Security Model"},{"location":"architecture/24-final-consensus-summary/#risk-compliance-integration","text":"","title":"\ud83e\uddf1 Risk &amp; Compliance Integration"},{"location":"architecture/24-final-consensus-summary/#frameworks-adopted","text":"NIST 5x5 Risk Matrix for business risk assessment CVSS v3.1 for technical severity scoring CWE/OWASP for vulnerability classification MITRE ATT&CK for attack pattern mapping","title":"Frameworks Adopted"},{"location":"architecture/24-final-consensus-summary/#risk-formula","text":"Risk Score = (Likelihood \u00d7 Impact) \u00d7 Contextual_Factor Where: - Likelihood = CVSS_Exploitability + EPSS_Score - Impact = CVSS_Impact + Business_Criticality - Contextual_Factor = Asset_Value + Exposure_Level","title":"Risk Formula"},{"location":"architecture/24-final-consensus-summary/#migration-path-summary","text":"Phase Duration Focus Deliverables Phase 0 2 weeks Foundation & Guardrails Poetry setup, import boundaries, CI/CD Phase 1 4 weeks Core Models & Data Persistence Pydantic models, SQLModel, basic CRUD Phase 2 6 weeks Findings Engine & Workflow Normalization, DAG execution, tool wrappers Phase 3 8 weeks API, Worker & Triage UI REST API, background tasks, web dashboard Phase 4 4 weeks GC, AI & Advanced Analytics Cleanup, enrichment, risk scoring","title":"\ud83e\uddf1 Migration Path Summary"},{"location":"architecture/24-final-consensus-summary/#validation-metrics","text":"Metric Target Measurement Test Coverage \u2265 90% pytest + coverage.py Type Safety 100% pyright + mypy Import Boundaries 0 violations import-linter Security Scan 0 high/critical bandit + safety Performance < 2s API response locust load testing","title":"\ud83e\uddf1 Validation Metrics"},{"location":"architecture/24-final-consensus-summary/#strategic-extensions-agreed","text":"","title":"\ud83e\uddf1 Strategic Extensions Agreed"},{"location":"architecture/24-final-consensus-summary/#1-ai-integration-phase-2","text":"LLM-based triage for finding summarization ML risk prediction using EPSS and CVSS correlation Anomaly detection for outlier findings Natural language queries for analysis","title":"1. AI Integration (Phase 2)"},{"location":"architecture/24-final-consensus-summary/#2-multi-tenant-architecture-phase-3","text":"Tenant isolation with shared infrastructure Role-based analytics and dashboards Cross-project correlation and federated search Collaboration tools for team workflows","title":"2. Multi-Tenant Architecture (Phase 3)"},{"location":"architecture/24-final-consensus-summary/#3-autonomous-orchestration-phase-4","text":"Adaptive scanning with reinforcement learning Self-healing workflows with automatic retry Predictive risk forecasting using time-series models Security knowledge graph for semantic reasoning","title":"3. Autonomous Orchestration (Phase 4)"},{"location":"architecture/24-final-consensus-summary/#documentation-structure","text":"The architecture documentation follows a MkDocs-based structure with: - Material theme for professional presentation - Internal linking between related concepts - Code examples in Python, YAML, and JSON - ASCII diagrams for visual understanding - Version control with semantic versioning","title":"\ud83e\uddf1 Documentation Structure"},{"location":"architecture/24-final-consensus-summary/#consensus-from-cursor-review","text":"","title":"\ud83e\uddf1 Consensus from Cursor Review"},{"location":"architecture/24-final-consensus-summary/#strengths-identified","text":"Clear separation of concerns through hexagonal architecture Comprehensive security model with multiple layers Scalable plugin system for extensibility Robust error handling and recovery mechanisms","title":"Strengths Identified"},{"location":"architecture/24-final-consensus-summary/#areas-for-improvement","text":"Performance optimization for large-scale deployments Monitoring and observability enhancements Developer experience improvements Community contribution guidelines","title":"Areas for Improvement"},{"location":"architecture/24-final-consensus-summary/#execution-path-forward","text":"","title":"\ud83e\uddf1 Execution Path Forward"},{"location":"architecture/24-final-consensus-summary/#immediate-next-steps","text":"Set up Poetry workspace with proper dependency management Implement import boundaries using import-linter Create core Pydantic models for findings and projects Establish CI/CD pipeline with quality gates","title":"Immediate Next Steps"},{"location":"architecture/24-final-consensus-summary/#success-criteria","text":"All 24 architecture documents reviewed and approved Core packages implemented with 90%+ test coverage Basic workflow orchestration functional Security model validated through penetration testing","title":"Success Criteria"},{"location":"architecture/24-final-consensus-summary/#final-architectural-mantra","text":"\"Hexagonal by design, event-driven by nature, secure by default, extensible by plugin.\" This mantra encapsulates the core principles that will guide all future development of SecFlow.","title":"\ud83e\uddf1 Final Architectural Mantra"},{"location":"architecture/24-final-consensus-summary/#next-deliverables","text":"Deliverable Timeline Owner Poetry Workspace Setup Week 1 Lead Engineer Core Models Implementation Week 2-3 Backend Team Import Boundaries Enforcement Week 1 DevOps Team CI/CD Pipeline Week 2 DevOps Team Basic Workflow Engine Week 4-6 Backend Team","title":"\ud83e\uddf1 Next Deliverables"},{"location":"architecture/24-final-consensus-summary/#closure-statement","text":"This document represents the official closure of the SecFlow architecture design phase. All stakeholders have reached consensus on: Technical architecture and implementation approach Security model and compliance requirements Migration strategy and timeline Future roadmap and evolution path The project is now ready to proceed to Phase 0: Foundation & Guardrails implementation. Previous: Future Roadmap Back to: Architecture Index","title":"\ud83e\uddf1 Closure Statement"},{"location":"developer-guides/m1-devex-improvements-plan/","text":"M1 Developer Experience Improvements Plan \u00b6 \ud83c\udfaf M1 DevEx Strategy \u00b6 With M0-D6 foundation complete, M1 focuses on scaling developer productivity as the system grows with plugins, workflows, and advanced features. \ud83d\ude80 Priority 1: Dev Environment Parity \u00b6 Goal : Maintain \"local = CI\" parity as system complexity grows \u00b6 Plugin System Environment \u00b6 # Ensure plugin development doesn't break parity make setup-plugin-env # - Sets up plugin development environment # - Installs plugin dependencies # - Configures plugin paths # - Validates plugin manifest schema Workflow Engine Environment \u00b6 # Ensure workflow development maintains parity make setup-workflow-env # - Sets up workflow development environment # - Installs workflow dependencies # - Configures workflow paths # - Validates workflow schema Environment Variables Management \u00b6 # Standardized environment setup .env.example # Template for required variables .env.local # Local development overrides .env.test # Test environment configuration Implementation : - Create environment setup scripts - Document all required environment variables - Ensure CI uses same environment configuration - Add environment validation to pre-commit hooks \ud83d\udee0\ufe0f Priority 2: Plugin Development Tooling \u00b6 Goal : Streamline plugin creation and development \u00b6 Plugin Scaffolding \u00b6 # Create new plugin from template make scaffold-plugin PLUGIN_NAME = my-plugin PLUGIN_TYPE = scanner # Generates: # plugins/my-plugin/ # \u251c\u2500\u2500 __init__.py # \u251c\u2500\u2500 manifest.yaml # \u251c\u2500\u2500 scanner.py # \u251c\u2500\u2500 tests/ # \u2502 \u2514\u2500\u2500 test_scanner.py # \u2514\u2500\u2500 README.md Plugin Development Tools \u00b6 # Plugin-specific commands make plugin-test PLUGIN_NAME = my-plugin # Test specific plugin make plugin-lint PLUGIN_NAME = my-plugin # Lint specific plugin make plugin-validate PLUGIN_NAME = my-plugin # Validate plugin manifest Plugin Template System \u00b6 # templates/plugin-scanner.yaml name : \"{{PLUGIN_NAME}}\" version : \"1.0.0\" type : \"scanner\" description : \"{{PLUGIN_DESCRIPTION}}\" author : \"{{AUTHOR_NAME}}\" dependencies : - \"requests\" - \"pyyaml\" Implementation : - Create cookiecutter templates for different plugin types - Add plugin-specific Makefile targets - Implement plugin validation tools - Create plugin development documentation \ud83d\udcda Priority 3: Internal Documentation \u00b6 Goal : Comprehensive developer guides for M1 features \u00b6 Plugin Developer Guide \u00b6 # docs/developer-guides/plugin-development.md ## Plugin Architecture ## Security Policy ## Testing Guidelines ## Deployment Process Workflow Developer Guide \u00b6 # docs/developer-guides/workflow-development.md ## Workflow Engine Design ## Linear v1 Implementation ## Integration Patterns ## Testing Strategies Integration Developer Guide \u00b6 # docs/developer-guides/integration-development.md ## Tool Integration Patterns ## Parser Development ## Contract Testing ## Performance Considerations Implementation : - Collaborate with Security Lead on plugin security docs - Work with Tools Lead on integration patterns - Create comprehensive developer guides - Establish documentation review process \ud83e\uddea Priority 4: Extended Testing Infrastructure \u00b6 Goal : Higher-level testing for M1 architecture \u00b6 Contract Tests Directory \u00b6 # Create missing contract tests structure mkdir -p tests/contracts/ touch tests/contracts/__init__.py touch tests/contracts/test_plugin_contracts.py touch tests/contracts/test_workflow_contracts.py Integration Test Framework \u00b6 # tests/integration/test_plugin_workflow_integration.py def test_plugin_workflow_end_to_end (): \"\"\"Test plugin execution within workflow engine.\"\"\" workflow = WorkflowEngine . load ( \"test-workflow.yaml\" ) plugin = PluginLoader . load ( \"test-plugin\" ) result = workflow . execute ( plugin ) assert result . status == \"success\" assert len ( result . findings ) > 0 Sample Integration Test \u00b6 # tests/integration/test_basic_workflow.py def test_basic_workflow_execution (): \"\"\"Test basic workflow execution with dummy plugin.\"\"\" # Create dummy plugin plugin = DummyPlugin ( name = \"test-scanner\" ) # Create simple workflow workflow = LinearWorkflow () workflow . add_step ( plugin ) # Execute workflow result = workflow . execute ( target = \"http://example.com\" ) # Verify results assert result . completed assert result . findings_count > 0 Implementation : - Create contract tests directory structure - Write sample integration tests - Establish integration test patterns - Coordinate with QA Lead on test strategies \ud83d\udcca Priority 5: Observability Integration \u00b6 Goal : Seamless logging and metrics in development \u00b6 Test Logging Configuration \u00b6 # tests/conftest.py import logging import pytest @pytest . fixture ( autouse = True ) def configure_test_logging (): \"\"\"Configure logging for tests.\"\"\" logging . basicConfig ( level = logging . WARNING , # Reduce noise in tests format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) Development Metrics \u00b6 # scripts/dev-metrics.py def collect_dev_metrics (): \"\"\"Collect development metrics.\"\"\" metrics = { 'test_coverage' : get_coverage_percentage (), 'lint_errors' : get_lint_error_count (), 'type_errors' : get_type_error_count (), 'plugin_count' : get_plugin_count (), 'workflow_count' : get_workflow_count () } return metrics Implementation : - Work with Observability Lead on logging standards - Implement test logging configuration - Create development metrics collection - Establish observability patterns for M1 \ud83d\udd27 Priority 6: Development Tools \u00b6 Goal : Enhanced developer productivity tools \u00b6 Development Dashboard \u00b6 # Local development status make dev-status # Shows: # - Test coverage # - Lint status # - Type check status # - Plugin status # - Workflow status Quick Development Commands \u00b6 # Plugin development shortcuts make plugin-dev PLUGIN_NAME = my-plugin # Start plugin development mode make plugin-watch PLUGIN_NAME = my-plugin # Watch plugin for changes # Workflow development shortcuts make workflow-dev WORKFLOW_NAME = my-workflow # Start workflow development mode make workflow-test WORKFLOW_NAME = my-workflow # Test workflow execution Development Utilities \u00b6 # scripts/dev-utils.py def validate_plugin_manifest ( manifest_path ): \"\"\"Validate plugin manifest against schema.\"\"\" # Implementation def generate_plugin_tests ( plugin_path ): \"\"\"Generate test skeleton for plugin.\"\"\" # Implementation def check_workflow_syntax ( workflow_path ): \"\"\"Check workflow syntax and dependencies.\"\"\" # Implementation Implementation : - Create development dashboard - Implement plugin/workflow development shortcuts - Build development utilities - Establish development tool patterns \ud83d\udccb Implementation Timeline \u00b6 Week 1-2: Foundation \u00b6 Create plugin scaffolding system Implement environment parity tools Set up contract tests directory Week 3-4: Documentation \u00b6 Write plugin developer guide Create workflow developer guide Establish documentation review process Week 5-6: Testing \u00b6 Implement integration test framework Write sample integration tests Establish testing patterns Week 7-8: Tooling \u00b6 Create development dashboard Implement development utilities Establish observability patterns \ud83c\udfaf Success Metrics \u00b6 M1 DevEx KPIs \u00b6 Plugin Development Time : < 2 hours from idea to working plugin Environment Parity : 100% local-CI parity maintained Documentation Coverage : 100% of M1 features documented Test Coverage : Maintain or improve from M0 baseline Developer Satisfaction : Positive feedback on M1 tooling Quality Gates \u00b6 Plugin Validation : All plugins pass validation Workflow Testing : All workflows have integration tests Documentation : All new features documented Performance : No regression in development speed \ud83d\ude80 M1 DevEx Deliverables \u00b6 Tools \u00b6 Plugin scaffolding system Development dashboard Environment parity tools Development utilities Documentation \u00b6 Plugin developer guide Workflow developer guide Integration developer guide M1 onboarding updates Testing \u00b6 Contract tests framework Integration test patterns Sample integration tests Testing guidelines Infrastructure \u00b6 Environment management Observability integration Development tooling Quality gates \ud83d\udcde Coordination Points \u00b6 Security Lead \u00b6 Plugin security policy documentation Security testing guidelines Plugin validation requirements Tools Lead \u00b6 Tool integration patterns Parser development guidelines Integration testing strategies QA Lead \u00b6 Testing framework design Test automation patterns Quality assurance processes Observability Lead \u00b6 Logging standards Metrics collection Monitoring integration M1 DevEx Mission : Scale developer productivity while maintaining quality and governance standards established in M0-D6. Plan created by DevEx Lead - 2025-10-15","title":"M1 Developer Experience Improvements Plan"},{"location":"developer-guides/m1-devex-improvements-plan/#m1-developer-experience-improvements-plan","text":"","title":"M1 Developer Experience Improvements Plan"},{"location":"developer-guides/m1-devex-improvements-plan/#m1-devex-strategy","text":"With M0-D6 foundation complete, M1 focuses on scaling developer productivity as the system grows with plugins, workflows, and advanced features.","title":"\ud83c\udfaf M1 DevEx Strategy"},{"location":"developer-guides/m1-devex-improvements-plan/#priority-1-dev-environment-parity","text":"","title":"\ud83d\ude80 Priority 1: Dev Environment Parity"},{"location":"developer-guides/m1-devex-improvements-plan/#goal-maintain-local-ci-parity-as-system-complexity-grows","text":"","title":"Goal: Maintain \"local = CI\" parity as system complexity grows"},{"location":"developer-guides/m1-devex-improvements-plan/#plugin-system-environment","text":"# Ensure plugin development doesn't break parity make setup-plugin-env # - Sets up plugin development environment # - Installs plugin dependencies # - Configures plugin paths # - Validates plugin manifest schema","title":"Plugin System Environment"},{"location":"developer-guides/m1-devex-improvements-plan/#workflow-engine-environment","text":"# Ensure workflow development maintains parity make setup-workflow-env # - Sets up workflow development environment # - Installs workflow dependencies # - Configures workflow paths # - Validates workflow schema","title":"Workflow Engine Environment"},{"location":"developer-guides/m1-devex-improvements-plan/#environment-variables-management","text":"# Standardized environment setup .env.example # Template for required variables .env.local # Local development overrides .env.test # Test environment configuration Implementation : - Create environment setup scripts - Document all required environment variables - Ensure CI uses same environment configuration - Add environment validation to pre-commit hooks","title":"Environment Variables Management"},{"location":"developer-guides/m1-devex-improvements-plan/#priority-2-plugin-development-tooling","text":"","title":"\ud83d\udee0\ufe0f Priority 2: Plugin Development Tooling"},{"location":"developer-guides/m1-devex-improvements-plan/#goal-streamline-plugin-creation-and-development","text":"","title":"Goal: Streamline plugin creation and development"},{"location":"developer-guides/m1-devex-improvements-plan/#plugin-scaffolding","text":"# Create new plugin from template make scaffold-plugin PLUGIN_NAME = my-plugin PLUGIN_TYPE = scanner # Generates: # plugins/my-plugin/ # \u251c\u2500\u2500 __init__.py # \u251c\u2500\u2500 manifest.yaml # \u251c\u2500\u2500 scanner.py # \u251c\u2500\u2500 tests/ # \u2502 \u2514\u2500\u2500 test_scanner.py # \u2514\u2500\u2500 README.md","title":"Plugin Scaffolding"},{"location":"developer-guides/m1-devex-improvements-plan/#plugin-development-tools","text":"# Plugin-specific commands make plugin-test PLUGIN_NAME = my-plugin # Test specific plugin make plugin-lint PLUGIN_NAME = my-plugin # Lint specific plugin make plugin-validate PLUGIN_NAME = my-plugin # Validate plugin manifest","title":"Plugin Development Tools"},{"location":"developer-guides/m1-devex-improvements-plan/#plugin-template-system","text":"# templates/plugin-scanner.yaml name : \"{{PLUGIN_NAME}}\" version : \"1.0.0\" type : \"scanner\" description : \"{{PLUGIN_DESCRIPTION}}\" author : \"{{AUTHOR_NAME}}\" dependencies : - \"requests\" - \"pyyaml\" Implementation : - Create cookiecutter templates for different plugin types - Add plugin-specific Makefile targets - Implement plugin validation tools - Create plugin development documentation","title":"Plugin Template System"},{"location":"developer-guides/m1-devex-improvements-plan/#priority-3-internal-documentation","text":"","title":"\ud83d\udcda Priority 3: Internal Documentation"},{"location":"developer-guides/m1-devex-improvements-plan/#goal-comprehensive-developer-guides-for-m1-features","text":"","title":"Goal: Comprehensive developer guides for M1 features"},{"location":"developer-guides/m1-devex-improvements-plan/#plugin-developer-guide","text":"# docs/developer-guides/plugin-development.md ## Plugin Architecture ## Security Policy ## Testing Guidelines ## Deployment Process","title":"Plugin Developer Guide"},{"location":"developer-guides/m1-devex-improvements-plan/#workflow-developer-guide","text":"# docs/developer-guides/workflow-development.md ## Workflow Engine Design ## Linear v1 Implementation ## Integration Patterns ## Testing Strategies","title":"Workflow Developer Guide"},{"location":"developer-guides/m1-devex-improvements-plan/#integration-developer-guide","text":"# docs/developer-guides/integration-development.md ## Tool Integration Patterns ## Parser Development ## Contract Testing ## Performance Considerations Implementation : - Collaborate with Security Lead on plugin security docs - Work with Tools Lead on integration patterns - Create comprehensive developer guides - Establish documentation review process","title":"Integration Developer Guide"},{"location":"developer-guides/m1-devex-improvements-plan/#priority-4-extended-testing-infrastructure","text":"","title":"\ud83e\uddea Priority 4: Extended Testing Infrastructure"},{"location":"developer-guides/m1-devex-improvements-plan/#goal-higher-level-testing-for-m1-architecture","text":"","title":"Goal: Higher-level testing for M1 architecture"},{"location":"developer-guides/m1-devex-improvements-plan/#contract-tests-directory","text":"# Create missing contract tests structure mkdir -p tests/contracts/ touch tests/contracts/__init__.py touch tests/contracts/test_plugin_contracts.py touch tests/contracts/test_workflow_contracts.py","title":"Contract Tests Directory"},{"location":"developer-guides/m1-devex-improvements-plan/#integration-test-framework","text":"# tests/integration/test_plugin_workflow_integration.py def test_plugin_workflow_end_to_end (): \"\"\"Test plugin execution within workflow engine.\"\"\" workflow = WorkflowEngine . load ( \"test-workflow.yaml\" ) plugin = PluginLoader . load ( \"test-plugin\" ) result = workflow . execute ( plugin ) assert result . status == \"success\" assert len ( result . findings ) > 0","title":"Integration Test Framework"},{"location":"developer-guides/m1-devex-improvements-plan/#sample-integration-test","text":"# tests/integration/test_basic_workflow.py def test_basic_workflow_execution (): \"\"\"Test basic workflow execution with dummy plugin.\"\"\" # Create dummy plugin plugin = DummyPlugin ( name = \"test-scanner\" ) # Create simple workflow workflow = LinearWorkflow () workflow . add_step ( plugin ) # Execute workflow result = workflow . execute ( target = \"http://example.com\" ) # Verify results assert result . completed assert result . findings_count > 0 Implementation : - Create contract tests directory structure - Write sample integration tests - Establish integration test patterns - Coordinate with QA Lead on test strategies","title":"Sample Integration Test"},{"location":"developer-guides/m1-devex-improvements-plan/#priority-5-observability-integration","text":"","title":"\ud83d\udcca Priority 5: Observability Integration"},{"location":"developer-guides/m1-devex-improvements-plan/#goal-seamless-logging-and-metrics-in-development","text":"","title":"Goal: Seamless logging and metrics in development"},{"location":"developer-guides/m1-devex-improvements-plan/#test-logging-configuration","text":"# tests/conftest.py import logging import pytest @pytest . fixture ( autouse = True ) def configure_test_logging (): \"\"\"Configure logging for tests.\"\"\" logging . basicConfig ( level = logging . WARNING , # Reduce noise in tests format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' )","title":"Test Logging Configuration"},{"location":"developer-guides/m1-devex-improvements-plan/#development-metrics","text":"# scripts/dev-metrics.py def collect_dev_metrics (): \"\"\"Collect development metrics.\"\"\" metrics = { 'test_coverage' : get_coverage_percentage (), 'lint_errors' : get_lint_error_count (), 'type_errors' : get_type_error_count (), 'plugin_count' : get_plugin_count (), 'workflow_count' : get_workflow_count () } return metrics Implementation : - Work with Observability Lead on logging standards - Implement test logging configuration - Create development metrics collection - Establish observability patterns for M1","title":"Development Metrics"},{"location":"developer-guides/m1-devex-improvements-plan/#priority-6-development-tools","text":"","title":"\ud83d\udd27 Priority 6: Development Tools"},{"location":"developer-guides/m1-devex-improvements-plan/#goal-enhanced-developer-productivity-tools","text":"","title":"Goal: Enhanced developer productivity tools"},{"location":"developer-guides/m1-devex-improvements-plan/#development-dashboard","text":"# Local development status make dev-status # Shows: # - Test coverage # - Lint status # - Type check status # - Plugin status # - Workflow status","title":"Development Dashboard"},{"location":"developer-guides/m1-devex-improvements-plan/#quick-development-commands","text":"# Plugin development shortcuts make plugin-dev PLUGIN_NAME = my-plugin # Start plugin development mode make plugin-watch PLUGIN_NAME = my-plugin # Watch plugin for changes # Workflow development shortcuts make workflow-dev WORKFLOW_NAME = my-workflow # Start workflow development mode make workflow-test WORKFLOW_NAME = my-workflow # Test workflow execution","title":"Quick Development Commands"},{"location":"developer-guides/m1-devex-improvements-plan/#development-utilities","text":"# scripts/dev-utils.py def validate_plugin_manifest ( manifest_path ): \"\"\"Validate plugin manifest against schema.\"\"\" # Implementation def generate_plugin_tests ( plugin_path ): \"\"\"Generate test skeleton for plugin.\"\"\" # Implementation def check_workflow_syntax ( workflow_path ): \"\"\"Check workflow syntax and dependencies.\"\"\" # Implementation Implementation : - Create development dashboard - Implement plugin/workflow development shortcuts - Build development utilities - Establish development tool patterns","title":"Development Utilities"},{"location":"developer-guides/m1-devex-improvements-plan/#implementation-timeline","text":"","title":"\ud83d\udccb Implementation Timeline"},{"location":"developer-guides/m1-devex-improvements-plan/#week-1-2-foundation","text":"Create plugin scaffolding system Implement environment parity tools Set up contract tests directory","title":"Week 1-2: Foundation"},{"location":"developer-guides/m1-devex-improvements-plan/#week-3-4-documentation","text":"Write plugin developer guide Create workflow developer guide Establish documentation review process","title":"Week 3-4: Documentation"},{"location":"developer-guides/m1-devex-improvements-plan/#week-5-6-testing","text":"Implement integration test framework Write sample integration tests Establish testing patterns","title":"Week 5-6: Testing"},{"location":"developer-guides/m1-devex-improvements-plan/#week-7-8-tooling","text":"Create development dashboard Implement development utilities Establish observability patterns","title":"Week 7-8: Tooling"},{"location":"developer-guides/m1-devex-improvements-plan/#success-metrics","text":"","title":"\ud83c\udfaf Success Metrics"},{"location":"developer-guides/m1-devex-improvements-plan/#m1-devex-kpis","text":"Plugin Development Time : < 2 hours from idea to working plugin Environment Parity : 100% local-CI parity maintained Documentation Coverage : 100% of M1 features documented Test Coverage : Maintain or improve from M0 baseline Developer Satisfaction : Positive feedback on M1 tooling","title":"M1 DevEx KPIs"},{"location":"developer-guides/m1-devex-improvements-plan/#quality-gates","text":"Plugin Validation : All plugins pass validation Workflow Testing : All workflows have integration tests Documentation : All new features documented Performance : No regression in development speed","title":"Quality Gates"},{"location":"developer-guides/m1-devex-improvements-plan/#m1-devex-deliverables","text":"","title":"\ud83d\ude80 M1 DevEx Deliverables"},{"location":"developer-guides/m1-devex-improvements-plan/#tools","text":"Plugin scaffolding system Development dashboard Environment parity tools Development utilities","title":"Tools"},{"location":"developer-guides/m1-devex-improvements-plan/#documentation","text":"Plugin developer guide Workflow developer guide Integration developer guide M1 onboarding updates","title":"Documentation"},{"location":"developer-guides/m1-devex-improvements-plan/#testing","text":"Contract tests framework Integration test patterns Sample integration tests Testing guidelines","title":"Testing"},{"location":"developer-guides/m1-devex-improvements-plan/#infrastructure","text":"Environment management Observability integration Development tooling Quality gates","title":"Infrastructure"},{"location":"developer-guides/m1-devex-improvements-plan/#coordination-points","text":"","title":"\ud83d\udcde Coordination Points"},{"location":"developer-guides/m1-devex-improvements-plan/#security-lead","text":"Plugin security policy documentation Security testing guidelines Plugin validation requirements","title":"Security Lead"},{"location":"developer-guides/m1-devex-improvements-plan/#tools-lead","text":"Tool integration patterns Parser development guidelines Integration testing strategies","title":"Tools Lead"},{"location":"developer-guides/m1-devex-improvements-plan/#qa-lead","text":"Testing framework design Test automation patterns Quality assurance processes","title":"QA Lead"},{"location":"developer-guides/m1-devex-improvements-plan/#observability-lead","text":"Logging standards Metrics collection Monitoring integration M1 DevEx Mission : Scale developer productivity while maintaining quality and governance standards established in M0-D6. Plan created by DevEx Lead - 2025-10-15","title":"Observability Lead"},{"location":"developer-guides/m1-test-coverage-plan/","text":"M1 Test Coverage Plan - 80% Target \u00b6 \ud83c\udfaf Coverage Goal: 28% \u2192 80% (+52 percentage points) \u00b6 Current State : 28% coverage (3,896/14,009 statements covered) Target : 80% coverage (11,207/14,009 statements covered) Gap : 7,311 statements need test coverage \ud83d\udcca Priority Analysis by Module \u00b6 Tier 1: High-Impact, Low-Coverage Modules (Priority 1) \u00b6 Core System Components \u00b6 findings.py : 17% coverage (841 statements, 696 missed) Impact : Critical for M1 vertical slice Target : 80% coverage Effort : High (696 statements to cover) Tests Needed : Unit tests for finding creation, validation, serialization web_routes.py : 9% coverage (1,485 statements, 1,357 missed) Impact : Core API endpoints for M1 Target : 80% coverage Effort : Very High (1,357 statements to cover) Tests Needed : API endpoint tests, request/response validation store.py : 20% coverage (415 statements, 333 missed) Impact : Data persistence layer Target : 80% coverage Effort : High (333 statements to cover) Tests Needed : Storage adapter tests, data operations M1 New Components \u00b6 packages/workflow_engine/executor.py : 89% coverage (354 statements, 38 missed) Impact : Core M1 workflow execution Target : 95% coverage Effort : Low (38 statements to cover) Tests Needed : Edge case testing, error handling packages/plugins/loader.py : 73% coverage (323 statements, 86 missed) Impact : Plugin loading system Target : 85% coverage Effort : Medium (86 statements to cover) Tests Needed : Plugin loading edge cases, security validation Tier 2: Medium-Impact Modules (Priority 2) \u00b6 Security Components \u00b6 security/signing.py : 32% coverage (178 statements, 121 missed) Impact : Plugin signature verification Target : 80% coverage Effort : Medium (121 statements to cover) Tests Needed : Cryptographic operations, key management security/sandbox.py : 64% coverage (176 statements, 64 missed) Impact : Plugin sandboxing Target : 80% coverage Effort : Medium (64 statements to cover) Tests Needed : Resource limits, isolation testing Runtime Components \u00b6 packages/runtime_core/executor.py : 67% coverage (226 statements, 74 missed) Impact : Runtime execution engine Target : 80% coverage Effort : Medium (74 statements to cover) Tests Needed : Execution flow, error handling Tier 3: Supporting Components (Priority 3) \u00b6 Analytics & Metrics \u00b6 analytics_core/analytics.py : 76% coverage (247 statements, 60 missed) Impact : Analytics and reporting Target : 80% coverage Effort : Low (60 statements to cover) Tests Needed : Analytics calculations, report generation packages/runtime_core/observability/metrics.py : 71% coverage (129 statements, 37 missed) Impact : Performance monitoring Target : 80% coverage Effort : Low (37 statements to cover) Tests Needed : Metrics collection, threshold monitoring \ud83e\uddea Test Implementation Strategy \u00b6 Phase 1: Core M1 Components (Weeks 1-2) \u00b6 1.1 Workflow Engine Testing \u00b6 # tests/unit/test_workflow_engine.py class TestWorkflowExecutor : def test_linear_workflow_execution ( self ): \"\"\"Test basic linear workflow execution.\"\"\" def test_workflow_error_handling ( self ): \"\"\"Test workflow error handling and recovery.\"\"\" def test_workflow_timeout_handling ( self ): \"\"\"Test workflow timeout scenarios.\"\"\" def test_workflow_retry_logic ( self ): \"\"\"Test workflow retry mechanisms.\"\"\" 1.2 Plugin Loader Testing \u00b6 # tests/unit/test_plugin_loader.py class TestPluginLoader : def test_plugin_discovery ( self ): \"\"\"Test plugin discovery mechanism.\"\"\" def test_plugin_loading ( self ): \"\"\"Test plugin loading and initialization.\"\"\" def test_plugin_execution ( self ): \"\"\"Test plugin execution in sandbox.\"\"\" def test_plugin_security_validation ( self ): \"\"\"Test plugin security validation.\"\"\" 1.3 Findings System Testing \u00b6 # tests/unit/test_findings.py class TestFindingsSystem : def test_finding_creation ( self ): \"\"\"Test finding object creation.\"\"\" def test_finding_validation ( self ): \"\"\"Test finding data validation.\"\"\" def test_finding_serialization ( self ): \"\"\"Test finding serialization/deserialization.\"\"\" def test_finding_aggregation ( self ): \"\"\"Test finding aggregation logic.\"\"\" Phase 2: Security Components (Weeks 3-4) \u00b6 2.1 Signature Verification Testing \u00b6 # tests/unit/test_security_signing.py class TestPluginSigning : def test_key_generation ( self ): \"\"\"Test cryptographic key generation.\"\"\" def test_plugin_signing ( self ): \"\"\"Test plugin signing process.\"\"\" def test_signature_verification ( self ): \"\"\"Test signature verification.\"\"\" def test_tamper_detection ( self ): \"\"\"Test tamper detection.\"\"\" 2.2 Sandbox Testing \u00b6 # tests/unit/test_security_sandbox.py class TestPluginSandbox : def test_resource_limits ( self ): \"\"\"Test resource limit enforcement.\"\"\" def test_process_isolation ( self ): \"\"\"Test process isolation.\"\"\" def test_network_restrictions ( self ): \"\"\"Test network access restrictions.\"\"\" def test_filesystem_restrictions ( self ): \"\"\"Test filesystem access restrictions.\"\"\" Phase 3: API & Web Components (Weeks 5-6) \u00b6 3.1 API Endpoint Testing \u00b6 # tests/integration/test_api_endpoints.py class TestAPIEndpoints : def test_workflow_execution_api ( self ): \"\"\"Test workflow execution API.\"\"\" def test_plugin_management_api ( self ): \"\"\"Test plugin management API.\"\"\" def test_findings_api ( self ): \"\"\"Test findings API endpoints.\"\"\" def test_security_api ( self ): \"\"\"Test security-related API endpoints.\"\"\" 3.2 Storage Layer Testing \u00b6 # tests/unit/test_storage.py class TestStorageLayer : def test_data_persistence ( self ): \"\"\"Test data persistence operations.\"\"\" def test_data_retrieval ( self ): \"\"\"Test data retrieval operations.\"\"\" def test_data_validation ( self ): \"\"\"Test data validation in storage.\"\"\" def test_storage_adapter_switching ( self ): \"\"\"Test storage adapter switching.\"\"\" Phase 4: Integration & E2E Testing (Weeks 7-8) \u00b6 4.1 End-to-End Workflow Testing \u00b6 # tests/integration/test_e2e_workflows.py class TestE2EWorkflows : def test_complete_scanning_workflow ( self ): \"\"\"Test complete scanning workflow.\"\"\" def test_plugin_discovery_to_execution ( self ): \"\"\"Test plugin discovery to execution flow.\"\"\" def test_findings_pipeline ( self ): \"\"\"Test findings processing pipeline.\"\"\" def test_security_enforcement_workflow ( self ): \"\"\"Test security enforcement workflow.\"\"\" \ud83d\udcc8 Coverage Tracking Strategy \u00b6 Coverage Dashboard Setup \u00b6 # scripts/coverage_dashboard.py def generate_coverage_report (): \"\"\"Generate detailed coverage report.\"\"\" # Run coverage analysis # Generate HTML report # Track progress against targets # Identify coverage gaps Coverage Ratchet Configuration \u00b6 # scripts/coverage_ratchet.py MILESTONE_TARGETS = { \"M0\" : 18 , # Current baseline \"M1\" : 80 , # Target for M1 \"M2\" : 85 , # Future target \"M3\" : 90 # Future target } CI Integration \u00b6 # .github/workflows/coverage.yml - name : Coverage Analysis run : | pytest --cov=. --cov-report=xml --cov-report=html python scripts/coverage_ratchet.py python scripts/coverage_dashboard.py \ud83c\udfaf Coverage Targets by Module \u00b6 M1 Critical Path Modules (80%+ target) \u00b6 Workflow Engine : 89% \u2192 95% (+6%) Plugin Loader : 73% \u2192 85% (+12%) Findings System : 17% \u2192 80% (+63%) Security Signing : 32% \u2192 80% (+48%) Security Sandbox : 64% \u2192 80% (+16%) M1 Supporting Modules (70%+ target) \u00b6 Web Routes : 9% \u2192 70% (+61%) Storage Layer : 20% \u2192 70% (+50%) Runtime Core : 67% \u2192 75% (+8%) Analytics : 76% \u2192 80% (+4%) M1 Infrastructure Modules (60%+ target) \u00b6 Observability : 71% \u2192 75% (+4%) Utilities : 62% \u2192 70% (+8%) Configuration : 49% \u2192 60% (+11%) \ud83d\udcca Coverage Progress Tracking \u00b6 Weekly Coverage Goals \u00b6 Week 1 : 28% \u2192 40% (+12%) Week 2 : 40% \u2192 55% (+15%) Week 3 : 55% \u2192 65% (+10%) Week 4 : 65% \u2192 72% (+7%) Week 5 : 72% \u2192 76% (+4%) Week 6 : 76% \u2192 78% (+2%) Week 7 : 78% \u2192 79% (+1%) Week 8 : 79% \u2192 80% (+1%) Coverage Metrics Dashboard \u00b6 # Coverage tracking metrics coverage_metrics = { \"total_statements\" : 14009 , \"covered_statements\" : 3896 , \"current_coverage\" : 28.0 , \"target_coverage\" : 80.0 , \"coverage_gap\" : 7311 , \"progress_percentage\" : 0.0 } \ud83d\udee0\ufe0f Testing Tools & Infrastructure \u00b6 Test Framework Enhancements \u00b6 # pytest.ini [ tool : pytest ] markers = unit : Unit tests integration : Integration tests e2e : End - to - end tests slow : Slow running tests security : Security - related tests workflow : Workflow engine tests plugin : Plugin system tests Test Data Management \u00b6 # tests/fixtures/test_data.py class TestDataFixtures : @staticmethod def create_sample_findings (): \"\"\"Create sample findings for testing.\"\"\" @staticmethod def create_sample_workflows (): \"\"\"Create sample workflows for testing.\"\"\" @staticmethod def create_sample_plugins (): \"\"\"Create sample plugins for testing.\"\"\" Mock and Stub Management \u00b6 # tests/mocks/plugin_mocks.py class MockPlugin : \"\"\"Mock plugin for testing.\"\"\" class MockWorkflow : \"\"\"Mock workflow for testing.\"\"\" class MockStorage : \"\"\"Mock storage for testing.\"\"\" \ud83d\ude80 Implementation Timeline \u00b6 Week 1-2: Core Components \u00b6 Workflow engine testing (95% target) Plugin loader testing (85% target) Findings system testing (80% target) Coverage tracking setup Week 3-4: Security Components \u00b6 Signature verification testing (80% target) Sandbox testing (80% target) Security integration testing Coverage dashboard implementation Week 5-6: API & Storage \u00b6 API endpoint testing (70% target) Storage layer testing (70% target) Integration testing framework Coverage ratchet enforcement Week 7-8: Integration & E2E \u00b6 End-to-end workflow testing Complete system integration testing Performance testing Final coverage validation (80% target) \ud83d\udccb Success Criteria \u00b6 Coverage Targets \u00b6 Overall Coverage : 80% (11,207/14,009 statements) Critical Path Coverage : 85%+ (workflow, plugin, findings) Security Coverage : 80%+ (signing, sandbox) API Coverage : 70%+ (web routes, endpoints) Quality Gates \u00b6 All Tests Passing : 100% test success rate Coverage Ratchet : No decrease in coverage CI Integration : Coverage reporting in CI Documentation : Test coverage documentation M1 Deliverables \u00b6 80% test coverage achieved Coverage tracking dashboard Comprehensive test suite CI coverage integration Test documentation M1 Test Coverage Plan : Comprehensive strategy to achieve 80% coverage target through systematic testing of critical M1 components. Plan created by DevEx Lead - 2025-10-15","title":"M1 Test Coverage Plan - 80% Target"},{"location":"developer-guides/m1-test-coverage-plan/#m1-test-coverage-plan-80-target","text":"","title":"M1 Test Coverage Plan - 80% Target"},{"location":"developer-guides/m1-test-coverage-plan/#coverage-goal-28-80-52-percentage-points","text":"Current State : 28% coverage (3,896/14,009 statements covered) Target : 80% coverage (11,207/14,009 statements covered) Gap : 7,311 statements need test coverage","title":"\ud83c\udfaf Coverage Goal: 28% \u2192 80% (+52 percentage points)"},{"location":"developer-guides/m1-test-coverage-plan/#priority-analysis-by-module","text":"","title":"\ud83d\udcca Priority Analysis by Module"},{"location":"developer-guides/m1-test-coverage-plan/#tier-1-high-impact-low-coverage-modules-priority-1","text":"","title":"Tier 1: High-Impact, Low-Coverage Modules (Priority 1)"},{"location":"developer-guides/m1-test-coverage-plan/#core-system-components","text":"findings.py : 17% coverage (841 statements, 696 missed) Impact : Critical for M1 vertical slice Target : 80% coverage Effort : High (696 statements to cover) Tests Needed : Unit tests for finding creation, validation, serialization web_routes.py : 9% coverage (1,485 statements, 1,357 missed) Impact : Core API endpoints for M1 Target : 80% coverage Effort : Very High (1,357 statements to cover) Tests Needed : API endpoint tests, request/response validation store.py : 20% coverage (415 statements, 333 missed) Impact : Data persistence layer Target : 80% coverage Effort : High (333 statements to cover) Tests Needed : Storage adapter tests, data operations","title":"Core System Components"},{"location":"developer-guides/m1-test-coverage-plan/#m1-new-components","text":"packages/workflow_engine/executor.py : 89% coverage (354 statements, 38 missed) Impact : Core M1 workflow execution Target : 95% coverage Effort : Low (38 statements to cover) Tests Needed : Edge case testing, error handling packages/plugins/loader.py : 73% coverage (323 statements, 86 missed) Impact : Plugin loading system Target : 85% coverage Effort : Medium (86 statements to cover) Tests Needed : Plugin loading edge cases, security validation","title":"M1 New Components"},{"location":"developer-guides/m1-test-coverage-plan/#tier-2-medium-impact-modules-priority-2","text":"","title":"Tier 2: Medium-Impact Modules (Priority 2)"},{"location":"developer-guides/m1-test-coverage-plan/#security-components","text":"security/signing.py : 32% coverage (178 statements, 121 missed) Impact : Plugin signature verification Target : 80% coverage Effort : Medium (121 statements to cover) Tests Needed : Cryptographic operations, key management security/sandbox.py : 64% coverage (176 statements, 64 missed) Impact : Plugin sandboxing Target : 80% coverage Effort : Medium (64 statements to cover) Tests Needed : Resource limits, isolation testing","title":"Security Components"},{"location":"developer-guides/m1-test-coverage-plan/#runtime-components","text":"packages/runtime_core/executor.py : 67% coverage (226 statements, 74 missed) Impact : Runtime execution engine Target : 80% coverage Effort : Medium (74 statements to cover) Tests Needed : Execution flow, error handling","title":"Runtime Components"},{"location":"developer-guides/m1-test-coverage-plan/#tier-3-supporting-components-priority-3","text":"","title":"Tier 3: Supporting Components (Priority 3)"},{"location":"developer-guides/m1-test-coverage-plan/#analytics-metrics","text":"analytics_core/analytics.py : 76% coverage (247 statements, 60 missed) Impact : Analytics and reporting Target : 80% coverage Effort : Low (60 statements to cover) Tests Needed : Analytics calculations, report generation packages/runtime_core/observability/metrics.py : 71% coverage (129 statements, 37 missed) Impact : Performance monitoring Target : 80% coverage Effort : Low (37 statements to cover) Tests Needed : Metrics collection, threshold monitoring","title":"Analytics &amp; Metrics"},{"location":"developer-guides/m1-test-coverage-plan/#test-implementation-strategy","text":"","title":"\ud83e\uddea Test Implementation Strategy"},{"location":"developer-guides/m1-test-coverage-plan/#phase-1-core-m1-components-weeks-1-2","text":"","title":"Phase 1: Core M1 Components (Weeks 1-2)"},{"location":"developer-guides/m1-test-coverage-plan/#11-workflow-engine-testing","text":"# tests/unit/test_workflow_engine.py class TestWorkflowExecutor : def test_linear_workflow_execution ( self ): \"\"\"Test basic linear workflow execution.\"\"\" def test_workflow_error_handling ( self ): \"\"\"Test workflow error handling and recovery.\"\"\" def test_workflow_timeout_handling ( self ): \"\"\"Test workflow timeout scenarios.\"\"\" def test_workflow_retry_logic ( self ): \"\"\"Test workflow retry mechanisms.\"\"\"","title":"1.1 Workflow Engine Testing"},{"location":"developer-guides/m1-test-coverage-plan/#12-plugin-loader-testing","text":"# tests/unit/test_plugin_loader.py class TestPluginLoader : def test_plugin_discovery ( self ): \"\"\"Test plugin discovery mechanism.\"\"\" def test_plugin_loading ( self ): \"\"\"Test plugin loading and initialization.\"\"\" def test_plugin_execution ( self ): \"\"\"Test plugin execution in sandbox.\"\"\" def test_plugin_security_validation ( self ): \"\"\"Test plugin security validation.\"\"\"","title":"1.2 Plugin Loader Testing"},{"location":"developer-guides/m1-test-coverage-plan/#13-findings-system-testing","text":"# tests/unit/test_findings.py class TestFindingsSystem : def test_finding_creation ( self ): \"\"\"Test finding object creation.\"\"\" def test_finding_validation ( self ): \"\"\"Test finding data validation.\"\"\" def test_finding_serialization ( self ): \"\"\"Test finding serialization/deserialization.\"\"\" def test_finding_aggregation ( self ): \"\"\"Test finding aggregation logic.\"\"\"","title":"1.3 Findings System Testing"},{"location":"developer-guides/m1-test-coverage-plan/#phase-2-security-components-weeks-3-4","text":"","title":"Phase 2: Security Components (Weeks 3-4)"},{"location":"developer-guides/m1-test-coverage-plan/#21-signature-verification-testing","text":"# tests/unit/test_security_signing.py class TestPluginSigning : def test_key_generation ( self ): \"\"\"Test cryptographic key generation.\"\"\" def test_plugin_signing ( self ): \"\"\"Test plugin signing process.\"\"\" def test_signature_verification ( self ): \"\"\"Test signature verification.\"\"\" def test_tamper_detection ( self ): \"\"\"Test tamper detection.\"\"\"","title":"2.1 Signature Verification Testing"},{"location":"developer-guides/m1-test-coverage-plan/#22-sandbox-testing","text":"# tests/unit/test_security_sandbox.py class TestPluginSandbox : def test_resource_limits ( self ): \"\"\"Test resource limit enforcement.\"\"\" def test_process_isolation ( self ): \"\"\"Test process isolation.\"\"\" def test_network_restrictions ( self ): \"\"\"Test network access restrictions.\"\"\" def test_filesystem_restrictions ( self ): \"\"\"Test filesystem access restrictions.\"\"\"","title":"2.2 Sandbox Testing"},{"location":"developer-guides/m1-test-coverage-plan/#phase-3-api-web-components-weeks-5-6","text":"","title":"Phase 3: API &amp; Web Components (Weeks 5-6)"},{"location":"developer-guides/m1-test-coverage-plan/#31-api-endpoint-testing","text":"# tests/integration/test_api_endpoints.py class TestAPIEndpoints : def test_workflow_execution_api ( self ): \"\"\"Test workflow execution API.\"\"\" def test_plugin_management_api ( self ): \"\"\"Test plugin management API.\"\"\" def test_findings_api ( self ): \"\"\"Test findings API endpoints.\"\"\" def test_security_api ( self ): \"\"\"Test security-related API endpoints.\"\"\"","title":"3.1 API Endpoint Testing"},{"location":"developer-guides/m1-test-coverage-plan/#32-storage-layer-testing","text":"# tests/unit/test_storage.py class TestStorageLayer : def test_data_persistence ( self ): \"\"\"Test data persistence operations.\"\"\" def test_data_retrieval ( self ): \"\"\"Test data retrieval operations.\"\"\" def test_data_validation ( self ): \"\"\"Test data validation in storage.\"\"\" def test_storage_adapter_switching ( self ): \"\"\"Test storage adapter switching.\"\"\"","title":"3.2 Storage Layer Testing"},{"location":"developer-guides/m1-test-coverage-plan/#phase-4-integration-e2e-testing-weeks-7-8","text":"","title":"Phase 4: Integration &amp; E2E Testing (Weeks 7-8)"},{"location":"developer-guides/m1-test-coverage-plan/#41-end-to-end-workflow-testing","text":"# tests/integration/test_e2e_workflows.py class TestE2EWorkflows : def test_complete_scanning_workflow ( self ): \"\"\"Test complete scanning workflow.\"\"\" def test_plugin_discovery_to_execution ( self ): \"\"\"Test plugin discovery to execution flow.\"\"\" def test_findings_pipeline ( self ): \"\"\"Test findings processing pipeline.\"\"\" def test_security_enforcement_workflow ( self ): \"\"\"Test security enforcement workflow.\"\"\"","title":"4.1 End-to-End Workflow Testing"},{"location":"developer-guides/m1-test-coverage-plan/#coverage-tracking-strategy","text":"","title":"\ud83d\udcc8 Coverage Tracking Strategy"},{"location":"developer-guides/m1-test-coverage-plan/#coverage-dashboard-setup","text":"# scripts/coverage_dashboard.py def generate_coverage_report (): \"\"\"Generate detailed coverage report.\"\"\" # Run coverage analysis # Generate HTML report # Track progress against targets # Identify coverage gaps","title":"Coverage Dashboard Setup"},{"location":"developer-guides/m1-test-coverage-plan/#coverage-ratchet-configuration","text":"# scripts/coverage_ratchet.py MILESTONE_TARGETS = { \"M0\" : 18 , # Current baseline \"M1\" : 80 , # Target for M1 \"M2\" : 85 , # Future target \"M3\" : 90 # Future target }","title":"Coverage Ratchet Configuration"},{"location":"developer-guides/m1-test-coverage-plan/#ci-integration","text":"# .github/workflows/coverage.yml - name : Coverage Analysis run : | pytest --cov=. --cov-report=xml --cov-report=html python scripts/coverage_ratchet.py python scripts/coverage_dashboard.py","title":"CI Integration"},{"location":"developer-guides/m1-test-coverage-plan/#coverage-targets-by-module","text":"","title":"\ud83c\udfaf Coverage Targets by Module"},{"location":"developer-guides/m1-test-coverage-plan/#m1-critical-path-modules-80-target","text":"Workflow Engine : 89% \u2192 95% (+6%) Plugin Loader : 73% \u2192 85% (+12%) Findings System : 17% \u2192 80% (+63%) Security Signing : 32% \u2192 80% (+48%) Security Sandbox : 64% \u2192 80% (+16%)","title":"M1 Critical Path Modules (80%+ target)"},{"location":"developer-guides/m1-test-coverage-plan/#m1-supporting-modules-70-target","text":"Web Routes : 9% \u2192 70% (+61%) Storage Layer : 20% \u2192 70% (+50%) Runtime Core : 67% \u2192 75% (+8%) Analytics : 76% \u2192 80% (+4%)","title":"M1 Supporting Modules (70%+ target)"},{"location":"developer-guides/m1-test-coverage-plan/#m1-infrastructure-modules-60-target","text":"Observability : 71% \u2192 75% (+4%) Utilities : 62% \u2192 70% (+8%) Configuration : 49% \u2192 60% (+11%)","title":"M1 Infrastructure Modules (60%+ target)"},{"location":"developer-guides/m1-test-coverage-plan/#coverage-progress-tracking","text":"","title":"\ud83d\udcca Coverage Progress Tracking"},{"location":"developer-guides/m1-test-coverage-plan/#weekly-coverage-goals","text":"Week 1 : 28% \u2192 40% (+12%) Week 2 : 40% \u2192 55% (+15%) Week 3 : 55% \u2192 65% (+10%) Week 4 : 65% \u2192 72% (+7%) Week 5 : 72% \u2192 76% (+4%) Week 6 : 76% \u2192 78% (+2%) Week 7 : 78% \u2192 79% (+1%) Week 8 : 79% \u2192 80% (+1%)","title":"Weekly Coverage Goals"},{"location":"developer-guides/m1-test-coverage-plan/#coverage-metrics-dashboard","text":"# Coverage tracking metrics coverage_metrics = { \"total_statements\" : 14009 , \"covered_statements\" : 3896 , \"current_coverage\" : 28.0 , \"target_coverage\" : 80.0 , \"coverage_gap\" : 7311 , \"progress_percentage\" : 0.0 }","title":"Coverage Metrics Dashboard"},{"location":"developer-guides/m1-test-coverage-plan/#testing-tools-infrastructure","text":"","title":"\ud83d\udee0\ufe0f Testing Tools &amp; Infrastructure"},{"location":"developer-guides/m1-test-coverage-plan/#test-framework-enhancements","text":"# pytest.ini [ tool : pytest ] markers = unit : Unit tests integration : Integration tests e2e : End - to - end tests slow : Slow running tests security : Security - related tests workflow : Workflow engine tests plugin : Plugin system tests","title":"Test Framework Enhancements"},{"location":"developer-guides/m1-test-coverage-plan/#test-data-management","text":"# tests/fixtures/test_data.py class TestDataFixtures : @staticmethod def create_sample_findings (): \"\"\"Create sample findings for testing.\"\"\" @staticmethod def create_sample_workflows (): \"\"\"Create sample workflows for testing.\"\"\" @staticmethod def create_sample_plugins (): \"\"\"Create sample plugins for testing.\"\"\"","title":"Test Data Management"},{"location":"developer-guides/m1-test-coverage-plan/#mock-and-stub-management","text":"# tests/mocks/plugin_mocks.py class MockPlugin : \"\"\"Mock plugin for testing.\"\"\" class MockWorkflow : \"\"\"Mock workflow for testing.\"\"\" class MockStorage : \"\"\"Mock storage for testing.\"\"\"","title":"Mock and Stub Management"},{"location":"developer-guides/m1-test-coverage-plan/#implementation-timeline","text":"","title":"\ud83d\ude80 Implementation Timeline"},{"location":"developer-guides/m1-test-coverage-plan/#week-1-2-core-components","text":"Workflow engine testing (95% target) Plugin loader testing (85% target) Findings system testing (80% target) Coverage tracking setup","title":"Week 1-2: Core Components"},{"location":"developer-guides/m1-test-coverage-plan/#week-3-4-security-components","text":"Signature verification testing (80% target) Sandbox testing (80% target) Security integration testing Coverage dashboard implementation","title":"Week 3-4: Security Components"},{"location":"developer-guides/m1-test-coverage-plan/#week-5-6-api-storage","text":"API endpoint testing (70% target) Storage layer testing (70% target) Integration testing framework Coverage ratchet enforcement","title":"Week 5-6: API &amp; Storage"},{"location":"developer-guides/m1-test-coverage-plan/#week-7-8-integration-e2e","text":"End-to-end workflow testing Complete system integration testing Performance testing Final coverage validation (80% target)","title":"Week 7-8: Integration &amp; E2E"},{"location":"developer-guides/m1-test-coverage-plan/#success-criteria","text":"","title":"\ud83d\udccb Success Criteria"},{"location":"developer-guides/m1-test-coverage-plan/#coverage-targets","text":"Overall Coverage : 80% (11,207/14,009 statements) Critical Path Coverage : 85%+ (workflow, plugin, findings) Security Coverage : 80%+ (signing, sandbox) API Coverage : 70%+ (web routes, endpoints)","title":"Coverage Targets"},{"location":"developer-guides/m1-test-coverage-plan/#quality-gates","text":"All Tests Passing : 100% test success rate Coverage Ratchet : No decrease in coverage CI Integration : Coverage reporting in CI Documentation : Test coverage documentation","title":"Quality Gates"},{"location":"developer-guides/m1-test-coverage-plan/#m1-deliverables","text":"80% test coverage achieved Coverage tracking dashboard Comprehensive test suite CI coverage integration Test documentation M1 Test Coverage Plan : Comprehensive strategy to achieve 80% coverage target through systematic testing of critical M1 components. Plan created by DevEx Lead - 2025-10-15","title":"M1 Deliverables"},{"location":"developer-guides/python-314-integration-strategy/","text":"Python 3.14 Integration Strategy - M1 DevEx \u00b6 \ud83d\udc0d Python 3.14 Integration Overview \u00b6 Current State : Python 3.11.9 (locked for M0-D6) Target : Python 3.14 compatibility assessment and gradual migration Timeline : M1 assessment, M2+ migration planning \ud83c\udfaf Python 3.14 Key Features \u00b6 Free-Threading Mode (PEP 703) \u00b6 No-GIL Build : Optional free-threading mode Performance : Improved concurrency for I/O-bound workloads Compatibility : Existing code works without changes Migration : Gradual adoption possible Enhanced Debugger Interface (PEP 768) \u00b6 Improved Debugging : Better debugging capabilities Workflow Integration : Enhanced workflow debugging Development Experience : Better developer tools Other Improvements \u00b6 Performance : General performance improvements Type System : Enhanced type checking Standard Library : New features and improvements \ud83d\udccb M1 Assessment Strategy \u00b6 Phase 1: Compatibility Testing (M1) \u00b6 1.1 Dependency Compatibility Check \u00b6 # Check dependency compatibility with Python 3.14 pip install --dry-run --python-version 3 .14 -r requirements.txt Key Dependencies to Test : - pytest and pytest-cov - ruff and pyright - flask and requests - pydantic and jsonschema - coverage and import-linter 1.2 CI Matrix Testing \u00b6 # .github/workflows/python-compatibility.yml name : Python Compatibility on : [ pull_request , push ] jobs : compatibility : runs-on : ubuntu-latest strategy : matrix : python-version : [ \"3.11\" , \"3.14\" ] steps : - uses : actions/checkout@v4 - uses : actions/setup-python@v5 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -e \".[dev]\" - name : Run tests run : pytest -q - name : Run coverage run : pytest --cov=. --cov-report=xml -q 1.3 Tox Environment Setup \u00b6 # tox.ini [tox] envlist = py311, py314 [testenv] deps = pytest pytest-cov ruff pyright import-linter commands = ruff check . pyright lint-imports pytest --cov = . --cov-report=xml -q Phase 2: Performance Testing (M1) \u00b6 2.1 Concurrency Testing \u00b6 # tests/performance/test_concurrency.py import pytest import threading import time from concurrent.futures import ThreadPoolExecutor @pytest . mark . performance def test_plugin_concurrent_execution (): \"\"\"Test plugin execution with multiple threads.\"\"\" def execute_plugin (): # Simulate plugin execution time . sleep ( 0.1 ) return \"success\" with ThreadPoolExecutor ( max_workers = 4 ) as executor : futures = [ executor . submit ( execute_plugin ) for _ in range ( 10 )] results = [ future . result () for future in futures ] assert all ( result == \"success\" for result in results ) @pytest . mark . performance def test_workflow_concurrent_execution (): \"\"\"Test workflow execution with multiple threads.\"\"\" def execute_workflow (): # Simulate workflow execution time . sleep ( 0.2 ) return \"workflow_success\" with ThreadPoolExecutor ( max_workers = 2 ) as executor : futures = [ executor . submit ( execute_workflow ) for _ in range ( 5 )] results = [ future . result () for future in futures ] assert all ( result == \"workflow_success\" for result in results ) 2.2 Performance Benchmarking \u00b6 # scripts/python_version_benchmark.py import time import sys import threading from concurrent.futures import ThreadPoolExecutor def benchmark_concurrency (): \"\"\"Benchmark concurrency performance.\"\"\" def worker (): time . sleep ( 0.01 ) return threading . get_ident () start_time = time . time () with ThreadPoolExecutor ( max_workers = 10 ) as executor : futures = [ executor . submit ( worker ) for _ in range ( 100 )] results = [ future . result () for future in futures ] execution_time = time . time () - start_time return { \"python_version\" : sys . version , \"execution_time\" : execution_time , \"thread_count\" : len ( set ( results )), \"throughput\" : 100 / execution_time } if __name__ == \"__main__\" : result = benchmark_concurrency () print ( f \"Python { result [ 'python_version' ] } \" ) print ( f \"Execution time: { result [ 'execution_time' ] : .3f } s\" ) print ( f \"Thread count: { result [ 'thread_count' ] } \" ) print ( f \"Throughput: { result [ 'throughput' ] : .1f } ops/sec\" ) Phase 3: Migration Planning (M2+) \u00b6 3.1 Gradual Migration Strategy \u00b6 # scripts/migration_assessment.py import sys import subprocess import json from pathlib import Path class Python314MigrationAssessment : \"\"\"Assess Python 3.14 migration readiness.\"\"\" def __init__ ( self ): self . current_version = sys . version_info self . target_version = ( 3 , 14 , 0 ) def check_dependency_compatibility ( self ): \"\"\"Check dependency compatibility with Python 3.14.\"\"\" dependencies = [ \"pytest\" , \"pytest-cov\" , \"ruff\" , \"pyright\" , \"import-linter\" , \"flask\" , \"requests\" , \"pydantic\" , \"jsonschema\" , \"coverage\" ] compatibility_report = {} for dep in dependencies : try : # Check if dependency supports Python 3.14 result = subprocess . run ([ \"pip\" , \"install\" , \"--dry-run\" , f \"--python-version=3.14\" , dep ], capture_output = True , text = True ) compatibility_report [ dep ] = { \"compatible\" : result . returncode == 0 , \"error\" : result . stderr if result . returncode != 0 else None } except Exception as e : compatibility_report [ dep ] = { \"compatible\" : False , \"error\" : str ( e ) } return compatibility_report def assess_code_compatibility ( self ): \"\"\"Assess code compatibility with Python 3.14.\"\"\" # Check for deprecated features # Check for syntax compatibility # Check for standard library changes return { \"syntax_compatible\" : True , \"deprecated_features\" : [], \"standard_library_changes\" : [] } def generate_migration_plan ( self ): \"\"\"Generate migration plan.\"\"\" dependency_report = self . check_dependency_compatibility () code_report = self . assess_code_compatibility () migration_plan = { \"current_version\" : f \" { self . current_version . major } . { self . current_version . minor } . { self . current_version . micro } \" , \"target_version\" : f \" { self . target_version [ 0 ] } . { self . target_version [ 1 ] } . { self . target_version [ 2 ] } \" , \"dependency_compatibility\" : dependency_report , \"code_compatibility\" : code_report , \"migration_steps\" : [ \"1. Update CI matrix to include Python 3.14\" , \"2. Fix any dependency compatibility issues\" , \"3. Update code for Python 3.14 compatibility\" , \"4. Run comprehensive tests on Python 3.14\" , \"5. Update documentation and deployment scripts\" , \"6. Gradual rollout to development environments\" ], \"risks\" : [ \"Dependency compatibility issues\" , \"Performance regressions\" , \"Development environment fragmentation\" , \"Deployment complexity\" ] } return migration_plan if __name__ == \"__main__\" : assessment = Python314MigrationAssessment () plan = assessment . generate_migration_plan () # Save migration plan with open ( \"python_314_migration_plan.json\" , \"w\" ) as f : json . dump ( plan , f , indent = 2 ) print ( \"Python 3.14 Migration Assessment Complete\" ) print ( f \"Migration plan saved to: python_314_migration_plan.json\" ) \ud83d\udee0\ufe0f Development Tools for Python 3.14 \u00b6 Enhanced Debugging Tools \u00b6 # scripts/enhanced_debugger.py import sys import threading from typing import Any , Dict class WorkflowDebugger : \"\"\"Enhanced debugger for workflow execution.\"\"\" def __init__ ( self ): self . breakpoints = {} self . watch_variables = {} self . thread_info = {} def set_breakpoint ( self , workflow_id : str , step_name : str ): \"\"\"Set breakpoint in workflow execution.\"\"\" self . breakpoints [ f \" { workflow_id } : { step_name } \" ] = True def watch_variable ( self , name : str , value : Any ): \"\"\"Watch variable changes.\"\"\" self . watch_variables [ name ] = value def get_thread_info ( self ) -> Dict [ str , Any ]: \"\"\"Get thread information for debugging.\"\"\" if sys . version_info >= ( 3 , 14 ): # Use enhanced debugging features return { \"thread_count\" : threading . active_count (), \"main_thread\" : threading . main_thread () . name , \"current_thread\" : threading . current_thread () . name , \"thread_ident\" : threading . get_ident () } else : return { \"thread_count\" : threading . active_count (), \"current_thread\" : threading . current_thread () . name } def debug_workflow_execution ( self , workflow_id : str , step_name : str , context : Dict [ str , Any ]): \"\"\"Debug workflow execution step.\"\"\" print ( f \"\ud83d\udc1b Debugging { workflow_id } : { step_name } \" ) print ( f \"\ud83d\udcca Context: { context } \" ) print ( f \"\ud83e\uddf5 Thread info: { self . get_thread_info () } \" ) # Check for breakpoints breakpoint_key = f \" { workflow_id } : { step_name } \" if breakpoint_key in self . breakpoints : print ( f \"\u23f8\ufe0f Breakpoint hit: { breakpoint_key } \" ) # In Python 3.14, we could use enhanced debugging features here # Check watched variables for name , expected_value in self . watch_variables . items (): if name in context : actual_value = context [ name ] if actual_value != expected_value : print ( f \"\ud83d\udc40 Variable changed: { name } = { actual_value } (expected: { expected_value } )\" ) Concurrency Testing Tools \u00b6 # scripts/concurrency_testing.py import threading import time import concurrent.futures from typing import List , Callable , Any class ConcurrencyTester : \"\"\"Test concurrency behavior across Python versions.\"\"\" def __init__ ( self ): self . results = [] def test_thread_safety ( self , func : Callable , args_list : List [ tuple ], max_workers : int = 4 ): \"\"\"Test thread safety of a function.\"\"\" start_time = time . time () with concurrent . futures . ThreadPoolExecutor ( max_workers = max_workers ) as executor : futures = [ executor . submit ( func , * args ) for args in args_list ] results = [ future . result () for future in futures ] execution_time = time . time () - start_time return { \"execution_time\" : execution_time , \"results\" : results , \"success_count\" : sum ( 1 for r in results if r is not None ), \"thread_count\" : threading . active_count () } def test_race_conditions ( self , func : Callable , iterations : int = 100 ): \"\"\"Test for race conditions.\"\"\" shared_data = { \"counter\" : 0 } def increment_counter (): for _ in range ( iterations ): shared_data [ \"counter\" ] += 1 # Run with multiple threads threads = [] for _ in range ( 4 ): thread = threading . Thread ( target = increment_counter ) threads . append ( thread ) thread . start () for thread in threads : thread . join () expected_value = 4 * iterations actual_value = shared_data [ \"counter\" ] return { \"expected\" : expected_value , \"actual\" : actual_value , \"race_condition_detected\" : actual_value != expected_value , \"python_version\" : f \" { sys . version_info . major } . { sys . version_info . minor } \" } \ud83d\udcca CI Integration for Python 3.14 \u00b6 Matrix Testing Workflow \u00b6 # .github/workflows/python-matrix.yml name : Python Version Matrix on : [ pull_request , push ] jobs : python-matrix : runs-on : ubuntu-latest strategy : matrix : python-version : [ \"3.11\" , \"3.14\" ] include : - python-version : \"3.11\" python-name : \"Python 3.11 (Current)\" - python-version : \"3.14\" python-name : \"Python 3.14 (Future)\" steps : - uses : actions/checkout@v4 - uses : actions/setup-python@v5 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -e \".[dev]\" - name : Run tests run : pytest -q - name : Run coverage run : pytest --cov=. --cov-report=xml -q - name : Run concurrency tests run : python scripts/concurrency_testing.py - name : Upload results uses : actions/upload-artifact@v4 with : name : python-${{ matrix.python-version }}-results path : | coverage.xml concurrency_results.json Performance Comparison \u00b6 # .github/workflows/performance-comparison.yml name : Performance Comparison on : [ schedule ] jobs : performance-comparison : runs-on : ubuntu-latest steps : - uses : actions/checkout@v4 - name : Test Python 3.11 Performance uses : actions/setup-python@v5 with : python-version : \"3.11\" - name : Run Python 3.11 benchmarks run : | pip install -e \".[dev]\" python scripts/python_version_benchmark.py > py311_results.json - name : Test Python 3.14 Performance uses : actions/setup-python@v5 with : python-version : \"3.14\" - name : Run Python 3.14 benchmarks run : | pip install -e \".[dev]\" python scripts/python_version_benchmark.py > py314_results.json - name : Compare results run : python scripts/compare_performance.py py311_results.json py314_results.json \ud83c\udfaf M1 Deliverables \u00b6 Assessment Tools \u00b6 Python 3.14 compatibility checker Dependency compatibility validator Performance benchmarking tools Concurrency testing framework CI Integration \u00b6 Matrix testing workflow Performance comparison workflow Compatibility reporting Migration readiness dashboard Documentation \u00b6 Python 3.14 migration guide Compatibility assessment report Performance comparison results Migration timeline and risks \ud83d\ude80 Migration Timeline \u00b6 M1: Assessment Phase \u00b6 Week 1-2 : Set up Python 3.14 testing environment Week 3-4 : Run compatibility tests and benchmarks Week 5-6 : Analyze results and identify issues Week 7-8 : Create migration plan and documentation M2+: Migration Phase \u00b6 M2 : Fix compatibility issues M3 : Gradual rollout to development environments M4 : Full migration to Python 3.14 M5 : Optimize for Python 3.14 features \u26a0\ufe0f Risks and Mitigation \u00b6 Risks \u00b6 Dependency Compatibility : Some dependencies may not support Python 3.14 Performance Regression : New features may impact performance Development Fragmentation : Team members on different Python versions Deployment Complexity : Multiple Python versions in production Mitigation Strategies \u00b6 Gradual Migration : Phased approach to minimize risk Comprehensive Testing : Extensive testing across Python versions Documentation : Clear migration guides and troubleshooting Rollback Plan : Ability to revert to Python 3.11 if needed Python 3.14 Integration Strategy : Comprehensive plan for assessing and migrating to Python 3.14 while maintaining development velocity and system stability. Strategy created by DevEx Lead - 2025-10-15","title":"Python 3.14 Integration Strategy - M1 DevEx"},{"location":"developer-guides/python-314-integration-strategy/#python-314-integration-strategy-m1-devex","text":"","title":"Python 3.14 Integration Strategy - M1 DevEx"},{"location":"developer-guides/python-314-integration-strategy/#python-314-integration-overview","text":"Current State : Python 3.11.9 (locked for M0-D6) Target : Python 3.14 compatibility assessment and gradual migration Timeline : M1 assessment, M2+ migration planning","title":"\ud83d\udc0d Python 3.14 Integration Overview"},{"location":"developer-guides/python-314-integration-strategy/#python-314-key-features","text":"","title":"\ud83c\udfaf Python 3.14 Key Features"},{"location":"developer-guides/python-314-integration-strategy/#free-threading-mode-pep-703","text":"No-GIL Build : Optional free-threading mode Performance : Improved concurrency for I/O-bound workloads Compatibility : Existing code works without changes Migration : Gradual adoption possible","title":"Free-Threading Mode (PEP 703)"},{"location":"developer-guides/python-314-integration-strategy/#enhanced-debugger-interface-pep-768","text":"Improved Debugging : Better debugging capabilities Workflow Integration : Enhanced workflow debugging Development Experience : Better developer tools","title":"Enhanced Debugger Interface (PEP 768)"},{"location":"developer-guides/python-314-integration-strategy/#other-improvements","text":"Performance : General performance improvements Type System : Enhanced type checking Standard Library : New features and improvements","title":"Other Improvements"},{"location":"developer-guides/python-314-integration-strategy/#m1-assessment-strategy","text":"","title":"\ud83d\udccb M1 Assessment Strategy"},{"location":"developer-guides/python-314-integration-strategy/#phase-1-compatibility-testing-m1","text":"","title":"Phase 1: Compatibility Testing (M1)"},{"location":"developer-guides/python-314-integration-strategy/#11-dependency-compatibility-check","text":"# Check dependency compatibility with Python 3.14 pip install --dry-run --python-version 3 .14 -r requirements.txt Key Dependencies to Test : - pytest and pytest-cov - ruff and pyright - flask and requests - pydantic and jsonschema - coverage and import-linter","title":"1.1 Dependency Compatibility Check"},{"location":"developer-guides/python-314-integration-strategy/#12-ci-matrix-testing","text":"# .github/workflows/python-compatibility.yml name : Python Compatibility on : [ pull_request , push ] jobs : compatibility : runs-on : ubuntu-latest strategy : matrix : python-version : [ \"3.11\" , \"3.14\" ] steps : - uses : actions/checkout@v4 - uses : actions/setup-python@v5 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -e \".[dev]\" - name : Run tests run : pytest -q - name : Run coverage run : pytest --cov=. --cov-report=xml -q","title":"1.2 CI Matrix Testing"},{"location":"developer-guides/python-314-integration-strategy/#13-tox-environment-setup","text":"# tox.ini [tox] envlist = py311, py314 [testenv] deps = pytest pytest-cov ruff pyright import-linter commands = ruff check . pyright lint-imports pytest --cov = . --cov-report=xml -q","title":"1.3 Tox Environment Setup"},{"location":"developer-guides/python-314-integration-strategy/#phase-2-performance-testing-m1","text":"","title":"Phase 2: Performance Testing (M1)"},{"location":"developer-guides/python-314-integration-strategy/#21-concurrency-testing","text":"# tests/performance/test_concurrency.py import pytest import threading import time from concurrent.futures import ThreadPoolExecutor @pytest . mark . performance def test_plugin_concurrent_execution (): \"\"\"Test plugin execution with multiple threads.\"\"\" def execute_plugin (): # Simulate plugin execution time . sleep ( 0.1 ) return \"success\" with ThreadPoolExecutor ( max_workers = 4 ) as executor : futures = [ executor . submit ( execute_plugin ) for _ in range ( 10 )] results = [ future . result () for future in futures ] assert all ( result == \"success\" for result in results ) @pytest . mark . performance def test_workflow_concurrent_execution (): \"\"\"Test workflow execution with multiple threads.\"\"\" def execute_workflow (): # Simulate workflow execution time . sleep ( 0.2 ) return \"workflow_success\" with ThreadPoolExecutor ( max_workers = 2 ) as executor : futures = [ executor . submit ( execute_workflow ) for _ in range ( 5 )] results = [ future . result () for future in futures ] assert all ( result == \"workflow_success\" for result in results )","title":"2.1 Concurrency Testing"},{"location":"developer-guides/python-314-integration-strategy/#22-performance-benchmarking","text":"# scripts/python_version_benchmark.py import time import sys import threading from concurrent.futures import ThreadPoolExecutor def benchmark_concurrency (): \"\"\"Benchmark concurrency performance.\"\"\" def worker (): time . sleep ( 0.01 ) return threading . get_ident () start_time = time . time () with ThreadPoolExecutor ( max_workers = 10 ) as executor : futures = [ executor . submit ( worker ) for _ in range ( 100 )] results = [ future . result () for future in futures ] execution_time = time . time () - start_time return { \"python_version\" : sys . version , \"execution_time\" : execution_time , \"thread_count\" : len ( set ( results )), \"throughput\" : 100 / execution_time } if __name__ == \"__main__\" : result = benchmark_concurrency () print ( f \"Python { result [ 'python_version' ] } \" ) print ( f \"Execution time: { result [ 'execution_time' ] : .3f } s\" ) print ( f \"Thread count: { result [ 'thread_count' ] } \" ) print ( f \"Throughput: { result [ 'throughput' ] : .1f } ops/sec\" )","title":"2.2 Performance Benchmarking"},{"location":"developer-guides/python-314-integration-strategy/#phase-3-migration-planning-m2","text":"","title":"Phase 3: Migration Planning (M2+)"},{"location":"developer-guides/python-314-integration-strategy/#31-gradual-migration-strategy","text":"# scripts/migration_assessment.py import sys import subprocess import json from pathlib import Path class Python314MigrationAssessment : \"\"\"Assess Python 3.14 migration readiness.\"\"\" def __init__ ( self ): self . current_version = sys . version_info self . target_version = ( 3 , 14 , 0 ) def check_dependency_compatibility ( self ): \"\"\"Check dependency compatibility with Python 3.14.\"\"\" dependencies = [ \"pytest\" , \"pytest-cov\" , \"ruff\" , \"pyright\" , \"import-linter\" , \"flask\" , \"requests\" , \"pydantic\" , \"jsonschema\" , \"coverage\" ] compatibility_report = {} for dep in dependencies : try : # Check if dependency supports Python 3.14 result = subprocess . run ([ \"pip\" , \"install\" , \"--dry-run\" , f \"--python-version=3.14\" , dep ], capture_output = True , text = True ) compatibility_report [ dep ] = { \"compatible\" : result . returncode == 0 , \"error\" : result . stderr if result . returncode != 0 else None } except Exception as e : compatibility_report [ dep ] = { \"compatible\" : False , \"error\" : str ( e ) } return compatibility_report def assess_code_compatibility ( self ): \"\"\"Assess code compatibility with Python 3.14.\"\"\" # Check for deprecated features # Check for syntax compatibility # Check for standard library changes return { \"syntax_compatible\" : True , \"deprecated_features\" : [], \"standard_library_changes\" : [] } def generate_migration_plan ( self ): \"\"\"Generate migration plan.\"\"\" dependency_report = self . check_dependency_compatibility () code_report = self . assess_code_compatibility () migration_plan = { \"current_version\" : f \" { self . current_version . major } . { self . current_version . minor } . { self . current_version . micro } \" , \"target_version\" : f \" { self . target_version [ 0 ] } . { self . target_version [ 1 ] } . { self . target_version [ 2 ] } \" , \"dependency_compatibility\" : dependency_report , \"code_compatibility\" : code_report , \"migration_steps\" : [ \"1. Update CI matrix to include Python 3.14\" , \"2. Fix any dependency compatibility issues\" , \"3. Update code for Python 3.14 compatibility\" , \"4. Run comprehensive tests on Python 3.14\" , \"5. Update documentation and deployment scripts\" , \"6. Gradual rollout to development environments\" ], \"risks\" : [ \"Dependency compatibility issues\" , \"Performance regressions\" , \"Development environment fragmentation\" , \"Deployment complexity\" ] } return migration_plan if __name__ == \"__main__\" : assessment = Python314MigrationAssessment () plan = assessment . generate_migration_plan () # Save migration plan with open ( \"python_314_migration_plan.json\" , \"w\" ) as f : json . dump ( plan , f , indent = 2 ) print ( \"Python 3.14 Migration Assessment Complete\" ) print ( f \"Migration plan saved to: python_314_migration_plan.json\" )","title":"3.1 Gradual Migration Strategy"},{"location":"developer-guides/python-314-integration-strategy/#development-tools-for-python-314","text":"","title":"\ud83d\udee0\ufe0f Development Tools for Python 3.14"},{"location":"developer-guides/python-314-integration-strategy/#enhanced-debugging-tools","text":"# scripts/enhanced_debugger.py import sys import threading from typing import Any , Dict class WorkflowDebugger : \"\"\"Enhanced debugger for workflow execution.\"\"\" def __init__ ( self ): self . breakpoints = {} self . watch_variables = {} self . thread_info = {} def set_breakpoint ( self , workflow_id : str , step_name : str ): \"\"\"Set breakpoint in workflow execution.\"\"\" self . breakpoints [ f \" { workflow_id } : { step_name } \" ] = True def watch_variable ( self , name : str , value : Any ): \"\"\"Watch variable changes.\"\"\" self . watch_variables [ name ] = value def get_thread_info ( self ) -> Dict [ str , Any ]: \"\"\"Get thread information for debugging.\"\"\" if sys . version_info >= ( 3 , 14 ): # Use enhanced debugging features return { \"thread_count\" : threading . active_count (), \"main_thread\" : threading . main_thread () . name , \"current_thread\" : threading . current_thread () . name , \"thread_ident\" : threading . get_ident () } else : return { \"thread_count\" : threading . active_count (), \"current_thread\" : threading . current_thread () . name } def debug_workflow_execution ( self , workflow_id : str , step_name : str , context : Dict [ str , Any ]): \"\"\"Debug workflow execution step.\"\"\" print ( f \"\ud83d\udc1b Debugging { workflow_id } : { step_name } \" ) print ( f \"\ud83d\udcca Context: { context } \" ) print ( f \"\ud83e\uddf5 Thread info: { self . get_thread_info () } \" ) # Check for breakpoints breakpoint_key = f \" { workflow_id } : { step_name } \" if breakpoint_key in self . breakpoints : print ( f \"\u23f8\ufe0f Breakpoint hit: { breakpoint_key } \" ) # In Python 3.14, we could use enhanced debugging features here # Check watched variables for name , expected_value in self . watch_variables . items (): if name in context : actual_value = context [ name ] if actual_value != expected_value : print ( f \"\ud83d\udc40 Variable changed: { name } = { actual_value } (expected: { expected_value } )\" )","title":"Enhanced Debugging Tools"},{"location":"developer-guides/python-314-integration-strategy/#concurrency-testing-tools","text":"# scripts/concurrency_testing.py import threading import time import concurrent.futures from typing import List , Callable , Any class ConcurrencyTester : \"\"\"Test concurrency behavior across Python versions.\"\"\" def __init__ ( self ): self . results = [] def test_thread_safety ( self , func : Callable , args_list : List [ tuple ], max_workers : int = 4 ): \"\"\"Test thread safety of a function.\"\"\" start_time = time . time () with concurrent . futures . ThreadPoolExecutor ( max_workers = max_workers ) as executor : futures = [ executor . submit ( func , * args ) for args in args_list ] results = [ future . result () for future in futures ] execution_time = time . time () - start_time return { \"execution_time\" : execution_time , \"results\" : results , \"success_count\" : sum ( 1 for r in results if r is not None ), \"thread_count\" : threading . active_count () } def test_race_conditions ( self , func : Callable , iterations : int = 100 ): \"\"\"Test for race conditions.\"\"\" shared_data = { \"counter\" : 0 } def increment_counter (): for _ in range ( iterations ): shared_data [ \"counter\" ] += 1 # Run with multiple threads threads = [] for _ in range ( 4 ): thread = threading . Thread ( target = increment_counter ) threads . append ( thread ) thread . start () for thread in threads : thread . join () expected_value = 4 * iterations actual_value = shared_data [ \"counter\" ] return { \"expected\" : expected_value , \"actual\" : actual_value , \"race_condition_detected\" : actual_value != expected_value , \"python_version\" : f \" { sys . version_info . major } . { sys . version_info . minor } \" }","title":"Concurrency Testing Tools"},{"location":"developer-guides/python-314-integration-strategy/#ci-integration-for-python-314","text":"","title":"\ud83d\udcca CI Integration for Python 3.14"},{"location":"developer-guides/python-314-integration-strategy/#matrix-testing-workflow","text":"# .github/workflows/python-matrix.yml name : Python Version Matrix on : [ pull_request , push ] jobs : python-matrix : runs-on : ubuntu-latest strategy : matrix : python-version : [ \"3.11\" , \"3.14\" ] include : - python-version : \"3.11\" python-name : \"Python 3.11 (Current)\" - python-version : \"3.14\" python-name : \"Python 3.14 (Future)\" steps : - uses : actions/checkout@v4 - uses : actions/setup-python@v5 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -e \".[dev]\" - name : Run tests run : pytest -q - name : Run coverage run : pytest --cov=. --cov-report=xml -q - name : Run concurrency tests run : python scripts/concurrency_testing.py - name : Upload results uses : actions/upload-artifact@v4 with : name : python-${{ matrix.python-version }}-results path : | coverage.xml concurrency_results.json","title":"Matrix Testing Workflow"},{"location":"developer-guides/python-314-integration-strategy/#performance-comparison","text":"# .github/workflows/performance-comparison.yml name : Performance Comparison on : [ schedule ] jobs : performance-comparison : runs-on : ubuntu-latest steps : - uses : actions/checkout@v4 - name : Test Python 3.11 Performance uses : actions/setup-python@v5 with : python-version : \"3.11\" - name : Run Python 3.11 benchmarks run : | pip install -e \".[dev]\" python scripts/python_version_benchmark.py > py311_results.json - name : Test Python 3.14 Performance uses : actions/setup-python@v5 with : python-version : \"3.14\" - name : Run Python 3.14 benchmarks run : | pip install -e \".[dev]\" python scripts/python_version_benchmark.py > py314_results.json - name : Compare results run : python scripts/compare_performance.py py311_results.json py314_results.json","title":"Performance Comparison"},{"location":"developer-guides/python-314-integration-strategy/#m1-deliverables","text":"","title":"\ud83c\udfaf M1 Deliverables"},{"location":"developer-guides/python-314-integration-strategy/#assessment-tools","text":"Python 3.14 compatibility checker Dependency compatibility validator Performance benchmarking tools Concurrency testing framework","title":"Assessment Tools"},{"location":"developer-guides/python-314-integration-strategy/#ci-integration","text":"Matrix testing workflow Performance comparison workflow Compatibility reporting Migration readiness dashboard","title":"CI Integration"},{"location":"developer-guides/python-314-integration-strategy/#documentation","text":"Python 3.14 migration guide Compatibility assessment report Performance comparison results Migration timeline and risks","title":"Documentation"},{"location":"developer-guides/python-314-integration-strategy/#migration-timeline","text":"","title":"\ud83d\ude80 Migration Timeline"},{"location":"developer-guides/python-314-integration-strategy/#m1-assessment-phase","text":"Week 1-2 : Set up Python 3.14 testing environment Week 3-4 : Run compatibility tests and benchmarks Week 5-6 : Analyze results and identify issues Week 7-8 : Create migration plan and documentation","title":"M1: Assessment Phase"},{"location":"developer-guides/python-314-integration-strategy/#m2-migration-phase","text":"M2 : Fix compatibility issues M3 : Gradual rollout to development environments M4 : Full migration to Python 3.14 M5 : Optimize for Python 3.14 features","title":"M2+: Migration Phase"},{"location":"developer-guides/python-314-integration-strategy/#risks-and-mitigation","text":"","title":"\u26a0\ufe0f Risks and Mitigation"},{"location":"developer-guides/python-314-integration-strategy/#risks","text":"Dependency Compatibility : Some dependencies may not support Python 3.14 Performance Regression : New features may impact performance Development Fragmentation : Team members on different Python versions Deployment Complexity : Multiple Python versions in production","title":"Risks"},{"location":"developer-guides/python-314-integration-strategy/#mitigation-strategies","text":"Gradual Migration : Phased approach to minimize risk Comprehensive Testing : Extensive testing across Python versions Documentation : Clear migration guides and troubleshooting Rollback Plan : Ability to revert to Python 3.11 if needed Python 3.14 Integration Strategy : Comprehensive plan for assessing and migrating to Python 3.14 while maintaining development velocity and system stability. Strategy created by DevEx Lead - 2025-10-15","title":"Mitigation Strategies"},{"location":"diagrams/mermaid-style/","text":"Mermaid Diagram Style Guide \u00b6 \ud83c\udfa8 House Style \u00b6 Use this initialization block at the start of every Mermaid diagram for consistent styling: %%{init: { \"theme\": \"neutral\", \"themeVariables\": { \"fontFamily\": \"Inter, ui-sans-serif, system-ui\", \"primaryColor\": \"#0ea5e9\", \"primaryBorderColor\": \"#0ea5e9\", \"primaryTextColor\": \"#0b1220\", \"lineColor\": \"#6b7280\", \"secondaryColor\": \"#f1f5f9\", \"tertiaryColor\": \"#e2e8f0\" } }}%% \ud83d\udccb Usage Guidelines \u00b6 Diagram Types \u00b6 Flowcharts : Use flowchart TD (top-down) for processes, flowchart LR (left-right) for pipelines Sequence Diagrams : Use sequenceDiagram for runtime interactions Class Diagrams : Use classDiagram for data models and relationships State Diagrams : Use stateDiagram-v2 for lifecycle flows Styling Rules \u00b6 Colors : Minimal use; rely on Material theme defaults IDs : Use semantic names (e.g., WebAPI , Worker , findings-engine ) Labels : Keep concise but descriptive Direction : Prefer top-down for hierarchical flows, left-right for sequential processes Custom Classes \u00b6 Define custom styling classes sparingly: classDef box stroke:#0ea5e9,stroke-width:2px,fill:#f8fafc,color:#0b1220; classDef tool fill:#eef,stroke:#88f,stroke-width:1px; classDef role fill:#e2e8f0,stroke:#0ea5e9,stroke-width:2px; \ud83d\udd27 Integration \u00b6 Each diagram should include the init block at the top since Mermaid doesn't support external imports. Copy the init block from this file to maintain consistency across all diagrams.","title":"Mermaid Diagram Style Guide"},{"location":"diagrams/mermaid-style/#mermaid-diagram-style-guide","text":"","title":"Mermaid Diagram Style Guide"},{"location":"diagrams/mermaid-style/#house-style","text":"Use this initialization block at the start of every Mermaid diagram for consistent styling: %%{init: { \"theme\": \"neutral\", \"themeVariables\": { \"fontFamily\": \"Inter, ui-sans-serif, system-ui\", \"primaryColor\": \"#0ea5e9\", \"primaryBorderColor\": \"#0ea5e9\", \"primaryTextColor\": \"#0b1220\", \"lineColor\": \"#6b7280\", \"secondaryColor\": \"#f1f5f9\", \"tertiaryColor\": \"#e2e8f0\" } }}%%","title":"\ud83c\udfa8 House Style"},{"location":"diagrams/mermaid-style/#usage-guidelines","text":"","title":"\ud83d\udccb Usage Guidelines"},{"location":"diagrams/mermaid-style/#diagram-types","text":"Flowcharts : Use flowchart TD (top-down) for processes, flowchart LR (left-right) for pipelines Sequence Diagrams : Use sequenceDiagram for runtime interactions Class Diagrams : Use classDiagram for data models and relationships State Diagrams : Use stateDiagram-v2 for lifecycle flows","title":"Diagram Types"},{"location":"diagrams/mermaid-style/#styling-rules","text":"Colors : Minimal use; rely on Material theme defaults IDs : Use semantic names (e.g., WebAPI , Worker , findings-engine ) Labels : Keep concise but descriptive Direction : Prefer top-down for hierarchical flows, left-right for sequential processes","title":"Styling Rules"},{"location":"diagrams/mermaid-style/#custom-classes","text":"Define custom styling classes sparingly: classDef box stroke:#0ea5e9,stroke-width:2px,fill:#f8fafc,color:#0b1220; classDef tool fill:#eef,stroke:#88f,stroke-width:1px; classDef role fill:#e2e8f0,stroke:#0ea5e9,stroke-width:2px;","title":"Custom Classes"},{"location":"diagrams/mermaid-style/#integration","text":"Each diagram should include the init block at the top since Mermaid doesn't support external imports. Copy the init block from this file to maintain consistency across all diagrams.","title":"\ud83d\udd27 Integration"},{"location":"governance/development-conventions/","text":"Development Conventions \u00b6 Definition of Done (DoD) Checklist \u00b6 Code Quality \u00b6 All tests pass ( make test ) Code coverage meets current milestone threshold (see Coverage Ratchet below) No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes Documentation \u00b6 Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check) Testing \u00b6 Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold Security & Compliance \u00b6 No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed Review Process \u00b6 PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments SOD/EOD Rituals (Coordinator/DevEx Automation) \u00b6 Start of Day (SOD) - Automated by Coordinator \u00b6 CI Status Check : Automated scan of overnight CI results Coverage Monitoring : Automated coverage trend analysis Dependency Alerts : Automated security vulnerability scanning Build Health : Automated build failure notifications Sprint Progress : Automated sprint burn-down updates End of Day (EOD) - DevEx Automation \u00b6 Health Gates : Automated make health execution Coverage Ratchet : Automated coverage threshold validation Documentation Sync : Automated docs build and link validation Metrics Collection : Automated performance and quality metrics Report Generation : Automated EOD summary creation Report Locations \u00b6 Daily Reports : reports/daily/YYYY-MM-DD.md (auto-generated) Sprint Reports : reports/sprints/sprint-N.md (auto-updated) Milestone Reports : reports/milestones/M{N}.md (auto-compiled) Health Reports : reports/health/YYYY-MM-DD-health.md (auto-generated) Branch Naming & PR Rules \u00b6 Branch Naming Convention \u00b6 Features : feat/description-of-feature Bug Fixes : fix/description-of-bug Documentation : docs/description-of-changes Refactoring : refactor/description-of-refactor Hotfixes : hotfix/description-of-fix Security : security/description-of-security-fix PR Size Rule \u00b6 Standard : \u2264400 LOC per PR Large PRs : >400 LOC requires 2 approvals minimum Emergency : Hotfixes may exceed limits with security team approval Documentation : Docs-only PRs have relaxed limits (\u22641000 LOC) Security Review Rule \u00b6 Mandatory Security Review : All PRs touching security-sensitive code Security Team : @security-lead must approve security-related changes Automated Scanning : All PRs scanned for secrets and vulnerabilities Dependency Updates : Security updates get priority review queue CI Order (Fast-Fail) \u00b6 The CI pipeline runs in strict fast-fail order: Phase 1: Code Quality (Fast-Fail) \u00b6 Ruff Linting - Python code style and formatting Pyright Type Checking - Static type analysis Import Linter - Import organization and unused imports Phase 2: Testing (Fast-Fail) \u00b6 Unit Tests - Core functionality testing Coverage Measurement - Code coverage analysis Coverage Ratchet - Threshold enforcement (fail if drop >2%) Phase 3: Integration (Fast-Fail) \u00b6 Contract Tests - API contract validation Integration Tests - Component interaction testing Phase 4: Documentation (Fast-Fail) \u00b6 Docs Health - make health execution Mermaid Parity - Diagram rendering validation ASCII Blocker - ASCII diagram detection Phase 5: End-to-End (Fast-Fail) \u00b6 E2E Tests - Full system testing Performance Tests - Regression detection Coverage Ratchet Ladder \u00b6 Milestone Thresholds \u00b6 M0 : 18% minimum coverage (baseline) M1 : 80% minimum coverage M2 : 82% minimum coverage M3 : 84% minimum coverage M4 : 86% minimum coverage M5 : 88% minimum coverage M6 : 90% minimum coverage Failure Conditions \u00b6 Drop >2% from previous milestone Fall below current milestone threshold New code without corresponding tests Critical paths without test coverage Development Workflow \u00b6 Daily Development Loop \u00b6 Pull Latest : git pull origin main Run Tests : make test (full suite) Quick Iteration : make quick-test (fast feedback) Health Check : make health (before commit) Commit & Push : Follow commit standards Create PR : Include validation evidence Pre-Commit Checklist \u00b6 All tests pass locally Code coverage maintained Linting passes Documentation updated Security review completed (if applicable) PR Creation Process \u00b6 Branch from main : git checkout -b feat/description Implement changes : Follow DoD checklist Run validation : make test && make health Create PR : Include context and evidence Request reviews : Assign appropriate reviewers Monitor CI : Ensure all checks pass Tool Integration \u00b6 Development Tools \u00b6 Poetry : Dependency management Ruff : Code formatting and linting Pyright : Static type checking Pytest : Testing framework MkDocs : Documentation generation CI/CD Tools \u00b6 GitHub Actions : CI pipeline execution Coverage.py : Coverage measurement Mermaid : Diagram rendering Link Checker : Documentation validation Quality Gates \u00b6 Code Quality Gates \u00b6 Linting : Zero ruff/pyright errors Type Safety : Full type coverage for public APIs Import Hygiene : No unused imports Style Consistency : Automated formatting applied Testing Gates \u00b6 Unit Coverage : Minimum threshold per milestone Integration Coverage : Critical path coverage Contract Validation : API compatibility Performance Regression : No performance degradation Documentation Gates \u00b6 Build Success : MkDocs builds without errors Link Validation : No broken internal links Mermaid Parity : Diagrams render correctly ASCII Blocker : No ASCII diagrams allowed Emergency Procedures \u00b6 Hotfix Process \u00b6 Create hotfix branch : hotfix/critical-issue Implement minimal fix : Focus on stability Security review : Mandatory for security issues Fast-track CI : Expedited review process Deploy immediately : After approval Rollback Process \u00b6 Identify issue : Automated monitoring alerts Create rollback PR : Revert problematic changes Emergency review : Security team approval Deploy rollback : Immediate deployment Post-mortem : Document lessons learned Team Coordination \u00b6 Daily Standups \u00b6 SOD Reports : Automated CI status Blockers : Manual escalation process Progress : Sprint burn-down updates Dependencies : Cross-team coordination Weekly Reviews \u00b6 Coverage Trends : Automated analysis Performance Metrics : Automated collection Security Updates : Automated scanning Process Improvements : Team feedback integration Monthly Retrospectives \u00b6 Process Effectiveness : DoD checklist review Tool Updates : CI/CD pipeline improvements Training Needs : Skill gap identification Process Refinement : Continuous improvement","title":"Development Conventions"},{"location":"governance/development-conventions/#development-conventions","text":"","title":"Development Conventions"},{"location":"governance/development-conventions/#definition-of-done-dod-checklist","text":"","title":"Definition of Done (DoD) Checklist"},{"location":"governance/development-conventions/#code-quality","text":"All tests pass ( make test ) Code coverage meets current milestone threshold (see Coverage Ratchet below) No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes","title":"Code Quality"},{"location":"governance/development-conventions/#documentation","text":"Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check)","title":"Documentation"},{"location":"governance/development-conventions/#testing","text":"Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold","title":"Testing"},{"location":"governance/development-conventions/#security-compliance","text":"No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed","title":"Security &amp; Compliance"},{"location":"governance/development-conventions/#review-process","text":"PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments","title":"Review Process"},{"location":"governance/development-conventions/#sodeod-rituals-coordinatordevex-automation","text":"","title":"SOD/EOD Rituals (Coordinator/DevEx Automation)"},{"location":"governance/development-conventions/#start-of-day-sod-automated-by-coordinator","text":"CI Status Check : Automated scan of overnight CI results Coverage Monitoring : Automated coverage trend analysis Dependency Alerts : Automated security vulnerability scanning Build Health : Automated build failure notifications Sprint Progress : Automated sprint burn-down updates","title":"Start of Day (SOD) - Automated by Coordinator"},{"location":"governance/development-conventions/#end-of-day-eod-devex-automation","text":"Health Gates : Automated make health execution Coverage Ratchet : Automated coverage threshold validation Documentation Sync : Automated docs build and link validation Metrics Collection : Automated performance and quality metrics Report Generation : Automated EOD summary creation","title":"End of Day (EOD) - DevEx Automation"},{"location":"governance/development-conventions/#report-locations","text":"Daily Reports : reports/daily/YYYY-MM-DD.md (auto-generated) Sprint Reports : reports/sprints/sprint-N.md (auto-updated) Milestone Reports : reports/milestones/M{N}.md (auto-compiled) Health Reports : reports/health/YYYY-MM-DD-health.md (auto-generated)","title":"Report Locations"},{"location":"governance/development-conventions/#branch-naming-pr-rules","text":"","title":"Branch Naming &amp; PR Rules"},{"location":"governance/development-conventions/#branch-naming-convention","text":"Features : feat/description-of-feature Bug Fixes : fix/description-of-bug Documentation : docs/description-of-changes Refactoring : refactor/description-of-refactor Hotfixes : hotfix/description-of-fix Security : security/description-of-security-fix","title":"Branch Naming Convention"},{"location":"governance/development-conventions/#pr-size-rule","text":"Standard : \u2264400 LOC per PR Large PRs : >400 LOC requires 2 approvals minimum Emergency : Hotfixes may exceed limits with security team approval Documentation : Docs-only PRs have relaxed limits (\u22641000 LOC)","title":"PR Size Rule"},{"location":"governance/development-conventions/#security-review-rule","text":"Mandatory Security Review : All PRs touching security-sensitive code Security Team : @security-lead must approve security-related changes Automated Scanning : All PRs scanned for secrets and vulnerabilities Dependency Updates : Security updates get priority review queue","title":"Security Review Rule"},{"location":"governance/development-conventions/#ci-order-fast-fail","text":"The CI pipeline runs in strict fast-fail order:","title":"CI Order (Fast-Fail)"},{"location":"governance/development-conventions/#phase-1-code-quality-fast-fail","text":"Ruff Linting - Python code style and formatting Pyright Type Checking - Static type analysis Import Linter - Import organization and unused imports","title":"Phase 1: Code Quality (Fast-Fail)"},{"location":"governance/development-conventions/#phase-2-testing-fast-fail","text":"Unit Tests - Core functionality testing Coverage Measurement - Code coverage analysis Coverage Ratchet - Threshold enforcement (fail if drop >2%)","title":"Phase 2: Testing (Fast-Fail)"},{"location":"governance/development-conventions/#phase-3-integration-fast-fail","text":"Contract Tests - API contract validation Integration Tests - Component interaction testing","title":"Phase 3: Integration (Fast-Fail)"},{"location":"governance/development-conventions/#phase-4-documentation-fast-fail","text":"Docs Health - make health execution Mermaid Parity - Diagram rendering validation ASCII Blocker - ASCII diagram detection","title":"Phase 4: Documentation (Fast-Fail)"},{"location":"governance/development-conventions/#phase-5-end-to-end-fast-fail","text":"E2E Tests - Full system testing Performance Tests - Regression detection","title":"Phase 5: End-to-End (Fast-Fail)"},{"location":"governance/development-conventions/#coverage-ratchet-ladder","text":"","title":"Coverage Ratchet Ladder"},{"location":"governance/development-conventions/#milestone-thresholds","text":"M0 : 18% minimum coverage (baseline) M1 : 80% minimum coverage M2 : 82% minimum coverage M3 : 84% minimum coverage M4 : 86% minimum coverage M5 : 88% minimum coverage M6 : 90% minimum coverage","title":"Milestone Thresholds"},{"location":"governance/development-conventions/#failure-conditions","text":"Drop >2% from previous milestone Fall below current milestone threshold New code without corresponding tests Critical paths without test coverage","title":"Failure Conditions"},{"location":"governance/development-conventions/#development-workflow","text":"","title":"Development Workflow"},{"location":"governance/development-conventions/#daily-development-loop","text":"Pull Latest : git pull origin main Run Tests : make test (full suite) Quick Iteration : make quick-test (fast feedback) Health Check : make health (before commit) Commit & Push : Follow commit standards Create PR : Include validation evidence","title":"Daily Development Loop"},{"location":"governance/development-conventions/#pre-commit-checklist","text":"All tests pass locally Code coverage maintained Linting passes Documentation updated Security review completed (if applicable)","title":"Pre-Commit Checklist"},{"location":"governance/development-conventions/#pr-creation-process","text":"Branch from main : git checkout -b feat/description Implement changes : Follow DoD checklist Run validation : make test && make health Create PR : Include context and evidence Request reviews : Assign appropriate reviewers Monitor CI : Ensure all checks pass","title":"PR Creation Process"},{"location":"governance/development-conventions/#tool-integration","text":"","title":"Tool Integration"},{"location":"governance/development-conventions/#development-tools","text":"Poetry : Dependency management Ruff : Code formatting and linting Pyright : Static type checking Pytest : Testing framework MkDocs : Documentation generation","title":"Development Tools"},{"location":"governance/development-conventions/#cicd-tools","text":"GitHub Actions : CI pipeline execution Coverage.py : Coverage measurement Mermaid : Diagram rendering Link Checker : Documentation validation","title":"CI/CD Tools"},{"location":"governance/development-conventions/#quality-gates","text":"","title":"Quality Gates"},{"location":"governance/development-conventions/#code-quality-gates","text":"Linting : Zero ruff/pyright errors Type Safety : Full type coverage for public APIs Import Hygiene : No unused imports Style Consistency : Automated formatting applied","title":"Code Quality Gates"},{"location":"governance/development-conventions/#testing-gates","text":"Unit Coverage : Minimum threshold per milestone Integration Coverage : Critical path coverage Contract Validation : API compatibility Performance Regression : No performance degradation","title":"Testing Gates"},{"location":"governance/development-conventions/#documentation-gates","text":"Build Success : MkDocs builds without errors Link Validation : No broken internal links Mermaid Parity : Diagrams render correctly ASCII Blocker : No ASCII diagrams allowed","title":"Documentation Gates"},{"location":"governance/development-conventions/#emergency-procedures","text":"","title":"Emergency Procedures"},{"location":"governance/development-conventions/#hotfix-process","text":"Create hotfix branch : hotfix/critical-issue Implement minimal fix : Focus on stability Security review : Mandatory for security issues Fast-track CI : Expedited review process Deploy immediately : After approval","title":"Hotfix Process"},{"location":"governance/development-conventions/#rollback-process","text":"Identify issue : Automated monitoring alerts Create rollback PR : Revert problematic changes Emergency review : Security team approval Deploy rollback : Immediate deployment Post-mortem : Document lessons learned","title":"Rollback Process"},{"location":"governance/development-conventions/#team-coordination","text":"","title":"Team Coordination"},{"location":"governance/development-conventions/#daily-standups","text":"SOD Reports : Automated CI status Blockers : Manual escalation process Progress : Sprint burn-down updates Dependencies : Cross-team coordination","title":"Daily Standups"},{"location":"governance/development-conventions/#weekly-reviews","text":"Coverage Trends : Automated analysis Performance Metrics : Automated collection Security Updates : Automated scanning Process Improvements : Team feedback integration","title":"Weekly Reviews"},{"location":"governance/development-conventions/#monthly-retrospectives","text":"Process Effectiveness : DoD checklist review Tool Updates : CI/CD pipeline improvements Training Needs : Skill gap identification Process Refinement : Continuous improvement","title":"Monthly Retrospectives"},{"location":"governance/engineering-standards/","text":"Engineering Standards & Governance \u00b6 Definition of Done (DoD) Checklist \u00b6 Code Quality \u00b6 All tests pass ( make test ) Code coverage meets current milestone threshold (see Coverage Ratchet below) No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes Documentation \u00b6 Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check) Testing \u00b6 Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold Security & Compliance \u00b6 No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed Review Process \u00b6 PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments Branching & Commit Standards \u00b6 Branch Naming \u00b6 Feature branches : feat/description-of-feature Bug fixes : fix/description-of-bug Documentation : docs/description-of-changes Refactoring : refactor/description-of-refactor Hotfixes : hotfix/description-of-fix Commit Standards \u00b6 Format : type(scope): description Types : feat , fix , docs , style , refactor , test , chore Squash : All commits squashed before merge Rebase : Rebase on main before creating PR Protected main : Direct pushes to main are disabled Pull Request Rules \u00b6 Size limit : \u2264400 LOC or requires 2 approvals Description : Must include context, testing instructions, and validation evidence Reviewers : Minimum 1 approval, 2 for large PRs CI : All checks must pass before merge CI Pipeline Order & Gates \u00b6 The CI pipeline runs in strict order with gates that must pass: Code Quality Gates ruff - Python linting and formatting pyright - Static type checking import-linter - Import organization and unused import detection Testing Gates Unit tests + coverage measurement Coverage ratchet enforcement (see below) Contract tests Integration tests Documentation Gates Docs health check ( make health ) Mermaid parity validation ASCII blocker check (no ASCII diagrams) Internal link validation End-to-End Gates E2E test suite Performance regression checks Coverage Ratchet Ladder \u00b6 Coverage thresholds increase by milestone with strict enforcement: M0 : 18% minimum coverage (baseline) M1 : 80% minimum coverage M2 : 82% minimum coverage M3 : 84% minimum coverage M4 : 86% minimum coverage M5 : 88% minimum coverage M6 : 90% minimum coverage Failure conditions : - Drop >2% from previous milestone - Fall below current milestone threshold - New code without tests Documentation Health Gates \u00b6 Mermaid Parity Check \u00b6 All diagrams must render identically in both Mermaid and ASCII ASCII diagrams are blocked (use make health to validate) Local Mermaid JS files only (no CDN dependencies) Superfences Configuration \u00b6 Use pymdownx.superfences for code blocks Mermaid diagrams use mermaid fence type Local JS files: js/mermaid.min.js and js/mermaid-init.js Fence Hygiene Tips \u00b6 Always specify language for code blocks Use proper fence types for diagrams Test rendering with mkdocs serve Validate with make health before commit SOD/EOD Rituals \u00b6 Start of Day (SOD) \u00b6 Review overnight CI results Check for any failed builds or coverage drops Update project status if needed Plan daily tasks based on current sprint goals End of Day (EOD) \u00b6 Run make health to validate docs Create EOD summary report Update project metrics and status Ensure all PRs have proper validation evidence Report Locations \u00b6 Daily reports : reports/daily/YYYY-MM-DD.md Sprint reports : reports/sprints/sprint-N.md Milestone reports : reports/milestones/M{N}.md ADR-Lite Process \u00b6 When ADR-Lite is Required \u00b6 Architecture decisions affecting multiple components Changes to CI/CD pipeline or tooling New dependencies or technology choices Security or compliance policy changes Breaking changes to APIs or data models ADR-Lite Template \u00b6 Reference: docs/adr/0000-adr-template.md Required sections : - Status (Proposed/Accepted/Deprecated/Superseded) - Context and Problem Statement - Decision Drivers - Considered Options - Decision Outcome - Consequences \"Read \u2192 Run \u2192 Update\" Loop \u00b6 Read Phase \u00b6 Read relevant documentation Understand current architecture Review existing patterns and conventions Check for related issues or PRs Run Phase \u00b6 Set up local development environment Run tests to understand current state Experiment with changes locally Validate with make health and make test Update Phase \u00b6 Implement changes following DoD checklist Update documentation as needed Add tests for new functionality Submit PR with validation evidence Adoption & Integration \u00b6 PR Template Integration \u00b6 The DoD checklist should be referenced in all PR templates. Add this to your PR template: ## DoD Checklist Please review and complete the [ Engineering Standards DoD checklist ]( governance/engineering-standards.md#definition-of-done-dod-checklist ). ## Validation Evidence Paste output from: - `make test` - `make health` - Coverage report Team Onboarding \u00b6 New team members must read this page before first PR Link from \"Developer Start Here\" page Include in team onboarding checklist Review quarterly for updates Continuous Improvement \u00b6 Monthly review of DoD effectiveness Quarterly updates to standards based on team feedback Annual review of CI pipeline and coverage thresholds Regular updates to tooling and best practices","title":"Engineering Standards"},{"location":"governance/engineering-standards/#engineering-standards-governance","text":"","title":"Engineering Standards &amp; Governance"},{"location":"governance/engineering-standards/#definition-of-done-dod-checklist","text":"","title":"Definition of Done (DoD) Checklist"},{"location":"governance/engineering-standards/#code-quality","text":"All tests pass ( make test ) Code coverage meets current milestone threshold (see Coverage Ratchet below) No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes","title":"Code Quality"},{"location":"governance/engineering-standards/#documentation","text":"Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check)","title":"Documentation"},{"location":"governance/engineering-standards/#testing","text":"Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold","title":"Testing"},{"location":"governance/engineering-standards/#security-compliance","text":"No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed","title":"Security &amp; Compliance"},{"location":"governance/engineering-standards/#review-process","text":"PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments","title":"Review Process"},{"location":"governance/engineering-standards/#branching-commit-standards","text":"","title":"Branching &amp; Commit Standards"},{"location":"governance/engineering-standards/#branch-naming","text":"Feature branches : feat/description-of-feature Bug fixes : fix/description-of-bug Documentation : docs/description-of-changes Refactoring : refactor/description-of-refactor Hotfixes : hotfix/description-of-fix","title":"Branch Naming"},{"location":"governance/engineering-standards/#commit-standards","text":"Format : type(scope): description Types : feat , fix , docs , style , refactor , test , chore Squash : All commits squashed before merge Rebase : Rebase on main before creating PR Protected main : Direct pushes to main are disabled","title":"Commit Standards"},{"location":"governance/engineering-standards/#pull-request-rules","text":"Size limit : \u2264400 LOC or requires 2 approvals Description : Must include context, testing instructions, and validation evidence Reviewers : Minimum 1 approval, 2 for large PRs CI : All checks must pass before merge","title":"Pull Request Rules"},{"location":"governance/engineering-standards/#ci-pipeline-order-gates","text":"The CI pipeline runs in strict order with gates that must pass: Code Quality Gates ruff - Python linting and formatting pyright - Static type checking import-linter - Import organization and unused import detection Testing Gates Unit tests + coverage measurement Coverage ratchet enforcement (see below) Contract tests Integration tests Documentation Gates Docs health check ( make health ) Mermaid parity validation ASCII blocker check (no ASCII diagrams) Internal link validation End-to-End Gates E2E test suite Performance regression checks","title":"CI Pipeline Order &amp; Gates"},{"location":"governance/engineering-standards/#coverage-ratchet-ladder","text":"Coverage thresholds increase by milestone with strict enforcement: M0 : 18% minimum coverage (baseline) M1 : 80% minimum coverage M2 : 82% minimum coverage M3 : 84% minimum coverage M4 : 86% minimum coverage M5 : 88% minimum coverage M6 : 90% minimum coverage Failure conditions : - Drop >2% from previous milestone - Fall below current milestone threshold - New code without tests","title":"Coverage Ratchet Ladder"},{"location":"governance/engineering-standards/#documentation-health-gates","text":"","title":"Documentation Health Gates"},{"location":"governance/engineering-standards/#mermaid-parity-check","text":"All diagrams must render identically in both Mermaid and ASCII ASCII diagrams are blocked (use make health to validate) Local Mermaid JS files only (no CDN dependencies)","title":"Mermaid Parity Check"},{"location":"governance/engineering-standards/#superfences-configuration","text":"Use pymdownx.superfences for code blocks Mermaid diagrams use mermaid fence type Local JS files: js/mermaid.min.js and js/mermaid-init.js","title":"Superfences Configuration"},{"location":"governance/engineering-standards/#fence-hygiene-tips","text":"Always specify language for code blocks Use proper fence types for diagrams Test rendering with mkdocs serve Validate with make health before commit","title":"Fence Hygiene Tips"},{"location":"governance/engineering-standards/#sodeod-rituals","text":"","title":"SOD/EOD Rituals"},{"location":"governance/engineering-standards/#start-of-day-sod","text":"Review overnight CI results Check for any failed builds or coverage drops Update project status if needed Plan daily tasks based on current sprint goals","title":"Start of Day (SOD)"},{"location":"governance/engineering-standards/#end-of-day-eod","text":"Run make health to validate docs Create EOD summary report Update project metrics and status Ensure all PRs have proper validation evidence","title":"End of Day (EOD)"},{"location":"governance/engineering-standards/#report-locations","text":"Daily reports : reports/daily/YYYY-MM-DD.md Sprint reports : reports/sprints/sprint-N.md Milestone reports : reports/milestones/M{N}.md","title":"Report Locations"},{"location":"governance/engineering-standards/#adr-lite-process","text":"","title":"ADR-Lite Process"},{"location":"governance/engineering-standards/#when-adr-lite-is-required","text":"Architecture decisions affecting multiple components Changes to CI/CD pipeline or tooling New dependencies or technology choices Security or compliance policy changes Breaking changes to APIs or data models","title":"When ADR-Lite is Required"},{"location":"governance/engineering-standards/#adr-lite-template","text":"Reference: docs/adr/0000-adr-template.md Required sections : - Status (Proposed/Accepted/Deprecated/Superseded) - Context and Problem Statement - Decision Drivers - Considered Options - Decision Outcome - Consequences","title":"ADR-Lite Template"},{"location":"governance/engineering-standards/#read-run-update-loop","text":"","title":"\"Read \u2192 Run \u2192 Update\" Loop"},{"location":"governance/engineering-standards/#read-phase","text":"Read relevant documentation Understand current architecture Review existing patterns and conventions Check for related issues or PRs","title":"Read Phase"},{"location":"governance/engineering-standards/#run-phase","text":"Set up local development environment Run tests to understand current state Experiment with changes locally Validate with make health and make test","title":"Run Phase"},{"location":"governance/engineering-standards/#update-phase","text":"Implement changes following DoD checklist Update documentation as needed Add tests for new functionality Submit PR with validation evidence","title":"Update Phase"},{"location":"governance/engineering-standards/#adoption-integration","text":"","title":"Adoption &amp; Integration"},{"location":"governance/engineering-standards/#pr-template-integration","text":"The DoD checklist should be referenced in all PR templates. Add this to your PR template: ## DoD Checklist Please review and complete the [ Engineering Standards DoD checklist ]( governance/engineering-standards.md#definition-of-done-dod-checklist ). ## Validation Evidence Paste output from: - `make test` - `make health` - Coverage report","title":"PR Template Integration"},{"location":"governance/engineering-standards/#team-onboarding","text":"New team members must read this page before first PR Link from \"Developer Start Here\" page Include in team onboarding checklist Review quarterly for updates","title":"Team Onboarding"},{"location":"governance/engineering-standards/#continuous-improvement","text":"Monthly review of DoD effectiveness Quarterly updates to standards based on team feedback Annual review of CI pipeline and coverage thresholds Regular updates to tooling and best practices","title":"Continuous Improvement"},{"location":"qa/test-coverage-coordination/","text":"\"\"\" Test Coverage Coordination Plan for SecFlow M1 This document outlines the QA Lead's coordination strategy for achieving \u226580% test coverage across all M1 deliverables and ensuring comprehensive testing of core features. \"\"\" Test Coverage Coordination Strategy \u00b6 Overview \u00b6 The QA Lead will coordinate with all team leads to ensure comprehensive test coverage across: - Plugin Loader (Security Lead) - Workflow Engine (Workflow Lead) - Storage Layer (Storage Lead) - API Layer (API Lead) - CLI Tools (Tools Lead) - Documentation (Docs Lead) Coverage Targets by Component \u00b6 Plugin Loader (Security Lead) \u00b6 Target Coverage: 85% - Plugin loading and validation - Signature verification - Capability restrictions - Error handling for invalid plugins - Security isolation testing Key Test Areas: - test_plugin_loading.py - Basic loading functionality - test_plugin_security.py - Security validation - test_plugin_isolation.py - Isolation and sandboxing - test_plugin_error_handling.py - Error scenarios Edge Cases to Cover: - Two plugins with same name - Plugin trying to load another module - Malicious plugin behavior - Plugin timeout scenarios - Invalid plugin manifests Workflow Engine (Workflow Lead) \u00b6 Target Coverage: 80% - Workflow execution - Step orchestration - Error handling and recovery - Performance monitoring Key Test Areas: - test_workflow_execution.py - Core execution logic - test_workflow_orchestration.py - Step management - test_workflow_error_handling.py - Error scenarios - test_workflow_performance.py - Performance monitoring Edge Cases to Cover: - Workflow with no steps - Circular dependencies between steps - Step timeout scenarios - Resource exhaustion - Concurrent workflow execution Storage Layer (Storage Lead) \u00b6 Target Coverage: 90% - Finding storage and retrieval - Project management - Data persistence - Thread safety Key Test Areas: - test_storage_operations.py - Basic CRUD operations - test_storage_thread_safety.py - Concurrency safety - test_storage_persistence.py - Data persistence - test_storage_performance.py - Performance characteristics Edge Cases to Cover: - Storage corruption scenarios - Disk space exhaustion - Concurrent access patterns - Data migration scenarios - Backup and recovery API Layer (API Lead) \u00b6 Target Coverage: 75% - REST API endpoints - Request/response handling - Authentication and authorization - Error responses Key Test Areas: - test_api_endpoints.py - Endpoint functionality - test_api_authentication.py - Auth mechanisms - test_api_error_handling.py - Error responses - test_api_performance.py - API performance Edge Cases to Cover: - Invalid request formats - Authentication failures - Rate limiting scenarios - Large payload handling - Concurrent API requests CLI Tools (Tools Lead) \u00b6 Target Coverage: 70% - Command-line interfaces - Argument parsing - Output formatting - Error handling Key Test Areas: - test_cli_commands.py - Command functionality - test_cli_parsing.py - Argument parsing - test_cli_output.py - Output formatting - test_cli_error_handling.py - Error scenarios Edge Cases to Cover: - Invalid command arguments - Missing required files - Permission errors - Output redirection - Interactive mode Integration Test Coverage \u00b6 End-to-End Workflows \u00b6 Target Coverage: 100% of critical paths - Happy path workflow execution - Error handling workflows - Security validation workflows - Performance monitoring workflows Cross-Component Integration \u00b6 Plugin \u2192 Workflow Engine integration Workflow Engine \u2192 Storage integration API \u2192 Workflow Engine integration CLI \u2192 All components integration Test Data and Fixtures \u00b6 Test Data Requirements \u00b6 Sample workflow configurations Mock plugin manifests Test findings data Performance benchmarks Security test cases Fixture Management \u00b6 Centralized fixture repository Reusable test data generators Environment-specific configurations Cleanup and isolation utilities Quality Gates and Enforcement \u00b6 Coverage Thresholds \u00b6 Unit Tests: \u226580% overall coverage Integration Tests: 100% critical path coverage Component-Specific: As defined above Regression Prevention: 0% coverage decrease Quality Gate Checks \u00b6 Pre-commit hooks for coverage validation CI/CD pipeline coverage reporting Coverage trend monitoring Coverage gap analysis Coordination Process \u00b6 Daily Standups \u00b6 Coverage progress updates Test failure triage Coverage gap identification Resource allocation needs Weekly Reviews \u00b6 Coverage trend analysis Test quality assessment Edge case coverage review Performance test results Sprint Planning \u00b6 Test coverage goals Test case prioritization Resource allocation Risk assessment Test Execution Strategy \u00b6 Local Development \u00b6 Pre-commit test execution Coverage validation Performance benchmarking Security testing CI/CD Pipeline \u00b6 Automated test execution Coverage reporting Performance monitoring Security scanning Manual Testing \u00b6 User acceptance testing Exploratory testing Performance validation Security validation Risk Management \u00b6 Coverage Risks \u00b6 Low Coverage Areas: Identify and prioritize Untested Edge Cases: Document and plan Performance Regressions: Monitor and alert Security Gaps: Validate and remediate Mitigation Strategies \u00b6 Pair testing with developers Test case review sessions Coverage gap analysis Risk-based testing prioritization Success Metrics \u00b6 Coverage Metrics \u00b6 Overall test coverage percentage Component-specific coverage Critical path coverage Edge case coverage Quality Metrics \u00b6 Test failure rate Test execution time Test maintenance cost Bug escape rate Process Metrics \u00b6 Test case creation rate Test execution frequency Coverage improvement rate Test quality score Tools and Infrastructure \u00b6 Testing Tools \u00b6 pytest - Test framework pytest-cov - Coverage reporting pytest-xdist - Parallel execution pytest-mock - Mocking utilities Coverage Tools \u00b6 coverage.py - Coverage measurement coverage-html - HTML reports coverage-xml - XML reports coverage-json - JSON reports CI/CD Integration \u00b6 GitHub Actions - Automated testing Coverage reporting - Trend analysis Performance monitoring - Regression detection Security scanning - Vulnerability detection Implementation Timeline \u00b6 Week 1-2: Foundation \u00b6 Set up test infrastructure Create base test frameworks Establish coverage baselines Implement quality gates Week 3-4: Component Testing \u00b6 Plugin Loader test suite Workflow Engine test suite Storage Layer test suite API Layer test suite Week 5-6: Integration Testing \u00b6 End-to-end workflow tests Cross-component integration Performance testing Security testing Week 7-8: Validation and Optimization \u00b6 Coverage gap analysis Test optimization Performance tuning Security validation Communication and Reporting \u00b6 Daily Reports \u00b6 Test execution status Coverage progress Test failures Resource needs Weekly Reports \u00b6 Coverage trends Test quality metrics Risk assessment Improvement recommendations Sprint Reports \u00b6 Coverage achievements Test suite completeness Quality improvements Next sprint planning Conclusion \u00b6 This coordination plan ensures comprehensive test coverage across all M1 deliverables while maintaining high quality standards and efficient resource utilization. The QA Lead will work closely with all team leads to achieve the \u226580% coverage goal and validate that \"all workflows run in non-error state with real plugins\".","title":"Test coverage coordination"},{"location":"qa/test-coverage-coordination/#test-coverage-coordination-strategy","text":"","title":"Test Coverage Coordination Strategy"},{"location":"qa/test-coverage-coordination/#overview","text":"The QA Lead will coordinate with all team leads to ensure comprehensive test coverage across: - Plugin Loader (Security Lead) - Workflow Engine (Workflow Lead) - Storage Layer (Storage Lead) - API Layer (API Lead) - CLI Tools (Tools Lead) - Documentation (Docs Lead)","title":"Overview"},{"location":"qa/test-coverage-coordination/#coverage-targets-by-component","text":"","title":"Coverage Targets by Component"},{"location":"qa/test-coverage-coordination/#plugin-loader-security-lead","text":"Target Coverage: 85% - Plugin loading and validation - Signature verification - Capability restrictions - Error handling for invalid plugins - Security isolation testing Key Test Areas: - test_plugin_loading.py - Basic loading functionality - test_plugin_security.py - Security validation - test_plugin_isolation.py - Isolation and sandboxing - test_plugin_error_handling.py - Error scenarios Edge Cases to Cover: - Two plugins with same name - Plugin trying to load another module - Malicious plugin behavior - Plugin timeout scenarios - Invalid plugin manifests","title":"Plugin Loader (Security Lead)"},{"location":"qa/test-coverage-coordination/#workflow-engine-workflow-lead","text":"Target Coverage: 80% - Workflow execution - Step orchestration - Error handling and recovery - Performance monitoring Key Test Areas: - test_workflow_execution.py - Core execution logic - test_workflow_orchestration.py - Step management - test_workflow_error_handling.py - Error scenarios - test_workflow_performance.py - Performance monitoring Edge Cases to Cover: - Workflow with no steps - Circular dependencies between steps - Step timeout scenarios - Resource exhaustion - Concurrent workflow execution","title":"Workflow Engine (Workflow Lead)"},{"location":"qa/test-coverage-coordination/#storage-layer-storage-lead","text":"Target Coverage: 90% - Finding storage and retrieval - Project management - Data persistence - Thread safety Key Test Areas: - test_storage_operations.py - Basic CRUD operations - test_storage_thread_safety.py - Concurrency safety - test_storage_persistence.py - Data persistence - test_storage_performance.py - Performance characteristics Edge Cases to Cover: - Storage corruption scenarios - Disk space exhaustion - Concurrent access patterns - Data migration scenarios - Backup and recovery","title":"Storage Layer (Storage Lead)"},{"location":"qa/test-coverage-coordination/#api-layer-api-lead","text":"Target Coverage: 75% - REST API endpoints - Request/response handling - Authentication and authorization - Error responses Key Test Areas: - test_api_endpoints.py - Endpoint functionality - test_api_authentication.py - Auth mechanisms - test_api_error_handling.py - Error responses - test_api_performance.py - API performance Edge Cases to Cover: - Invalid request formats - Authentication failures - Rate limiting scenarios - Large payload handling - Concurrent API requests","title":"API Layer (API Lead)"},{"location":"qa/test-coverage-coordination/#cli-tools-tools-lead","text":"Target Coverage: 70% - Command-line interfaces - Argument parsing - Output formatting - Error handling Key Test Areas: - test_cli_commands.py - Command functionality - test_cli_parsing.py - Argument parsing - test_cli_output.py - Output formatting - test_cli_error_handling.py - Error scenarios Edge Cases to Cover: - Invalid command arguments - Missing required files - Permission errors - Output redirection - Interactive mode","title":"CLI Tools (Tools Lead)"},{"location":"qa/test-coverage-coordination/#integration-test-coverage","text":"","title":"Integration Test Coverage"},{"location":"qa/test-coverage-coordination/#end-to-end-workflows","text":"Target Coverage: 100% of critical paths - Happy path workflow execution - Error handling workflows - Security validation workflows - Performance monitoring workflows","title":"End-to-End Workflows"},{"location":"qa/test-coverage-coordination/#cross-component-integration","text":"Plugin \u2192 Workflow Engine integration Workflow Engine \u2192 Storage integration API \u2192 Workflow Engine integration CLI \u2192 All components integration","title":"Cross-Component Integration"},{"location":"qa/test-coverage-coordination/#test-data-and-fixtures","text":"","title":"Test Data and Fixtures"},{"location":"qa/test-coverage-coordination/#test-data-requirements","text":"Sample workflow configurations Mock plugin manifests Test findings data Performance benchmarks Security test cases","title":"Test Data Requirements"},{"location":"qa/test-coverage-coordination/#fixture-management","text":"Centralized fixture repository Reusable test data generators Environment-specific configurations Cleanup and isolation utilities","title":"Fixture Management"},{"location":"qa/test-coverage-coordination/#quality-gates-and-enforcement","text":"","title":"Quality Gates and Enforcement"},{"location":"qa/test-coverage-coordination/#coverage-thresholds","text":"Unit Tests: \u226580% overall coverage Integration Tests: 100% critical path coverage Component-Specific: As defined above Regression Prevention: 0% coverage decrease","title":"Coverage Thresholds"},{"location":"qa/test-coverage-coordination/#quality-gate-checks","text":"Pre-commit hooks for coverage validation CI/CD pipeline coverage reporting Coverage trend monitoring Coverage gap analysis","title":"Quality Gate Checks"},{"location":"qa/test-coverage-coordination/#coordination-process","text":"","title":"Coordination Process"},{"location":"qa/test-coverage-coordination/#daily-standups","text":"Coverage progress updates Test failure triage Coverage gap identification Resource allocation needs","title":"Daily Standups"},{"location":"qa/test-coverage-coordination/#weekly-reviews","text":"Coverage trend analysis Test quality assessment Edge case coverage review Performance test results","title":"Weekly Reviews"},{"location":"qa/test-coverage-coordination/#sprint-planning","text":"Test coverage goals Test case prioritization Resource allocation Risk assessment","title":"Sprint Planning"},{"location":"qa/test-coverage-coordination/#test-execution-strategy","text":"","title":"Test Execution Strategy"},{"location":"qa/test-coverage-coordination/#local-development","text":"Pre-commit test execution Coverage validation Performance benchmarking Security testing","title":"Local Development"},{"location":"qa/test-coverage-coordination/#cicd-pipeline","text":"Automated test execution Coverage reporting Performance monitoring Security scanning","title":"CI/CD Pipeline"},{"location":"qa/test-coverage-coordination/#manual-testing","text":"User acceptance testing Exploratory testing Performance validation Security validation","title":"Manual Testing"},{"location":"qa/test-coverage-coordination/#risk-management","text":"","title":"Risk Management"},{"location":"qa/test-coverage-coordination/#coverage-risks","text":"Low Coverage Areas: Identify and prioritize Untested Edge Cases: Document and plan Performance Regressions: Monitor and alert Security Gaps: Validate and remediate","title":"Coverage Risks"},{"location":"qa/test-coverage-coordination/#mitigation-strategies","text":"Pair testing with developers Test case review sessions Coverage gap analysis Risk-based testing prioritization","title":"Mitigation Strategies"},{"location":"qa/test-coverage-coordination/#success-metrics","text":"","title":"Success Metrics"},{"location":"qa/test-coverage-coordination/#coverage-metrics","text":"Overall test coverage percentage Component-specific coverage Critical path coverage Edge case coverage","title":"Coverage Metrics"},{"location":"qa/test-coverage-coordination/#quality-metrics","text":"Test failure rate Test execution time Test maintenance cost Bug escape rate","title":"Quality Metrics"},{"location":"qa/test-coverage-coordination/#process-metrics","text":"Test case creation rate Test execution frequency Coverage improvement rate Test quality score","title":"Process Metrics"},{"location":"qa/test-coverage-coordination/#tools-and-infrastructure","text":"","title":"Tools and Infrastructure"},{"location":"qa/test-coverage-coordination/#testing-tools","text":"pytest - Test framework pytest-cov - Coverage reporting pytest-xdist - Parallel execution pytest-mock - Mocking utilities","title":"Testing Tools"},{"location":"qa/test-coverage-coordination/#coverage-tools","text":"coverage.py - Coverage measurement coverage-html - HTML reports coverage-xml - XML reports coverage-json - JSON reports","title":"Coverage Tools"},{"location":"qa/test-coverage-coordination/#cicd-integration","text":"GitHub Actions - Automated testing Coverage reporting - Trend analysis Performance monitoring - Regression detection Security scanning - Vulnerability detection","title":"CI/CD Integration"},{"location":"qa/test-coverage-coordination/#implementation-timeline","text":"","title":"Implementation Timeline"},{"location":"qa/test-coverage-coordination/#week-1-2-foundation","text":"Set up test infrastructure Create base test frameworks Establish coverage baselines Implement quality gates","title":"Week 1-2: Foundation"},{"location":"qa/test-coverage-coordination/#week-3-4-component-testing","text":"Plugin Loader test suite Workflow Engine test suite Storage Layer test suite API Layer test suite","title":"Week 3-4: Component Testing"},{"location":"qa/test-coverage-coordination/#week-5-6-integration-testing","text":"End-to-end workflow tests Cross-component integration Performance testing Security testing","title":"Week 5-6: Integration Testing"},{"location":"qa/test-coverage-coordination/#week-7-8-validation-and-optimization","text":"Coverage gap analysis Test optimization Performance tuning Security validation","title":"Week 7-8: Validation and Optimization"},{"location":"qa/test-coverage-coordination/#communication-and-reporting","text":"","title":"Communication and Reporting"},{"location":"qa/test-coverage-coordination/#daily-reports","text":"Test execution status Coverage progress Test failures Resource needs","title":"Daily Reports"},{"location":"qa/test-coverage-coordination/#weekly-reports","text":"Coverage trends Test quality metrics Risk assessment Improvement recommendations","title":"Weekly Reports"},{"location":"qa/test-coverage-coordination/#sprint-reports","text":"Coverage achievements Test suite completeness Quality improvements Next sprint planning","title":"Sprint Reports"},{"location":"qa/test-coverage-coordination/#conclusion","text":"This coordination plan ensures comprehensive test coverage across all M1 deliverables while maintaining high quality standards and efficient resource utilization. The QA Lead will work closely with all team leads to achieve the \u226580% coverage goal and validate that \"all workflows run in non-error state with real plugins\".","title":"Conclusion"},{"location":"review/REVIEW_GUIDELINES/","text":"SecFlow Architecture Review Guidelines \u00b6 \ud83c\udfaf Overview \u00b6 This document outlines the review and validation process for the SecFlow architecture documentation suite. It provides structured guidelines for development team members to conduct thorough reviews of the 24 architecture documents. \ud83d\udc65 Review Team Structure \u00b6 Role Responsibilities Documents to Review Lead Architect Overall architecture consistency, design patterns All documents (00-24) Backend Engineers Core packages, data models, API design 04, 05, 12, 13, 16, 17, 18 Frontend Engineers UX design, web interface, CLI experience 08, 22 DevOps Engineers CI/CD, deployment, infrastructure 03, 21, 15, 17 Security Engineers Security model, compliance, risk assessment 16, 19, 14, 11 QA Engineers Testing strategy, validation criteria 21, 18, 12 Product Manager Business requirements, user experience 01, 08, 20, 23 \ud83d\udccb Review Checklist \u00b6 1. Technical Accuracy \u00b6 Code examples are syntactically correct Architecture diagrams are accurate and complete Technical specifications are feasible Dependencies and integrations are realistic Performance requirements are achievable 2. Completeness \u00b6 All required sections are present Cross-references between documents are valid Examples are comprehensive and relevant Edge cases and error scenarios are covered Future considerations are addressed 3. Consistency \u00b6 Terminology is consistent across documents Design patterns align with overall architecture Data models are consistent between documents Security principles are uniformly applied Naming conventions are followed 4. Clarity and Usability \u00b6 Concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Document structure is logical Language is professional and accessible 5. Security and Compliance \u00b6 Security requirements are clearly defined Compliance frameworks are properly referenced Risk assessment methodology is sound Data protection measures are adequate Audit requirements are met \ud83d\udd0d Review Process \u00b6 Phase 1: Individual Review (Week 1) \u00b6 Each team member reviews their assigned documents using the checklist above. Phase 2: Cross-Review (Week 2) \u00b6 Team members review documents outside their primary expertise to catch inconsistencies. Phase 3: Group Review (Week 3) \u00b6 Scheduled review sessions for each document category with all stakeholders. Phase 4: Final Validation (Week 4) \u00b6 Lead architect consolidates feedback and validates final changes. \ud83d\udcdd Review Template \u00b6 Document: [Document Name] \u00b6 Reviewer: [Name] Date: [Date] Review Type: [Individual/Cross/Group] Technical Accuracy \u00b6 Issues Found: [List specific issues] Recommendations: [Suggestions for improvement] Code Examples: [Any syntax or logic errors] Completeness \u00b6 Missing Sections: [List any missing content] Incomplete Examples: [Identify incomplete examples] Unclear Requirements: [Highlight unclear specifications] Consistency \u00b6 Terminology Issues: [Inconsistent terms or definitions] Design Pattern Conflicts: [Any architectural inconsistencies] Cross-Reference Problems: [Broken or incorrect links] Clarity and Usability \u00b6 Confusing Sections: [Identify unclear content] Missing Context: [Areas needing more explanation] Diagram Issues: [Problems with visual representations] Security and Compliance \u00b6 Security Gaps: [Missing security considerations] Compliance Issues: [Regulatory or policy concerns] Risk Assessment Problems: [Issues with risk methodology] Overall Assessment \u00b6 Strengths: [What works well] Critical Issues: [Must-fix problems] Enhancement Opportunities: [Nice-to-have improvements] Recommendation: [Approve/Revise/Reject] \ud83d\udea8 Critical Issues Escalation \u00b6 Immediate Escalation Required \u00b6 Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Performance bottlenecks Escalation Process \u00b6 Identify the critical issue Document the problem with evidence Notify the lead architect immediately Schedule emergency review session Resolve with all stakeholders present \ud83d\udcca Review Metrics \u00b6 Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met \ud83d\udd04 Review Workflow \u00b6 graph TD A[Document Ready] --> B[Assign Reviewers] B --> C[Individual Review] C --> D[Cross-Review] D --> E[Group Review] E --> F[Issue Resolution] F --> G[Final Validation] G --> H[Approval] H --> I[Implementation Ready] F --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> F J -->|No| G \ud83d\udcc5 Timeline \u00b6 Week Activities Deliverables Week 1 Individual reviews Initial feedback reports Week 2 Cross-reviews Consolidated issue list Week 3 Group review sessions Resolution plan Week 4 Final validation Approved architecture \ud83c\udfaf Success Criteria \u00b6 The architecture review is considered successful when: - [ ] All 24 documents pass technical accuracy review - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team \ud83d\udcde Contact Information \u00b6 Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Next Steps: Begin individual reviews using the provided checklist and template. ```","title":"Guidelines"},{"location":"review/REVIEW_GUIDELINES/#secflow-architecture-review-guidelines","text":"","title":"SecFlow Architecture Review Guidelines"},{"location":"review/REVIEW_GUIDELINES/#overview","text":"This document outlines the review and validation process for the SecFlow architecture documentation suite. It provides structured guidelines for development team members to conduct thorough reviews of the 24 architecture documents.","title":"\ud83c\udfaf Overview"},{"location":"review/REVIEW_GUIDELINES/#review-team-structure","text":"Role Responsibilities Documents to Review Lead Architect Overall architecture consistency, design patterns All documents (00-24) Backend Engineers Core packages, data models, API design 04, 05, 12, 13, 16, 17, 18 Frontend Engineers UX design, web interface, CLI experience 08, 22 DevOps Engineers CI/CD, deployment, infrastructure 03, 21, 15, 17 Security Engineers Security model, compliance, risk assessment 16, 19, 14, 11 QA Engineers Testing strategy, validation criteria 21, 18, 12 Product Manager Business requirements, user experience 01, 08, 20, 23","title":"\ud83d\udc65 Review Team Structure"},{"location":"review/REVIEW_GUIDELINES/#review-checklist","text":"","title":"\ud83d\udccb Review Checklist"},{"location":"review/REVIEW_GUIDELINES/#1-technical-accuracy","text":"Code examples are syntactically correct Architecture diagrams are accurate and complete Technical specifications are feasible Dependencies and integrations are realistic Performance requirements are achievable","title":"1. Technical Accuracy"},{"location":"review/REVIEW_GUIDELINES/#2-completeness","text":"All required sections are present Cross-references between documents are valid Examples are comprehensive and relevant Edge cases and error scenarios are covered Future considerations are addressed","title":"2. Completeness"},{"location":"review/REVIEW_GUIDELINES/#3-consistency","text":"Terminology is consistent across documents Design patterns align with overall architecture Data models are consistent between documents Security principles are uniformly applied Naming conventions are followed","title":"3. Consistency"},{"location":"review/REVIEW_GUIDELINES/#4-clarity-and-usability","text":"Concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Document structure is logical Language is professional and accessible","title":"4. Clarity and Usability"},{"location":"review/REVIEW_GUIDELINES/#5-security-and-compliance","text":"Security requirements are clearly defined Compliance frameworks are properly referenced Risk assessment methodology is sound Data protection measures are adequate Audit requirements are met","title":"5. Security and Compliance"},{"location":"review/REVIEW_GUIDELINES/#review-process","text":"","title":"\ud83d\udd0d Review Process"},{"location":"review/REVIEW_GUIDELINES/#phase-1-individual-review-week-1","text":"Each team member reviews their assigned documents using the checklist above.","title":"Phase 1: Individual Review (Week 1)"},{"location":"review/REVIEW_GUIDELINES/#phase-2-cross-review-week-2","text":"Team members review documents outside their primary expertise to catch inconsistencies.","title":"Phase 2: Cross-Review (Week 2)"},{"location":"review/REVIEW_GUIDELINES/#phase-3-group-review-week-3","text":"Scheduled review sessions for each document category with all stakeholders.","title":"Phase 3: Group Review (Week 3)"},{"location":"review/REVIEW_GUIDELINES/#phase-4-final-validation-week-4","text":"Lead architect consolidates feedback and validates final changes.","title":"Phase 4: Final Validation (Week 4)"},{"location":"review/REVIEW_GUIDELINES/#review-template","text":"","title":"\ud83d\udcdd Review Template"},{"location":"review/REVIEW_GUIDELINES/#document-document-name","text":"Reviewer: [Name] Date: [Date] Review Type: [Individual/Cross/Group]","title":"Document: [Document Name]"},{"location":"review/REVIEW_GUIDELINES/#technical-accuracy","text":"Issues Found: [List specific issues] Recommendations: [Suggestions for improvement] Code Examples: [Any syntax or logic errors]","title":"Technical Accuracy"},{"location":"review/REVIEW_GUIDELINES/#completeness","text":"Missing Sections: [List any missing content] Incomplete Examples: [Identify incomplete examples] Unclear Requirements: [Highlight unclear specifications]","title":"Completeness"},{"location":"review/REVIEW_GUIDELINES/#consistency","text":"Terminology Issues: [Inconsistent terms or definitions] Design Pattern Conflicts: [Any architectural inconsistencies] Cross-Reference Problems: [Broken or incorrect links]","title":"Consistency"},{"location":"review/REVIEW_GUIDELINES/#clarity-and-usability","text":"Confusing Sections: [Identify unclear content] Missing Context: [Areas needing more explanation] Diagram Issues: [Problems with visual representations]","title":"Clarity and Usability"},{"location":"review/REVIEW_GUIDELINES/#security-and-compliance","text":"Security Gaps: [Missing security considerations] Compliance Issues: [Regulatory or policy concerns] Risk Assessment Problems: [Issues with risk methodology]","title":"Security and Compliance"},{"location":"review/REVIEW_GUIDELINES/#overall-assessment","text":"Strengths: [What works well] Critical Issues: [Must-fix problems] Enhancement Opportunities: [Nice-to-have improvements] Recommendation: [Approve/Revise/Reject]","title":"Overall Assessment"},{"location":"review/REVIEW_GUIDELINES/#critical-issues-escalation","text":"","title":"\ud83d\udea8 Critical Issues Escalation"},{"location":"review/REVIEW_GUIDELINES/#immediate-escalation-required","text":"Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Performance bottlenecks","title":"Immediate Escalation Required"},{"location":"review/REVIEW_GUIDELINES/#escalation-process","text":"Identify the critical issue Document the problem with evidence Notify the lead architect immediately Schedule emergency review session Resolve with all stakeholders present","title":"Escalation Process"},{"location":"review/REVIEW_GUIDELINES/#review-metrics","text":"Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met","title":"\ud83d\udcca Review Metrics"},{"location":"review/REVIEW_GUIDELINES/#review-workflow","text":"graph TD A[Document Ready] --> B[Assign Reviewers] B --> C[Individual Review] C --> D[Cross-Review] D --> E[Group Review] E --> F[Issue Resolution] F --> G[Final Validation] G --> H[Approval] H --> I[Implementation Ready] F --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> F J -->|No| G","title":"\ud83d\udd04 Review Workflow"},{"location":"review/REVIEW_GUIDELINES/#timeline","text":"Week Activities Deliverables Week 1 Individual reviews Initial feedback reports Week 2 Cross-reviews Consolidated issue list Week 3 Group review sessions Resolution plan Week 4 Final validation Approved architecture","title":"\ud83d\udcc5 Timeline"},{"location":"review/REVIEW_GUIDELINES/#success-criteria","text":"The architecture review is considered successful when: - [ ] All 24 documents pass technical accuracy review - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team","title":"\ud83c\udfaf Success Criteria"},{"location":"review/REVIEW_GUIDELINES/#contact-information","text":"Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Next Steps: Begin individual reviews using the provided checklist and template. ```","title":"\ud83d\udcde Contact Information"},{"location":"review/REVIEW_STATUS/","text":"SecFlow Architecture Review Status \u00b6 \ud83c\udfaf Review and Validation Summary \u00b6 Date: October 6, 2025 Status: \u2705 Ready for Development Team Review Critical Issues: 0 Warnings: 89 \ud83d\udcca Validation Results \u00b6 Automated Validation \u00b6 Total Documents: 25 Total Checks: 89 Critical Issues: 0 \u274c Warnings: 89 \u26a0\ufe0f Passed: 0 \u2705 Critical Issues Resolved \u00b6 \u2705 Cross-reference validation - Fixed broken internal links \u2705 Document naming consistency - Corrected reference patterns \u2705 YAML metadata validation - All documents have proper frontmatter \u2705 Code example validation - All code blocks are properly formatted \u26a0\ufe0f Remaining Warnings (Non-Critical) \u00b6 Document Structure (15 warnings) \u00b6 Missing recommended \"## \ud83e\udded Overview\" sections in some documents Minor heading hierarchy issues These are style preferences, not functional issues ASCII Diagrams (8 warnings) \u00b6 Some diagrams use multiple box drawing characters Still readable and functional Could be simplified for consistency Code Examples (25 warnings) \u00b6 Some code examples contain placeholder content ( ... , TODO , FIXME ) These are intentional placeholders for implementation Will be completed during development phase Consistency (41 warnings) \u00b6 Mixed case usage for technical terms (e.g., \"SecFlow\" vs \"secflow\") Terminology variations across documents These are style inconsistencies, not functional issues \ud83d\ude80 Ready for Development Team Review \u00b6 The architecture documentation suite is now ready for comprehensive review by the development team. All critical issues have been resolved, and the remaining warnings are minor style and consistency issues that can be addressed during the review process. \ud83d\udccb Next Steps \u00b6 1. Development Team Review (Week 1-2) \u00b6 Backend Engineers: Review core packages, data models, API design Frontend Engineers: Review UX design, web interface, CLI experience DevOps Engineers: Review CI/CD, deployment, infrastructure Security Engineers: Review security model, compliance, risk assessment QA Engineers: Review testing strategy, validation criteria Product Manager: Review business requirements, user experience 2. Review Process \u00b6 Use provided review templates and checklists Follow the structured review workflow Document all feedback and recommendations Address critical issues immediately Plan resolution for non-critical issues 3. Final Validation \u00b6 Consolidate all review feedback Update documents based on team input Run final validation checks Obtain stakeholder approval Proceed to implementation \ud83d\udcc1 Review Resources \u00b6 Review Guidelines \u00b6 REVIEW_GUIDELINES.md - Comprehensive review process validation_checklist.md - Detailed validation criteria review_assignments.md - Team member assignments review_workflow.md - Structured workflow process Validation Tools \u00b6 automated_validation.py - Automated validation script validation_report.md - Latest validation results Review Templates \u00b6 Individual review template (in REVIEW_GUIDELINES.md) Group review session template (in REVIEW_GUIDELINES.md) Issue tracking and resolution templates \ud83c\udfaf Success Criteria \u00b6 The review process will be considered successful when: All 24 documents reviewed by assigned team members All critical issues identified and resolved Cross-document consistency validated All stakeholders approve their respective sections Implementation roadmap validated as feasible Development team ready to begin implementation \ud83d\udcde Contact Information \u00b6 Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Status: \u2705 Ready for Development Team Review Next Action: Begin structured review process according to assigned roles and timeline [Auto] Validation Summary updated \u2014 5 warnings, 0 criticals (2025-10-07 12:28) [Auto] DQI recalculated \u2014 99.0/100 (Trend: \u2192) [Auto] Snapshot generated and uploaded \u2014 2025-10-07 [Auto] Code fence language hints added \u2014 287 fences fixed, warnings: 127\u219269 (-58), DQI: 74.6\u219286.2 (+11.6) (2025-10-07) [Auto] Terminology normalized \u2014 75 fixes applied, warnings: 17\u21920 (-17), DQI: 92.6\u219296.0 (+3.4) (2025-10-07) [Auto] JSON/YAML syntax tightened \u2014 32 fixes applied, syntax errors: 18\u219212 (-6), DQI: 96.0\u219297.4 (+1.4), 16-security-model.md: \u2705 clean (2025-10-07) [Auto] Dashboard polished \u2014 Top 5 fixes + DQI trend added, accessible via Review & Validation \u2192 AI Summary (2025-10-07) [Auto] Phase 2.6 Green Across Board \u2014 Python syntax errors: 7\u21924 (-3), illegal annotation: 1\u21920 (-1), warnings: 13\u219210 (-3), DQI: 97.4\u219298.0 (+0.6) (2025-10-07) [Auto] YAML syntax fixed \u2014 YAML errors: 3\u21920 (-3), warnings: 10\u21926 (-4), DQI: 98.0\u219298.8 (+0.8) (2025-10-07) [Auto] Placeholders replaced \u2014 placeholder warnings: 2\u21920 (-2), warnings: 6\u21926 (stable), DQI: 98.8\u219298.8 (stable) (2025-10-07) [Auto] Risk framework cleaned \u2014 syntax errors: 1\u21920 (-1), warnings: 6\u21925 (-1), DQI: 98.8\u219299.0 (+0.2) (2025-10-07) [Auto] Full pipeline completed \u2014 warnings: 5, DQI: 99.0/100 (\u226599 \u2705), snapshot: docs_snapshot_2025-10-07.tgz (928KB) (2025-10-07)","title":"Status"},{"location":"review/REVIEW_STATUS/#secflow-architecture-review-status","text":"","title":"SecFlow Architecture Review Status"},{"location":"review/REVIEW_STATUS/#review-and-validation-summary","text":"Date: October 6, 2025 Status: \u2705 Ready for Development Team Review Critical Issues: 0 Warnings: 89","title":"\ud83c\udfaf Review and Validation Summary"},{"location":"review/REVIEW_STATUS/#validation-results","text":"","title":"\ud83d\udcca Validation Results"},{"location":"review/REVIEW_STATUS/#automated-validation","text":"Total Documents: 25 Total Checks: 89 Critical Issues: 0 \u274c Warnings: 89 \u26a0\ufe0f Passed: 0 \u2705","title":"Automated Validation"},{"location":"review/REVIEW_STATUS/#critical-issues-resolved","text":"\u2705 Cross-reference validation - Fixed broken internal links \u2705 Document naming consistency - Corrected reference patterns \u2705 YAML metadata validation - All documents have proper frontmatter \u2705 Code example validation - All code blocks are properly formatted","title":"Critical Issues Resolved"},{"location":"review/REVIEW_STATUS/#remaining-warnings-non-critical","text":"","title":"\u26a0\ufe0f Remaining Warnings (Non-Critical)"},{"location":"review/REVIEW_STATUS/#document-structure-15-warnings","text":"Missing recommended \"## \ud83e\udded Overview\" sections in some documents Minor heading hierarchy issues These are style preferences, not functional issues","title":"Document Structure (15 warnings)"},{"location":"review/REVIEW_STATUS/#ascii-diagrams-8-warnings","text":"Some diagrams use multiple box drawing characters Still readable and functional Could be simplified for consistency","title":"ASCII Diagrams (8 warnings)"},{"location":"review/REVIEW_STATUS/#code-examples-25-warnings","text":"Some code examples contain placeholder content ( ... , TODO , FIXME ) These are intentional placeholders for implementation Will be completed during development phase","title":"Code Examples (25 warnings)"},{"location":"review/REVIEW_STATUS/#consistency-41-warnings","text":"Mixed case usage for technical terms (e.g., \"SecFlow\" vs \"secflow\") Terminology variations across documents These are style inconsistencies, not functional issues","title":"Consistency (41 warnings)"},{"location":"review/REVIEW_STATUS/#ready-for-development-team-review","text":"The architecture documentation suite is now ready for comprehensive review by the development team. All critical issues have been resolved, and the remaining warnings are minor style and consistency issues that can be addressed during the review process.","title":"\ud83d\ude80 Ready for Development Team Review"},{"location":"review/REVIEW_STATUS/#next-steps","text":"","title":"\ud83d\udccb Next Steps"},{"location":"review/REVIEW_STATUS/#1-development-team-review-week-1-2","text":"Backend Engineers: Review core packages, data models, API design Frontend Engineers: Review UX design, web interface, CLI experience DevOps Engineers: Review CI/CD, deployment, infrastructure Security Engineers: Review security model, compliance, risk assessment QA Engineers: Review testing strategy, validation criteria Product Manager: Review business requirements, user experience","title":"1. Development Team Review (Week 1-2)"},{"location":"review/REVIEW_STATUS/#2-review-process","text":"Use provided review templates and checklists Follow the structured review workflow Document all feedback and recommendations Address critical issues immediately Plan resolution for non-critical issues","title":"2. Review Process"},{"location":"review/REVIEW_STATUS/#3-final-validation","text":"Consolidate all review feedback Update documents based on team input Run final validation checks Obtain stakeholder approval Proceed to implementation","title":"3. Final Validation"},{"location":"review/REVIEW_STATUS/#review-resources","text":"","title":"\ud83d\udcc1 Review Resources"},{"location":"review/REVIEW_STATUS/#review-guidelines","text":"REVIEW_GUIDELINES.md - Comprehensive review process validation_checklist.md - Detailed validation criteria review_assignments.md - Team member assignments review_workflow.md - Structured workflow process","title":"Review Guidelines"},{"location":"review/REVIEW_STATUS/#validation-tools","text":"automated_validation.py - Automated validation script validation_report.md - Latest validation results","title":"Validation Tools"},{"location":"review/REVIEW_STATUS/#review-templates","text":"Individual review template (in REVIEW_GUIDELINES.md) Group review session template (in REVIEW_GUIDELINES.md) Issue tracking and resolution templates","title":"Review Templates"},{"location":"review/REVIEW_STATUS/#success-criteria","text":"The review process will be considered successful when: All 24 documents reviewed by assigned team members All critical issues identified and resolved Cross-document consistency validated All stakeholders approve their respective sections Implementation roadmap validated as feasible Development team ready to begin implementation","title":"\ud83c\udfaf Success Criteria"},{"location":"review/REVIEW_STATUS/#contact-information","text":"Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Status: \u2705 Ready for Development Team Review Next Action: Begin structured review process according to assigned roles and timeline [Auto] Validation Summary updated \u2014 5 warnings, 0 criticals (2025-10-07 12:28) [Auto] DQI recalculated \u2014 99.0/100 (Trend: \u2192) [Auto] Snapshot generated and uploaded \u2014 2025-10-07 [Auto] Code fence language hints added \u2014 287 fences fixed, warnings: 127\u219269 (-58), DQI: 74.6\u219286.2 (+11.6) (2025-10-07) [Auto] Terminology normalized \u2014 75 fixes applied, warnings: 17\u21920 (-17), DQI: 92.6\u219296.0 (+3.4) (2025-10-07) [Auto] JSON/YAML syntax tightened \u2014 32 fixes applied, syntax errors: 18\u219212 (-6), DQI: 96.0\u219297.4 (+1.4), 16-security-model.md: \u2705 clean (2025-10-07) [Auto] Dashboard polished \u2014 Top 5 fixes + DQI trend added, accessible via Review & Validation \u2192 AI Summary (2025-10-07) [Auto] Phase 2.6 Green Across Board \u2014 Python syntax errors: 7\u21924 (-3), illegal annotation: 1\u21920 (-1), warnings: 13\u219210 (-3), DQI: 97.4\u219298.0 (+0.6) (2025-10-07) [Auto] YAML syntax fixed \u2014 YAML errors: 3\u21920 (-3), warnings: 10\u21926 (-4), DQI: 98.0\u219298.8 (+0.8) (2025-10-07) [Auto] Placeholders replaced \u2014 placeholder warnings: 2\u21920 (-2), warnings: 6\u21926 (stable), DQI: 98.8\u219298.8 (stable) (2025-10-07) [Auto] Risk framework cleaned \u2014 syntax errors: 1\u21920 (-1), warnings: 6\u21925 (-1), DQI: 98.8\u219299.0 (+0.2) (2025-10-07) [Auto] Full pipeline completed \u2014 warnings: 5, DQI: 99.0/100 (\u226599 \u2705), snapshot: docs_snapshot_2025-10-07.tgz (928KB) (2025-10-07)","title":"\ud83d\udcde Contact Information"},{"location":"review/VALIDATION_SUMMARY/","text":"AI-Assisted Validation Summary \u00b6 Generated : 2025-10-07 12:28:23 Source : docs/review/validation_report.md \ud83d\udcca Overview \u00b6 Total Documents : 25 Critical Issues : 0 Warnings : 5 Categories : 2 \ud83d\udcc8 Documentation Quality Index (DQI) \u00b6 Score : 99.0/100 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent \ud83c\udff7\ufe0f Warnings by Category \u00b6 Code Block Issues \u00b6 Count : 3 Issues : Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax Terminology / Glossary Inconsistencies \u00b6 Count : 2 Issues : Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) \ud83d\udd25 Top 10 Most Frequent Issues \u00b6 1x - Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 1x - Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 1x - Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 1x - Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 1x - Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value \ud83d\udca1 Suggested Actions \u00b6 Add language hints to code blocks (3 issues): Specify python , yaml , json etc. for better syntax highlighting Standardize terminology (2 issues): Use consistent capitalization (e.g., 'SecFlow' not 'secflow') Run validation regularly : Use make validate before committing changes Review validation summary : Check VALIDATION_SUMMARY.md for detailed insights Follow glossary standards : Use docs/review/glossary.yml for terminology consistency Complete code examples : Replace placeholders with working code snippets \ud83d\udcc8 Document Health Score \u00b6 Document Warnings Criticals Health Score 00-index.md 0 0 \ud83d\udfe2 100% 01-title-and-executive-summary.md 0 0 \ud83d\udfe2 100% 02-architecture-philosophy.md 0 0 \ud83d\udfe2 100% 03-repository-layout.md 0 0 \ud83d\udfe2 100% 04-core-packages-and-responsibilities.md 0 0 \ud83d\udfe2 100% 05-orchestration-and-workflow-engine.md 0 0 \ud83d\udfe2 100% 07-tools-integration-model.md 0 0 \ud83d\udfe2 100% 09-resource-registry.md 0 0 \ud83d\udfe2 100% 10-wordlist-and-output-sharing.md 0 0 \ud83d\udfe2 100% 11-project-isolation-and-data-sharing.md 0 0 \ud83d\udfe2 100% 13-cve-cwe-poc-enrichment-layer.md 0 0 \ud83d\udfe2 100% 15-garbage-collection-and-retention.md 0 0 \ud83d\udfe2 100% 16-security-model.md 0 0 \ud83d\udfe2 100% 18-error-handling-and-recovery.md 0 0 \ud83d\udfe2 100% 19-risk-assessment-framework.md 0 0 \ud83d\udfe2 100% 20-migration-and-implementation-phases.md 0 0 \ud83d\udfe2 100% 21-ci-cd-and-testing-strategy.md 0 0 \ud83d\udfe2 100% 22-developer-experience-and-docs.md 0 0 \ud83d\udfe2 100% 23-future-roadmap.md 0 0 \ud83d\udfe2 100% 24-final-consensus-summary.md 0 0 \ud83d\udfe2 100% 06-plugin-system.md 1 0 \ud83d\udfe2 98% 08-tool-manager-and-ux-design.md 1 0 \ud83d\udfe2 98% 12-findings-model-and-schema.md 1 0 \ud83d\udfe2 98% 14-poc-sources-and-legal-guidelines.md 1 0 \ud83d\udfe2 98% 17-observability-logging-and-metrics.md 1 0 \ud83d\udfe2 98% \ud83c\udfaf Priority Recommendations \u00b6 Focus on high-warning documents : 06-plugin-system.md, 08-tool-manager-and-ux-design.md, 12-findings-model-and-schema.md Standardize terminology : Important for professional presentation Regular validation : Run make validate before each commit Team review : Use structured review process for major changes Continuous improvement : Monitor validation trends over time Generated by AI-Assisted Validation Summary Generator Last updated: 2025-10-07 12:28:23","title":"AI Summary"},{"location":"review/VALIDATION_SUMMARY/#ai-assisted-validation-summary","text":"Generated : 2025-10-07 12:28:23 Source : docs/review/validation_report.md","title":"AI-Assisted Validation Summary"},{"location":"review/VALIDATION_SUMMARY/#overview","text":"Total Documents : 25 Critical Issues : 0 Warnings : 5 Categories : 2","title":"\ud83d\udcca Overview"},{"location":"review/VALIDATION_SUMMARY/#documentation-quality-index-dqi","text":"Score : 99.0/100 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent","title":"\ud83d\udcc8 Documentation Quality Index (DQI)"},{"location":"review/VALIDATION_SUMMARY/#warnings-by-category","text":"","title":"\ud83c\udff7\ufe0f Warnings by Category"},{"location":"review/VALIDATION_SUMMARY/#code-block-issues","text":"Count : 3 Issues : Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax","title":"Code Block Issues"},{"location":"review/VALIDATION_SUMMARY/#terminology-glossary-inconsistencies","text":"Count : 2 Issues : Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow )","title":"Terminology / Glossary Inconsistencies"},{"location":"review/VALIDATION_SUMMARY/#top-10-most-frequent-issues","text":"1x - Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 1x - Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 1x - Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 1x - Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 1x - Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value","title":"\ud83d\udd25 Top 10 Most Frequent Issues"},{"location":"review/VALIDATION_SUMMARY/#suggested-actions","text":"Add language hints to code blocks (3 issues): Specify python , yaml , json etc. for better syntax highlighting Standardize terminology (2 issues): Use consistent capitalization (e.g., 'SecFlow' not 'secflow') Run validation regularly : Use make validate before committing changes Review validation summary : Check VALIDATION_SUMMARY.md for detailed insights Follow glossary standards : Use docs/review/glossary.yml for terminology consistency Complete code examples : Replace placeholders with working code snippets","title":"\ud83d\udca1 Suggested Actions"},{"location":"review/VALIDATION_SUMMARY/#document-health-score","text":"Document Warnings Criticals Health Score 00-index.md 0 0 \ud83d\udfe2 100% 01-title-and-executive-summary.md 0 0 \ud83d\udfe2 100% 02-architecture-philosophy.md 0 0 \ud83d\udfe2 100% 03-repository-layout.md 0 0 \ud83d\udfe2 100% 04-core-packages-and-responsibilities.md 0 0 \ud83d\udfe2 100% 05-orchestration-and-workflow-engine.md 0 0 \ud83d\udfe2 100% 07-tools-integration-model.md 0 0 \ud83d\udfe2 100% 09-resource-registry.md 0 0 \ud83d\udfe2 100% 10-wordlist-and-output-sharing.md 0 0 \ud83d\udfe2 100% 11-project-isolation-and-data-sharing.md 0 0 \ud83d\udfe2 100% 13-cve-cwe-poc-enrichment-layer.md 0 0 \ud83d\udfe2 100% 15-garbage-collection-and-retention.md 0 0 \ud83d\udfe2 100% 16-security-model.md 0 0 \ud83d\udfe2 100% 18-error-handling-and-recovery.md 0 0 \ud83d\udfe2 100% 19-risk-assessment-framework.md 0 0 \ud83d\udfe2 100% 20-migration-and-implementation-phases.md 0 0 \ud83d\udfe2 100% 21-ci-cd-and-testing-strategy.md 0 0 \ud83d\udfe2 100% 22-developer-experience-and-docs.md 0 0 \ud83d\udfe2 100% 23-future-roadmap.md 0 0 \ud83d\udfe2 100% 24-final-consensus-summary.md 0 0 \ud83d\udfe2 100% 06-plugin-system.md 1 0 \ud83d\udfe2 98% 08-tool-manager-and-ux-design.md 1 0 \ud83d\udfe2 98% 12-findings-model-and-schema.md 1 0 \ud83d\udfe2 98% 14-poc-sources-and-legal-guidelines.md 1 0 \ud83d\udfe2 98% 17-observability-logging-and-metrics.md 1 0 \ud83d\udfe2 98%","title":"\ud83d\udcc8 Document Health Score"},{"location":"review/VALIDATION_SUMMARY/#priority-recommendations","text":"Focus on high-warning documents : 06-plugin-system.md, 08-tool-manager-and-ux-design.md, 12-findings-model-and-schema.md Standardize terminology : Important for professional presentation Regular validation : Run make validate before each commit Team review : Use structured review process for major changes Continuous improvement : Monitor validation trends over time Generated by AI-Assisted Validation Summary Generator Last updated: 2025-10-07 12:28:23","title":"\ud83c\udfaf Priority Recommendations"},{"location":"review/mermaid-smoketest/","text":"Mermaid Smoketest \u00b6 This page tests that Mermaid diagrams are rendering correctly using the superfences + local Mermaid approach. \ud83e\uddea Test Diagram \u00b6 flowchart LR A[Start] --> B[Process] B --> C[End] \u2705 Expected Result \u00b6 You should see a flowchart diagram above with three boxes connected by arrows. \ud83d\udd04 Sequence Diagram Test \u00b6 sequenceDiagram participant A as Alice participant B as Bob A->>B: Hello Bob, how are you? B-->>A: Great! A-)B: See you later! \ud83d\udcca Class Diagram Test \u00b6 classDiagram class Animal { +String name +int age +makeSound() } class Dog { +String breed +bark() } Animal <|-- Dog \ud83c\udfaf State Diagram Test \u00b6 stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] \ud83d\udcc8 Expected Results \u00b6 All four diagrams above should render as interactive Mermaid diagrams, not as code blocks. ```","title":"Mermaid Smoketest"},{"location":"review/mermaid-smoketest/#mermaid-smoketest","text":"This page tests that Mermaid diagrams are rendering correctly using the superfences + local Mermaid approach.","title":"Mermaid Smoketest"},{"location":"review/mermaid-smoketest/#test-diagram","text":"flowchart LR A[Start] --> B[Process] B --> C[End]","title":"\ud83e\uddea Test Diagram"},{"location":"review/mermaid-smoketest/#expected-result","text":"You should see a flowchart diagram above with three boxes connected by arrows.","title":"\u2705 Expected Result"},{"location":"review/mermaid-smoketest/#sequence-diagram-test","text":"sequenceDiagram participant A as Alice participant B as Bob A->>B: Hello Bob, how are you? B-->>A: Great! A-)B: See you later!","title":"\ud83d\udd04 Sequence Diagram Test"},{"location":"review/mermaid-smoketest/#class-diagram-test","text":"classDiagram class Animal { +String name +int age +makeSound() } class Dog { +String breed +bark() } Animal <|-- Dog","title":"\ud83d\udcca Class Diagram Test"},{"location":"review/mermaid-smoketest/#state-diagram-test","text":"stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*]","title":"\ud83c\udfaf State Diagram Test"},{"location":"review/mermaid-smoketest/#expected-results","text":"All four diagrams above should render as interactive Mermaid diagrams, not as code blocks. ```","title":"\ud83d\udcc8 Expected Results"},{"location":"review/review_assignments/","text":"SecFlow Architecture Review Assignments \u00b6 \ud83d\udc65 Review Team Assignments \u00b6 This document outlines the specific review assignments for each team member, ensuring comprehensive coverage of all 24 architecture documents. \ud83c\udfaf Lead Architect Review (All Documents) \u00b6 Reviewer: Lead Architect Scope: All documents (00-24) Focus: Overall architecture consistency, design patterns, cross-document coherence Priority Documents \u00b6 00-index.md - Navigation and structure 02-architecture-philosophy.md - Core architectural principles 04-core-packages-and-responsibilities.md - Package structure and relationships 05-orchestration-and-workflow-engine.md - Workflow orchestration design 24-final-consensus-summary.md - Final architectural consensus Review Criteria \u00b6 Architectural consistency across all documents Design pattern alignment with hexagonal architecture Cross-document references and dependencies Overall system coherence and feasibility Strategic alignment with project goals \ud83d\udd27 Backend Engineers Review \u00b6 Reviewers: Senior Backend Engineer, Backend Developer Scope: Core technical implementation documents Focus: Data models, API design, database integration, workflow execution Assigned Documents \u00b6 04-core-packages-and-responsibilities.md Package structure and responsibilities Interface definitions and contracts Cross-package dependencies 05-orchestration-and-workflow-engine.md DAG execution engine design Workflow specification schema Node executor implementation 12-findings-model-and-schema.md Finding data model and normalization Database schema design Data validation and transformation 13-cve-cwe-poc-enrichment-layer.md Enrichment pipeline design External API integrations Data caching and synchronization 16-security-model.md Authentication and authorization implementation Security middleware design Audit logging mechanisms 17-observability-logging-and-metrics.md Logging infrastructure design Metrics collection and export Distributed tracing implementation 18-error-handling-and-recovery.md Error handling strategies Retry logic and circuit breakers Dead letter queue implementation Review Criteria \u00b6 Technical feasibility of implementation Code examples are syntactically correct Data models are complete and consistent API design follows RESTful principles Database schema is normalized and efficient Error handling is comprehensive Performance requirements are achievable \ud83c\udfa8 Frontend Engineers Review \u00b6 Reviewers: Frontend Developer, UX Designer Scope: User interface and experience documents Focus: UI/UX design, web interface, CLI experience, user workflows Assigned Documents \u00b6 08-tool-manager-and-ux-design.md Tool Manager UX design CLI and web interface specifications User workflow design 22-developer-experience-and-docs.md Developer tools and CLI utilities Documentation system design Local development workflow Review Criteria \u00b6 UI/UX design is intuitive and accessible CLI commands are logical and well-organized Web interface specifications are complete User workflows are efficient and clear Developer experience is optimized Documentation is user-friendly \ud83d\ude80 DevOps Engineers Review \u00b6 Reviewers: DevOps Engineer, Infrastructure Engineer Scope: Infrastructure, deployment, and operational documents Focus: CI/CD, deployment strategies, infrastructure requirements, monitoring Assigned Documents \u00b6 03-repository-layout.md Mono-repo structure and tooling Import boundaries and enforcement Development workflow automation 15-garbage-collection-and-retention.md Data lifecycle management Cleanup and retention policies Storage optimization strategies 17-observability-logging-and-metrics.md Monitoring and observability infrastructure Log aggregation and analysis Metrics collection and alerting 21-ci-cd-and-testing-strategy.md CI/CD pipeline design Testing framework and automation Deployment strategies and rollback Review Criteria \u00b6 Infrastructure requirements are realistic CI/CD pipeline is efficient and reliable Deployment strategies are safe and scalable Monitoring and observability are comprehensive Backup and recovery procedures are adequate Security scanning and compliance checks are integrated \ud83d\udd12 Security Engineers Review \u00b6 Reviewers: Security Engineer, Compliance Officer Scope: Security, compliance, and risk assessment documents Focus: Security model, compliance requirements, risk assessment, data protection Assigned Documents \u00b6 11-project-isolation-and-data-sharing.md Project isolation mechanisms Data sharing policies and controls Access control and permissions 14-poc-sources-and-legal-guidelines.md PoC governance and legal compliance Sandbox execution security Legal and ethical guidelines 16-security-model.md Authentication and authorization design Security middleware and controls Audit logging and compliance 19-risk-assessment-framework.md Risk scoring methodology NIST framework implementation CVSS and CWE integration Review Criteria \u00b6 Security controls are comprehensive and effective Compliance requirements are met Risk assessment methodology is sound Data protection measures are adequate Audit requirements are satisfied Legal and ethical guidelines are followed \ud83e\uddea QA Engineers Review \u00b6 Reviewers: QA Engineer, Test Automation Engineer Scope: Testing strategy and quality assurance documents Focus: Testing frameworks, quality gates, validation criteria, test automation Assigned Documents \u00b6 12-findings-model-and-schema.md Data validation and transformation testing Schema validation and error handling Data integrity testing 18-error-handling-and-recovery.md Error handling testing strategies Recovery mechanism validation Fault tolerance testing 21-ci-cd-and-testing-strategy.md Testing framework and automation Quality gates and validation criteria Test coverage and reporting Review Criteria \u00b6 Testing strategies are comprehensive Quality gates are appropriate and measurable Test automation is feasible and maintainable Error scenarios are adequately covered Performance testing requirements are defined Security testing is integrated \ud83d\udcca Product Manager Review \u00b6 Reviewer: Product Manager Scope: Business requirements and user experience documents Focus: Business value, user requirements, market fit, strategic alignment Assigned Documents \u00b6 01-title-and-executive-summary.md Project vision and goals Business value proposition Market positioning 08-tool-manager-and-ux-design.md User experience design Tool management workflows User interface specifications 20-migration-and-implementation-phases.md Implementation roadmap and timeline Phase deliverables and milestones Success criteria and metrics 23-future-roadmap.md Strategic evolution and roadmap AI integration and future capabilities Market trends and competitive analysis Review Criteria \u00b6 Business requirements are clearly defined User experience meets market expectations Implementation timeline is realistic Success criteria are measurable Strategic alignment with company goals Competitive advantages are clear \ud83d\udcc5 Review Schedule \u00b6 Week 1: Individual Reviews \u00b6 Monday-Tuesday: Backend Engineers review assigned documents Wednesday-Thursday: Frontend Engineers review assigned documents Friday: DevOps Engineers review assigned documents Week 2: Cross-Reviews \u00b6 Monday: Security Engineers review assigned documents Tuesday: QA Engineers review assigned documents Wednesday: Product Manager review assigned documents Thursday-Friday: Cross-review sessions (each team reviews outside their expertise) Week 3: Group Review Sessions \u00b6 Monday: Core Architecture Review (Documents 00-05) Tuesday: Implementation Review (Documents 06-12) Wednesday: Operations Review (Documents 13-18) Thursday: Strategy Review (Documents 19-24) Friday: Final Consolidation and Issue Resolution Week 4: Final Validation \u00b6 Monday-Tuesday: Lead Architect final review Wednesday: Stakeholder approval sessions Thursday: Final validation and sign-off Friday: Implementation readiness assessment \ud83d\udcdd Review Deliverables \u00b6 Individual Review Deliverables \u00b6 Completed review checklist for each assigned document Detailed feedback report with specific issues and recommendations Technical accuracy validation results Consistency and completeness assessment Cross-Review Deliverables \u00b6 Cross-document consistency validation Integration point verification Cross-functional requirement validation Stakeholder alignment confirmation Group Review Deliverables \u00b6 Consolidated issue list with priorities Resolution plan with timelines Final approval recommendations Implementation readiness assessment \ud83d\udea8 Escalation Process \u00b6 Critical Issues \u00b6 Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Timeline conflicts Escalation Path \u00b6 Identify critical issue during review Document issue with evidence and impact Notify Lead Architect immediately Schedule emergency review session Resolve with all stakeholders present Update review assignments if needed \u2705 Success Criteria \u00b6 The review process is considered successful when: - [ ] All 24 documents are reviewed by assigned team members - [ ] All critical issues are identified and resolved - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Development team is ready to begin implementation Next Steps: Begin individual reviews according to the assigned schedule and scope.","title":"Assignments"},{"location":"review/review_assignments/#secflow-architecture-review-assignments","text":"","title":"SecFlow Architecture Review Assignments"},{"location":"review/review_assignments/#review-team-assignments","text":"This document outlines the specific review assignments for each team member, ensuring comprehensive coverage of all 24 architecture documents.","title":"\ud83d\udc65 Review Team Assignments"},{"location":"review/review_assignments/#lead-architect-review-all-documents","text":"Reviewer: Lead Architect Scope: All documents (00-24) Focus: Overall architecture consistency, design patterns, cross-document coherence","title":"\ud83c\udfaf Lead Architect Review (All Documents)"},{"location":"review/review_assignments/#priority-documents","text":"00-index.md - Navigation and structure 02-architecture-philosophy.md - Core architectural principles 04-core-packages-and-responsibilities.md - Package structure and relationships 05-orchestration-and-workflow-engine.md - Workflow orchestration design 24-final-consensus-summary.md - Final architectural consensus","title":"Priority Documents"},{"location":"review/review_assignments/#review-criteria","text":"Architectural consistency across all documents Design pattern alignment with hexagonal architecture Cross-document references and dependencies Overall system coherence and feasibility Strategic alignment with project goals","title":"Review Criteria"},{"location":"review/review_assignments/#backend-engineers-review","text":"Reviewers: Senior Backend Engineer, Backend Developer Scope: Core technical implementation documents Focus: Data models, API design, database integration, workflow execution","title":"\ud83d\udd27 Backend Engineers Review"},{"location":"review/review_assignments/#assigned-documents","text":"04-core-packages-and-responsibilities.md Package structure and responsibilities Interface definitions and contracts Cross-package dependencies 05-orchestration-and-workflow-engine.md DAG execution engine design Workflow specification schema Node executor implementation 12-findings-model-and-schema.md Finding data model and normalization Database schema design Data validation and transformation 13-cve-cwe-poc-enrichment-layer.md Enrichment pipeline design External API integrations Data caching and synchronization 16-security-model.md Authentication and authorization implementation Security middleware design Audit logging mechanisms 17-observability-logging-and-metrics.md Logging infrastructure design Metrics collection and export Distributed tracing implementation 18-error-handling-and-recovery.md Error handling strategies Retry logic and circuit breakers Dead letter queue implementation","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_1","text":"Technical feasibility of implementation Code examples are syntactically correct Data models are complete and consistent API design follows RESTful principles Database schema is normalized and efficient Error handling is comprehensive Performance requirements are achievable","title":"Review Criteria"},{"location":"review/review_assignments/#frontend-engineers-review","text":"Reviewers: Frontend Developer, UX Designer Scope: User interface and experience documents Focus: UI/UX design, web interface, CLI experience, user workflows","title":"\ud83c\udfa8 Frontend Engineers Review"},{"location":"review/review_assignments/#assigned-documents_1","text":"08-tool-manager-and-ux-design.md Tool Manager UX design CLI and web interface specifications User workflow design 22-developer-experience-and-docs.md Developer tools and CLI utilities Documentation system design Local development workflow","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_2","text":"UI/UX design is intuitive and accessible CLI commands are logical and well-organized Web interface specifications are complete User workflows are efficient and clear Developer experience is optimized Documentation is user-friendly","title":"Review Criteria"},{"location":"review/review_assignments/#devops-engineers-review","text":"Reviewers: DevOps Engineer, Infrastructure Engineer Scope: Infrastructure, deployment, and operational documents Focus: CI/CD, deployment strategies, infrastructure requirements, monitoring","title":"\ud83d\ude80 DevOps Engineers Review"},{"location":"review/review_assignments/#assigned-documents_2","text":"03-repository-layout.md Mono-repo structure and tooling Import boundaries and enforcement Development workflow automation 15-garbage-collection-and-retention.md Data lifecycle management Cleanup and retention policies Storage optimization strategies 17-observability-logging-and-metrics.md Monitoring and observability infrastructure Log aggregation and analysis Metrics collection and alerting 21-ci-cd-and-testing-strategy.md CI/CD pipeline design Testing framework and automation Deployment strategies and rollback","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_3","text":"Infrastructure requirements are realistic CI/CD pipeline is efficient and reliable Deployment strategies are safe and scalable Monitoring and observability are comprehensive Backup and recovery procedures are adequate Security scanning and compliance checks are integrated","title":"Review Criteria"},{"location":"review/review_assignments/#security-engineers-review","text":"Reviewers: Security Engineer, Compliance Officer Scope: Security, compliance, and risk assessment documents Focus: Security model, compliance requirements, risk assessment, data protection","title":"\ud83d\udd12 Security Engineers Review"},{"location":"review/review_assignments/#assigned-documents_3","text":"11-project-isolation-and-data-sharing.md Project isolation mechanisms Data sharing policies and controls Access control and permissions 14-poc-sources-and-legal-guidelines.md PoC governance and legal compliance Sandbox execution security Legal and ethical guidelines 16-security-model.md Authentication and authorization design Security middleware and controls Audit logging and compliance 19-risk-assessment-framework.md Risk scoring methodology NIST framework implementation CVSS and CWE integration","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_4","text":"Security controls are comprehensive and effective Compliance requirements are met Risk assessment methodology is sound Data protection measures are adequate Audit requirements are satisfied Legal and ethical guidelines are followed","title":"Review Criteria"},{"location":"review/review_assignments/#qa-engineers-review","text":"Reviewers: QA Engineer, Test Automation Engineer Scope: Testing strategy and quality assurance documents Focus: Testing frameworks, quality gates, validation criteria, test automation","title":"\ud83e\uddea QA Engineers Review"},{"location":"review/review_assignments/#assigned-documents_4","text":"12-findings-model-and-schema.md Data validation and transformation testing Schema validation and error handling Data integrity testing 18-error-handling-and-recovery.md Error handling testing strategies Recovery mechanism validation Fault tolerance testing 21-ci-cd-and-testing-strategy.md Testing framework and automation Quality gates and validation criteria Test coverage and reporting","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_5","text":"Testing strategies are comprehensive Quality gates are appropriate and measurable Test automation is feasible and maintainable Error scenarios are adequately covered Performance testing requirements are defined Security testing is integrated","title":"Review Criteria"},{"location":"review/review_assignments/#product-manager-review","text":"Reviewer: Product Manager Scope: Business requirements and user experience documents Focus: Business value, user requirements, market fit, strategic alignment","title":"\ud83d\udcca Product Manager Review"},{"location":"review/review_assignments/#assigned-documents_5","text":"01-title-and-executive-summary.md Project vision and goals Business value proposition Market positioning 08-tool-manager-and-ux-design.md User experience design Tool management workflows User interface specifications 20-migration-and-implementation-phases.md Implementation roadmap and timeline Phase deliverables and milestones Success criteria and metrics 23-future-roadmap.md Strategic evolution and roadmap AI integration and future capabilities Market trends and competitive analysis","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_6","text":"Business requirements are clearly defined User experience meets market expectations Implementation timeline is realistic Success criteria are measurable Strategic alignment with company goals Competitive advantages are clear","title":"Review Criteria"},{"location":"review/review_assignments/#review-schedule","text":"","title":"\ud83d\udcc5 Review Schedule"},{"location":"review/review_assignments/#week-1-individual-reviews","text":"Monday-Tuesday: Backend Engineers review assigned documents Wednesday-Thursday: Frontend Engineers review assigned documents Friday: DevOps Engineers review assigned documents","title":"Week 1: Individual Reviews"},{"location":"review/review_assignments/#week-2-cross-reviews","text":"Monday: Security Engineers review assigned documents Tuesday: QA Engineers review assigned documents Wednesday: Product Manager review assigned documents Thursday-Friday: Cross-review sessions (each team reviews outside their expertise)","title":"Week 2: Cross-Reviews"},{"location":"review/review_assignments/#week-3-group-review-sessions","text":"Monday: Core Architecture Review (Documents 00-05) Tuesday: Implementation Review (Documents 06-12) Wednesday: Operations Review (Documents 13-18) Thursday: Strategy Review (Documents 19-24) Friday: Final Consolidation and Issue Resolution","title":"Week 3: Group Review Sessions"},{"location":"review/review_assignments/#week-4-final-validation","text":"Monday-Tuesday: Lead Architect final review Wednesday: Stakeholder approval sessions Thursday: Final validation and sign-off Friday: Implementation readiness assessment","title":"Week 4: Final Validation"},{"location":"review/review_assignments/#review-deliverables","text":"","title":"\ud83d\udcdd Review Deliverables"},{"location":"review/review_assignments/#individual-review-deliverables","text":"Completed review checklist for each assigned document Detailed feedback report with specific issues and recommendations Technical accuracy validation results Consistency and completeness assessment","title":"Individual Review Deliverables"},{"location":"review/review_assignments/#cross-review-deliverables","text":"Cross-document consistency validation Integration point verification Cross-functional requirement validation Stakeholder alignment confirmation","title":"Cross-Review Deliverables"},{"location":"review/review_assignments/#group-review-deliverables","text":"Consolidated issue list with priorities Resolution plan with timelines Final approval recommendations Implementation readiness assessment","title":"Group Review Deliverables"},{"location":"review/review_assignments/#escalation-process","text":"","title":"\ud83d\udea8 Escalation Process"},{"location":"review/review_assignments/#critical-issues","text":"Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Timeline conflicts","title":"Critical Issues"},{"location":"review/review_assignments/#escalation-path","text":"Identify critical issue during review Document issue with evidence and impact Notify Lead Architect immediately Schedule emergency review session Resolve with all stakeholders present Update review assignments if needed","title":"Escalation Path"},{"location":"review/review_assignments/#success-criteria","text":"The review process is considered successful when: - [ ] All 24 documents are reviewed by assigned team members - [ ] All critical issues are identified and resolved - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Development team is ready to begin implementation Next Steps: Begin individual reviews according to the assigned schedule and scope.","title":"\u2705 Success Criteria"},{"location":"review/review_workflow/","text":"SecFlow Architecture Review Workflow \u00b6 \ud83c\udfaf Overview \u00b6 This document defines the structured review workflow for the SecFlow architecture documentation suite, ensuring comprehensive validation and stakeholder approval before implementation begins. \ud83d\udccb Workflow Phases \u00b6 Phase 1: Pre-Review Setup (Day 1) \u00b6 Duration: 1 day Participants: Lead Architect, Review Coordinator Activities \u00b6 Review Environment Setup Create review workspace with all documents Set up collaboration tools (GitHub, Slack, etc.) Configure review tracking system Prepare review templates and checklists Stakeholder Notification Send review invitations to all team members Distribute review assignments and timelines Schedule review sessions and meetings Provide access to review tools and resources Review Preparation Run automated validation checks Generate initial validation report Identify any obvious issues for quick fixes Prepare review guidelines and instructions Deliverables \u00b6 Review workspace configured All stakeholders notified and onboarded Review tools and templates ready Initial validation report generated Phase 2: Individual Reviews (Days 2-6) \u00b6 Duration: 5 days Participants: All assigned reviewers Activities \u00b6 Document-Specific Reviews Each reviewer reviews their assigned documents Use provided checklists and templates Document all issues, suggestions, and feedback Validate technical accuracy and completeness Cross-Document Reviews Review documents outside primary expertise Check for consistency and integration issues Validate cross-references and dependencies Identify architectural inconsistencies Feedback Documentation Complete review templates for each document Categorize issues by severity and type Provide specific recommendations for improvements Document approval status for each document Deliverables \u00b6 Individual review reports for all documents Consolidated issue list with priorities Cross-document consistency validation Initial approval recommendations Phase 3: Cross-Review Sessions (Days 7-8) \u00b6 Duration: 2 days Participants: All reviewers Activities \u00b6 Cross-Functional Reviews Review documents outside primary expertise Validate integration points and dependencies Check for cross-functional requirements Ensure stakeholder alignment Consistency Validation Validate terminology and naming conventions Check architectural pattern consistency Verify data model consistency Validate security model alignment Issue Resolution Planning Prioritize issues by severity and impact Assign resolution responsibilities Set timelines for issue resolution Plan for emergency escalations Deliverables \u00b6 Cross-review validation reports Prioritized issue resolution plan Consistency validation results Stakeholder alignment confirmation Phase 4: Group Review Sessions (Days 9-11) \u00b6 Duration: 3 days Participants: All stakeholders Activities \u00b6 Document Category Reviews Day 9: Core Architecture (Documents 00-05) Day 10: Implementation (Documents 06-12) Day 11: Operations & Strategy (Documents 13-24) Stakeholder Approval Sessions Present review findings for each category Discuss critical issues and resolutions Validate implementation feasibility Obtain formal approval from stakeholders Issue Resolution Address critical issues immediately Plan resolution for non-critical issues Update documents based on feedback Validate changes and improvements Deliverables \u00b6 Group review session reports Stakeholder approval confirmations Resolved critical issues Updated documents with improvements Phase 5: Final Validation (Days 12-14) \u00b6 Duration: 3 days Participants: Lead Architect, Review Coordinator Activities \u00b6 Final Validation Run comprehensive validation checks Verify all issues have been addressed Validate document consistency and accuracy Confirm stakeholder approvals Implementation Readiness Assessment Validate technical feasibility Confirm resource availability Verify timeline alignment Assess risk mitigation strategies Documentation Finalization Update documents with final changes Generate final validation report Prepare implementation guidelines Create handoff documentation Deliverables \u00b6 Final validation report Implementation readiness assessment Updated architecture documentation Implementation handoff package \ud83d\udd04 Review Process Flow \u00b6 graph TD A[Pre-Review Setup] --> B[Individual Reviews] B --> C[Cross-Review Sessions] C --> D[Group Review Sessions] D --> E[Final Validation] E --> F[Implementation Ready] B --> G[Issue Identification] G --> H[Issue Resolution] H --> I[Document Updates] I --> C D --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> H J -->|No| E \ud83d\udcca Review Metrics and KPIs \u00b6 Quality Metrics \u00b6 Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met Process Metrics \u00b6 Metric Target Measurement Review Completion 100% All assigned reviews completed on time Stakeholder Approval 100% All stakeholders approve their sections Issue Response Time < 24 hours Time to respond to critical issues Document Update Time < 48 hours Time to update documents after feedback \ud83d\udea8 Escalation Procedures \u00b6 Critical Issues \u00b6 Definition: Issues that could prevent implementation or compromise security Examples: - Security vulnerabilities in design - Infeasible technical requirements - Major architectural inconsistencies - Compliance violations - Timeline conflicts Escalation Process: 1. Identify critical issue during review 2. Document issue with evidence and impact 3. Notify Lead Architect immediately 4. Schedule emergency review session within 4 hours 5. Resolve with all stakeholders present 6. Update review assignments if needed Non-Critical Issues \u00b6 Definition: Issues that should be addressed but don't block implementation Examples: - Minor formatting inconsistencies - Documentation improvements - Code style suggestions - Performance optimizations Resolution Process: 1. Document issue in review report 2. Assign resolution responsibility 3. Set timeline for resolution 4. Track progress in issue tracking system 5. Validate resolution before final approval \ud83d\udcdd Review Templates and Tools \u00b6 Individual Review Template \u00b6 ## Document Review: [Document Name] **Reviewer:** [Name] **Date:** [Date] **Review Type:** [Individual/Cross/Group] ### Technical Accuracy - **Issues Found:** [List specific issues] - **Recommendations:** [Suggestions for improvement] - **Code Examples:** [Any syntax or logic errors] ### Completeness - **Missing Sections:** [List any missing content] - **Incomplete Examples:** [Identify incomplete examples] - **Unclear Requirements:** [Highlight unclear specifications] ### Consistency - **Terminology Issues:** [Inconsistent terms or definitions] - **Design Pattern Conflicts:** [Any architectural inconsistencies] - **Cross-Reference Problems:** [Broken or incorrect links] ### Overall Assessment - **Strengths:** [What works well] - **Critical Issues:** [Must-fix problems] - **Enhancement Opportunities:** [Nice-to-have improvements] - **Recommendation:** [Approve/Revise/Reject] Group Review Session Template \u00b6 ## Group Review Session: [Category] **Date:** [Date] **Participants:** [List participants] ### Documents Reviewed - [List documents reviewed] ### Key Issues Identified - [List critical issues] - [List non-critical issues] ### Resolutions Agreed - [List agreed resolutions] - [Assign responsibilities] - [Set timelines] ### Approval Status - [Document approval status] - [Stakeholder confirmations] - [Next steps] \ud83c\udfaf Success Criteria \u00b6 The review workflow is considered successful when: Quality Criteria \u00b6 All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Process Criteria \u00b6 All review phases completed on schedule All stakeholders actively participated All issues properly documented and resolved All approvals formally obtained Implementation team ready to begin Deliverable Criteria \u00b6 Final validation report generated Implementation readiness assessment completed Updated architecture documentation ready Implementation handoff package prepared Development team onboarded and ready \ud83d\udcde Contact Information \u00b6 Review Team \u00b6 Lead Architect: [Name] - [email] - [phone] Review Coordinator: [Name] - [email] - [phone] Backend Lead: [Name] - [email] - [phone] Frontend Lead: [Name] - [email] - [phone] DevOps Lead: [Name] - [email] - [phone] Security Lead: [Name] - [email] - [phone] Emergency Contacts \u00b6 Critical Issues: Lead Architect - [phone] Process Issues: Review Coordinator - [phone] Technical Issues: Backend Lead - [phone] \ud83d\udcc5 Timeline Summary \u00b6 Phase Duration Key Activities Deliverables Phase 1 1 day Pre-review setup Review environment ready Phase 2 5 days Individual reviews Review reports completed Phase 3 2 days Cross-review sessions Consistency validated Phase 4 3 days Group review sessions Stakeholder approvals Phase 5 3 days Final validation Implementation ready Total Duration: 14 days Target Completion: [Date] Next Steps: Begin Phase 1 - Pre-Review Setup","title":"Workflow"},{"location":"review/review_workflow/#secflow-architecture-review-workflow","text":"","title":"SecFlow Architecture Review Workflow"},{"location":"review/review_workflow/#overview","text":"This document defines the structured review workflow for the SecFlow architecture documentation suite, ensuring comprehensive validation and stakeholder approval before implementation begins.","title":"\ud83c\udfaf Overview"},{"location":"review/review_workflow/#workflow-phases","text":"","title":"\ud83d\udccb Workflow Phases"},{"location":"review/review_workflow/#phase-1-pre-review-setup-day-1","text":"Duration: 1 day Participants: Lead Architect, Review Coordinator","title":"Phase 1: Pre-Review Setup (Day 1)"},{"location":"review/review_workflow/#activities","text":"Review Environment Setup Create review workspace with all documents Set up collaboration tools (GitHub, Slack, etc.) Configure review tracking system Prepare review templates and checklists Stakeholder Notification Send review invitations to all team members Distribute review assignments and timelines Schedule review sessions and meetings Provide access to review tools and resources Review Preparation Run automated validation checks Generate initial validation report Identify any obvious issues for quick fixes Prepare review guidelines and instructions","title":"Activities"},{"location":"review/review_workflow/#deliverables","text":"Review workspace configured All stakeholders notified and onboarded Review tools and templates ready Initial validation report generated","title":"Deliverables"},{"location":"review/review_workflow/#phase-2-individual-reviews-days-2-6","text":"Duration: 5 days Participants: All assigned reviewers","title":"Phase 2: Individual Reviews (Days 2-6)"},{"location":"review/review_workflow/#activities_1","text":"Document-Specific Reviews Each reviewer reviews their assigned documents Use provided checklists and templates Document all issues, suggestions, and feedback Validate technical accuracy and completeness Cross-Document Reviews Review documents outside primary expertise Check for consistency and integration issues Validate cross-references and dependencies Identify architectural inconsistencies Feedback Documentation Complete review templates for each document Categorize issues by severity and type Provide specific recommendations for improvements Document approval status for each document","title":"Activities"},{"location":"review/review_workflow/#deliverables_1","text":"Individual review reports for all documents Consolidated issue list with priorities Cross-document consistency validation Initial approval recommendations","title":"Deliverables"},{"location":"review/review_workflow/#phase-3-cross-review-sessions-days-7-8","text":"Duration: 2 days Participants: All reviewers","title":"Phase 3: Cross-Review Sessions (Days 7-8)"},{"location":"review/review_workflow/#activities_2","text":"Cross-Functional Reviews Review documents outside primary expertise Validate integration points and dependencies Check for cross-functional requirements Ensure stakeholder alignment Consistency Validation Validate terminology and naming conventions Check architectural pattern consistency Verify data model consistency Validate security model alignment Issue Resolution Planning Prioritize issues by severity and impact Assign resolution responsibilities Set timelines for issue resolution Plan for emergency escalations","title":"Activities"},{"location":"review/review_workflow/#deliverables_2","text":"Cross-review validation reports Prioritized issue resolution plan Consistency validation results Stakeholder alignment confirmation","title":"Deliverables"},{"location":"review/review_workflow/#phase-4-group-review-sessions-days-9-11","text":"Duration: 3 days Participants: All stakeholders","title":"Phase 4: Group Review Sessions (Days 9-11)"},{"location":"review/review_workflow/#activities_3","text":"Document Category Reviews Day 9: Core Architecture (Documents 00-05) Day 10: Implementation (Documents 06-12) Day 11: Operations & Strategy (Documents 13-24) Stakeholder Approval Sessions Present review findings for each category Discuss critical issues and resolutions Validate implementation feasibility Obtain formal approval from stakeholders Issue Resolution Address critical issues immediately Plan resolution for non-critical issues Update documents based on feedback Validate changes and improvements","title":"Activities"},{"location":"review/review_workflow/#deliverables_3","text":"Group review session reports Stakeholder approval confirmations Resolved critical issues Updated documents with improvements","title":"Deliverables"},{"location":"review/review_workflow/#phase-5-final-validation-days-12-14","text":"Duration: 3 days Participants: Lead Architect, Review Coordinator","title":"Phase 5: Final Validation (Days 12-14)"},{"location":"review/review_workflow/#activities_4","text":"Final Validation Run comprehensive validation checks Verify all issues have been addressed Validate document consistency and accuracy Confirm stakeholder approvals Implementation Readiness Assessment Validate technical feasibility Confirm resource availability Verify timeline alignment Assess risk mitigation strategies Documentation Finalization Update documents with final changes Generate final validation report Prepare implementation guidelines Create handoff documentation","title":"Activities"},{"location":"review/review_workflow/#deliverables_4","text":"Final validation report Implementation readiness assessment Updated architecture documentation Implementation handoff package","title":"Deliverables"},{"location":"review/review_workflow/#review-process-flow","text":"graph TD A[Pre-Review Setup] --> B[Individual Reviews] B --> C[Cross-Review Sessions] C --> D[Group Review Sessions] D --> E[Final Validation] E --> F[Implementation Ready] B --> G[Issue Identification] G --> H[Issue Resolution] H --> I[Document Updates] I --> C D --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> H J -->|No| E","title":"\ud83d\udd04 Review Process Flow"},{"location":"review/review_workflow/#review-metrics-and-kpis","text":"","title":"\ud83d\udcca Review Metrics and KPIs"},{"location":"review/review_workflow/#quality-metrics","text":"Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met","title":"Quality Metrics"},{"location":"review/review_workflow/#process-metrics","text":"Metric Target Measurement Review Completion 100% All assigned reviews completed on time Stakeholder Approval 100% All stakeholders approve their sections Issue Response Time < 24 hours Time to respond to critical issues Document Update Time < 48 hours Time to update documents after feedback","title":"Process Metrics"},{"location":"review/review_workflow/#escalation-procedures","text":"","title":"\ud83d\udea8 Escalation Procedures"},{"location":"review/review_workflow/#critical-issues","text":"Definition: Issues that could prevent implementation or compromise security Examples: - Security vulnerabilities in design - Infeasible technical requirements - Major architectural inconsistencies - Compliance violations - Timeline conflicts Escalation Process: 1. Identify critical issue during review 2. Document issue with evidence and impact 3. Notify Lead Architect immediately 4. Schedule emergency review session within 4 hours 5. Resolve with all stakeholders present 6. Update review assignments if needed","title":"Critical Issues"},{"location":"review/review_workflow/#non-critical-issues","text":"Definition: Issues that should be addressed but don't block implementation Examples: - Minor formatting inconsistencies - Documentation improvements - Code style suggestions - Performance optimizations Resolution Process: 1. Document issue in review report 2. Assign resolution responsibility 3. Set timeline for resolution 4. Track progress in issue tracking system 5. Validate resolution before final approval","title":"Non-Critical Issues"},{"location":"review/review_workflow/#review-templates-and-tools","text":"","title":"\ud83d\udcdd Review Templates and Tools"},{"location":"review/review_workflow/#individual-review-template","text":"## Document Review: [Document Name] **Reviewer:** [Name] **Date:** [Date] **Review Type:** [Individual/Cross/Group] ### Technical Accuracy - **Issues Found:** [List specific issues] - **Recommendations:** [Suggestions for improvement] - **Code Examples:** [Any syntax or logic errors] ### Completeness - **Missing Sections:** [List any missing content] - **Incomplete Examples:** [Identify incomplete examples] - **Unclear Requirements:** [Highlight unclear specifications] ### Consistency - **Terminology Issues:** [Inconsistent terms or definitions] - **Design Pattern Conflicts:** [Any architectural inconsistencies] - **Cross-Reference Problems:** [Broken or incorrect links] ### Overall Assessment - **Strengths:** [What works well] - **Critical Issues:** [Must-fix problems] - **Enhancement Opportunities:** [Nice-to-have improvements] - **Recommendation:** [Approve/Revise/Reject]","title":"Individual Review Template"},{"location":"review/review_workflow/#group-review-session-template","text":"## Group Review Session: [Category] **Date:** [Date] **Participants:** [List participants] ### Documents Reviewed - [List documents reviewed] ### Key Issues Identified - [List critical issues] - [List non-critical issues] ### Resolutions Agreed - [List agreed resolutions] - [Assign responsibilities] - [Set timelines] ### Approval Status - [Document approval status] - [Stakeholder confirmations] - [Next steps]","title":"Group Review Session Template"},{"location":"review/review_workflow/#success-criteria","text":"The review workflow is considered successful when:","title":"\ud83c\udfaf Success Criteria"},{"location":"review/review_workflow/#quality-criteria","text":"All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible","title":"Quality Criteria"},{"location":"review/review_workflow/#process-criteria","text":"All review phases completed on schedule All stakeholders actively participated All issues properly documented and resolved All approvals formally obtained Implementation team ready to begin","title":"Process Criteria"},{"location":"review/review_workflow/#deliverable-criteria","text":"Final validation report generated Implementation readiness assessment completed Updated architecture documentation ready Implementation handoff package prepared Development team onboarded and ready","title":"Deliverable Criteria"},{"location":"review/review_workflow/#contact-information","text":"","title":"\ud83d\udcde Contact Information"},{"location":"review/review_workflow/#review-team","text":"Lead Architect: [Name] - [email] - [phone] Review Coordinator: [Name] - [email] - [phone] Backend Lead: [Name] - [email] - [phone] Frontend Lead: [Name] - [email] - [phone] DevOps Lead: [Name] - [email] - [phone] Security Lead: [Name] - [email] - [phone]","title":"Review Team"},{"location":"review/review_workflow/#emergency-contacts","text":"Critical Issues: Lead Architect - [phone] Process Issues: Review Coordinator - [phone] Technical Issues: Backend Lead - [phone]","title":"Emergency Contacts"},{"location":"review/review_workflow/#timeline-summary","text":"Phase Duration Key Activities Deliverables Phase 1 1 day Pre-review setup Review environment ready Phase 2 5 days Individual reviews Review reports completed Phase 3 2 days Cross-review sessions Consistency validated Phase 4 3 days Group review sessions Stakeholder approvals Phase 5 3 days Final validation Implementation ready Total Duration: 14 days Target Completion: [Date] Next Steps: Begin Phase 1 - Pre-Review Setup","title":"\ud83d\udcc5 Timeline Summary"},{"location":"review/validation_checklist/","text":"SecFlow Architecture Validation Checklist \u00b6 \ud83c\udfaf Overview \u00b6 This comprehensive validation checklist ensures all architecture documents meet quality standards before implementation begins. \ud83d\udccb Document Structure Validation \u00b6 YAML Metadata Validation \u00b6 All documents have required YAML frontmatter Title, author, codename, version, and date are present Version format follows semantic versioning (e.g., \"1.0\") Date format is consistent (YYYY-MM-DD) Content Structure Validation \u00b6 Each document has a clear overview section ASCII diagrams are properly formatted Code examples are syntax-highlighted Tables are properly formatted Internal links use correct MkDocs format Section headers follow consistent hierarchy \ud83d\udd0d Technical Content Validation \u00b6 Code Examples \u00b6 Python code examples are syntactically correct YAML configurations are valid JSON examples are properly formatted Import statements are accurate Function signatures are complete Error handling is demonstrated Architecture Diagrams \u00b6 ASCII diagrams are readable and accurate Component relationships are clearly shown Data flow directions are indicated Layer boundaries are defined External dependencies are identified Data Models \u00b6 Pydantic models are complete and valid Field types are correctly specified Validation rules are appropriate Relationships between models are clear Database schemas are consistent \ud83c\udfd7\ufe0f Architecture Consistency Validation \u00b6 Design Patterns \u00b6 Hexagonal architecture principles are consistently applied Port and adapter patterns are properly implemented Event-driven design is consistently used Immutable data flow is maintained Dependency inversion is followed Naming Conventions \u00b6 Package names follow Python conventions Class names use PascalCase Function names use snake_case Constants use UPPER_CASE File names are descriptive and consistent Cross-Document References \u00b6 All internal links are valid Referenced concepts are defined Data model references are consistent API endpoint references are accurate Configuration references are valid \ud83d\udd12 Security Validation \u00b6 Security Model \u00b6 Authentication mechanisms are clearly defined Authorization model is comprehensive Data encryption requirements are specified Audit logging is properly designed Security boundaries are clearly defined Compliance Requirements \u00b6 NIST framework references are accurate CVSS scoring methodology is correct CWE/OWASP mappings are valid MITRE ATT&CK references are current Data protection requirements are met Risk Assessment \u00b6 Risk scoring formulas are mathematically sound Risk factors are properly weighted Risk thresholds are realistic Risk mitigation strategies are feasible Risk reporting is comprehensive \ud83d\ude80 Implementation Feasibility Validation \u00b6 Technical Requirements \u00b6 All dependencies are available and maintained Performance requirements are achievable Scalability requirements are realistic Integration points are well-defined Error handling strategies are complete Resource Requirements \u00b6 Infrastructure requirements are realistic Development effort estimates are accurate Testing requirements are comprehensive Deployment complexity is manageable Maintenance requirements are clear Migration Strategy \u00b6 Phase transitions are clearly defined Rollback strategies are specified Data migration plans are feasible Testing strategies are comprehensive Success criteria are measurable \ud83d\udcca Quality Metrics Validation \u00b6 Completeness \u00b6 All required sections are present Examples cover all major use cases Edge cases are addressed Error scenarios are documented Future considerations are included Clarity \u00b6 Technical concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Language is professional and accessible Document structure is logical Accuracy \u00b6 Technical specifications are correct Code examples are functional Architecture diagrams are accurate Data models are consistent Integration points are valid \ud83d\udd04 Process Validation \u00b6 Review Process \u00b6 All stakeholders have reviewed their sections Cross-reviews have been completed Group review sessions have been conducted All issues have been resolved Final approval has been obtained Documentation Standards \u00b6 All documents follow the established template Formatting is consistent across documents Links and references are valid Version control is properly maintained Change tracking is documented \u2705 Final Validation Checklist \u00b6 Pre-Implementation Validation \u00b6 All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Risk assessment methodology is approved by security team Implementation Readiness \u00b6 Development environment setup is documented CI/CD pipeline configuration is ready Testing framework is established Security scanning tools are configured Monitoring and observability tools are ready Documentation generation process is automated \ud83d\udea8 Critical Issues Requiring Immediate Attention \u00b6 Security Issues \u00b6 Any security vulnerabilities in design Missing security controls Inadequate data protection measures Compliance violations Audit trail gaps Technical Issues \u00b6 Infeasible technical requirements Major architectural inconsistencies Performance bottlenecks Integration failures Data model conflicts Process Issues \u00b6 Missing stakeholder approvals Incomplete review process Unresolved critical issues Timeline conflicts Resource constraints \ud83d\udcdd Validation Report Template \u00b6 Document: [Document Name] \u00b6 Validator: [Name] Date: [Date] Status: [Pass/Fail/Needs Revision] Technical Validation \u00b6 Code Examples: [Pass/Fail] - [Comments] Architecture Diagrams: [Pass/Fail] - [Comments] Data Models: [Pass/Fail] - [Comments] Integration Points: [Pass/Fail] - [Comments] Security Validation \u00b6 Security Model: [Pass/Fail] - [Comments] Compliance Requirements: [Pass/Fail] - [Comments] Risk Assessment: [Pass/Fail] - [Comments] Data Protection: [Pass/Fail] - [Comments] Process Validation \u00b6 Review Process: [Pass/Fail] - [Comments] Documentation Standards: [Pass/Fail] - [Comments] Stakeholder Approval: [Pass/Fail] - [Comments] Implementation Readiness: [Pass/Fail] - [Comments] Overall Assessment \u00b6 Strengths: [List strengths] Issues Found: [List issues] Recommendations: [List recommendations] Final Recommendation: [Approve/Revise/Reject] \ud83c\udfaf Success Criteria \u00b6 The architecture validation is considered successful when: - [ ] All documents pass technical accuracy validation - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team - [ ] Development team is ready to begin implementation Next Steps: Use this checklist to validate each document systematically before proceeding to implementation.","title":"Checklist"},{"location":"review/validation_checklist/#secflow-architecture-validation-checklist","text":"","title":"SecFlow Architecture Validation Checklist"},{"location":"review/validation_checklist/#overview","text":"This comprehensive validation checklist ensures all architecture documents meet quality standards before implementation begins.","title":"\ud83c\udfaf Overview"},{"location":"review/validation_checklist/#document-structure-validation","text":"","title":"\ud83d\udccb Document Structure Validation"},{"location":"review/validation_checklist/#yaml-metadata-validation","text":"All documents have required YAML frontmatter Title, author, codename, version, and date are present Version format follows semantic versioning (e.g., \"1.0\") Date format is consistent (YYYY-MM-DD)","title":"YAML Metadata Validation"},{"location":"review/validation_checklist/#content-structure-validation","text":"Each document has a clear overview section ASCII diagrams are properly formatted Code examples are syntax-highlighted Tables are properly formatted Internal links use correct MkDocs format Section headers follow consistent hierarchy","title":"Content Structure Validation"},{"location":"review/validation_checklist/#technical-content-validation","text":"","title":"\ud83d\udd0d Technical Content Validation"},{"location":"review/validation_checklist/#code-examples","text":"Python code examples are syntactically correct YAML configurations are valid JSON examples are properly formatted Import statements are accurate Function signatures are complete Error handling is demonstrated","title":"Code Examples"},{"location":"review/validation_checklist/#architecture-diagrams","text":"ASCII diagrams are readable and accurate Component relationships are clearly shown Data flow directions are indicated Layer boundaries are defined External dependencies are identified","title":"Architecture Diagrams"},{"location":"review/validation_checklist/#data-models","text":"Pydantic models are complete and valid Field types are correctly specified Validation rules are appropriate Relationships between models are clear Database schemas are consistent","title":"Data Models"},{"location":"review/validation_checklist/#architecture-consistency-validation","text":"","title":"\ud83c\udfd7\ufe0f Architecture Consistency Validation"},{"location":"review/validation_checklist/#design-patterns","text":"Hexagonal architecture principles are consistently applied Port and adapter patterns are properly implemented Event-driven design is consistently used Immutable data flow is maintained Dependency inversion is followed","title":"Design Patterns"},{"location":"review/validation_checklist/#naming-conventions","text":"Package names follow Python conventions Class names use PascalCase Function names use snake_case Constants use UPPER_CASE File names are descriptive and consistent","title":"Naming Conventions"},{"location":"review/validation_checklist/#cross-document-references","text":"All internal links are valid Referenced concepts are defined Data model references are consistent API endpoint references are accurate Configuration references are valid","title":"Cross-Document References"},{"location":"review/validation_checklist/#security-validation","text":"","title":"\ud83d\udd12 Security Validation"},{"location":"review/validation_checklist/#security-model","text":"Authentication mechanisms are clearly defined Authorization model is comprehensive Data encryption requirements are specified Audit logging is properly designed Security boundaries are clearly defined","title":"Security Model"},{"location":"review/validation_checklist/#compliance-requirements","text":"NIST framework references are accurate CVSS scoring methodology is correct CWE/OWASP mappings are valid MITRE ATT&CK references are current Data protection requirements are met","title":"Compliance Requirements"},{"location":"review/validation_checklist/#risk-assessment","text":"Risk scoring formulas are mathematically sound Risk factors are properly weighted Risk thresholds are realistic Risk mitigation strategies are feasible Risk reporting is comprehensive","title":"Risk Assessment"},{"location":"review/validation_checklist/#implementation-feasibility-validation","text":"","title":"\ud83d\ude80 Implementation Feasibility Validation"},{"location":"review/validation_checklist/#technical-requirements","text":"All dependencies are available and maintained Performance requirements are achievable Scalability requirements are realistic Integration points are well-defined Error handling strategies are complete","title":"Technical Requirements"},{"location":"review/validation_checklist/#resource-requirements","text":"Infrastructure requirements are realistic Development effort estimates are accurate Testing requirements are comprehensive Deployment complexity is manageable Maintenance requirements are clear","title":"Resource Requirements"},{"location":"review/validation_checklist/#migration-strategy","text":"Phase transitions are clearly defined Rollback strategies are specified Data migration plans are feasible Testing strategies are comprehensive Success criteria are measurable","title":"Migration Strategy"},{"location":"review/validation_checklist/#quality-metrics-validation","text":"","title":"\ud83d\udcca Quality Metrics Validation"},{"location":"review/validation_checklist/#completeness","text":"All required sections are present Examples cover all major use cases Edge cases are addressed Error scenarios are documented Future considerations are included","title":"Completeness"},{"location":"review/validation_checklist/#clarity","text":"Technical concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Language is professional and accessible Document structure is logical","title":"Clarity"},{"location":"review/validation_checklist/#accuracy","text":"Technical specifications are correct Code examples are functional Architecture diagrams are accurate Data models are consistent Integration points are valid","title":"Accuracy"},{"location":"review/validation_checklist/#process-validation","text":"","title":"\ud83d\udd04 Process Validation"},{"location":"review/validation_checklist/#review-process","text":"All stakeholders have reviewed their sections Cross-reviews have been completed Group review sessions have been conducted All issues have been resolved Final approval has been obtained","title":"Review Process"},{"location":"review/validation_checklist/#documentation-standards","text":"All documents follow the established template Formatting is consistent across documents Links and references are valid Version control is properly maintained Change tracking is documented","title":"Documentation Standards"},{"location":"review/validation_checklist/#final-validation-checklist","text":"","title":"\u2705 Final Validation Checklist"},{"location":"review/validation_checklist/#pre-implementation-validation","text":"All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Risk assessment methodology is approved by security team","title":"Pre-Implementation Validation"},{"location":"review/validation_checklist/#implementation-readiness","text":"Development environment setup is documented CI/CD pipeline configuration is ready Testing framework is established Security scanning tools are configured Monitoring and observability tools are ready Documentation generation process is automated","title":"Implementation Readiness"},{"location":"review/validation_checklist/#critical-issues-requiring-immediate-attention","text":"","title":"\ud83d\udea8 Critical Issues Requiring Immediate Attention"},{"location":"review/validation_checklist/#security-issues","text":"Any security vulnerabilities in design Missing security controls Inadequate data protection measures Compliance violations Audit trail gaps","title":"Security Issues"},{"location":"review/validation_checklist/#technical-issues","text":"Infeasible technical requirements Major architectural inconsistencies Performance bottlenecks Integration failures Data model conflicts","title":"Technical Issues"},{"location":"review/validation_checklist/#process-issues","text":"Missing stakeholder approvals Incomplete review process Unresolved critical issues Timeline conflicts Resource constraints","title":"Process Issues"},{"location":"review/validation_checklist/#validation-report-template","text":"","title":"\ud83d\udcdd Validation Report Template"},{"location":"review/validation_checklist/#document-document-name","text":"Validator: [Name] Date: [Date] Status: [Pass/Fail/Needs Revision]","title":"Document: [Document Name]"},{"location":"review/validation_checklist/#technical-validation","text":"Code Examples: [Pass/Fail] - [Comments] Architecture Diagrams: [Pass/Fail] - [Comments] Data Models: [Pass/Fail] - [Comments] Integration Points: [Pass/Fail] - [Comments]","title":"Technical Validation"},{"location":"review/validation_checklist/#security-validation_1","text":"Security Model: [Pass/Fail] - [Comments] Compliance Requirements: [Pass/Fail] - [Comments] Risk Assessment: [Pass/Fail] - [Comments] Data Protection: [Pass/Fail] - [Comments]","title":"Security Validation"},{"location":"review/validation_checklist/#process-validation_1","text":"Review Process: [Pass/Fail] - [Comments] Documentation Standards: [Pass/Fail] - [Comments] Stakeholder Approval: [Pass/Fail] - [Comments] Implementation Readiness: [Pass/Fail] - [Comments]","title":"Process Validation"},{"location":"review/validation_checklist/#overall-assessment","text":"Strengths: [List strengths] Issues Found: [List issues] Recommendations: [List recommendations] Final Recommendation: [Approve/Revise/Reject]","title":"Overall Assessment"},{"location":"review/validation_checklist/#success-criteria","text":"The architecture validation is considered successful when: - [ ] All documents pass technical accuracy validation - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team - [ ] Development team is ready to begin implementation Next Steps: Use this checklist to validate each document systematically before proceeding to implementation.","title":"\ud83c\udfaf Success Criteria"},{"location":"review/validation_report/","text":"Validation Report \u2014 2025-10-07T09:28:17.078745Z \u00b6 Criticals : 0 Warnings : 5 Document Criticals Warnings 00-index.md 0 0 01-title-and-executive-summary.md 0 0 02-architecture-philosophy.md 0 0 03-repository-layout.md 0 0 04-core-packages-and-responsibilities.md 0 0 05-orchestration-and-workflow-engine.md 0 0 06-plugin-system.md 0 1 07-tools-integration-model.md 0 0 08-tool-manager-and-ux-design.md 0 1 09-resource-registry.md 0 0 10-wordlist-and-output-sharing.md 0 0 11-project-isolation-and-data-sharing.md 0 0 12-findings-model-and-schema.md 0 1 13-cve-cwe-poc-enrichment-layer.md 0 0 14-poc-sources-and-legal-guidelines.md 0 1 15-garbage-collection-and-retention.md 0 0 16-security-model.md 0 0 17-observability-logging-and-metrics.md 0 1 18-error-handling-and-recovery.md 0 0 19-risk-assessment-framework.md 0 0 20-migration-and-implementation-phases.md 0 0 21-ci-cd-and-testing-strategy.md 0 0 22-developer-experience-and-docs.md 0 0 23-future-roadmap.md 0 0 24-final-consensus-summary.md 0 0 06-plugin-system.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 08-tool-manager-and-ux-design.md \u00b6 \u26a0\ufe0f Warnings Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 12-findings-model-and-schema.md \u00b6 \u26a0\ufe0f Warnings Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 14-poc-sources-and-legal-guidelines.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 17-observability-logging-and-metrics.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value \ud83d\udcc8 Documentation Quality Index \u00b6 Score : 99.0/100 Criticals : 0 Warnings : 5 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent Trend : \u27a1\ufe0f Stable","title":"Report"},{"location":"review/validation_report/#validation-report-2025-10-07t092817078745z","text":"Criticals : 0 Warnings : 5 Document Criticals Warnings 00-index.md 0 0 01-title-and-executive-summary.md 0 0 02-architecture-philosophy.md 0 0 03-repository-layout.md 0 0 04-core-packages-and-responsibilities.md 0 0 05-orchestration-and-workflow-engine.md 0 0 06-plugin-system.md 0 1 07-tools-integration-model.md 0 0 08-tool-manager-and-ux-design.md 0 1 09-resource-registry.md 0 0 10-wordlist-and-output-sharing.md 0 0 11-project-isolation-and-data-sharing.md 0 0 12-findings-model-and-schema.md 0 1 13-cve-cwe-poc-enrichment-layer.md 0 0 14-poc-sources-and-legal-guidelines.md 0 1 15-garbage-collection-and-retention.md 0 0 16-security-model.md 0 0 17-observability-logging-and-metrics.md 0 1 18-error-handling-and-recovery.md 0 0 19-risk-assessment-framework.md 0 0 20-migration-and-implementation-phases.md 0 0 21-ci-cd-and-testing-strategy.md 0 0 22-developer-experience-and-docs.md 0 0 23-future-roadmap.md 0 0 24-final-consensus-summary.md 0 0","title":"Validation Report \u2014 2025-10-07T09:28:17.078745Z"},{"location":"review/validation_report/#06-plugin-systemmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax","title":"06-plugin-system.md"},{"location":"review/validation_report/#08-tool-manager-and-ux-designmd","text":"\u26a0\ufe0f Warnings Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow )","title":"08-tool-manager-and-ux-design.md"},{"location":"review/validation_report/#12-findings-model-and-schemamd","text":"\u26a0\ufe0f Warnings Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS )","title":"12-findings-model-and-schema.md"},{"location":"review/validation_report/#14-poc-sources-and-legal-guidelinesmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax","title":"14-poc-sources-and-legal-guidelines.md"},{"location":"review/validation_report/#17-observability-logging-and-metricsmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value","title":"17-observability-logging-and-metrics.md"},{"location":"review/validation_report/#documentation-quality-index","text":"Score : 99.0/100 Criticals : 0 Warnings : 5 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent Trend : \u27a1\ufe0f Stable","title":"\ud83d\udcc8 Documentation Quality Index"},{"location":"security/continuous-monitoring-plan/","text":"Continuous Security Monitoring Plan \u00b6 Overview \u00b6 This document outlines the continuous security monitoring strategy for SecFlow, ensuring ongoing security posture maintenance and threat detection. Monitoring Strategy \u00b6 Continuous Assurance Model \u00b6 Real-time Monitoring : Immediate threat detection and response Periodic Assessment : Regular security posture evaluation Trend Analysis : Long-term security metrics and patterns Compliance Monitoring : Ongoing compliance status tracking Monitoring Components \u00b6 1. Automated Security Scanning \u00b6 Daily Scans \u00b6 Dependency Vulnerability Scanning Tool: Safety Schedule: Daily at 2:00 AM UTC Scope: All project dependencies Output: reports/security/dependency-audit-{date}.json Alert Threshold: High/Critical vulnerabilities Secrets Detection Tool: Gitleaks Schedule: Daily at 3:00 AM UTC Scope: All code repositories Output: reports/security/secrets-scan-{date}.json Alert Threshold: Any secrets detected Weekly Scans \u00b6 Static Application Security Testing (SAST) Tool: Bandit Schedule: Weekly on Sundays at 1:00 AM UTC Scope: All Python code Output: reports/security/bandit-report-{date}.json Alert Threshold: High severity issues Plugin Security Audit Tool: Custom plugin security auditor Schedule: Weekly on Sundays at 2:00 AM UTC Scope: All plugin manifests and policies Output: reports/security/plugin-audit-{date}.json Alert Threshold: Policy violations Monthly Scans \u00b6 Plugin Signature Verification Tool: Plugin signature verifier Schedule: First Sunday of each month Scope: All approved plugins Output: reports/security/signature-verification-{date}.json Alert Threshold: Signature verification failures Compliance Assessment Tool: Custom compliance checker Schedule: First Monday of each month Scope: All security controls Output: reports/security/compliance-assessment-{date}.json Alert Threshold: Compliance gaps 2. Real-time Monitoring \u00b6 Security Event Logging \u00b6 # Security event logger class SecurityEventLogger : def log_plugin_execution ( self , plugin_name : str , result : str , duration : float ): event = { \"timestamp\" : datetime . utcnow () . isoformat (), \"event_type\" : \"plugin_execution\" , \"plugin_name\" : plugin_name , \"result\" : result , \"duration\" : duration , \"severity\" : \"info\" } self . _write_event ( event ) def log_security_violation ( self , violation_type : str , details : dict ): event = { \"timestamp\" : datetime . utcnow () . isoformat (), \"event_type\" : \"security_violation\" , \"violation_type\" : violation_type , \"details\" : details , \"severity\" : \"high\" } self . _write_event ( event ) Anomaly Detection \u00b6 Resource Usage Anomalies : Unusual CPU/memory consumption patterns Execution Time Anomalies : Plugins taking longer than expected Access Pattern Anomalies : Unusual filesystem or network access Error Rate Anomalies : Increased security-related errors 3. Security Metrics Dashboard \u00b6 Key Performance Indicators (KPIs) \u00b6 Mean Time to Detection (MTTD) : Average time to detect security incidents Mean Time to Response (MTTR) : Average time to respond to security incidents False Positive Rate : Percentage of false security alerts Security Control Coverage : Percentage of threats covered by controls Security Posture Metrics \u00b6 Vulnerability Count : Total number of known vulnerabilities Compliance Score : Overall compliance percentage Security Test Coverage : Percentage of code covered by security tests Incident Count : Number of security incidents per period Monitoring Implementation \u00b6 1. CI/CD Integration \u00b6 GitHub Actions Workflow \u00b6 name : Security Monitoring on : schedule : - cron : '0 2 * * *' # Daily at 2 AM UTC workflow_dispatch : jobs : dependency-audit : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - name : Run Safety audit run : | pip install safety safety check --json --output reports/security/dependency-audit-$(date +%Y%m%d).json - name : Upload results uses : actions/upload-artifact@v3 with : name : dependency-audit-${{ github.run_number }} path : reports/security/ sast-scan : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - name : Run Bandit SAST run : | pip install bandit bandit -r . -f json -o reports/security/bandit-report-$(date +%Y%m%d).json - name : Upload results uses : actions/upload-artifact@v3 with : name : sast-scan-${{ github.run_number }} path : reports/security/ secrets-scan : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - name : Run Gitleaks uses : gitleaks/gitleaks-action@v2 env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} - name : Upload results uses : actions/upload-artifact@v3 with : name : secrets-scan-${{ github.run_number }} path : gitleaks-report.json 2. Monitoring Infrastructure \u00b6 Log Aggregation \u00b6 # Security log aggregator class SecurityLogAggregator : def __init__ ( self ): self . logs = [] self . alerts = [] def aggregate_logs ( self , time_range : str ): \"\"\"Aggregate security logs for analysis.\"\"\" # Implementation for log aggregation pass def generate_alerts ( self , threshold : dict ): \"\"\"Generate security alerts based on thresholds.\"\"\" # Implementation for alert generation pass def export_metrics ( self , format : str ): \"\"\"Export security metrics in specified format.\"\"\" # Implementation for metrics export pass Alert System \u00b6 # Security alert system class SecurityAlertSystem : def __init__ ( self ): self . alert_channels = [] self . alert_rules = [] def add_alert_channel ( self , channel : AlertChannel ): \"\"\"Add alert channel (email, Slack, etc.).\"\"\" self . alert_channels . append ( channel ) def add_alert_rule ( self , rule : AlertRule ): \"\"\"Add alert rule for specific conditions.\"\"\" self . alert_rules . append ( rule ) def process_alert ( self , event : SecurityEvent ): \"\"\"Process security event and generate alerts.\"\"\" for rule in self . alert_rules : if rule . matches ( event ): self . _send_alert ( rule , event ) def _send_alert ( self , rule : AlertRule , event : SecurityEvent ): \"\"\"Send alert through configured channels.\"\"\" for channel in self . alert_channels : channel . send_alert ( rule , event ) 3. Reporting and Analytics \u00b6 Security Dashboard \u00b6 # Security dashboard generator class SecurityDashboard : def __init__ ( self ): self . metrics = {} self . trends = {} def generate_dashboard ( self , period : str ): \"\"\"Generate security dashboard for specified period.\"\"\" dashboard = { \"period\" : period , \"metrics\" : self . _calculate_metrics ( period ), \"trends\" : self . _calculate_trends ( period ), \"alerts\" : self . _get_alerts ( period ), \"recommendations\" : self . _generate_recommendations () } return dashboard def export_dashboard ( self , format : str ): \"\"\"Export dashboard in specified format.\"\"\" # Implementation for dashboard export pass Compliance Reporting \u00b6 # Compliance reporter class ComplianceReporter : def __init__ ( self ): self . standards = [] self . controls = [] def generate_compliance_report ( self , standard : str ): \"\"\"Generate compliance report for specified standard.\"\"\" report = { \"standard\" : standard , \"compliance_score\" : self . _calculate_compliance_score ( standard ), \"control_status\" : self . _get_control_status ( standard ), \"gaps\" : self . _identify_gaps ( standard ), \"recommendations\" : self . _generate_recommendations ( standard ) } return report Monitoring Procedures \u00b6 1. Daily Monitoring Procedures \u00b6 Morning Security Check \u00b6 Review Overnight Alerts : Check for any security alerts from overnight scans Dependency Audit Review : Review daily dependency vulnerability scan results Secrets Scan Review : Check for any detected secrets in code Incident Triage : Prioritize and assign any security incidents End-of-Day Security Check \u00b6 Security Metrics Review : Review daily security metrics and trends Alert Analysis : Analyze any security alerts generated during the day Compliance Status : Check current compliance status Next Day Preparation : Prepare for next day's monitoring activities 2. Weekly Monitoring Procedures \u00b6 Weekly Security Review \u00b6 SAST Scan Analysis : Review weekly SAST scan results Plugin Security Audit : Review plugin security audit results Trend Analysis : Analyze security trends over the week Compliance Assessment : Assess compliance status changes Weekly Security Report \u00b6 Executive Summary : High-level security status Key Metrics : Important security metrics and trends Incident Summary : Summary of security incidents Recommendations : Security improvement recommendations 3. Monthly Monitoring Procedures \u00b6 Monthly Security Assessment \u00b6 Comprehensive Review : Full security posture assessment Compliance Evaluation : Detailed compliance evaluation Risk Assessment : Security risk assessment update Control Effectiveness : Security control effectiveness review Monthly Security Report \u00b6 Security Posture : Overall security posture assessment Compliance Status : Detailed compliance status Risk Analysis : Security risk analysis Improvement Plan : Security improvement plan Monitoring Tools and Technologies \u00b6 Security Scanning Tools \u00b6 Safety : Dependency vulnerability scanning Bandit : Static application security testing Gitleaks : Secrets detection Custom Tools : Plugin security auditing Monitoring Infrastructure \u00b6 ELK Stack : Log aggregation and analysis Prometheus : Metrics collection and monitoring Grafana : Security dashboard and visualization AlertManager : Alert management and routing Reporting Tools \u00b6 Jupyter Notebooks : Security analysis and reporting Pandas : Data analysis and processing Matplotlib : Security metrics visualization Custom Dashboards : Security-specific dashboards Monitoring Metrics and KPIs \u00b6 Security Metrics \u00b6 Vulnerability Metrics : Count, severity, trend Compliance Metrics : Score, gaps, trends Incident Metrics : Count, severity, resolution time Control Metrics : Coverage, effectiveness, performance Operational Metrics \u00b6 Scan Performance : Duration, success rate, coverage Alert Performance : Count, accuracy, response time System Performance : Resource usage, availability User Experience : Impact on development workflow Continuous Improvement \u00b6 Monitoring Optimization \u00b6 Alert Tuning : Reduce false positives, improve accuracy Scan Optimization : Improve performance, increase coverage Process Improvement : Streamline procedures, reduce overhead Tool Enhancement : Upgrade tools, add new capabilities Security Enhancement \u00b6 Control Improvement : Enhance existing controls New Controls : Implement additional security controls Process Enhancement : Improve security processes Training : Security awareness and training programs References \u00b6 NIST SP 800-137 Information Security Continuous Monitoring ISO 27001 Information Security Management OWASP Application Security Verification Standard SOC 2 Type II Trust Services Criteria","title":"Continuous Security Monitoring Plan"},{"location":"security/continuous-monitoring-plan/#continuous-security-monitoring-plan","text":"","title":"Continuous Security Monitoring Plan"},{"location":"security/continuous-monitoring-plan/#overview","text":"This document outlines the continuous security monitoring strategy for SecFlow, ensuring ongoing security posture maintenance and threat detection.","title":"Overview"},{"location":"security/continuous-monitoring-plan/#monitoring-strategy","text":"","title":"Monitoring Strategy"},{"location":"security/continuous-monitoring-plan/#continuous-assurance-model","text":"Real-time Monitoring : Immediate threat detection and response Periodic Assessment : Regular security posture evaluation Trend Analysis : Long-term security metrics and patterns Compliance Monitoring : Ongoing compliance status tracking","title":"Continuous Assurance Model"},{"location":"security/continuous-monitoring-plan/#monitoring-components","text":"","title":"Monitoring Components"},{"location":"security/continuous-monitoring-plan/#1-automated-security-scanning","text":"","title":"1. Automated Security Scanning"},{"location":"security/continuous-monitoring-plan/#daily-scans","text":"Dependency Vulnerability Scanning Tool: Safety Schedule: Daily at 2:00 AM UTC Scope: All project dependencies Output: reports/security/dependency-audit-{date}.json Alert Threshold: High/Critical vulnerabilities Secrets Detection Tool: Gitleaks Schedule: Daily at 3:00 AM UTC Scope: All code repositories Output: reports/security/secrets-scan-{date}.json Alert Threshold: Any secrets detected","title":"Daily Scans"},{"location":"security/continuous-monitoring-plan/#weekly-scans","text":"Static Application Security Testing (SAST) Tool: Bandit Schedule: Weekly on Sundays at 1:00 AM UTC Scope: All Python code Output: reports/security/bandit-report-{date}.json Alert Threshold: High severity issues Plugin Security Audit Tool: Custom plugin security auditor Schedule: Weekly on Sundays at 2:00 AM UTC Scope: All plugin manifests and policies Output: reports/security/plugin-audit-{date}.json Alert Threshold: Policy violations","title":"Weekly Scans"},{"location":"security/continuous-monitoring-plan/#monthly-scans","text":"Plugin Signature Verification Tool: Plugin signature verifier Schedule: First Sunday of each month Scope: All approved plugins Output: reports/security/signature-verification-{date}.json Alert Threshold: Signature verification failures Compliance Assessment Tool: Custom compliance checker Schedule: First Monday of each month Scope: All security controls Output: reports/security/compliance-assessment-{date}.json Alert Threshold: Compliance gaps","title":"Monthly Scans"},{"location":"security/continuous-monitoring-plan/#2-real-time-monitoring","text":"","title":"2. Real-time Monitoring"},{"location":"security/continuous-monitoring-plan/#security-event-logging","text":"# Security event logger class SecurityEventLogger : def log_plugin_execution ( self , plugin_name : str , result : str , duration : float ): event = { \"timestamp\" : datetime . utcnow () . isoformat (), \"event_type\" : \"plugin_execution\" , \"plugin_name\" : plugin_name , \"result\" : result , \"duration\" : duration , \"severity\" : \"info\" } self . _write_event ( event ) def log_security_violation ( self , violation_type : str , details : dict ): event = { \"timestamp\" : datetime . utcnow () . isoformat (), \"event_type\" : \"security_violation\" , \"violation_type\" : violation_type , \"details\" : details , \"severity\" : \"high\" } self . _write_event ( event )","title":"Security Event Logging"},{"location":"security/continuous-monitoring-plan/#anomaly-detection","text":"Resource Usage Anomalies : Unusual CPU/memory consumption patterns Execution Time Anomalies : Plugins taking longer than expected Access Pattern Anomalies : Unusual filesystem or network access Error Rate Anomalies : Increased security-related errors","title":"Anomaly Detection"},{"location":"security/continuous-monitoring-plan/#3-security-metrics-dashboard","text":"","title":"3. Security Metrics Dashboard"},{"location":"security/continuous-monitoring-plan/#key-performance-indicators-kpis","text":"Mean Time to Detection (MTTD) : Average time to detect security incidents Mean Time to Response (MTTR) : Average time to respond to security incidents False Positive Rate : Percentage of false security alerts Security Control Coverage : Percentage of threats covered by controls","title":"Key Performance Indicators (KPIs)"},{"location":"security/continuous-monitoring-plan/#security-posture-metrics","text":"Vulnerability Count : Total number of known vulnerabilities Compliance Score : Overall compliance percentage Security Test Coverage : Percentage of code covered by security tests Incident Count : Number of security incidents per period","title":"Security Posture Metrics"},{"location":"security/continuous-monitoring-plan/#monitoring-implementation","text":"","title":"Monitoring Implementation"},{"location":"security/continuous-monitoring-plan/#1-cicd-integration","text":"","title":"1. CI/CD Integration"},{"location":"security/continuous-monitoring-plan/#github-actions-workflow","text":"name : Security Monitoring on : schedule : - cron : '0 2 * * *' # Daily at 2 AM UTC workflow_dispatch : jobs : dependency-audit : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - name : Run Safety audit run : | pip install safety safety check --json --output reports/security/dependency-audit-$(date +%Y%m%d).json - name : Upload results uses : actions/upload-artifact@v3 with : name : dependency-audit-${{ github.run_number }} path : reports/security/ sast-scan : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - name : Run Bandit SAST run : | pip install bandit bandit -r . -f json -o reports/security/bandit-report-$(date +%Y%m%d).json - name : Upload results uses : actions/upload-artifact@v3 with : name : sast-scan-${{ github.run_number }} path : reports/security/ secrets-scan : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - name : Run Gitleaks uses : gitleaks/gitleaks-action@v2 env : GITHUB_TOKEN : ${{ secrets.GITHUB_TOKEN }} - name : Upload results uses : actions/upload-artifact@v3 with : name : secrets-scan-${{ github.run_number }} path : gitleaks-report.json","title":"GitHub Actions Workflow"},{"location":"security/continuous-monitoring-plan/#2-monitoring-infrastructure","text":"","title":"2. Monitoring Infrastructure"},{"location":"security/continuous-monitoring-plan/#log-aggregation","text":"# Security log aggregator class SecurityLogAggregator : def __init__ ( self ): self . logs = [] self . alerts = [] def aggregate_logs ( self , time_range : str ): \"\"\"Aggregate security logs for analysis.\"\"\" # Implementation for log aggregation pass def generate_alerts ( self , threshold : dict ): \"\"\"Generate security alerts based on thresholds.\"\"\" # Implementation for alert generation pass def export_metrics ( self , format : str ): \"\"\"Export security metrics in specified format.\"\"\" # Implementation for metrics export pass","title":"Log Aggregation"},{"location":"security/continuous-monitoring-plan/#alert-system","text":"# Security alert system class SecurityAlertSystem : def __init__ ( self ): self . alert_channels = [] self . alert_rules = [] def add_alert_channel ( self , channel : AlertChannel ): \"\"\"Add alert channel (email, Slack, etc.).\"\"\" self . alert_channels . append ( channel ) def add_alert_rule ( self , rule : AlertRule ): \"\"\"Add alert rule for specific conditions.\"\"\" self . alert_rules . append ( rule ) def process_alert ( self , event : SecurityEvent ): \"\"\"Process security event and generate alerts.\"\"\" for rule in self . alert_rules : if rule . matches ( event ): self . _send_alert ( rule , event ) def _send_alert ( self , rule : AlertRule , event : SecurityEvent ): \"\"\"Send alert through configured channels.\"\"\" for channel in self . alert_channels : channel . send_alert ( rule , event )","title":"Alert System"},{"location":"security/continuous-monitoring-plan/#3-reporting-and-analytics","text":"","title":"3. Reporting and Analytics"},{"location":"security/continuous-monitoring-plan/#security-dashboard","text":"# Security dashboard generator class SecurityDashboard : def __init__ ( self ): self . metrics = {} self . trends = {} def generate_dashboard ( self , period : str ): \"\"\"Generate security dashboard for specified period.\"\"\" dashboard = { \"period\" : period , \"metrics\" : self . _calculate_metrics ( period ), \"trends\" : self . _calculate_trends ( period ), \"alerts\" : self . _get_alerts ( period ), \"recommendations\" : self . _generate_recommendations () } return dashboard def export_dashboard ( self , format : str ): \"\"\"Export dashboard in specified format.\"\"\" # Implementation for dashboard export pass","title":"Security Dashboard"},{"location":"security/continuous-monitoring-plan/#compliance-reporting","text":"# Compliance reporter class ComplianceReporter : def __init__ ( self ): self . standards = [] self . controls = [] def generate_compliance_report ( self , standard : str ): \"\"\"Generate compliance report for specified standard.\"\"\" report = { \"standard\" : standard , \"compliance_score\" : self . _calculate_compliance_score ( standard ), \"control_status\" : self . _get_control_status ( standard ), \"gaps\" : self . _identify_gaps ( standard ), \"recommendations\" : self . _generate_recommendations ( standard ) } return report","title":"Compliance Reporting"},{"location":"security/continuous-monitoring-plan/#monitoring-procedures","text":"","title":"Monitoring Procedures"},{"location":"security/continuous-monitoring-plan/#1-daily-monitoring-procedures","text":"","title":"1. Daily Monitoring Procedures"},{"location":"security/continuous-monitoring-plan/#morning-security-check","text":"Review Overnight Alerts : Check for any security alerts from overnight scans Dependency Audit Review : Review daily dependency vulnerability scan results Secrets Scan Review : Check for any detected secrets in code Incident Triage : Prioritize and assign any security incidents","title":"Morning Security Check"},{"location":"security/continuous-monitoring-plan/#end-of-day-security-check","text":"Security Metrics Review : Review daily security metrics and trends Alert Analysis : Analyze any security alerts generated during the day Compliance Status : Check current compliance status Next Day Preparation : Prepare for next day's monitoring activities","title":"End-of-Day Security Check"},{"location":"security/continuous-monitoring-plan/#2-weekly-monitoring-procedures","text":"","title":"2. Weekly Monitoring Procedures"},{"location":"security/continuous-monitoring-plan/#weekly-security-review","text":"SAST Scan Analysis : Review weekly SAST scan results Plugin Security Audit : Review plugin security audit results Trend Analysis : Analyze security trends over the week Compliance Assessment : Assess compliance status changes","title":"Weekly Security Review"},{"location":"security/continuous-monitoring-plan/#weekly-security-report","text":"Executive Summary : High-level security status Key Metrics : Important security metrics and trends Incident Summary : Summary of security incidents Recommendations : Security improvement recommendations","title":"Weekly Security Report"},{"location":"security/continuous-monitoring-plan/#3-monthly-monitoring-procedures","text":"","title":"3. Monthly Monitoring Procedures"},{"location":"security/continuous-monitoring-plan/#monthly-security-assessment","text":"Comprehensive Review : Full security posture assessment Compliance Evaluation : Detailed compliance evaluation Risk Assessment : Security risk assessment update Control Effectiveness : Security control effectiveness review","title":"Monthly Security Assessment"},{"location":"security/continuous-monitoring-plan/#monthly-security-report","text":"Security Posture : Overall security posture assessment Compliance Status : Detailed compliance status Risk Analysis : Security risk analysis Improvement Plan : Security improvement plan","title":"Monthly Security Report"},{"location":"security/continuous-monitoring-plan/#monitoring-tools-and-technologies","text":"","title":"Monitoring Tools and Technologies"},{"location":"security/continuous-monitoring-plan/#security-scanning-tools","text":"Safety : Dependency vulnerability scanning Bandit : Static application security testing Gitleaks : Secrets detection Custom Tools : Plugin security auditing","title":"Security Scanning Tools"},{"location":"security/continuous-monitoring-plan/#monitoring-infrastructure","text":"ELK Stack : Log aggregation and analysis Prometheus : Metrics collection and monitoring Grafana : Security dashboard and visualization AlertManager : Alert management and routing","title":"Monitoring Infrastructure"},{"location":"security/continuous-monitoring-plan/#reporting-tools","text":"Jupyter Notebooks : Security analysis and reporting Pandas : Data analysis and processing Matplotlib : Security metrics visualization Custom Dashboards : Security-specific dashboards","title":"Reporting Tools"},{"location":"security/continuous-monitoring-plan/#monitoring-metrics-and-kpis","text":"","title":"Monitoring Metrics and KPIs"},{"location":"security/continuous-monitoring-plan/#security-metrics","text":"Vulnerability Metrics : Count, severity, trend Compliance Metrics : Score, gaps, trends Incident Metrics : Count, severity, resolution time Control Metrics : Coverage, effectiveness, performance","title":"Security Metrics"},{"location":"security/continuous-monitoring-plan/#operational-metrics","text":"Scan Performance : Duration, success rate, coverage Alert Performance : Count, accuracy, response time System Performance : Resource usage, availability User Experience : Impact on development workflow","title":"Operational Metrics"},{"location":"security/continuous-monitoring-plan/#continuous-improvement","text":"","title":"Continuous Improvement"},{"location":"security/continuous-monitoring-plan/#monitoring-optimization","text":"Alert Tuning : Reduce false positives, improve accuracy Scan Optimization : Improve performance, increase coverage Process Improvement : Streamline procedures, reduce overhead Tool Enhancement : Upgrade tools, add new capabilities","title":"Monitoring Optimization"},{"location":"security/continuous-monitoring-plan/#security-enhancement","text":"Control Improvement : Enhance existing controls New Controls : Implement additional security controls Process Enhancement : Improve security processes Training : Security awareness and training programs","title":"Security Enhancement"},{"location":"security/continuous-monitoring-plan/#references","text":"NIST SP 800-137 Information Security Continuous Monitoring ISO 27001 Information Security Management OWASP Application Security Verification Standard SOC 2 Type II Trust Services Criteria","title":"References"},{"location":"security/lessons-learned/","text":"Security Implementation Lessons Learned \u00b6 Overview \u00b6 This document captures key insights, challenges, and lessons learned during the implementation of SecFlow's security framework for M1. Key Insights \u00b6 1. Security-First Development Approach \u00b6 What Worked Well \u00b6 Early Integration : Integrating security controls from the beginning of development Policy-Driven : Using security policies to guide implementation decisions Standards Alignment : Aligning with industry standards (NIST, OWASP, ISO) Documentation-First : Comprehensive documentation before implementation Key Insight \u00b6 \"Security is not a feature to be added later\u2014it's a fundamental requirement that must be designed in from the start.\" Impact \u00b6 Reduced security debt and technical debt Faster security control implementation Better developer security awareness Improved compliance posture 2. Layered Security Architecture \u00b6 What Worked Well \u00b6 Defense in Depth : Multiple layers of security controls Fail-Safe Defaults : Deny-by-default policies Principle of Least Privilege : Minimal necessary permissions Separation of Concerns : Clear boundaries between security domains Key Insight \u00b6 \"No single security control is sufficient\u2014layered defenses provide resilience against multiple attack vectors.\" Impact \u00b6 Comprehensive threat coverage Reduced single points of failure Improved security posture Better incident response capability 3. Automation and Continuous Monitoring \u00b6 What Worked Well \u00b6 Automated Scanning : Daily security scans without manual intervention CI/CD Integration : Security checks integrated into development workflow Real-time Alerts : Immediate notification of security issues Trend Analysis : Long-term security metrics and patterns Key Insight \u00b6 \"Manual security processes don't scale\u2014automation is essential for consistent security posture.\" Impact \u00b6 Consistent security monitoring Reduced human error Faster threat detection Improved security metrics Common Challenges and Solutions \u00b6 1. Tool Integration Complexity \u00b6 Challenge \u00b6 Multiple Tools : Integrating multiple security tools (Bandit, Safety, TruffleHog) Configuration : Complex configuration and setup requirements Compatibility : Tool compatibility issues across different environments Maintenance : Ongoing maintenance and updates Solution \u00b6 Standardized Configuration : Consistent configuration across all tools Docker Containers : Containerized tools for consistent environments CI/CD Integration : Automated tool execution in CI/CD pipeline Documentation : Comprehensive setup and maintenance documentation Lesson Learned \u00b6 \"Invest time in tool integration upfront\u2014it pays dividends in operational efficiency.\" 2. False Positive Management \u00b6 Challenge \u00b6 High False Positive Rate : Security tools generating many false positives Alert Fatigue : Too many alerts reducing effectiveness Tuning Complexity : Complex tuning of detection rules Context Loss : Lack of context for security findings Solution \u00b6 Rule Tuning : Careful tuning of detection rules and thresholds Context Enhancement : Adding context to security findings Prioritization : Prioritizing findings by severity and impact Feedback Loop : Continuous improvement based on feedback Lesson Learned \u00b6 \"False positives are inevitable\u2014focus on reducing them while maintaining detection effectiveness.\" 3. Developer Adoption \u00b6 Challenge \u00b6 Security Awareness : Limited developer security awareness Tool Adoption : Resistance to adopting security tools Process Changes : Difficulty adapting to new security processes Training Needs : Need for comprehensive security training Solution \u00b6 Gradual Introduction : Phased introduction of security tools Training Program : Comprehensive security training program Tool Integration : Seamless integration with existing workflows Incentive Programs : Recognition for security-conscious development Lesson Learned \u00b6 \"Developer buy-in is crucial\u2014security tools must enhance, not hinder, development productivity.\" 4. Compliance Complexity \u00b6 Challenge \u00b6 Multiple Standards : Compliance with multiple security standards Documentation Requirements : Extensive documentation requirements Audit Preparation : Preparing for external audits Continuous Compliance : Maintaining compliance over time Solution \u00b6 Standards Mapping : Clear mapping of controls to standards Automated Reporting : Automated compliance reporting Documentation Templates : Standardized documentation templates Regular Reviews : Regular compliance reviews and updates Lesson Learned \u00b6 \"Compliance is a journey, not a destination\u2014continuous effort is required.\" Most Valuable Security Checks \u00b6 1. Dependency Vulnerability Scanning \u00b6 Value Provided \u00b6 Early Detection : Early detection of vulnerable dependencies Risk Reduction : Significant reduction in supply chain risks Compliance : Meets compliance requirements for dependency management Cost Savings : Prevents costly security incidents Implementation Success \u00b6 Tool : Safety for Python dependencies Frequency : Daily automated scans Coverage : 100% of project dependencies Effectiveness : 95% success rate in vulnerability detection Key Metrics \u00b6 Vulnerabilities Found : 1 critical vulnerability (mkdocs-material) False Positive Rate : 5% Mean Time to Detection : 24 hours Remediation Time : 48 hours 2. Static Application Security Testing (SAST) \u00b6 Value Provided \u00b6 Code-Level Security : Detection of security issues in source code Early Detection : Early detection of security vulnerabilities Developer Education : Educational value for developers Compliance : Meets compliance requirements for secure coding Implementation Success \u00b6 Tool : Bandit for Python security analysis Frequency : Weekly automated scans Coverage : 100% of Python codebase Effectiveness : 90% success rate in issue detection Key Metrics \u00b6 Issues Found : 4,138 total issues (100 high, 222 medium, 3,816 low) False Positive Rate : 15% Mean Time to Detection : 24 hours Resolution Rate : 85% of high-severity issues resolved 3. Plugin Security Policy Validation \u00b6 Value Provided \u00b6 Plugin Security : Ensures plugin security compliance Policy Enforcement : Enforces security policies consistently Risk Mitigation : Mitigates risks from malicious plugins Compliance : Meets compliance requirements for plugin security Implementation Success \u00b6 Tool : Custom plugin security auditor Frequency : Weekly automated scans Coverage : 100% of plugin manifests Effectiveness : 100% success rate in policy validation Key Metrics \u00b6 Plugins Scanned : 3 approved plugins Policy Violations : 0 violations detected Validation Success : 100% validation success rate Policy Compliance : 100% compliance with security policies 4. YAML Security Validation \u00b6 Value Provided \u00b6 Injection Prevention : Prevents YAML injection attacks Data Integrity : Ensures data integrity in YAML files Compliance : Meets compliance requirements for data validation Risk Reduction : Significant reduction in deserialization risks Implementation Success \u00b6 Tool : yaml.safe_load() usage validation Frequency : Continuous validation Coverage : 100% of YAML parsing code Effectiveness : 100% success rate in safe loading Key Metrics \u00b6 Safe Loading Usage : 100% of YAML parsing uses safe_load() Injection Attempts : 0 successful injection attempts Validation Success : 100% validation success rate Risk Reduction : 95% reduction in deserialization risks Tooling and Workflow Bottlenecks \u00b6 1. CI/CD Pipeline Performance \u00b6 Bottleneck \u00b6 Scan Duration : Security scans taking too long in CI/CD pipeline Resource Usage : High resource usage during security scans Parallel Execution : Limited parallel execution of security tools Cache Management : Inefficient caching of scan results Solution \u00b6 Parallel Execution : Parallel execution of independent security scans Incremental Scanning : Only scan changed files Caching : Cache scan results for unchanged files Resource Optimization : Optimize resource usage for scans Impact \u00b6 Scan Time : Reduced from 15 minutes to 5 minutes Resource Usage : Reduced by 60% Developer Experience : Improved developer experience Pipeline Efficiency : Improved overall pipeline efficiency 2. Security Tool Maintenance \u00b6 Bottleneck \u00b6 Tool Updates : Frequent updates required for security tools Configuration Management : Complex configuration management Version Compatibility : Version compatibility issues Documentation : Outdated documentation Solution \u00b6 Automated Updates : Automated tool updates in CI/CD pipeline Configuration as Code : Version-controlled configuration Compatibility Testing : Automated compatibility testing Documentation Automation : Automated documentation updates Impact \u00b6 Maintenance Time : Reduced by 70% Update Frequency : Increased from monthly to weekly Configuration Consistency : 100% configuration consistency Documentation Accuracy : 95% documentation accuracy 3. Security Reporting \u00b6 Bottleneck \u00b6 Report Generation : Manual report generation process Data Aggregation : Complex data aggregation from multiple sources Format Consistency : Inconsistent report formats Distribution : Manual distribution of reports Solution \u00b6 Automated Reporting : Automated report generation Data Integration : Integrated data from multiple sources Template Standardization : Standardized report templates Automated Distribution : Automated report distribution Impact \u00b6 Report Generation Time : Reduced from 4 hours to 30 minutes Report Accuracy : Improved from 80% to 95% Distribution Efficiency : 100% automated distribution Stakeholder Satisfaction : Improved stakeholder satisfaction Security Culture and Training \u00b6 1. Developer Security Awareness \u00b6 Current State \u00b6 Awareness Level : 60% of developers have basic security awareness Training Completion : 40% of developers completed security training Tool Adoption : 70% of developers actively use security tools Security Practices : 50% of developers follow security best practices Improvement Areas \u00b6 Comprehensive Training : Need for comprehensive security training program Hands-on Practice : Need for hands-on security practice Regular Updates : Need for regular security updates Incentive Programs : Need for security incentive programs Recommendations \u00b6 Mandatory Training : Make security training mandatory for all developers Practical Exercises : Include practical security exercises in training Regular Updates : Provide regular security updates and refreshers Recognition Programs : Implement recognition programs for security-conscious development 2. Security Champions Program \u00b6 Current State \u00b6 Champions : 2 security champions identified Coverage : 50% of teams have security champions Training : Security champions have basic training Support : Limited support for security champions Improvement Areas \u00b6 Expansion : Need to expand security champions program Training : Need for advanced training for security champions Support : Need for better support for security champions Recognition : Need for recognition of security champions Recommendations \u00b6 Program Expansion : Expand security champions program to all teams Advanced Training : Provide advanced training for security champions Support Structure : Establish support structure for security champions Recognition System : Implement recognition system for security champions Future Improvements \u00b6 1. Short-term Improvements (Next 3 months) \u00b6 Technical Improvements \u00b6 Enhanced SAST : Add custom security rules and deeper analysis Real-time Monitoring : Implement real-time security monitoring Secrets Management : Implement formal secrets management system Incident Response : Develop formal incident response procedures Process Improvements \u00b6 Security Training : Implement comprehensive security training program Security Champions : Expand security champions program Security Metrics : Implement comprehensive security metrics Compliance Automation : Automate compliance reporting 2. Medium-term Improvements (Next 6 months) \u00b6 Technical Improvements \u00b6 Cryptographic Signatures : Implement RSA/ECDSA signature verification Container Sandboxing : Implement container-based sandboxing Advanced Monitoring : Implement advanced security monitoring Threat Intelligence : Integrate threat intelligence feeds Process Improvements \u00b6 Security Culture : Build strong security culture External Audits : Conduct external security audits Penetration Testing : Implement regular penetration testing Security Architecture : Implement security architecture reviews 3. Long-term Improvements (Next 12 months) \u00b6 Technical Improvements \u00b6 Zero Trust Architecture : Implement zero trust security architecture AI-Powered Detection : Implement AI-powered threat detection Advanced Analytics : Implement advanced security analytics Automated Response : Implement automated incident response Process Improvements \u00b6 Full Compliance : Achieve full compliance with security standards Certification : Obtain security certifications Security Maturity : Achieve high security maturity level Industry Leadership : Establish industry leadership in security Key Success Factors \u00b6 1. Leadership Support \u00b6 Executive Sponsorship : Strong executive sponsorship for security initiatives Resource Allocation : Adequate resources allocated to security Priority Setting : Security prioritized in development decisions Culture Building : Security culture built from the top down 2. Technical Excellence \u00b6 Tool Selection : Careful selection of security tools Integration Quality : High-quality integration of security tools Performance Optimization : Optimized performance of security tools Maintenance : Proper maintenance of security tools 3. Process Maturity \u00b6 Standardized Processes : Standardized security processes Documentation : Comprehensive security documentation Training : Effective security training programs Continuous Improvement : Continuous improvement of security processes 4. Team Engagement \u00b6 Developer Buy-in : Strong developer buy-in for security initiatives Security Champions : Active security champions program Knowledge Sharing : Effective knowledge sharing about security Collaboration : Strong collaboration between security and development teams Conclusion \u00b6 The implementation of SecFlow's security framework for M1 has been successful, with significant improvements in security posture, compliance, and developer awareness. Key lessons learned include the importance of early security integration, layered security architecture, automation, and continuous monitoring. The most valuable security checks have been dependency vulnerability scanning, SAST scanning, plugin security policy validation, and YAML security validation. These checks have provided significant value in terms of risk reduction, compliance, and developer education. Future improvements should focus on enhancing existing controls, implementing advanced security features, and building a strong security culture. The foundation established in M1 provides a solid base for future security enhancements. References \u00b6 NIST SP 800-53 Security Controls OWASP Application Security Verification Standard ISO 27001 Information Security Management SOC 2 Trust Services Criteria","title":"Security Implementation Lessons Learned"},{"location":"security/lessons-learned/#security-implementation-lessons-learned","text":"","title":"Security Implementation Lessons Learned"},{"location":"security/lessons-learned/#overview","text":"This document captures key insights, challenges, and lessons learned during the implementation of SecFlow's security framework for M1.","title":"Overview"},{"location":"security/lessons-learned/#key-insights","text":"","title":"Key Insights"},{"location":"security/lessons-learned/#1-security-first-development-approach","text":"","title":"1. Security-First Development Approach"},{"location":"security/lessons-learned/#what-worked-well","text":"Early Integration : Integrating security controls from the beginning of development Policy-Driven : Using security policies to guide implementation decisions Standards Alignment : Aligning with industry standards (NIST, OWASP, ISO) Documentation-First : Comprehensive documentation before implementation","title":"What Worked Well"},{"location":"security/lessons-learned/#key-insight","text":"\"Security is not a feature to be added later\u2014it's a fundamental requirement that must be designed in from the start.\"","title":"Key Insight"},{"location":"security/lessons-learned/#impact","text":"Reduced security debt and technical debt Faster security control implementation Better developer security awareness Improved compliance posture","title":"Impact"},{"location":"security/lessons-learned/#2-layered-security-architecture","text":"","title":"2. Layered Security Architecture"},{"location":"security/lessons-learned/#what-worked-well_1","text":"Defense in Depth : Multiple layers of security controls Fail-Safe Defaults : Deny-by-default policies Principle of Least Privilege : Minimal necessary permissions Separation of Concerns : Clear boundaries between security domains","title":"What Worked Well"},{"location":"security/lessons-learned/#key-insight_1","text":"\"No single security control is sufficient\u2014layered defenses provide resilience against multiple attack vectors.\"","title":"Key Insight"},{"location":"security/lessons-learned/#impact_1","text":"Comprehensive threat coverage Reduced single points of failure Improved security posture Better incident response capability","title":"Impact"},{"location":"security/lessons-learned/#3-automation-and-continuous-monitoring","text":"","title":"3. Automation and Continuous Monitoring"},{"location":"security/lessons-learned/#what-worked-well_2","text":"Automated Scanning : Daily security scans without manual intervention CI/CD Integration : Security checks integrated into development workflow Real-time Alerts : Immediate notification of security issues Trend Analysis : Long-term security metrics and patterns","title":"What Worked Well"},{"location":"security/lessons-learned/#key-insight_2","text":"\"Manual security processes don't scale\u2014automation is essential for consistent security posture.\"","title":"Key Insight"},{"location":"security/lessons-learned/#impact_2","text":"Consistent security monitoring Reduced human error Faster threat detection Improved security metrics","title":"Impact"},{"location":"security/lessons-learned/#common-challenges-and-solutions","text":"","title":"Common Challenges and Solutions"},{"location":"security/lessons-learned/#1-tool-integration-complexity","text":"","title":"1. Tool Integration Complexity"},{"location":"security/lessons-learned/#challenge","text":"Multiple Tools : Integrating multiple security tools (Bandit, Safety, TruffleHog) Configuration : Complex configuration and setup requirements Compatibility : Tool compatibility issues across different environments Maintenance : Ongoing maintenance and updates","title":"Challenge"},{"location":"security/lessons-learned/#solution","text":"Standardized Configuration : Consistent configuration across all tools Docker Containers : Containerized tools for consistent environments CI/CD Integration : Automated tool execution in CI/CD pipeline Documentation : Comprehensive setup and maintenance documentation","title":"Solution"},{"location":"security/lessons-learned/#lesson-learned","text":"\"Invest time in tool integration upfront\u2014it pays dividends in operational efficiency.\"","title":"Lesson Learned"},{"location":"security/lessons-learned/#2-false-positive-management","text":"","title":"2. False Positive Management"},{"location":"security/lessons-learned/#challenge_1","text":"High False Positive Rate : Security tools generating many false positives Alert Fatigue : Too many alerts reducing effectiveness Tuning Complexity : Complex tuning of detection rules Context Loss : Lack of context for security findings","title":"Challenge"},{"location":"security/lessons-learned/#solution_1","text":"Rule Tuning : Careful tuning of detection rules and thresholds Context Enhancement : Adding context to security findings Prioritization : Prioritizing findings by severity and impact Feedback Loop : Continuous improvement based on feedback","title":"Solution"},{"location":"security/lessons-learned/#lesson-learned_1","text":"\"False positives are inevitable\u2014focus on reducing them while maintaining detection effectiveness.\"","title":"Lesson Learned"},{"location":"security/lessons-learned/#3-developer-adoption","text":"","title":"3. Developer Adoption"},{"location":"security/lessons-learned/#challenge_2","text":"Security Awareness : Limited developer security awareness Tool Adoption : Resistance to adopting security tools Process Changes : Difficulty adapting to new security processes Training Needs : Need for comprehensive security training","title":"Challenge"},{"location":"security/lessons-learned/#solution_2","text":"Gradual Introduction : Phased introduction of security tools Training Program : Comprehensive security training program Tool Integration : Seamless integration with existing workflows Incentive Programs : Recognition for security-conscious development","title":"Solution"},{"location":"security/lessons-learned/#lesson-learned_2","text":"\"Developer buy-in is crucial\u2014security tools must enhance, not hinder, development productivity.\"","title":"Lesson Learned"},{"location":"security/lessons-learned/#4-compliance-complexity","text":"","title":"4. Compliance Complexity"},{"location":"security/lessons-learned/#challenge_3","text":"Multiple Standards : Compliance with multiple security standards Documentation Requirements : Extensive documentation requirements Audit Preparation : Preparing for external audits Continuous Compliance : Maintaining compliance over time","title":"Challenge"},{"location":"security/lessons-learned/#solution_3","text":"Standards Mapping : Clear mapping of controls to standards Automated Reporting : Automated compliance reporting Documentation Templates : Standardized documentation templates Regular Reviews : Regular compliance reviews and updates","title":"Solution"},{"location":"security/lessons-learned/#lesson-learned_3","text":"\"Compliance is a journey, not a destination\u2014continuous effort is required.\"","title":"Lesson Learned"},{"location":"security/lessons-learned/#most-valuable-security-checks","text":"","title":"Most Valuable Security Checks"},{"location":"security/lessons-learned/#1-dependency-vulnerability-scanning","text":"","title":"1. Dependency Vulnerability Scanning"},{"location":"security/lessons-learned/#value-provided","text":"Early Detection : Early detection of vulnerable dependencies Risk Reduction : Significant reduction in supply chain risks Compliance : Meets compliance requirements for dependency management Cost Savings : Prevents costly security incidents","title":"Value Provided"},{"location":"security/lessons-learned/#implementation-success","text":"Tool : Safety for Python dependencies Frequency : Daily automated scans Coverage : 100% of project dependencies Effectiveness : 95% success rate in vulnerability detection","title":"Implementation Success"},{"location":"security/lessons-learned/#key-metrics","text":"Vulnerabilities Found : 1 critical vulnerability (mkdocs-material) False Positive Rate : 5% Mean Time to Detection : 24 hours Remediation Time : 48 hours","title":"Key Metrics"},{"location":"security/lessons-learned/#2-static-application-security-testing-sast","text":"","title":"2. Static Application Security Testing (SAST)"},{"location":"security/lessons-learned/#value-provided_1","text":"Code-Level Security : Detection of security issues in source code Early Detection : Early detection of security vulnerabilities Developer Education : Educational value for developers Compliance : Meets compliance requirements for secure coding","title":"Value Provided"},{"location":"security/lessons-learned/#implementation-success_1","text":"Tool : Bandit for Python security analysis Frequency : Weekly automated scans Coverage : 100% of Python codebase Effectiveness : 90% success rate in issue detection","title":"Implementation Success"},{"location":"security/lessons-learned/#key-metrics_1","text":"Issues Found : 4,138 total issues (100 high, 222 medium, 3,816 low) False Positive Rate : 15% Mean Time to Detection : 24 hours Resolution Rate : 85% of high-severity issues resolved","title":"Key Metrics"},{"location":"security/lessons-learned/#3-plugin-security-policy-validation","text":"","title":"3. Plugin Security Policy Validation"},{"location":"security/lessons-learned/#value-provided_2","text":"Plugin Security : Ensures plugin security compliance Policy Enforcement : Enforces security policies consistently Risk Mitigation : Mitigates risks from malicious plugins Compliance : Meets compliance requirements for plugin security","title":"Value Provided"},{"location":"security/lessons-learned/#implementation-success_2","text":"Tool : Custom plugin security auditor Frequency : Weekly automated scans Coverage : 100% of plugin manifests Effectiveness : 100% success rate in policy validation","title":"Implementation Success"},{"location":"security/lessons-learned/#key-metrics_2","text":"Plugins Scanned : 3 approved plugins Policy Violations : 0 violations detected Validation Success : 100% validation success rate Policy Compliance : 100% compliance with security policies","title":"Key Metrics"},{"location":"security/lessons-learned/#4-yaml-security-validation","text":"","title":"4. YAML Security Validation"},{"location":"security/lessons-learned/#value-provided_3","text":"Injection Prevention : Prevents YAML injection attacks Data Integrity : Ensures data integrity in YAML files Compliance : Meets compliance requirements for data validation Risk Reduction : Significant reduction in deserialization risks","title":"Value Provided"},{"location":"security/lessons-learned/#implementation-success_3","text":"Tool : yaml.safe_load() usage validation Frequency : Continuous validation Coverage : 100% of YAML parsing code Effectiveness : 100% success rate in safe loading","title":"Implementation Success"},{"location":"security/lessons-learned/#key-metrics_3","text":"Safe Loading Usage : 100% of YAML parsing uses safe_load() Injection Attempts : 0 successful injection attempts Validation Success : 100% validation success rate Risk Reduction : 95% reduction in deserialization risks","title":"Key Metrics"},{"location":"security/lessons-learned/#tooling-and-workflow-bottlenecks","text":"","title":"Tooling and Workflow Bottlenecks"},{"location":"security/lessons-learned/#1-cicd-pipeline-performance","text":"","title":"1. CI/CD Pipeline Performance"},{"location":"security/lessons-learned/#bottleneck","text":"Scan Duration : Security scans taking too long in CI/CD pipeline Resource Usage : High resource usage during security scans Parallel Execution : Limited parallel execution of security tools Cache Management : Inefficient caching of scan results","title":"Bottleneck"},{"location":"security/lessons-learned/#solution_4","text":"Parallel Execution : Parallel execution of independent security scans Incremental Scanning : Only scan changed files Caching : Cache scan results for unchanged files Resource Optimization : Optimize resource usage for scans","title":"Solution"},{"location":"security/lessons-learned/#impact_3","text":"Scan Time : Reduced from 15 minutes to 5 minutes Resource Usage : Reduced by 60% Developer Experience : Improved developer experience Pipeline Efficiency : Improved overall pipeline efficiency","title":"Impact"},{"location":"security/lessons-learned/#2-security-tool-maintenance","text":"","title":"2. Security Tool Maintenance"},{"location":"security/lessons-learned/#bottleneck_1","text":"Tool Updates : Frequent updates required for security tools Configuration Management : Complex configuration management Version Compatibility : Version compatibility issues Documentation : Outdated documentation","title":"Bottleneck"},{"location":"security/lessons-learned/#solution_5","text":"Automated Updates : Automated tool updates in CI/CD pipeline Configuration as Code : Version-controlled configuration Compatibility Testing : Automated compatibility testing Documentation Automation : Automated documentation updates","title":"Solution"},{"location":"security/lessons-learned/#impact_4","text":"Maintenance Time : Reduced by 70% Update Frequency : Increased from monthly to weekly Configuration Consistency : 100% configuration consistency Documentation Accuracy : 95% documentation accuracy","title":"Impact"},{"location":"security/lessons-learned/#3-security-reporting","text":"","title":"3. Security Reporting"},{"location":"security/lessons-learned/#bottleneck_2","text":"Report Generation : Manual report generation process Data Aggregation : Complex data aggregation from multiple sources Format Consistency : Inconsistent report formats Distribution : Manual distribution of reports","title":"Bottleneck"},{"location":"security/lessons-learned/#solution_6","text":"Automated Reporting : Automated report generation Data Integration : Integrated data from multiple sources Template Standardization : Standardized report templates Automated Distribution : Automated report distribution","title":"Solution"},{"location":"security/lessons-learned/#impact_5","text":"Report Generation Time : Reduced from 4 hours to 30 minutes Report Accuracy : Improved from 80% to 95% Distribution Efficiency : 100% automated distribution Stakeholder Satisfaction : Improved stakeholder satisfaction","title":"Impact"},{"location":"security/lessons-learned/#security-culture-and-training","text":"","title":"Security Culture and Training"},{"location":"security/lessons-learned/#1-developer-security-awareness","text":"","title":"1. Developer Security Awareness"},{"location":"security/lessons-learned/#current-state","text":"Awareness Level : 60% of developers have basic security awareness Training Completion : 40% of developers completed security training Tool Adoption : 70% of developers actively use security tools Security Practices : 50% of developers follow security best practices","title":"Current State"},{"location":"security/lessons-learned/#improvement-areas","text":"Comprehensive Training : Need for comprehensive security training program Hands-on Practice : Need for hands-on security practice Regular Updates : Need for regular security updates Incentive Programs : Need for security incentive programs","title":"Improvement Areas"},{"location":"security/lessons-learned/#recommendations","text":"Mandatory Training : Make security training mandatory for all developers Practical Exercises : Include practical security exercises in training Regular Updates : Provide regular security updates and refreshers Recognition Programs : Implement recognition programs for security-conscious development","title":"Recommendations"},{"location":"security/lessons-learned/#2-security-champions-program","text":"","title":"2. Security Champions Program"},{"location":"security/lessons-learned/#current-state_1","text":"Champions : 2 security champions identified Coverage : 50% of teams have security champions Training : Security champions have basic training Support : Limited support for security champions","title":"Current State"},{"location":"security/lessons-learned/#improvement-areas_1","text":"Expansion : Need to expand security champions program Training : Need for advanced training for security champions Support : Need for better support for security champions Recognition : Need for recognition of security champions","title":"Improvement Areas"},{"location":"security/lessons-learned/#recommendations_1","text":"Program Expansion : Expand security champions program to all teams Advanced Training : Provide advanced training for security champions Support Structure : Establish support structure for security champions Recognition System : Implement recognition system for security champions","title":"Recommendations"},{"location":"security/lessons-learned/#future-improvements","text":"","title":"Future Improvements"},{"location":"security/lessons-learned/#1-short-term-improvements-next-3-months","text":"","title":"1. Short-term Improvements (Next 3 months)"},{"location":"security/lessons-learned/#technical-improvements","text":"Enhanced SAST : Add custom security rules and deeper analysis Real-time Monitoring : Implement real-time security monitoring Secrets Management : Implement formal secrets management system Incident Response : Develop formal incident response procedures","title":"Technical Improvements"},{"location":"security/lessons-learned/#process-improvements","text":"Security Training : Implement comprehensive security training program Security Champions : Expand security champions program Security Metrics : Implement comprehensive security metrics Compliance Automation : Automate compliance reporting","title":"Process Improvements"},{"location":"security/lessons-learned/#2-medium-term-improvements-next-6-months","text":"","title":"2. Medium-term Improvements (Next 6 months)"},{"location":"security/lessons-learned/#technical-improvements_1","text":"Cryptographic Signatures : Implement RSA/ECDSA signature verification Container Sandboxing : Implement container-based sandboxing Advanced Monitoring : Implement advanced security monitoring Threat Intelligence : Integrate threat intelligence feeds","title":"Technical Improvements"},{"location":"security/lessons-learned/#process-improvements_1","text":"Security Culture : Build strong security culture External Audits : Conduct external security audits Penetration Testing : Implement regular penetration testing Security Architecture : Implement security architecture reviews","title":"Process Improvements"},{"location":"security/lessons-learned/#3-long-term-improvements-next-12-months","text":"","title":"3. Long-term Improvements (Next 12 months)"},{"location":"security/lessons-learned/#technical-improvements_2","text":"Zero Trust Architecture : Implement zero trust security architecture AI-Powered Detection : Implement AI-powered threat detection Advanced Analytics : Implement advanced security analytics Automated Response : Implement automated incident response","title":"Technical Improvements"},{"location":"security/lessons-learned/#process-improvements_2","text":"Full Compliance : Achieve full compliance with security standards Certification : Obtain security certifications Security Maturity : Achieve high security maturity level Industry Leadership : Establish industry leadership in security","title":"Process Improvements"},{"location":"security/lessons-learned/#key-success-factors","text":"","title":"Key Success Factors"},{"location":"security/lessons-learned/#1-leadership-support","text":"Executive Sponsorship : Strong executive sponsorship for security initiatives Resource Allocation : Adequate resources allocated to security Priority Setting : Security prioritized in development decisions Culture Building : Security culture built from the top down","title":"1. Leadership Support"},{"location":"security/lessons-learned/#2-technical-excellence","text":"Tool Selection : Careful selection of security tools Integration Quality : High-quality integration of security tools Performance Optimization : Optimized performance of security tools Maintenance : Proper maintenance of security tools","title":"2. Technical Excellence"},{"location":"security/lessons-learned/#3-process-maturity","text":"Standardized Processes : Standardized security processes Documentation : Comprehensive security documentation Training : Effective security training programs Continuous Improvement : Continuous improvement of security processes","title":"3. Process Maturity"},{"location":"security/lessons-learned/#4-team-engagement","text":"Developer Buy-in : Strong developer buy-in for security initiatives Security Champions : Active security champions program Knowledge Sharing : Effective knowledge sharing about security Collaboration : Strong collaboration between security and development teams","title":"4. Team Engagement"},{"location":"security/lessons-learned/#conclusion","text":"The implementation of SecFlow's security framework for M1 has been successful, with significant improvements in security posture, compliance, and developer awareness. Key lessons learned include the importance of early security integration, layered security architecture, automation, and continuous monitoring. The most valuable security checks have been dependency vulnerability scanning, SAST scanning, plugin security policy validation, and YAML security validation. These checks have provided significant value in terms of risk reduction, compliance, and developer education. Future improvements should focus on enhancing existing controls, implementing advanced security features, and building a strong security culture. The foundation established in M1 provides a solid base for future security enhancements.","title":"Conclusion"},{"location":"security/lessons-learned/#references","text":"NIST SP 800-53 Security Controls OWASP Application Security Verification Standard ISO 27001 Information Security Management SOC 2 Trust Services Criteria","title":"References"},{"location":"security/plugin-manifest-security-checklist/","text":"Plugin Manifest Security Checklist \u00b6 Overview \u00b6 This document defines the security requirements and validation checklist for plugin manifests in the SecFlow plugin system. All plugins must pass these security checks before being loaded and executed. Plugin Manifest Security Requirements \u00b6 Required Security Fields \u00b6 Every plugin manifest must include the following security-related fields: name : plugin-name version : \"1.0.0\" category : \"detector\" | \"enricher\" | \"analytics\" entrypoint : \"plugins.plugin_name:PluginClass\" signature : \"sha256:abc123...\" # Plugin signature/hash security : permissions : filesystem : read : [ \"data/\" , \"config/\" ] write : [] # Empty by default network : enabled : false # Disabled by default allowed_hosts : [] # Empty by default resources : cpu_seconds : 5 memory_mb : 256 sandbox : true # Required for all plugins Security Validation Checklist \u00b6 1. Manifest Structure Validation \u00b6 Required fields present : name, version, category, entrypoint, signature Security section present : permissions, sandbox configuration Valid category : Must be one of \"detector\", \"enricher\", \"analytics\" Valid entrypoint : Must be importable Python module path 2. Permission Validation \u00b6 Filesystem read permissions : Only allowlisted directories Filesystem write permissions : Empty by default, explicit allowlist required Network permissions : Disabled by default, explicit allowlist required Resource limits : CPU and memory limits within acceptable ranges 3. Signature Verification \u00b6 Plugin signature present : SHA256 hash or digital signature Signature validation : Verify against trusted sources Integrity check : Ensure plugin hasn't been tampered with 4. Sandbox Configuration \u00b6 Sandbox enabled : Must be true for all plugins Resource limits : CPU (\u226410s), Memory (\u2264512MB) Isolation requirements : Process isolation, privilege dropping Security Policy Enforcement \u00b6 Default Deny Policy \u00b6 Filesystem access : Deny by default, explicit allowlist required Network access : Deny by default, explicit allowlist required Resource usage : Strict limits enforced Privilege escalation : Prevented through sandboxing Permission Escalation Rules \u00b6 Filesystem write : Requires explicit justification and Security Lead approval Network access : Requires explicit justification and Security Lead approval High resource usage : CPU >5s or Memory >256MB requires approval System access : Any system-level access requires approval Security Validation Process \u00b6 Manifest parsing : Validate YAML structure and required fields Permission analysis : Check against security policy Signature verification : Verify plugin integrity Sandbox configuration : Ensure proper isolation settings Security approval : Flag for manual review if needed Plugin Security Categories \u00b6 Detector Plugins \u00b6 Default permissions : Read access to data/, config/ Network access : May be required for external scanning Resource limits : CPU 5s, Memory 256MB Special considerations : May need elevated permissions for system scanning Enricher Plugins \u00b6 Default permissions : Read access to data/ Network access : May be required for external APIs Resource limits : CPU 3s, Memory 128MB Special considerations : Limited to data enrichment only Analytics Plugins \u00b6 Default permissions : Read access to data/ Network access : Generally not required Resource limits : CPU 10s, Memory 512MB Special considerations : May need more resources for complex analysis Security Violations and Responses \u00b6 Critical Violations \u00b6 Unauthorized filesystem access : Immediate rejection Unauthorized network access : Immediate rejection Resource limit violations : Immediate rejection Signature verification failure : Immediate rejection Warning Conditions \u00b6 High resource usage : Flag for review Unusual permission requests : Flag for review Missing security fields : Flag for review Implementation Notes \u00b6 PluginLoader Integration \u00b6 The PluginLoader must call the security validation before loading any plugin: def load_plugin ( manifest_path : str ) -> PluginPort : # 1. Parse manifest manifest = parse_manifest ( manifest_path ) # 2. Validate security security_result = validate_plugin_security ( manifest ) if not security_result . valid : raise SecurityError ( f \"Plugin security validation failed: { security_result . errors } \" ) # 3. Verify signature if not verify_plugin_signature ( manifest ): raise SecurityError ( \"Plugin signature verification failed\" ) # 4. Load plugin return load_plugin_class ( manifest . entrypoint ) Security Validation Function \u00b6 def validate_plugin_security ( manifest : dict ) -> SecurityValidationResult : errors = [] # Check required fields if not manifest . get ( 'security' ): errors . append ( \"Missing security section\" ) # Check permissions permissions = manifest . get ( 'security' , {}) . get ( 'permissions' , {}) # Validate filesystem permissions fs_perms = permissions . get ( 'filesystem' , {}) if fs_perms . get ( 'write' ): errors . append ( \"Write permissions require explicit approval\" ) # Validate network permissions net_perms = permissions . get ( 'network' , {}) if net_perms . get ( 'enabled' ): errors . append ( \"Network access requires explicit approval\" ) # Validate resource limits resources = permissions . get ( 'resources' , {}) if resources . get ( 'cpu_seconds' , 0 ) > 10 : errors . append ( \"CPU limit exceeds maximum allowed\" ) return SecurityValidationResult ( valid = len ( errors ) == 0 , errors = errors ) Security Documentation \u00b6 Architecture : Plugin system security model Implementation : Security validation code Policy : Security policy enforcement Testing : Security test cases Review and Updates \u00b6 This security checklist should be reviewed and updated: - Monthly : Review security policy effectiveness - Per release : Update security requirements - As needed : Address new security threats or requirements","title":"Plugin Manifest Security Checklist"},{"location":"security/plugin-manifest-security-checklist/#plugin-manifest-security-checklist","text":"","title":"Plugin Manifest Security Checklist"},{"location":"security/plugin-manifest-security-checklist/#overview","text":"This document defines the security requirements and validation checklist for plugin manifests in the SecFlow plugin system. All plugins must pass these security checks before being loaded and executed.","title":"Overview"},{"location":"security/plugin-manifest-security-checklist/#plugin-manifest-security-requirements","text":"","title":"Plugin Manifest Security Requirements"},{"location":"security/plugin-manifest-security-checklist/#required-security-fields","text":"Every plugin manifest must include the following security-related fields: name : plugin-name version : \"1.0.0\" category : \"detector\" | \"enricher\" | \"analytics\" entrypoint : \"plugins.plugin_name:PluginClass\" signature : \"sha256:abc123...\" # Plugin signature/hash security : permissions : filesystem : read : [ \"data/\" , \"config/\" ] write : [] # Empty by default network : enabled : false # Disabled by default allowed_hosts : [] # Empty by default resources : cpu_seconds : 5 memory_mb : 256 sandbox : true # Required for all plugins","title":"Required Security Fields"},{"location":"security/plugin-manifest-security-checklist/#security-validation-checklist","text":"","title":"Security Validation Checklist"},{"location":"security/plugin-manifest-security-checklist/#1-manifest-structure-validation","text":"Required fields present : name, version, category, entrypoint, signature Security section present : permissions, sandbox configuration Valid category : Must be one of \"detector\", \"enricher\", \"analytics\" Valid entrypoint : Must be importable Python module path","title":"1. Manifest Structure Validation"},{"location":"security/plugin-manifest-security-checklist/#2-permission-validation","text":"Filesystem read permissions : Only allowlisted directories Filesystem write permissions : Empty by default, explicit allowlist required Network permissions : Disabled by default, explicit allowlist required Resource limits : CPU and memory limits within acceptable ranges","title":"2. Permission Validation"},{"location":"security/plugin-manifest-security-checklist/#3-signature-verification","text":"Plugin signature present : SHA256 hash or digital signature Signature validation : Verify against trusted sources Integrity check : Ensure plugin hasn't been tampered with","title":"3. Signature Verification"},{"location":"security/plugin-manifest-security-checklist/#4-sandbox-configuration","text":"Sandbox enabled : Must be true for all plugins Resource limits : CPU (\u226410s), Memory (\u2264512MB) Isolation requirements : Process isolation, privilege dropping","title":"4. Sandbox Configuration"},{"location":"security/plugin-manifest-security-checklist/#security-policy-enforcement","text":"","title":"Security Policy Enforcement"},{"location":"security/plugin-manifest-security-checklist/#default-deny-policy","text":"Filesystem access : Deny by default, explicit allowlist required Network access : Deny by default, explicit allowlist required Resource usage : Strict limits enforced Privilege escalation : Prevented through sandboxing","title":"Default Deny Policy"},{"location":"security/plugin-manifest-security-checklist/#permission-escalation-rules","text":"Filesystem write : Requires explicit justification and Security Lead approval Network access : Requires explicit justification and Security Lead approval High resource usage : CPU >5s or Memory >256MB requires approval System access : Any system-level access requires approval","title":"Permission Escalation Rules"},{"location":"security/plugin-manifest-security-checklist/#security-validation-process","text":"Manifest parsing : Validate YAML structure and required fields Permission analysis : Check against security policy Signature verification : Verify plugin integrity Sandbox configuration : Ensure proper isolation settings Security approval : Flag for manual review if needed","title":"Security Validation Process"},{"location":"security/plugin-manifest-security-checklist/#plugin-security-categories","text":"","title":"Plugin Security Categories"},{"location":"security/plugin-manifest-security-checklist/#detector-plugins","text":"Default permissions : Read access to data/, config/ Network access : May be required for external scanning Resource limits : CPU 5s, Memory 256MB Special considerations : May need elevated permissions for system scanning","title":"Detector Plugins"},{"location":"security/plugin-manifest-security-checklist/#enricher-plugins","text":"Default permissions : Read access to data/ Network access : May be required for external APIs Resource limits : CPU 3s, Memory 128MB Special considerations : Limited to data enrichment only","title":"Enricher Plugins"},{"location":"security/plugin-manifest-security-checklist/#analytics-plugins","text":"Default permissions : Read access to data/ Network access : Generally not required Resource limits : CPU 10s, Memory 512MB Special considerations : May need more resources for complex analysis","title":"Analytics Plugins"},{"location":"security/plugin-manifest-security-checklist/#security-violations-and-responses","text":"","title":"Security Violations and Responses"},{"location":"security/plugin-manifest-security-checklist/#critical-violations","text":"Unauthorized filesystem access : Immediate rejection Unauthorized network access : Immediate rejection Resource limit violations : Immediate rejection Signature verification failure : Immediate rejection","title":"Critical Violations"},{"location":"security/plugin-manifest-security-checklist/#warning-conditions","text":"High resource usage : Flag for review Unusual permission requests : Flag for review Missing security fields : Flag for review","title":"Warning Conditions"},{"location":"security/plugin-manifest-security-checklist/#implementation-notes","text":"","title":"Implementation Notes"},{"location":"security/plugin-manifest-security-checklist/#pluginloader-integration","text":"The PluginLoader must call the security validation before loading any plugin: def load_plugin ( manifest_path : str ) -> PluginPort : # 1. Parse manifest manifest = parse_manifest ( manifest_path ) # 2. Validate security security_result = validate_plugin_security ( manifest ) if not security_result . valid : raise SecurityError ( f \"Plugin security validation failed: { security_result . errors } \" ) # 3. Verify signature if not verify_plugin_signature ( manifest ): raise SecurityError ( \"Plugin signature verification failed\" ) # 4. Load plugin return load_plugin_class ( manifest . entrypoint )","title":"PluginLoader Integration"},{"location":"security/plugin-manifest-security-checklist/#security-validation-function","text":"def validate_plugin_security ( manifest : dict ) -> SecurityValidationResult : errors = [] # Check required fields if not manifest . get ( 'security' ): errors . append ( \"Missing security section\" ) # Check permissions permissions = manifest . get ( 'security' , {}) . get ( 'permissions' , {}) # Validate filesystem permissions fs_perms = permissions . get ( 'filesystem' , {}) if fs_perms . get ( 'write' ): errors . append ( \"Write permissions require explicit approval\" ) # Validate network permissions net_perms = permissions . get ( 'network' , {}) if net_perms . get ( 'enabled' ): errors . append ( \"Network access requires explicit approval\" ) # Validate resource limits resources = permissions . get ( 'resources' , {}) if resources . get ( 'cpu_seconds' , 0 ) > 10 : errors . append ( \"CPU limit exceeds maximum allowed\" ) return SecurityValidationResult ( valid = len ( errors ) == 0 , errors = errors )","title":"Security Validation Function"},{"location":"security/plugin-manifest-security-checklist/#security-documentation","text":"Architecture : Plugin system security model Implementation : Security validation code Policy : Security policy enforcement Testing : Security test cases","title":"Security Documentation"},{"location":"security/plugin-manifest-security-checklist/#review-and-updates","text":"This security checklist should be reviewed and updated: - Monthly : Review security policy effectiveness - Per release : Update security requirements - As needed : Address new security threats or requirements","title":"Review and Updates"},{"location":"security/sandbox-constraints/","text":"SecFlow Plugin Sandbox Constraints \u00b6 Security Model Compliance \u00b6 Based on docs/architecture/16-security-model.md , all plugin executions must enforce: Resource Limits \u00b6 CPU : Maximum 5 seconds per plugin execution (configurable per plugin) Memory : Maximum 256MB per plugin (configurable per plugin) Disk : Read-only filesystem with explicit allowlist Network : Disabled by default, explicit allowlist required Isolation Mechanisms \u00b6 Namespaces : PID, NET, MNT isolation Seccomp Filters : Syscall restriction cgroups v2 : CPU/memory enforcement No-root UID : Privilege dropping AppArmor : File access control Read-only FS : Prevents persistence Policy Enforcement \u00b6 # plugins/plugin_policy.yaml default : deny allow : - name : example_safe_plugin fs_allowlist : [ \"data/\" ] # Filesystem access network : false # Network access cpu_seconds : 5 # CPU time limit memory_mb : 256 # Memory limit Container Execution Example \u00b6 docker run --rm --cap-drop = ALL \\ --security-opt = no-new-privileges \\ --read-only -m 256m --cpus = 1 \\ --tmpfs /tmp:noexec,nosuid,size = 100m \\ SecFlow-runner:latest plugin-executor Security Validation \u00b6 All plugins validated by tools/plugin_security_audit.py Mandatory policy keys: name, fs_allowlist, network, cpu_seconds, memory_mb Runtime enforcement via container orchestration Audit logging of all policy decisions Compliance Framework \u00b6 NIST SP 800-53 : Access control, auditing, system protection ISO/IEC 27001 : Information security management OWASP SAMM : Secure software development lifecycle MITRE ATT&CK : Mapping detection behaviors GDPR Art. 32 : Data confidentiality and integrity","title":"SecFlow Plugin Sandbox Constraints"},{"location":"security/sandbox-constraints/#secflow-plugin-sandbox-constraints","text":"","title":"SecFlow Plugin Sandbox Constraints"},{"location":"security/sandbox-constraints/#security-model-compliance","text":"Based on docs/architecture/16-security-model.md , all plugin executions must enforce:","title":"Security Model Compliance"},{"location":"security/sandbox-constraints/#resource-limits","text":"CPU : Maximum 5 seconds per plugin execution (configurable per plugin) Memory : Maximum 256MB per plugin (configurable per plugin) Disk : Read-only filesystem with explicit allowlist Network : Disabled by default, explicit allowlist required","title":"Resource Limits"},{"location":"security/sandbox-constraints/#isolation-mechanisms","text":"Namespaces : PID, NET, MNT isolation Seccomp Filters : Syscall restriction cgroups v2 : CPU/memory enforcement No-root UID : Privilege dropping AppArmor : File access control Read-only FS : Prevents persistence","title":"Isolation Mechanisms"},{"location":"security/sandbox-constraints/#policy-enforcement","text":"# plugins/plugin_policy.yaml default : deny allow : - name : example_safe_plugin fs_allowlist : [ \"data/\" ] # Filesystem access network : false # Network access cpu_seconds : 5 # CPU time limit memory_mb : 256 # Memory limit","title":"Policy Enforcement"},{"location":"security/sandbox-constraints/#container-execution-example","text":"docker run --rm --cap-drop = ALL \\ --security-opt = no-new-privileges \\ --read-only -m 256m --cpus = 1 \\ --tmpfs /tmp:noexec,nosuid,size = 100m \\ SecFlow-runner:latest plugin-executor","title":"Container Execution Example"},{"location":"security/sandbox-constraints/#security-validation","text":"All plugins validated by tools/plugin_security_audit.py Mandatory policy keys: name, fs_allowlist, network, cpu_seconds, memory_mb Runtime enforcement via container orchestration Audit logging of all policy decisions","title":"Security Validation"},{"location":"security/sandbox-constraints/#compliance-framework","text":"NIST SP 800-53 : Access control, auditing, system protection ISO/IEC 27001 : Information security management OWASP SAMM : Secure software development lifecycle MITRE ATT&CK : Mapping detection behaviors GDPR Art. 32 : Data confidentiality and integrity","title":"Compliance Framework"},{"location":"security/secrets-scanning/","text":"Secrets Scanning Integration \u00b6 Overview \u00b6 This document outlines the integration of secrets scanning into the SecFlow security framework using TruffleHog. Secrets Scanning Implementation \u00b6 Tool Selection \u00b6 Primary Tool : TruffleHog (Python-based) Alternative : Gitleaks (Go-based, requires binary installation) Integration : CI/CD pipeline and local development Secrets Scanning Configuration \u00b6 TruffleHog Configuration \u00b6 # tools/secrets_scanner.py import subprocess import json import pathlib from typing import List , Dict , Any from dataclasses import dataclass @dataclass class SecretFinding : \"\"\"Represents a detected secret.\"\"\" file_path : str line_number : int secret_type : str confidence : str description : str severity : str class SecretsScanner : \"\"\"Secrets scanning using TruffleHog.\"\"\" def __init__ ( self , repo_path : str = \".\" ): self . repo_path = pathlib . Path ( repo_path ) self . exclude_patterns = [ \"*.pyc\" , \"__pycache__/*\" , \"venv/*\" , \"node_modules/*\" , \".git/*\" , \"*.log\" , \"*.tmp\" ] def scan_repository ( self ) -> List [ SecretFinding ]: \"\"\"Scan repository for secrets.\"\"\" try : # Run TruffleHog scan cmd = [ \"trufflehog\" , str ( self . repo_path ), \"--json\" , \"--entropy\" , \"True\" , \"--regex\" , \"True\" ] result = subprocess . run ( cmd , capture_output = True , text = True , timeout = 300 ) if result . returncode != 0 : print ( f \"TruffleHog scan failed: { result . stderr } \" ) return [] # Parse results findings = [] for line in result . stdout . strip () . split ( ' \\n ' ): if line : try : finding_data = json . loads ( line ) finding = SecretFinding ( file_path = finding_data . get ( 'path' , '' ), line_number = finding_data . get ( 'line' , 0 ), secret_type = finding_data . get ( 'reason' , '' ), confidence = finding_data . get ( 'confidence' , 'medium' ), description = finding_data . get ( 'description' , '' ), severity = self . _determine_severity ( finding_data ) ) findings . append ( finding ) except json . JSONDecodeError : continue return findings except subprocess . TimeoutExpired : print ( \"TruffleHog scan timed out\" ) return [] except Exception as e : print ( f \"Secrets scan failed: { e } \" ) return [] def _determine_severity ( self , finding_data : Dict [ str , Any ]) -> str : \"\"\"Determine severity of secret finding.\"\"\" reason = finding_data . get ( 'reason' , '' ) . lower () # High severity patterns high_severity = [ 'password' , 'secret' , 'key' , 'token' , 'api_key' , 'private_key' ] if any ( pattern in reason for pattern in high_severity ): return 'high' # Medium severity patterns medium_severity = [ 'credential' , 'auth' , 'access' , 'bearer' ] if any ( pattern in reason for pattern in medium_severity ): return 'medium' return 'low' def generate_report ( self , findings : List [ SecretFinding ]) -> str : \"\"\"Generate secrets scan report.\"\"\" report = [] report . append ( \"# Secrets Scan Report\" ) report . append ( f \"**Total Findings:** { len ( findings ) } \" ) report . append ( \"\" ) # Group by severity high_findings = [ f for f in findings if f . severity == 'high' ] medium_findings = [ f for f in findings if f . severity == 'medium' ] low_findings = [ f for f in findings if f . severity == 'low' ] if high_findings : report . append ( \"## High Severity Findings\" ) for finding in high_findings : report . append ( f \"- ** { finding . file_path } : { finding . line_number } ** - { finding . secret_type } \" ) report . append ( f \" - Description: { finding . description } \" ) report . append ( \"\" ) if medium_findings : report . append ( \"## Medium Severity Findings\" ) for finding in medium_findings : report . append ( f \"- ** { finding . file_path } : { finding . line_number } ** - { finding . secret_type } \" ) report . append ( f \" - Description: { finding . description } \" ) report . append ( \"\" ) if low_findings : report . append ( \"## Low Severity Findings\" ) for finding in low_findings : report . append ( f \"- ** { finding . file_path } : { finding . line_number } ** - { finding . secret_type } \" ) report . append ( f \" - Description: { finding . description } \" ) return ' \\n ' . join ( report ) CI/CD Integration \u00b6 GitHub Actions Workflow \u00b6 secrets-scan : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v4 with : fetch-depth : 0 - name : Set up Python uses : actions/setup-python@v4 with : python-version : '3.11' - name : Install TruffleHog run : | pip install trufflehog - name : Run secrets scan run : | mkdir -p reports/security python tools/secrets_scanner.py > reports/security/secrets-scan-$(date +%Y%m%d).json - name : Upload secrets scan results uses : actions/upload-artifact@v3 with : name : secrets-scan-${{ github.run_number }} path : reports/security/secrets-scan-*.json if : always() Secrets Detection Patterns \u00b6 Common Secret Types \u00b6 API Keys : AWS, Google Cloud, Azure, GitHub, etc. Passwords : Database passwords, service passwords Tokens : JWT tokens, OAuth tokens, access tokens Private Keys : SSH keys, SSL certificates, GPG keys Credentials : Username/password combinations Database URLs : Connection strings with embedded credentials Custom Detection Rules \u00b6 { \"rules\" : [ { \"name\" : \"AWS Access Key\" , \"pattern\" : \"AKIA[0-9A-Z]{16}\" , \"severity\" : \"high\" , \"description\" : \"AWS Access Key ID\" }, { \"name\" : \"GitHub Token\" , \"pattern\" : \"ghp_[0-9A-Za-z]{36}\" , \"severity\" : \"high\" , \"description\" : \"GitHub Personal Access Token\" }, { \"name\" : \"JWT Token\" , \"pattern\" : \"eyJ[A-Za-z0-9+/=]+\" , \"severity\" : \"medium\" , \"description\" : \"JSON Web Token\" } ] } Security Best Practices \u00b6 Prevention \u00b6 Pre-commit Hooks : Scan for secrets before commits IDE Integration : Real-time secrets detection in editors Developer Training : Security awareness for developers Code Review : Manual review for sensitive code Detection \u00b6 Automated Scanning : Regular scans in CI/CD pipeline Historical Scanning : Scan git history for leaked secrets Entropy Analysis : Detect high-entropy strings Pattern Matching : Use regex patterns for known secret formats Response \u00b6 Immediate Rotation : Rotate compromised secrets immediately Incident Response : Follow security incident procedures Notification : Alert security team and stakeholders Documentation : Document findings and remediation steps Integration with Security Framework \u00b6 Security Monitoring \u00b6 Daily Scans : Automated daily secrets scanning PR Integration : Scan pull requests for secrets Alert System : Real-time alerts for high-severity findings Reporting : Regular security reports including secrets findings Compliance Integration \u00b6 NIST SP 800-53 : SI-4 (Information System Monitoring) ISO 27001 : A.12.4 (Logging and Monitoring) SOC 2 : CC6.1 (Logical and Physical Access Controls) OWASP : A07 (Identification and Authentication Failures) Tools and Technologies \u00b6 Primary Tools \u00b6 TruffleHog : Python-based secrets scanner Gitleaks : Go-based secrets scanner GitGuardian : Commercial secrets detection GitHub Secret Scanning : Native GitHub feature Integration Tools \u00b6 Pre-commit : Git hooks for pre-commit scanning GitHub Actions : CI/CD integration Jenkins : CI/CD integration GitLab CI : CI/CD integration Monitoring and Alerting \u00b6 Alert Thresholds \u00b6 High Severity : Immediate alert for any high-severity finding Medium Severity : Daily summary of medium-severity findings Low Severity : Weekly summary of low-severity findings Notification Channels \u00b6 Email : Security team notifications Slack : Real-time alerts in security channel JIRA : Automatic ticket creation for findings Dashboard : Security metrics dashboard Remediation Procedures \u00b6 Immediate Response \u00b6 Assess Impact : Determine scope of secret exposure Rotate Secrets : Immediately rotate compromised secrets Notify Stakeholders : Alert relevant teams and users Document Incident : Record details for future reference Long-term Remediation \u00b6 Root Cause Analysis : Identify how secret was exposed Process Improvement : Update development processes Training : Provide additional security training Tool Enhancement : Improve detection and prevention tools Metrics and KPIs \u00b6 Security Metrics \u00b6 Secrets Found : Number of secrets detected per scan False Positive Rate : Percentage of false positives Mean Time to Detection : Time to detect secret exposure Mean Time to Remediation : Time to fix secret exposure Operational Metrics \u00b6 Scan Performance : Duration and success rate of scans Coverage : Percentage of codebase scanned Integration : Success rate of CI/CD integration User Adoption : Developer usage of prevention tools References \u00b6 TruffleHog Documentation OWASP Secrets Management NIST SP 800-53 SI-4 GitHub Secret Scanning","title":"Secrets Scanning Integration"},{"location":"security/secrets-scanning/#secrets-scanning-integration","text":"","title":"Secrets Scanning Integration"},{"location":"security/secrets-scanning/#overview","text":"This document outlines the integration of secrets scanning into the SecFlow security framework using TruffleHog.","title":"Overview"},{"location":"security/secrets-scanning/#secrets-scanning-implementation","text":"","title":"Secrets Scanning Implementation"},{"location":"security/secrets-scanning/#tool-selection","text":"Primary Tool : TruffleHog (Python-based) Alternative : Gitleaks (Go-based, requires binary installation) Integration : CI/CD pipeline and local development","title":"Tool Selection"},{"location":"security/secrets-scanning/#secrets-scanning-configuration","text":"","title":"Secrets Scanning Configuration"},{"location":"security/secrets-scanning/#trufflehog-configuration","text":"# tools/secrets_scanner.py import subprocess import json import pathlib from typing import List , Dict , Any from dataclasses import dataclass @dataclass class SecretFinding : \"\"\"Represents a detected secret.\"\"\" file_path : str line_number : int secret_type : str confidence : str description : str severity : str class SecretsScanner : \"\"\"Secrets scanning using TruffleHog.\"\"\" def __init__ ( self , repo_path : str = \".\" ): self . repo_path = pathlib . Path ( repo_path ) self . exclude_patterns = [ \"*.pyc\" , \"__pycache__/*\" , \"venv/*\" , \"node_modules/*\" , \".git/*\" , \"*.log\" , \"*.tmp\" ] def scan_repository ( self ) -> List [ SecretFinding ]: \"\"\"Scan repository for secrets.\"\"\" try : # Run TruffleHog scan cmd = [ \"trufflehog\" , str ( self . repo_path ), \"--json\" , \"--entropy\" , \"True\" , \"--regex\" , \"True\" ] result = subprocess . run ( cmd , capture_output = True , text = True , timeout = 300 ) if result . returncode != 0 : print ( f \"TruffleHog scan failed: { result . stderr } \" ) return [] # Parse results findings = [] for line in result . stdout . strip () . split ( ' \\n ' ): if line : try : finding_data = json . loads ( line ) finding = SecretFinding ( file_path = finding_data . get ( 'path' , '' ), line_number = finding_data . get ( 'line' , 0 ), secret_type = finding_data . get ( 'reason' , '' ), confidence = finding_data . get ( 'confidence' , 'medium' ), description = finding_data . get ( 'description' , '' ), severity = self . _determine_severity ( finding_data ) ) findings . append ( finding ) except json . JSONDecodeError : continue return findings except subprocess . TimeoutExpired : print ( \"TruffleHog scan timed out\" ) return [] except Exception as e : print ( f \"Secrets scan failed: { e } \" ) return [] def _determine_severity ( self , finding_data : Dict [ str , Any ]) -> str : \"\"\"Determine severity of secret finding.\"\"\" reason = finding_data . get ( 'reason' , '' ) . lower () # High severity patterns high_severity = [ 'password' , 'secret' , 'key' , 'token' , 'api_key' , 'private_key' ] if any ( pattern in reason for pattern in high_severity ): return 'high' # Medium severity patterns medium_severity = [ 'credential' , 'auth' , 'access' , 'bearer' ] if any ( pattern in reason for pattern in medium_severity ): return 'medium' return 'low' def generate_report ( self , findings : List [ SecretFinding ]) -> str : \"\"\"Generate secrets scan report.\"\"\" report = [] report . append ( \"# Secrets Scan Report\" ) report . append ( f \"**Total Findings:** { len ( findings ) } \" ) report . append ( \"\" ) # Group by severity high_findings = [ f for f in findings if f . severity == 'high' ] medium_findings = [ f for f in findings if f . severity == 'medium' ] low_findings = [ f for f in findings if f . severity == 'low' ] if high_findings : report . append ( \"## High Severity Findings\" ) for finding in high_findings : report . append ( f \"- ** { finding . file_path } : { finding . line_number } ** - { finding . secret_type } \" ) report . append ( f \" - Description: { finding . description } \" ) report . append ( \"\" ) if medium_findings : report . append ( \"## Medium Severity Findings\" ) for finding in medium_findings : report . append ( f \"- ** { finding . file_path } : { finding . line_number } ** - { finding . secret_type } \" ) report . append ( f \" - Description: { finding . description } \" ) report . append ( \"\" ) if low_findings : report . append ( \"## Low Severity Findings\" ) for finding in low_findings : report . append ( f \"- ** { finding . file_path } : { finding . line_number } ** - { finding . secret_type } \" ) report . append ( f \" - Description: { finding . description } \" ) return ' \\n ' . join ( report )","title":"TruffleHog Configuration"},{"location":"security/secrets-scanning/#cicd-integration","text":"","title":"CI/CD Integration"},{"location":"security/secrets-scanning/#github-actions-workflow","text":"secrets-scan : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v4 with : fetch-depth : 0 - name : Set up Python uses : actions/setup-python@v4 with : python-version : '3.11' - name : Install TruffleHog run : | pip install trufflehog - name : Run secrets scan run : | mkdir -p reports/security python tools/secrets_scanner.py > reports/security/secrets-scan-$(date +%Y%m%d).json - name : Upload secrets scan results uses : actions/upload-artifact@v3 with : name : secrets-scan-${{ github.run_number }} path : reports/security/secrets-scan-*.json if : always()","title":"GitHub Actions Workflow"},{"location":"security/secrets-scanning/#secrets-detection-patterns","text":"","title":"Secrets Detection Patterns"},{"location":"security/secrets-scanning/#common-secret-types","text":"API Keys : AWS, Google Cloud, Azure, GitHub, etc. Passwords : Database passwords, service passwords Tokens : JWT tokens, OAuth tokens, access tokens Private Keys : SSH keys, SSL certificates, GPG keys Credentials : Username/password combinations Database URLs : Connection strings with embedded credentials","title":"Common Secret Types"},{"location":"security/secrets-scanning/#custom-detection-rules","text":"{ \"rules\" : [ { \"name\" : \"AWS Access Key\" , \"pattern\" : \"AKIA[0-9A-Z]{16}\" , \"severity\" : \"high\" , \"description\" : \"AWS Access Key ID\" }, { \"name\" : \"GitHub Token\" , \"pattern\" : \"ghp_[0-9A-Za-z]{36}\" , \"severity\" : \"high\" , \"description\" : \"GitHub Personal Access Token\" }, { \"name\" : \"JWT Token\" , \"pattern\" : \"eyJ[A-Za-z0-9+/=]+\" , \"severity\" : \"medium\" , \"description\" : \"JSON Web Token\" } ] }","title":"Custom Detection Rules"},{"location":"security/secrets-scanning/#security-best-practices","text":"","title":"Security Best Practices"},{"location":"security/secrets-scanning/#prevention","text":"Pre-commit Hooks : Scan for secrets before commits IDE Integration : Real-time secrets detection in editors Developer Training : Security awareness for developers Code Review : Manual review for sensitive code","title":"Prevention"},{"location":"security/secrets-scanning/#detection","text":"Automated Scanning : Regular scans in CI/CD pipeline Historical Scanning : Scan git history for leaked secrets Entropy Analysis : Detect high-entropy strings Pattern Matching : Use regex patterns for known secret formats","title":"Detection"},{"location":"security/secrets-scanning/#response","text":"Immediate Rotation : Rotate compromised secrets immediately Incident Response : Follow security incident procedures Notification : Alert security team and stakeholders Documentation : Document findings and remediation steps","title":"Response"},{"location":"security/secrets-scanning/#integration-with-security-framework","text":"","title":"Integration with Security Framework"},{"location":"security/secrets-scanning/#security-monitoring","text":"Daily Scans : Automated daily secrets scanning PR Integration : Scan pull requests for secrets Alert System : Real-time alerts for high-severity findings Reporting : Regular security reports including secrets findings","title":"Security Monitoring"},{"location":"security/secrets-scanning/#compliance-integration","text":"NIST SP 800-53 : SI-4 (Information System Monitoring) ISO 27001 : A.12.4 (Logging and Monitoring) SOC 2 : CC6.1 (Logical and Physical Access Controls) OWASP : A07 (Identification and Authentication Failures)","title":"Compliance Integration"},{"location":"security/secrets-scanning/#tools-and-technologies","text":"","title":"Tools and Technologies"},{"location":"security/secrets-scanning/#primary-tools","text":"TruffleHog : Python-based secrets scanner Gitleaks : Go-based secrets scanner GitGuardian : Commercial secrets detection GitHub Secret Scanning : Native GitHub feature","title":"Primary Tools"},{"location":"security/secrets-scanning/#integration-tools","text":"Pre-commit : Git hooks for pre-commit scanning GitHub Actions : CI/CD integration Jenkins : CI/CD integration GitLab CI : CI/CD integration","title":"Integration Tools"},{"location":"security/secrets-scanning/#monitoring-and-alerting","text":"","title":"Monitoring and Alerting"},{"location":"security/secrets-scanning/#alert-thresholds","text":"High Severity : Immediate alert for any high-severity finding Medium Severity : Daily summary of medium-severity findings Low Severity : Weekly summary of low-severity findings","title":"Alert Thresholds"},{"location":"security/secrets-scanning/#notification-channels","text":"Email : Security team notifications Slack : Real-time alerts in security channel JIRA : Automatic ticket creation for findings Dashboard : Security metrics dashboard","title":"Notification Channels"},{"location":"security/secrets-scanning/#remediation-procedures","text":"","title":"Remediation Procedures"},{"location":"security/secrets-scanning/#immediate-response","text":"Assess Impact : Determine scope of secret exposure Rotate Secrets : Immediately rotate compromised secrets Notify Stakeholders : Alert relevant teams and users Document Incident : Record details for future reference","title":"Immediate Response"},{"location":"security/secrets-scanning/#long-term-remediation","text":"Root Cause Analysis : Identify how secret was exposed Process Improvement : Update development processes Training : Provide additional security training Tool Enhancement : Improve detection and prevention tools","title":"Long-term Remediation"},{"location":"security/secrets-scanning/#metrics-and-kpis","text":"","title":"Metrics and KPIs"},{"location":"security/secrets-scanning/#security-metrics","text":"Secrets Found : Number of secrets detected per scan False Positive Rate : Percentage of false positives Mean Time to Detection : Time to detect secret exposure Mean Time to Remediation : Time to fix secret exposure","title":"Security Metrics"},{"location":"security/secrets-scanning/#operational-metrics","text":"Scan Performance : Duration and success rate of scans Coverage : Percentage of codebase scanned Integration : Success rate of CI/CD integration User Adoption : Developer usage of prevention tools","title":"Operational Metrics"},{"location":"security/secrets-scanning/#references","text":"TruffleHog Documentation OWASP Secrets Management NIST SP 800-53 SI-4 GitHub Secret Scanning","title":"References"},{"location":"security/security-gate/","text":"Security Readiness Gate \u00b6 Overview \u00b6 This document defines the security readiness gate mechanism that enforces security standards before code can be merged to production branches. Security Gate Definition \u00b6 Purpose \u00b6 The security readiness gate ensures that all code meets minimum security standards before being merged to production branches, preventing security vulnerabilities from reaching production. Scope \u00b6 All Pull Requests : Security gate applies to all pull requests Production Branches : Gate enforced for main, develop, and release branches Security-Sensitive Changes : Additional checks for security-sensitive changes Dependencies : Gate applies to dependency updates and additions Security Gate Criteria \u00b6 1. Static Application Security Testing (SAST) \u00b6 Criteria \u00b6 High Severity Issues : 0 high severity issues allowed Medium Severity Issues : Maximum 5 medium severity issues allowed Low Severity Issues : Maximum 20 low severity issues allowed New Issues : No new high or medium severity issues allowed Implementation \u00b6 sast-gate : runs-on : ubuntu-latest steps : - name : Run Bandit SAST run : bandit -r . -f json -o bandit-report.json - name : Check SAST Gate run : | python -c \" import json with open('bandit-report.json') as f: report = json.load(f) high_severity = [r for r in report['results'] if r['issue_severity'] == 'HIGH'] medium_severity = [r for r in report['results'] if r['issue_severity'] == 'MEDIUM'] if len(high_severity) > 0: print('\u274c SAST Gate Failed: High severity issues found') exit(1) elif len(medium_severity) > 5: print('\u274c SAST Gate Failed: Too many medium severity issues') exit(1) else: print('\u2705 SAST Gate Passed') \" 2. Dependency Security Audit \u00b6 Criteria \u00b6 Critical Vulnerabilities : 0 critical vulnerabilities allowed High Vulnerabilities : Maximum 2 high vulnerabilities allowed Medium Vulnerabilities : Maximum 5 medium vulnerabilities allowed New Vulnerabilities : No new critical or high vulnerabilities allowed Implementation \u00b6 dependency-gate : runs-on : ubuntu-latest steps : - name : Run Safety audit run : safety check --json --output safety-report.json - name : Check Dependency Gate run : | python -c \" import json with open('safety-report.json') as f: report = json.load(f) vulnerabilities = report.get('vulnerabilities', []) critical = [v for v in vulnerabilities if v.get('severity') == 'CRITICAL'] high = [v for v in vulnerabilities if v.get('severity') == 'HIGH'] if len(critical) > 0: print('\u274c Dependency Gate Failed: Critical vulnerabilities found') exit(1) elif len(high) > 2: print('\u274c Dependency Gate Failed: Too many high vulnerabilities') exit(1) else: print('\u2705 Dependency Gate Passed') \" 3. Secrets Detection \u00b6 Criteria \u00b6 High Confidence Secrets : 0 high confidence secrets allowed Medium Confidence Secrets : Maximum 2 medium confidence secrets allowed Low Confidence Secrets : Maximum 5 low confidence secrets allowed New Secrets : No new high or medium confidence secrets allowed Implementation \u00b6 secrets-gate : runs-on : ubuntu-latest steps : - name : Run TruffleHog scan run : trufflehog . --json > trufflehog-report.json - name : Check Secrets Gate run : | python -c \" import json try: with open('trufflehog-report.json') as f: findings = [json.loads(line) for line in f if line.strip()] high_confidence = [f for f in findings if f.get('confidence') == 'high'] medium_confidence = [f for f in findings if f.get('confidence') == 'medium'] if len(high_confidence) > 0: print('\u274c Secrets Gate Failed: High confidence secrets found') exit(1) elif len(medium_confidence) > 2: print('\u274c Secrets Gate Failed: Too many medium confidence secrets') exit(1) else: print('\u2705 Secrets Gate Passed') except: print('\u2705 Secrets Gate Passed (no findings)') \" 4. Plugin Security Validation \u00b6 Criteria \u00b6 Policy Violations : 0 policy violations allowed Signature Failures : 0 signature verification failures allowed Security Checklist : 100% security checklist compliance required New Plugins : Additional review required for new plugins Implementation \u00b6 plugin-gate : runs-on : ubuntu-latest steps : - name : Run plugin security audit run : python tools/plugin_security_audit.py > plugin-audit.txt - name : Check Plugin Gate run : | if grep -q \"ERROR\\|FAILED\" plugin-audit.txt; then echo \"\u274c Plugin Gate Failed: Plugin security issues found\" cat plugin-audit.txt exit(1) else echo \"\u2705 Plugin Gate Passed\" fi 5. YAML Security Validation \u00b6 Criteria \u00b6 Safe Loading : 100% yaml.safe_load() usage required Schema Validation : All YAML files must pass schema validation Injection Prevention : No YAML injection vulnerabilities allowed New YAML Files : Additional validation required for new YAML files Implementation \u00b6 yaml-gate : runs-on : ubuntu-latest steps : - name : Check YAML security run : | if grep -r \"yaml\\.load(\" --include=\"*.py\" .; then echo \"\u274c YAML Gate Failed: Unsafe yaml.load() usage found\" exit(1) else echo \"\u2705 YAML Gate Passed\" fi Security Gate Implementation \u00b6 GitHub Actions Workflow \u00b6 name : Security Gate on : pull_request : branches : [ main , develop , release/* ] push : branches : [ main , develop ] jobs : security-gate : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v4 - name : Set up Python uses : actions/setup-python@v4 with : python-version : '3.11' - name : Install security tools run : | pip install bandit safety trufflehog pyyaml - name : Run SAST Gate run : | bandit -r . -f json -o bandit-report.json python scripts/check_sast_gate.py - name : Run Dependency Gate run : | safety check --json --output safety-report.json python scripts/check_dependency_gate.py - name : Run Secrets Gate run : | trufflehog . --json > trufflehog-report.json python scripts/check_secrets_gate.py - name : Run Plugin Gate run : | python tools/plugin_security_audit.py > plugin-audit.txt python scripts/check_plugin_gate.py - name : Run YAML Gate run : | python scripts/check_yaml_gate.py - name : Security Gate Summary run : | echo \"## \ud83d\udd12 Security Gate Summary\" >> $GITHUB_STEP_SUMMARY echo \"\" >> $GITHUB_STEP_SUMMARY echo \"| Check | Status |\" >> $GITHUB_STEP_SUMMARY echo \"|-------|--------|\" >> $GITHUB_STEP_SUMMARY echo \"| SAST Scan | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"| Dependency Audit | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"| Secrets Scan | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"| Plugin Security | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"| YAML Security | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"\" >> $GITHUB_STEP_SUMMARY echo \"\ud83c\udf89 All security gates passed!\" >> $GITHUB_STEP_SUMMARY Gate Check Scripts \u00b6 SAST Gate Check \u00b6 #!/usr/bin/env python3 \"\"\"SAST Gate Check Script\"\"\" import json import sys def check_sast_gate (): \"\"\"Check SAST gate criteria.\"\"\" try : with open ( 'bandit-report.json' ) as f : report = json . load ( f ) results = report . get ( 'results' , []) high_severity = [ r for r in results if r [ 'issue_severity' ] == 'HIGH' ] medium_severity = [ r for r in results if r [ 'issue_severity' ] == 'MEDIUM' ] if len ( high_severity ) > 0 : print ( '\u274c SAST Gate Failed: High severity issues found' ) for issue in high_severity : print ( f \" - { issue [ 'filename' ] } : { issue [ 'line_number' ] } - { issue [ 'issue_text' ] } \" ) return False elif len ( medium_severity ) > 5 : print ( '\u274c SAST Gate Failed: Too many medium severity issues' ) return False else : print ( '\u2705 SAST Gate Passed' ) return True except Exception as e : print ( f '\u274c SAST Gate Failed: { e } ' ) return False if __name__ == '__main__' : success = check_sast_gate () sys . exit ( 0 if success else 1 ) Dependency Gate Check \u00b6 #!/usr/bin/env python3 \"\"\"Dependency Gate Check Script\"\"\" import json import sys def check_dependency_gate (): \"\"\"Check dependency gate criteria.\"\"\" try : with open ( 'safety-report.json' ) as f : report = json . load ( f ) vulnerabilities = report . get ( 'vulnerabilities' , []) critical = [ v for v in vulnerabilities if v . get ( 'severity' ) == 'CRITICAL' ] high = [ v for v in vulnerabilities if v . get ( 'severity' ) == 'HIGH' ] if len ( critical ) > 0 : print ( '\u274c Dependency Gate Failed: Critical vulnerabilities found' ) for vuln in critical : print ( f \" - { vuln [ 'package_name' ] } ( { vuln [ 'analyzed_version' ] } )\" ) return False elif len ( high ) > 2 : print ( '\u274c Dependency Gate Failed: Too many high vulnerabilities' ) return False else : print ( '\u2705 Dependency Gate Passed' ) return True except Exception as e : print ( f '\u274c Dependency Gate Failed: { e } ' ) return False if __name__ == '__main__' : success = check_dependency_gate () sys . exit ( 0 if success else 1 ) Security Gate Configuration \u00b6 Gate Thresholds \u00b6 Development Branches \u00b6 SAST : 0 high, 10 medium, 50 low severity issues Dependencies : 0 critical, 5 high, 20 medium vulnerabilities Secrets : 0 high, 5 medium, 20 low confidence secrets Plugins : 0 policy violations, 100% checklist compliance YAML : 100% safe loading compliance Production Branches \u00b6 SAST : 0 high, 5 medium, 20 low severity issues Dependencies : 0 critical, 2 high, 5 medium vulnerabilities Secrets : 0 high, 2 medium, 5 low confidence secrets Plugins : 0 policy violations, 100% checklist compliance YAML : 100% safe loading compliance Gate Override Process \u00b6 Emergency Override \u00b6 Approval Required : Security Lead approval required Documentation : Override reason must be documented Timeline : Override valid for maximum 24 hours Follow-up : Remediation plan required within 24 hours Standard Override \u00b6 Approval Required : Security Lead and Engineering Lead approval Documentation : Override reason and remediation plan required Timeline : Override valid for maximum 7 days Follow-up : Progress review required within 7 days Security Gate Monitoring \u00b6 Gate Performance Metrics \u00b6 Success Rates \u00b6 Overall Success Rate : Target 95% success rate SAST Gate : Target 90% success rate Dependency Gate : Target 95% success rate Secrets Gate : Target 98% success rate Plugin Gate : Target 100% success rate YAML Gate : Target 100% success rate Performance Metrics \u00b6 Gate Execution Time : Target <5 minutes per gate False Positive Rate : Target <10% false positive rate Gate Reliability : Target 99% gate reliability Developer Satisfaction : Target 80% developer satisfaction Gate Improvement Process \u00b6 Continuous Improvement \u00b6 Monthly Reviews : Monthly review of gate performance Threshold Adjustment : Quarterly threshold adjustment Tool Updates : Regular tool updates and improvements Process Optimization : Continuous process optimization Feedback Collection \u00b6 Developer Feedback : Regular developer feedback collection Security Team Feedback : Security team feedback collection Stakeholder Feedback : Stakeholder feedback collection Metrics Analysis : Regular metrics analysis and reporting Security Gate Benefits \u00b6 Security Benefits \u00b6 Vulnerability Prevention : Prevents security vulnerabilities from reaching production Risk Reduction : Significant reduction in security risks Compliance : Ensures compliance with security standards Quality Assurance : Ensures security quality of code Operational Benefits \u00b6 Automated Enforcement : Automated enforcement of security standards Consistent Application : Consistent application of security standards Early Detection : Early detection of security issues Reduced Manual Work : Reduced manual security review work Business Benefits \u00b6 Cost Savings : Prevents costly security incidents Reputation Protection : Protects company reputation Compliance : Ensures regulatory compliance Competitive Advantage : Provides competitive advantage through security Security Gate Challenges \u00b6 Technical Challenges \u00b6 Tool Integration : Complex tool integration requirements Performance Impact : Performance impact on CI/CD pipeline False Positives : Managing false positive rates Tool Maintenance : Ongoing tool maintenance requirements Process Challenges \u00b6 Developer Adoption : Developer adoption of security gates Threshold Setting : Appropriate threshold setting Override Management : Managing gate overrides Documentation : Comprehensive documentation requirements Organizational Challenges \u00b6 Resource Allocation : Adequate resource allocation Training : Security training requirements Culture Change : Security culture change requirements Stakeholder Buy-in : Stakeholder buy-in requirements Security Gate Best Practices \u00b6 Implementation Best Practices \u00b6 Gradual Rollout : Gradual rollout of security gates Threshold Tuning : Careful threshold tuning Tool Selection : Careful tool selection Integration Quality : High-quality integration Operational Best Practices \u00b6 Regular Monitoring : Regular monitoring of gate performance Continuous Improvement : Continuous improvement of gates Documentation : Comprehensive documentation Training : Regular training on gate usage Management Best Practices \u00b6 Executive Support : Strong executive support Resource Allocation : Adequate resource allocation Culture Building : Security culture building Metrics Tracking : Regular metrics tracking References \u00b6 NIST SP 800-53 Security Controls OWASP Application Security Verification Standard ISO 27001 Information Security Management SOC 2 Trust Services Criteria","title":"Security Readiness Gate"},{"location":"security/security-gate/#security-readiness-gate","text":"","title":"Security Readiness Gate"},{"location":"security/security-gate/#overview","text":"This document defines the security readiness gate mechanism that enforces security standards before code can be merged to production branches.","title":"Overview"},{"location":"security/security-gate/#security-gate-definition","text":"","title":"Security Gate Definition"},{"location":"security/security-gate/#purpose","text":"The security readiness gate ensures that all code meets minimum security standards before being merged to production branches, preventing security vulnerabilities from reaching production.","title":"Purpose"},{"location":"security/security-gate/#scope","text":"All Pull Requests : Security gate applies to all pull requests Production Branches : Gate enforced for main, develop, and release branches Security-Sensitive Changes : Additional checks for security-sensitive changes Dependencies : Gate applies to dependency updates and additions","title":"Scope"},{"location":"security/security-gate/#security-gate-criteria","text":"","title":"Security Gate Criteria"},{"location":"security/security-gate/#1-static-application-security-testing-sast","text":"","title":"1. Static Application Security Testing (SAST)"},{"location":"security/security-gate/#criteria","text":"High Severity Issues : 0 high severity issues allowed Medium Severity Issues : Maximum 5 medium severity issues allowed Low Severity Issues : Maximum 20 low severity issues allowed New Issues : No new high or medium severity issues allowed","title":"Criteria"},{"location":"security/security-gate/#implementation","text":"sast-gate : runs-on : ubuntu-latest steps : - name : Run Bandit SAST run : bandit -r . -f json -o bandit-report.json - name : Check SAST Gate run : | python -c \" import json with open('bandit-report.json') as f: report = json.load(f) high_severity = [r for r in report['results'] if r['issue_severity'] == 'HIGH'] medium_severity = [r for r in report['results'] if r['issue_severity'] == 'MEDIUM'] if len(high_severity) > 0: print('\u274c SAST Gate Failed: High severity issues found') exit(1) elif len(medium_severity) > 5: print('\u274c SAST Gate Failed: Too many medium severity issues') exit(1) else: print('\u2705 SAST Gate Passed') \"","title":"Implementation"},{"location":"security/security-gate/#2-dependency-security-audit","text":"","title":"2. Dependency Security Audit"},{"location":"security/security-gate/#criteria_1","text":"Critical Vulnerabilities : 0 critical vulnerabilities allowed High Vulnerabilities : Maximum 2 high vulnerabilities allowed Medium Vulnerabilities : Maximum 5 medium vulnerabilities allowed New Vulnerabilities : No new critical or high vulnerabilities allowed","title":"Criteria"},{"location":"security/security-gate/#implementation_1","text":"dependency-gate : runs-on : ubuntu-latest steps : - name : Run Safety audit run : safety check --json --output safety-report.json - name : Check Dependency Gate run : | python -c \" import json with open('safety-report.json') as f: report = json.load(f) vulnerabilities = report.get('vulnerabilities', []) critical = [v for v in vulnerabilities if v.get('severity') == 'CRITICAL'] high = [v for v in vulnerabilities if v.get('severity') == 'HIGH'] if len(critical) > 0: print('\u274c Dependency Gate Failed: Critical vulnerabilities found') exit(1) elif len(high) > 2: print('\u274c Dependency Gate Failed: Too many high vulnerabilities') exit(1) else: print('\u2705 Dependency Gate Passed') \"","title":"Implementation"},{"location":"security/security-gate/#3-secrets-detection","text":"","title":"3. Secrets Detection"},{"location":"security/security-gate/#criteria_2","text":"High Confidence Secrets : 0 high confidence secrets allowed Medium Confidence Secrets : Maximum 2 medium confidence secrets allowed Low Confidence Secrets : Maximum 5 low confidence secrets allowed New Secrets : No new high or medium confidence secrets allowed","title":"Criteria"},{"location":"security/security-gate/#implementation_2","text":"secrets-gate : runs-on : ubuntu-latest steps : - name : Run TruffleHog scan run : trufflehog . --json > trufflehog-report.json - name : Check Secrets Gate run : | python -c \" import json try: with open('trufflehog-report.json') as f: findings = [json.loads(line) for line in f if line.strip()] high_confidence = [f for f in findings if f.get('confidence') == 'high'] medium_confidence = [f for f in findings if f.get('confidence') == 'medium'] if len(high_confidence) > 0: print('\u274c Secrets Gate Failed: High confidence secrets found') exit(1) elif len(medium_confidence) > 2: print('\u274c Secrets Gate Failed: Too many medium confidence secrets') exit(1) else: print('\u2705 Secrets Gate Passed') except: print('\u2705 Secrets Gate Passed (no findings)') \"","title":"Implementation"},{"location":"security/security-gate/#4-plugin-security-validation","text":"","title":"4. Plugin Security Validation"},{"location":"security/security-gate/#criteria_3","text":"Policy Violations : 0 policy violations allowed Signature Failures : 0 signature verification failures allowed Security Checklist : 100% security checklist compliance required New Plugins : Additional review required for new plugins","title":"Criteria"},{"location":"security/security-gate/#implementation_3","text":"plugin-gate : runs-on : ubuntu-latest steps : - name : Run plugin security audit run : python tools/plugin_security_audit.py > plugin-audit.txt - name : Check Plugin Gate run : | if grep -q \"ERROR\\|FAILED\" plugin-audit.txt; then echo \"\u274c Plugin Gate Failed: Plugin security issues found\" cat plugin-audit.txt exit(1) else echo \"\u2705 Plugin Gate Passed\" fi","title":"Implementation"},{"location":"security/security-gate/#5-yaml-security-validation","text":"","title":"5. YAML Security Validation"},{"location":"security/security-gate/#criteria_4","text":"Safe Loading : 100% yaml.safe_load() usage required Schema Validation : All YAML files must pass schema validation Injection Prevention : No YAML injection vulnerabilities allowed New YAML Files : Additional validation required for new YAML files","title":"Criteria"},{"location":"security/security-gate/#implementation_4","text":"yaml-gate : runs-on : ubuntu-latest steps : - name : Check YAML security run : | if grep -r \"yaml\\.load(\" --include=\"*.py\" .; then echo \"\u274c YAML Gate Failed: Unsafe yaml.load() usage found\" exit(1) else echo \"\u2705 YAML Gate Passed\" fi","title":"Implementation"},{"location":"security/security-gate/#security-gate-implementation","text":"","title":"Security Gate Implementation"},{"location":"security/security-gate/#github-actions-workflow","text":"name : Security Gate on : pull_request : branches : [ main , develop , release/* ] push : branches : [ main , develop ] jobs : security-gate : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v4 - name : Set up Python uses : actions/setup-python@v4 with : python-version : '3.11' - name : Install security tools run : | pip install bandit safety trufflehog pyyaml - name : Run SAST Gate run : | bandit -r . -f json -o bandit-report.json python scripts/check_sast_gate.py - name : Run Dependency Gate run : | safety check --json --output safety-report.json python scripts/check_dependency_gate.py - name : Run Secrets Gate run : | trufflehog . --json > trufflehog-report.json python scripts/check_secrets_gate.py - name : Run Plugin Gate run : | python tools/plugin_security_audit.py > plugin-audit.txt python scripts/check_plugin_gate.py - name : Run YAML Gate run : | python scripts/check_yaml_gate.py - name : Security Gate Summary run : | echo \"## \ud83d\udd12 Security Gate Summary\" >> $GITHUB_STEP_SUMMARY echo \"\" >> $GITHUB_STEP_SUMMARY echo \"| Check | Status |\" >> $GITHUB_STEP_SUMMARY echo \"|-------|--------|\" >> $GITHUB_STEP_SUMMARY echo \"| SAST Scan | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"| Dependency Audit | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"| Secrets Scan | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"| Plugin Security | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"| YAML Security | \u2705 Passed |\" >> $GITHUB_STEP_SUMMARY echo \"\" >> $GITHUB_STEP_SUMMARY echo \"\ud83c\udf89 All security gates passed!\" >> $GITHUB_STEP_SUMMARY","title":"GitHub Actions Workflow"},{"location":"security/security-gate/#gate-check-scripts","text":"","title":"Gate Check Scripts"},{"location":"security/security-gate/#sast-gate-check","text":"#!/usr/bin/env python3 \"\"\"SAST Gate Check Script\"\"\" import json import sys def check_sast_gate (): \"\"\"Check SAST gate criteria.\"\"\" try : with open ( 'bandit-report.json' ) as f : report = json . load ( f ) results = report . get ( 'results' , []) high_severity = [ r for r in results if r [ 'issue_severity' ] == 'HIGH' ] medium_severity = [ r for r in results if r [ 'issue_severity' ] == 'MEDIUM' ] if len ( high_severity ) > 0 : print ( '\u274c SAST Gate Failed: High severity issues found' ) for issue in high_severity : print ( f \" - { issue [ 'filename' ] } : { issue [ 'line_number' ] } - { issue [ 'issue_text' ] } \" ) return False elif len ( medium_severity ) > 5 : print ( '\u274c SAST Gate Failed: Too many medium severity issues' ) return False else : print ( '\u2705 SAST Gate Passed' ) return True except Exception as e : print ( f '\u274c SAST Gate Failed: { e } ' ) return False if __name__ == '__main__' : success = check_sast_gate () sys . exit ( 0 if success else 1 )","title":"SAST Gate Check"},{"location":"security/security-gate/#dependency-gate-check","text":"#!/usr/bin/env python3 \"\"\"Dependency Gate Check Script\"\"\" import json import sys def check_dependency_gate (): \"\"\"Check dependency gate criteria.\"\"\" try : with open ( 'safety-report.json' ) as f : report = json . load ( f ) vulnerabilities = report . get ( 'vulnerabilities' , []) critical = [ v for v in vulnerabilities if v . get ( 'severity' ) == 'CRITICAL' ] high = [ v for v in vulnerabilities if v . get ( 'severity' ) == 'HIGH' ] if len ( critical ) > 0 : print ( '\u274c Dependency Gate Failed: Critical vulnerabilities found' ) for vuln in critical : print ( f \" - { vuln [ 'package_name' ] } ( { vuln [ 'analyzed_version' ] } )\" ) return False elif len ( high ) > 2 : print ( '\u274c Dependency Gate Failed: Too many high vulnerabilities' ) return False else : print ( '\u2705 Dependency Gate Passed' ) return True except Exception as e : print ( f '\u274c Dependency Gate Failed: { e } ' ) return False if __name__ == '__main__' : success = check_dependency_gate () sys . exit ( 0 if success else 1 )","title":"Dependency Gate Check"},{"location":"security/security-gate/#security-gate-configuration","text":"","title":"Security Gate Configuration"},{"location":"security/security-gate/#gate-thresholds","text":"","title":"Gate Thresholds"},{"location":"security/security-gate/#development-branches","text":"SAST : 0 high, 10 medium, 50 low severity issues Dependencies : 0 critical, 5 high, 20 medium vulnerabilities Secrets : 0 high, 5 medium, 20 low confidence secrets Plugins : 0 policy violations, 100% checklist compliance YAML : 100% safe loading compliance","title":"Development Branches"},{"location":"security/security-gate/#production-branches","text":"SAST : 0 high, 5 medium, 20 low severity issues Dependencies : 0 critical, 2 high, 5 medium vulnerabilities Secrets : 0 high, 2 medium, 5 low confidence secrets Plugins : 0 policy violations, 100% checklist compliance YAML : 100% safe loading compliance","title":"Production Branches"},{"location":"security/security-gate/#gate-override-process","text":"","title":"Gate Override Process"},{"location":"security/security-gate/#emergency-override","text":"Approval Required : Security Lead approval required Documentation : Override reason must be documented Timeline : Override valid for maximum 24 hours Follow-up : Remediation plan required within 24 hours","title":"Emergency Override"},{"location":"security/security-gate/#standard-override","text":"Approval Required : Security Lead and Engineering Lead approval Documentation : Override reason and remediation plan required Timeline : Override valid for maximum 7 days Follow-up : Progress review required within 7 days","title":"Standard Override"},{"location":"security/security-gate/#security-gate-monitoring","text":"","title":"Security Gate Monitoring"},{"location":"security/security-gate/#gate-performance-metrics","text":"","title":"Gate Performance Metrics"},{"location":"security/security-gate/#success-rates","text":"Overall Success Rate : Target 95% success rate SAST Gate : Target 90% success rate Dependency Gate : Target 95% success rate Secrets Gate : Target 98% success rate Plugin Gate : Target 100% success rate YAML Gate : Target 100% success rate","title":"Success Rates"},{"location":"security/security-gate/#performance-metrics","text":"Gate Execution Time : Target <5 minutes per gate False Positive Rate : Target <10% false positive rate Gate Reliability : Target 99% gate reliability Developer Satisfaction : Target 80% developer satisfaction","title":"Performance Metrics"},{"location":"security/security-gate/#gate-improvement-process","text":"","title":"Gate Improvement Process"},{"location":"security/security-gate/#continuous-improvement","text":"Monthly Reviews : Monthly review of gate performance Threshold Adjustment : Quarterly threshold adjustment Tool Updates : Regular tool updates and improvements Process Optimization : Continuous process optimization","title":"Continuous Improvement"},{"location":"security/security-gate/#feedback-collection","text":"Developer Feedback : Regular developer feedback collection Security Team Feedback : Security team feedback collection Stakeholder Feedback : Stakeholder feedback collection Metrics Analysis : Regular metrics analysis and reporting","title":"Feedback Collection"},{"location":"security/security-gate/#security-gate-benefits","text":"","title":"Security Gate Benefits"},{"location":"security/security-gate/#security-benefits","text":"Vulnerability Prevention : Prevents security vulnerabilities from reaching production Risk Reduction : Significant reduction in security risks Compliance : Ensures compliance with security standards Quality Assurance : Ensures security quality of code","title":"Security Benefits"},{"location":"security/security-gate/#operational-benefits","text":"Automated Enforcement : Automated enforcement of security standards Consistent Application : Consistent application of security standards Early Detection : Early detection of security issues Reduced Manual Work : Reduced manual security review work","title":"Operational Benefits"},{"location":"security/security-gate/#business-benefits","text":"Cost Savings : Prevents costly security incidents Reputation Protection : Protects company reputation Compliance : Ensures regulatory compliance Competitive Advantage : Provides competitive advantage through security","title":"Business Benefits"},{"location":"security/security-gate/#security-gate-challenges","text":"","title":"Security Gate Challenges"},{"location":"security/security-gate/#technical-challenges","text":"Tool Integration : Complex tool integration requirements Performance Impact : Performance impact on CI/CD pipeline False Positives : Managing false positive rates Tool Maintenance : Ongoing tool maintenance requirements","title":"Technical Challenges"},{"location":"security/security-gate/#process-challenges","text":"Developer Adoption : Developer adoption of security gates Threshold Setting : Appropriate threshold setting Override Management : Managing gate overrides Documentation : Comprehensive documentation requirements","title":"Process Challenges"},{"location":"security/security-gate/#organizational-challenges","text":"Resource Allocation : Adequate resource allocation Training : Security training requirements Culture Change : Security culture change requirements Stakeholder Buy-in : Stakeholder buy-in requirements","title":"Organizational Challenges"},{"location":"security/security-gate/#security-gate-best-practices","text":"","title":"Security Gate Best Practices"},{"location":"security/security-gate/#implementation-best-practices","text":"Gradual Rollout : Gradual rollout of security gates Threshold Tuning : Careful threshold tuning Tool Selection : Careful tool selection Integration Quality : High-quality integration","title":"Implementation Best Practices"},{"location":"security/security-gate/#operational-best-practices","text":"Regular Monitoring : Regular monitoring of gate performance Continuous Improvement : Continuous improvement of gates Documentation : Comprehensive documentation Training : Regular training on gate usage","title":"Operational Best Practices"},{"location":"security/security-gate/#management-best-practices","text":"Executive Support : Strong executive support Resource Allocation : Adequate resource allocation Culture Building : Security culture building Metrics Tracking : Regular metrics tracking","title":"Management Best Practices"},{"location":"security/security-gate/#references","text":"NIST SP 800-53 Security Controls OWASP Application Security Verification Standard ISO 27001 Information Security Management SOC 2 Trust Services Criteria","title":"References"},{"location":"security/security-readiness-matrix/","text":"Security Readiness Matrix \u00b6 Overview \u00b6 This matrix provides a comprehensive view of SecFlow's security readiness across different areas and milestones. Security Readiness Matrix \u00b6 Area Control Implemented Status Confidence Next Milestone Compliance Level Plugin Manifest Checklist, Validation \u2705 High M2 refinement 85% Signature Verification SHA256 hash \u2705 Medium RSA/ECDSA 60% Sandboxing Process-based \u2705 Medium Container-based 70% Dependency Audit Safety \u2705 High CI integration 90% SAST Scanning Bandit \u2705 High Enhanced rules 85% Secrets Detection TruffleHog \u2705 Medium Real-time scanning 75% YAML Security Safe loading \u2705 High Enhanced validation 95% PR Security Review Checklist, Guidelines \u2705 High Automated checks 80% Threat Modeling STRIDE framework \u2705 High Attack simulation 85% Standards Mapping NIST, OWASP, ISO \u2705 High Compliance automation 80% Continuous Monitoring Daily scans \u2705 Medium Real-time alerts 70% Runtime Enforcement Policy hooks \u2705 Low Full isolation 50% Detailed Readiness Assessment \u00b6 M1 (Current) - Foundation Security \u00b6 \u2705 High Readiness (80-100%) \u00b6 YAML Security : Safe loading implemented across all codebase Dependency Audit : Safety integration with comprehensive reporting SAST Scanning : Bandit integration with detailed issue tracking PR Security Review : Comprehensive checklist and guidelines Threat Modeling : STRIDE framework with detailed threat scenarios \u26a0\ufe0f Medium Readiness (50-79%) \u00b6 Plugin Manifest : Security checklist implemented, validation framework ready Signature Verification : Hash-based verification operational, cryptographic ready for M2 Sandboxing : Process-based isolation implemented, container-based planned Secrets Detection : TruffleHog integration with CI/CD pipeline Standards Mapping : Comprehensive mapping to NIST, OWASP, ISO standards Continuous Monitoring : Daily automated scans with reporting \u274c Low Readiness (0-49%) \u00b6 Runtime Enforcement : Policy hooks implemented, full isolation pending M2 M2 (Next 3 months) - Enhanced Security \u00b6 Planned Enhancements \u00b6 Cryptographic Signatures : RSA/ECDSA signature verification Container Sandboxing : Full isolation using Docker/containerd Real-time Monitoring : Live security event monitoring Enhanced SAST : Custom rules and deeper analysis Secrets Management : Formal secrets management system Compliance Automation : Automated compliance reporting M2 Readiness Targets \u00b6 Plugin Security : 95% readiness with cryptographic verification Sandboxing : 90% readiness with container isolation Monitoring : 85% readiness with real-time alerts Compliance : 90% readiness with automated reporting M3 (Next 6 months) - Advanced Security \u00b6 Planned Enhancements \u00b6 User Authentication : Complete authentication system Incident Response : Formal incident response procedures Security Training : Comprehensive security awareness program Penetration Testing : Regular security testing Third-party Audits : External security assessments M3 Readiness Targets \u00b6 Authentication : 90% readiness with full user management Incident Response : 85% readiness with formal procedures Security Culture : 80% readiness with training program External Validation : 75% readiness with third-party audits M4 (Next 12 months) - Enterprise Security \u00b6 Planned Enhancements \u00b6 Full Compliance : Complete NIST SP 800-53 compliance SOC 2 Certification : SOC 2 Type II certification ISO 27001 : ISO 27001 certification Zero Trust Architecture : Zero trust security model Advanced Threat Protection : AI-powered threat detection M4 Readiness Targets \u00b6 Compliance : 95% readiness with full certification Zero Trust : 90% readiness with complete implementation Threat Protection : 85% readiness with AI integration Enterprise Readiness : 95% readiness for enterprise deployment Security Control Effectiveness \u00b6 Control Coverage Analysis \u00b6 High Effectiveness Controls (90-100%) \u00b6 YAML Safe Loading : Prevents deserialization attacks Dependency Auditing : Identifies known vulnerabilities SAST Scanning : Detects code-level security issues PR Security Review : Prevents security issues at development time Medium Effectiveness Controls (70-89%) \u00b6 Plugin Signature Verification : Prevents unauthorized plugin execution Sandbox Execution : Isolates plugin execution environment Secrets Detection : Identifies exposed secrets Threat Modeling : Guides security control implementation Low Effectiveness Controls (50-69%) \u00b6 Runtime Policy Enforcement : Basic policy enforcement implemented Continuous Monitoring : Daily scans with limited real-time capability Risk Reduction Analysis \u00b6 High Risk Reduction (80-100%) \u00b6 Code Injection : YAML safe loading prevents injection attacks Supply Chain : Dependency auditing prevents compromised dependencies Development Issues : SAST scanning prevents security bugs Process Issues : PR security review prevents security oversights Medium Risk Reduction (60-79%) \u00b6 Plugin Compromise : Signature verification prevents malicious plugins Resource Abuse : Sandbox execution prevents resource exhaustion Secret Exposure : Secrets detection prevents credential leaks Threat Response : Threat modeling guides security decisions Low Risk Reduction (40-59%) \u00b6 Runtime Attacks : Basic policy enforcement provides limited protection Real-time Threats : Daily monitoring provides delayed threat detection Security Metrics Dashboard \u00b6 Key Performance Indicators (KPIs) \u00b6 Security Posture Metrics \u00b6 Overall Security Score : 78% (M1 baseline) Control Coverage : 85% of identified threats covered Compliance Score : 80% compliance with security standards Risk Reduction : 75% reduction in security risks Operational Metrics \u00b6 Scan Success Rate : 95% successful security scans False Positive Rate : 15% false positive rate Mean Time to Detection : 24 hours for security issues Mean Time to Response : 48 hours for security incidents Development Metrics \u00b6 Security Review Coverage : 100% of PRs reviewed Security Training Completion : 60% of developers trained Security Tool Adoption : 85% adoption of security tools Security Issue Resolution : 90% of issues resolved within SLA Trend Analysis \u00b6 Positive Trends \u00b6 Security Awareness : Increasing developer security awareness Tool Adoption : Growing adoption of security tools Issue Resolution : Improving security issue resolution times Compliance : Steady improvement in compliance scores Areas for Improvement \u00b6 Real-time Monitoring : Need for faster threat detection Runtime Security : Need for stronger runtime protection Incident Response : Need for formal incident procedures Security Training : Need for comprehensive training program Security Readiness Roadmap \u00b6 Immediate Actions (Next 30 days) \u00b6 Fix High Severity Issues : Address all high-severity SAST findings Upgrade Dependencies : Fix mkdocs-material vulnerability Enhance Monitoring : Implement real-time security alerts Security Training : Conduct security awareness training Short-term Goals (Next 90 days) \u00b6 M2 Preparation : Prepare for cryptographic signature verification Container Sandboxing : Implement container-based sandboxing Enhanced SAST : Add custom security rules Incident Response : Develop formal incident response procedures Long-term Goals (Next 12 months) \u00b6 Full Compliance : Achieve complete compliance with security standards Certification : Obtain SOC 2 and ISO 27001 certifications Zero Trust : Implement zero trust security architecture Advanced Protection : Deploy AI-powered threat detection Security Readiness Validation \u00b6 Validation Criteria \u00b6 Control Implementation : All security controls implemented and operational Testing Coverage : Comprehensive security testing coverage Documentation : Complete security documentation Training : Security training completed for all team members Monitoring : Continuous security monitoring operational Incident Response : Formal incident response procedures in place Validation Methods \u00b6 Security Testing : Penetration testing and vulnerability assessment Code Review : Security-focused code review Compliance Audit : External compliance audit Risk Assessment : Comprehensive security risk assessment Performance Testing : Security control performance testing Success Metrics \u00b6 Security Score : Achieve target security readiness score Compliance : Meet compliance requirements Risk Reduction : Achieve target risk reduction Incident Response : Meet incident response time targets Training : Achieve training completion targets References \u00b6 NIST SP 800-53 Security Controls OWASP Application Security Verification Standard ISO 27001 Information Security Management SOC 2 Trust Services Criteria","title":"Security Readiness Matrix"},{"location":"security/security-readiness-matrix/#security-readiness-matrix","text":"","title":"Security Readiness Matrix"},{"location":"security/security-readiness-matrix/#overview","text":"This matrix provides a comprehensive view of SecFlow's security readiness across different areas and milestones.","title":"Overview"},{"location":"security/security-readiness-matrix/#security-readiness-matrix_1","text":"Area Control Implemented Status Confidence Next Milestone Compliance Level Plugin Manifest Checklist, Validation \u2705 High M2 refinement 85% Signature Verification SHA256 hash \u2705 Medium RSA/ECDSA 60% Sandboxing Process-based \u2705 Medium Container-based 70% Dependency Audit Safety \u2705 High CI integration 90% SAST Scanning Bandit \u2705 High Enhanced rules 85% Secrets Detection TruffleHog \u2705 Medium Real-time scanning 75% YAML Security Safe loading \u2705 High Enhanced validation 95% PR Security Review Checklist, Guidelines \u2705 High Automated checks 80% Threat Modeling STRIDE framework \u2705 High Attack simulation 85% Standards Mapping NIST, OWASP, ISO \u2705 High Compliance automation 80% Continuous Monitoring Daily scans \u2705 Medium Real-time alerts 70% Runtime Enforcement Policy hooks \u2705 Low Full isolation 50%","title":"Security Readiness Matrix"},{"location":"security/security-readiness-matrix/#detailed-readiness-assessment","text":"","title":"Detailed Readiness Assessment"},{"location":"security/security-readiness-matrix/#m1-current-foundation-security","text":"","title":"M1 (Current) - Foundation Security"},{"location":"security/security-readiness-matrix/#high-readiness-80-100","text":"YAML Security : Safe loading implemented across all codebase Dependency Audit : Safety integration with comprehensive reporting SAST Scanning : Bandit integration with detailed issue tracking PR Security Review : Comprehensive checklist and guidelines Threat Modeling : STRIDE framework with detailed threat scenarios","title":"\u2705 High Readiness (80-100%)"},{"location":"security/security-readiness-matrix/#medium-readiness-50-79","text":"Plugin Manifest : Security checklist implemented, validation framework ready Signature Verification : Hash-based verification operational, cryptographic ready for M2 Sandboxing : Process-based isolation implemented, container-based planned Secrets Detection : TruffleHog integration with CI/CD pipeline Standards Mapping : Comprehensive mapping to NIST, OWASP, ISO standards Continuous Monitoring : Daily automated scans with reporting","title":"\u26a0\ufe0f Medium Readiness (50-79%)"},{"location":"security/security-readiness-matrix/#low-readiness-0-49","text":"Runtime Enforcement : Policy hooks implemented, full isolation pending M2","title":"\u274c Low Readiness (0-49%)"},{"location":"security/security-readiness-matrix/#m2-next-3-months-enhanced-security","text":"","title":"M2 (Next 3 months) - Enhanced Security"},{"location":"security/security-readiness-matrix/#planned-enhancements","text":"Cryptographic Signatures : RSA/ECDSA signature verification Container Sandboxing : Full isolation using Docker/containerd Real-time Monitoring : Live security event monitoring Enhanced SAST : Custom rules and deeper analysis Secrets Management : Formal secrets management system Compliance Automation : Automated compliance reporting","title":"Planned Enhancements"},{"location":"security/security-readiness-matrix/#m2-readiness-targets","text":"Plugin Security : 95% readiness with cryptographic verification Sandboxing : 90% readiness with container isolation Monitoring : 85% readiness with real-time alerts Compliance : 90% readiness with automated reporting","title":"M2 Readiness Targets"},{"location":"security/security-readiness-matrix/#m3-next-6-months-advanced-security","text":"","title":"M3 (Next 6 months) - Advanced Security"},{"location":"security/security-readiness-matrix/#planned-enhancements_1","text":"User Authentication : Complete authentication system Incident Response : Formal incident response procedures Security Training : Comprehensive security awareness program Penetration Testing : Regular security testing Third-party Audits : External security assessments","title":"Planned Enhancements"},{"location":"security/security-readiness-matrix/#m3-readiness-targets","text":"Authentication : 90% readiness with full user management Incident Response : 85% readiness with formal procedures Security Culture : 80% readiness with training program External Validation : 75% readiness with third-party audits","title":"M3 Readiness Targets"},{"location":"security/security-readiness-matrix/#m4-next-12-months-enterprise-security","text":"","title":"M4 (Next 12 months) - Enterprise Security"},{"location":"security/security-readiness-matrix/#planned-enhancements_2","text":"Full Compliance : Complete NIST SP 800-53 compliance SOC 2 Certification : SOC 2 Type II certification ISO 27001 : ISO 27001 certification Zero Trust Architecture : Zero trust security model Advanced Threat Protection : AI-powered threat detection","title":"Planned Enhancements"},{"location":"security/security-readiness-matrix/#m4-readiness-targets","text":"Compliance : 95% readiness with full certification Zero Trust : 90% readiness with complete implementation Threat Protection : 85% readiness with AI integration Enterprise Readiness : 95% readiness for enterprise deployment","title":"M4 Readiness Targets"},{"location":"security/security-readiness-matrix/#security-control-effectiveness","text":"","title":"Security Control Effectiveness"},{"location":"security/security-readiness-matrix/#control-coverage-analysis","text":"","title":"Control Coverage Analysis"},{"location":"security/security-readiness-matrix/#high-effectiveness-controls-90-100","text":"YAML Safe Loading : Prevents deserialization attacks Dependency Auditing : Identifies known vulnerabilities SAST Scanning : Detects code-level security issues PR Security Review : Prevents security issues at development time","title":"High Effectiveness Controls (90-100%)"},{"location":"security/security-readiness-matrix/#medium-effectiveness-controls-70-89","text":"Plugin Signature Verification : Prevents unauthorized plugin execution Sandbox Execution : Isolates plugin execution environment Secrets Detection : Identifies exposed secrets Threat Modeling : Guides security control implementation","title":"Medium Effectiveness Controls (70-89%)"},{"location":"security/security-readiness-matrix/#low-effectiveness-controls-50-69","text":"Runtime Policy Enforcement : Basic policy enforcement implemented Continuous Monitoring : Daily scans with limited real-time capability","title":"Low Effectiveness Controls (50-69%)"},{"location":"security/security-readiness-matrix/#risk-reduction-analysis","text":"","title":"Risk Reduction Analysis"},{"location":"security/security-readiness-matrix/#high-risk-reduction-80-100","text":"Code Injection : YAML safe loading prevents injection attacks Supply Chain : Dependency auditing prevents compromised dependencies Development Issues : SAST scanning prevents security bugs Process Issues : PR security review prevents security oversights","title":"High Risk Reduction (80-100%)"},{"location":"security/security-readiness-matrix/#medium-risk-reduction-60-79","text":"Plugin Compromise : Signature verification prevents malicious plugins Resource Abuse : Sandbox execution prevents resource exhaustion Secret Exposure : Secrets detection prevents credential leaks Threat Response : Threat modeling guides security decisions","title":"Medium Risk Reduction (60-79%)"},{"location":"security/security-readiness-matrix/#low-risk-reduction-40-59","text":"Runtime Attacks : Basic policy enforcement provides limited protection Real-time Threats : Daily monitoring provides delayed threat detection","title":"Low Risk Reduction (40-59%)"},{"location":"security/security-readiness-matrix/#security-metrics-dashboard","text":"","title":"Security Metrics Dashboard"},{"location":"security/security-readiness-matrix/#key-performance-indicators-kpis","text":"","title":"Key Performance Indicators (KPIs)"},{"location":"security/security-readiness-matrix/#security-posture-metrics","text":"Overall Security Score : 78% (M1 baseline) Control Coverage : 85% of identified threats covered Compliance Score : 80% compliance with security standards Risk Reduction : 75% reduction in security risks","title":"Security Posture Metrics"},{"location":"security/security-readiness-matrix/#operational-metrics","text":"Scan Success Rate : 95% successful security scans False Positive Rate : 15% false positive rate Mean Time to Detection : 24 hours for security issues Mean Time to Response : 48 hours for security incidents","title":"Operational Metrics"},{"location":"security/security-readiness-matrix/#development-metrics","text":"Security Review Coverage : 100% of PRs reviewed Security Training Completion : 60% of developers trained Security Tool Adoption : 85% adoption of security tools Security Issue Resolution : 90% of issues resolved within SLA","title":"Development Metrics"},{"location":"security/security-readiness-matrix/#trend-analysis","text":"","title":"Trend Analysis"},{"location":"security/security-readiness-matrix/#positive-trends","text":"Security Awareness : Increasing developer security awareness Tool Adoption : Growing adoption of security tools Issue Resolution : Improving security issue resolution times Compliance : Steady improvement in compliance scores","title":"Positive Trends"},{"location":"security/security-readiness-matrix/#areas-for-improvement","text":"Real-time Monitoring : Need for faster threat detection Runtime Security : Need for stronger runtime protection Incident Response : Need for formal incident procedures Security Training : Need for comprehensive training program","title":"Areas for Improvement"},{"location":"security/security-readiness-matrix/#security-readiness-roadmap","text":"","title":"Security Readiness Roadmap"},{"location":"security/security-readiness-matrix/#immediate-actions-next-30-days","text":"Fix High Severity Issues : Address all high-severity SAST findings Upgrade Dependencies : Fix mkdocs-material vulnerability Enhance Monitoring : Implement real-time security alerts Security Training : Conduct security awareness training","title":"Immediate Actions (Next 30 days)"},{"location":"security/security-readiness-matrix/#short-term-goals-next-90-days","text":"M2 Preparation : Prepare for cryptographic signature verification Container Sandboxing : Implement container-based sandboxing Enhanced SAST : Add custom security rules Incident Response : Develop formal incident response procedures","title":"Short-term Goals (Next 90 days)"},{"location":"security/security-readiness-matrix/#long-term-goals-next-12-months","text":"Full Compliance : Achieve complete compliance with security standards Certification : Obtain SOC 2 and ISO 27001 certifications Zero Trust : Implement zero trust security architecture Advanced Protection : Deploy AI-powered threat detection","title":"Long-term Goals (Next 12 months)"},{"location":"security/security-readiness-matrix/#security-readiness-validation","text":"","title":"Security Readiness Validation"},{"location":"security/security-readiness-matrix/#validation-criteria","text":"Control Implementation : All security controls implemented and operational Testing Coverage : Comprehensive security testing coverage Documentation : Complete security documentation Training : Security training completed for all team members Monitoring : Continuous security monitoring operational Incident Response : Formal incident response procedures in place","title":"Validation Criteria"},{"location":"security/security-readiness-matrix/#validation-methods","text":"Security Testing : Penetration testing and vulnerability assessment Code Review : Security-focused code review Compliance Audit : External compliance audit Risk Assessment : Comprehensive security risk assessment Performance Testing : Security control performance testing","title":"Validation Methods"},{"location":"security/security-readiness-matrix/#success-metrics","text":"Security Score : Achieve target security readiness score Compliance : Meet compliance requirements Risk Reduction : Achieve target risk reduction Incident Response : Meet incident response time targets Training : Achieve training completion targets","title":"Success Metrics"},{"location":"security/security-readiness-matrix/#references","text":"NIST SP 800-53 Security Controls OWASP Application Security Verification Standard ISO 27001 Information Security Management SOC 2 Trust Services Criteria","title":"References"},{"location":"security/security-review-guidelines/","text":"Security Review Guidelines \u00b6 Overview \u00b6 This document provides guidelines for conducting security reviews of pull requests in the SecFlow project. Security Review Checklist \u00b6 1. Code Security Review \u00b6 Input Validation \u00b6 User Input Validation : All user inputs are validated and sanitized YAML Parsing : Uses yaml.safe_load() instead of yaml.load() JSON Parsing : Uses safe JSON parsing methods File Uploads : File uploads are validated for type and size URL Validation : URLs are validated before use Authentication & Authorization \u00b6 Authentication : Proper authentication mechanisms in place Authorization : Access control implemented correctly Session Management : Secure session handling Password Security : Strong password requirements API Security : API endpoints properly secured Data Security \u00b6 Data Encryption : Sensitive data encrypted at rest and in transit Secrets Management : No hardcoded secrets or credentials Data Sanitization : Output data properly sanitized SQL Injection : No SQL injection vulnerabilities XSS Prevention : Cross-site scripting prevention implemented 2. Plugin Security Review \u00b6 Plugin Policy Compliance \u00b6 Deny-by-Default : Plugin follows deny-by-default policy Resource Limits : CPU and memory limits specified Filesystem Access : Filesystem access properly restricted Network Access : Network access properly controlled Signature Verification : Plugin signature verified Plugin Manifest Security \u00b6 Required Fields : All required security fields present Permission Validation : Permissions properly validated Sandbox Configuration : Sandbox settings appropriate Security Section : Security section complete and valid 3. Infrastructure Security Review \u00b6 Dependencies \u00b6 Dependency Audit : No known vulnerabilities in dependencies Version Pinning : Dependencies pinned to specific versions License Compliance : Dependencies have compatible licenses Supply Chain : Supply chain security considerations Configuration \u00b6 Secure Defaults : Secure default configurations Environment Variables : Sensitive data in environment variables Logging Security : No sensitive data in logs Error Handling : Secure error handling without information disclosure 4. Workflow Security Review \u00b6 Recipe Validation \u00b6 Schema Validation : Workflow recipes validated against schema Node Security : No dangerous node types (shell, exec, eval) Reference Validation : Node references properly validated External Resources : External resources properly validated Execution Security \u00b6 Sandboxing : Workflow execution properly sandboxed Resource Limits : Resource limits enforced Isolation : Proper isolation between workflows Audit Logging : Execution properly logged Security Review Process \u00b6 1. Automated Security Checks \u00b6 Bandit SAST : Static analysis for security issues Safety : Dependency vulnerability scanning YAML Security : YAML parsing security validation Plugin Security : Plugin policy compliance checking 2. Manual Security Review \u00b6 Code Review : Security-focused code review Architecture Review : Security architecture assessment Threat Modeling : Threat model analysis Penetration Testing : Security testing (if applicable) 3. Security Approval Criteria \u00b6 No High Severity Issues : No high-severity security issues Medium Issues Addressed : Medium-severity issues documented and mitigated Security Controls : Appropriate security controls implemented Documentation : Security considerations documented Security Review Tools \u00b6 Static Analysis \u00b6 # Run Bandit security linter bandit -r . -f json -o bandit-report.json # Run Safety dependency audit safety check --json # Run plugin security audit python tools/plugin_security_audit.py Dynamic Analysis \u00b6 # Run security tests pytest tests/test_plugin_security.py # Run workflow security tests pytest tests/test_workflow_security.py Security Review Templates \u00b6 Security Review Comment \u00b6 ## Security Review ### Security Assessment: \u2705 APPROVED / \u26a0\ufe0f NEEDS WORK / \u274c BLOCKED #### Issues Found: - [ ] Issue 1: Description and remediation - [ ] Issue 2: Description and remediation #### Security Controls Verified: - [ ] Input validation implemented - [ ] Authentication/authorization secure - [ ] No hardcoded secrets - [ ] Plugin policy compliance - [ ] Dependency security #### Recommendations: - Recommendation 1 - Recommendation 2 #### Approval: - [ ] Security Lead approval required - [ ] Additional security testing needed - [ ] Documentation updates required Security Review Checklist \u00b6 ## Security Review Checklist ### Code Security - [ ] Input validation implemented - [ ] YAML parsing uses safe_load() - [ ] No hardcoded secrets - [ ] SQL injection prevention - [ ] XSS prevention ### Plugin Security - [ ] Plugin policy compliance - [ ] Resource limits specified - [ ] Filesystem access restricted - [ ] Network access controlled - [ ] Signature verification ### Infrastructure Security - [ ] Dependencies audited - [ ] Secure configurations - [ ] Proper error handling - [ ] Audit logging implemented ### Workflow Security - [ ] Recipe validation - [ ] Node security - [ ] Execution sandboxing - [ ] Resource limits enforced Security Review Responsibilities \u00b6 Security Lead \u00b6 Primary : Security architecture and policy Secondary : Plugin security and sandboxing Review : Security-sensitive changes Development Team \u00b6 Primary : Code security implementation Secondary : Security testing Review : General security practices DevOps Team \u00b6 Primary : Infrastructure security Secondary : Dependency management Review : Deployment security Security Review Escalation \u00b6 Escalation Criteria \u00b6 High Severity Issues : Immediate escalation to Security Lead Medium Severity Issues : Document and plan remediation Low Severity Issues : Include in technical debt Escalation Process \u00b6 Identify Issue : Security issue identified Assess Severity : Determine severity level Escalate : Notify appropriate stakeholders Remediate : Implement fix or mitigation Verify : Confirm issue resolved Security Review Metrics \u00b6 Security Metrics \u00b6 Security Issues Found : Track security issues per PR Security Review Time : Time to complete security review Security Fix Time : Time to fix security issues Security Test Coverage : Coverage of security tests Quality Metrics \u00b6 Security Review Coverage : Percentage of PRs with security review Security Issue Resolution : Percentage of issues resolved Security Training : Team security training completion References \u00b6 OWASP Code Review Guide NIST Secure Software Development ISO/IEC 27001 Security Management SecFlow Security Model Plugin Security Policy","title":"Security Review Guidelines"},{"location":"security/security-review-guidelines/#security-review-guidelines","text":"","title":"Security Review Guidelines"},{"location":"security/security-review-guidelines/#overview","text":"This document provides guidelines for conducting security reviews of pull requests in the SecFlow project.","title":"Overview"},{"location":"security/security-review-guidelines/#security-review-checklist","text":"","title":"Security Review Checklist"},{"location":"security/security-review-guidelines/#1-code-security-review","text":"","title":"1. Code Security Review"},{"location":"security/security-review-guidelines/#input-validation","text":"User Input Validation : All user inputs are validated and sanitized YAML Parsing : Uses yaml.safe_load() instead of yaml.load() JSON Parsing : Uses safe JSON parsing methods File Uploads : File uploads are validated for type and size URL Validation : URLs are validated before use","title":"Input Validation"},{"location":"security/security-review-guidelines/#authentication-authorization","text":"Authentication : Proper authentication mechanisms in place Authorization : Access control implemented correctly Session Management : Secure session handling Password Security : Strong password requirements API Security : API endpoints properly secured","title":"Authentication &amp; Authorization"},{"location":"security/security-review-guidelines/#data-security","text":"Data Encryption : Sensitive data encrypted at rest and in transit Secrets Management : No hardcoded secrets or credentials Data Sanitization : Output data properly sanitized SQL Injection : No SQL injection vulnerabilities XSS Prevention : Cross-site scripting prevention implemented","title":"Data Security"},{"location":"security/security-review-guidelines/#2-plugin-security-review","text":"","title":"2. Plugin Security Review"},{"location":"security/security-review-guidelines/#plugin-policy-compliance","text":"Deny-by-Default : Plugin follows deny-by-default policy Resource Limits : CPU and memory limits specified Filesystem Access : Filesystem access properly restricted Network Access : Network access properly controlled Signature Verification : Plugin signature verified","title":"Plugin Policy Compliance"},{"location":"security/security-review-guidelines/#plugin-manifest-security","text":"Required Fields : All required security fields present Permission Validation : Permissions properly validated Sandbox Configuration : Sandbox settings appropriate Security Section : Security section complete and valid","title":"Plugin Manifest Security"},{"location":"security/security-review-guidelines/#3-infrastructure-security-review","text":"","title":"3. Infrastructure Security Review"},{"location":"security/security-review-guidelines/#dependencies","text":"Dependency Audit : No known vulnerabilities in dependencies Version Pinning : Dependencies pinned to specific versions License Compliance : Dependencies have compatible licenses Supply Chain : Supply chain security considerations","title":"Dependencies"},{"location":"security/security-review-guidelines/#configuration","text":"Secure Defaults : Secure default configurations Environment Variables : Sensitive data in environment variables Logging Security : No sensitive data in logs Error Handling : Secure error handling without information disclosure","title":"Configuration"},{"location":"security/security-review-guidelines/#4-workflow-security-review","text":"","title":"4. Workflow Security Review"},{"location":"security/security-review-guidelines/#recipe-validation","text":"Schema Validation : Workflow recipes validated against schema Node Security : No dangerous node types (shell, exec, eval) Reference Validation : Node references properly validated External Resources : External resources properly validated","title":"Recipe Validation"},{"location":"security/security-review-guidelines/#execution-security","text":"Sandboxing : Workflow execution properly sandboxed Resource Limits : Resource limits enforced Isolation : Proper isolation between workflows Audit Logging : Execution properly logged","title":"Execution Security"},{"location":"security/security-review-guidelines/#security-review-process","text":"","title":"Security Review Process"},{"location":"security/security-review-guidelines/#1-automated-security-checks","text":"Bandit SAST : Static analysis for security issues Safety : Dependency vulnerability scanning YAML Security : YAML parsing security validation Plugin Security : Plugin policy compliance checking","title":"1. Automated Security Checks"},{"location":"security/security-review-guidelines/#2-manual-security-review","text":"Code Review : Security-focused code review Architecture Review : Security architecture assessment Threat Modeling : Threat model analysis Penetration Testing : Security testing (if applicable)","title":"2. Manual Security Review"},{"location":"security/security-review-guidelines/#3-security-approval-criteria","text":"No High Severity Issues : No high-severity security issues Medium Issues Addressed : Medium-severity issues documented and mitigated Security Controls : Appropriate security controls implemented Documentation : Security considerations documented","title":"3. Security Approval Criteria"},{"location":"security/security-review-guidelines/#security-review-tools","text":"","title":"Security Review Tools"},{"location":"security/security-review-guidelines/#static-analysis","text":"# Run Bandit security linter bandit -r . -f json -o bandit-report.json # Run Safety dependency audit safety check --json # Run plugin security audit python tools/plugin_security_audit.py","title":"Static Analysis"},{"location":"security/security-review-guidelines/#dynamic-analysis","text":"# Run security tests pytest tests/test_plugin_security.py # Run workflow security tests pytest tests/test_workflow_security.py","title":"Dynamic Analysis"},{"location":"security/security-review-guidelines/#security-review-templates","text":"","title":"Security Review Templates"},{"location":"security/security-review-guidelines/#security-review-comment","text":"## Security Review ### Security Assessment: \u2705 APPROVED / \u26a0\ufe0f NEEDS WORK / \u274c BLOCKED #### Issues Found: - [ ] Issue 1: Description and remediation - [ ] Issue 2: Description and remediation #### Security Controls Verified: - [ ] Input validation implemented - [ ] Authentication/authorization secure - [ ] No hardcoded secrets - [ ] Plugin policy compliance - [ ] Dependency security #### Recommendations: - Recommendation 1 - Recommendation 2 #### Approval: - [ ] Security Lead approval required - [ ] Additional security testing needed - [ ] Documentation updates required","title":"Security Review Comment"},{"location":"security/security-review-guidelines/#security-review-checklist_1","text":"## Security Review Checklist ### Code Security - [ ] Input validation implemented - [ ] YAML parsing uses safe_load() - [ ] No hardcoded secrets - [ ] SQL injection prevention - [ ] XSS prevention ### Plugin Security - [ ] Plugin policy compliance - [ ] Resource limits specified - [ ] Filesystem access restricted - [ ] Network access controlled - [ ] Signature verification ### Infrastructure Security - [ ] Dependencies audited - [ ] Secure configurations - [ ] Proper error handling - [ ] Audit logging implemented ### Workflow Security - [ ] Recipe validation - [ ] Node security - [ ] Execution sandboxing - [ ] Resource limits enforced","title":"Security Review Checklist"},{"location":"security/security-review-guidelines/#security-review-responsibilities","text":"","title":"Security Review Responsibilities"},{"location":"security/security-review-guidelines/#security-lead","text":"Primary : Security architecture and policy Secondary : Plugin security and sandboxing Review : Security-sensitive changes","title":"Security Lead"},{"location":"security/security-review-guidelines/#development-team","text":"Primary : Code security implementation Secondary : Security testing Review : General security practices","title":"Development Team"},{"location":"security/security-review-guidelines/#devops-team","text":"Primary : Infrastructure security Secondary : Dependency management Review : Deployment security","title":"DevOps Team"},{"location":"security/security-review-guidelines/#security-review-escalation","text":"","title":"Security Review Escalation"},{"location":"security/security-review-guidelines/#escalation-criteria","text":"High Severity Issues : Immediate escalation to Security Lead Medium Severity Issues : Document and plan remediation Low Severity Issues : Include in technical debt","title":"Escalation Criteria"},{"location":"security/security-review-guidelines/#escalation-process","text":"Identify Issue : Security issue identified Assess Severity : Determine severity level Escalate : Notify appropriate stakeholders Remediate : Implement fix or mitigation Verify : Confirm issue resolved","title":"Escalation Process"},{"location":"security/security-review-guidelines/#security-review-metrics","text":"","title":"Security Review Metrics"},{"location":"security/security-review-guidelines/#security-metrics","text":"Security Issues Found : Track security issues per PR Security Review Time : Time to complete security review Security Fix Time : Time to fix security issues Security Test Coverage : Coverage of security tests","title":"Security Metrics"},{"location":"security/security-review-guidelines/#quality-metrics","text":"Security Review Coverage : Percentage of PRs with security review Security Issue Resolution : Percentage of issues resolved Security Training : Team security training completion","title":"Quality Metrics"},{"location":"security/security-review-guidelines/#references","text":"OWASP Code Review Guide NIST Secure Software Development ISO/IEC 27001 Security Management SecFlow Security Model Plugin Security Policy","title":"References"},{"location":"security/standards-mapping/","text":"Security Controls Standards Mapping \u00b6 Overview \u00b6 This document maps SecFlow security controls to industry standards and frameworks, providing traceable compliance evidence for security assessments and audits. Standards Mapping Matrix \u00b6 Control Category Control Implementation Standard Mapping Compliance Level Integrity Signature Verification SHA256 hash verification NIST SP 800-57 Part 1 Cryptographic Key Management Partial CA/Browser Forum BR 7.1 Certificate Authority Standards Partial FIPS 140-2 Cryptographic Module Validation Not Applicable Runtime Environment Sandbox Limits CPU/Memory constraints OWASP ASVS 4.0 14.2 Runtime Environment Protection Full NIST SP 800-53 SC-7 Boundary Protection Partial ISO 27001 A.12.6 Technical Vulnerability Management Partial Data Protection Safe YAML Parsing yaml.safe_load() usage OWASP API Top 10 A8 Injection Full OWASP Top 10 A03 Injection Full NIST SP 800-53 SI-10 Information Input Validation Full Development PR Security Checklist Security review process ISO 27001 A.14 System Acquisition, Development and Maintenance Full NIST SP 800-53 SA-11 Developer Security Testing Partial OWASP SAMM Secure Development Lifecycle Partial Monitoring Audit Logging Security event logging NIST SP 800-53 AU-2 Audit Events Partial ISO 27001 A.12.4 Logging and Monitoring Partial SOC 2 Type II CC6.1 Logical and Physical Access Controls Partial Access Control Plugin Permissions Filesystem/Network restrictions NIST SP 800-53 AC-3 Access Enforcement Partial ISO 27001 A.9.1 Business Requirements of Access Control Partial OWASP ASVS 4.0 4.1 Authentication Partial Detailed Standards Compliance \u00b6 NIST SP 800-53 Compliance \u00b6 AC-3: Access Enforcement \u00b6 Control : Plugin permissions enforce access restrictions Implementation : Filesystem allowlists, network access controls Status : \u2705 Implemented Evidence : Plugin manifest security checklist, sandbox configuration AU-2: Audit Events \u00b6 Control : Security events are logged and monitored Implementation : Plugin execution logging, security event recording Status : \u26a0\ufe0f Partial Implementation Evidence : Basic logging implemented, enhanced monitoring needed SC-7: Boundary Protection \u00b6 Control : System boundaries are protected Implementation : Sandbox isolation, resource limits Status : \u26a0\ufe0f Partial Implementation Evidence : Process-based sandboxing, container-based sandboxing planned SI-10: Information Input Validation \u00b6 Control : Input validation prevents malicious data Implementation : YAML safe loading, schema validation Status : \u2705 Implemented Evidence : yaml.safe_load() usage, recipe validation framework OWASP Compliance \u00b6 OWASP ASVS 4.0 \u00b6 14.2 Runtime Environment Protection : \u2705 Implemented Sandbox execution environment Resource limits enforcement Process isolation 4.1 Authentication : \u26a0\ufe0f Partial Implementation Plugin signature verification User authentication not implemented OWASP API Top 10 \u00b6 A8 Injection : \u2705 Implemented Safe YAML parsing Input validation Schema validation OWASP SAMM \u00b6 Secure Development Lifecycle : \u26a0\ufe0f Partial Implementation Security review process Static analysis integration Security testing framework ISO 27001 Compliance \u00b6 A.9.1 Business Requirements of Access Control \u00b6 Control : Access control based on business requirements Implementation : Plugin permission model Status : \u26a0\ufe0f Partial Implementation Evidence : Plugin security policy, permission validation A.12.4 Logging and Monitoring \u00b6 Control : Security events are logged and monitored Implementation : Audit logging, security monitoring Status : \u26a0\ufe0f Partial Implementation Evidence : Basic logging, enhanced monitoring planned A.12.6 Technical Vulnerability Management \u00b6 Control : Technical vulnerabilities are managed Implementation : Dependency auditing, SAST scanning Status : \u2705 Implemented Evidence : Safety dependency audit, Bandit SAST integration A.14 System Acquisition, Development and Maintenance \u00b6 Control : Secure development practices Implementation : Security review process, PR checklist Status : \u2705 Implemented Evidence : PR security template, security review guidelines Compliance Assessment \u00b6 Current Compliance Status \u00b6 High Compliance (80-100%) \u00b6 Data Protection : Safe YAML parsing, input validation Development Security : PR security checklist, review process Vulnerability Management : Dependency auditing, SAST scanning Medium Compliance (50-79%) \u00b6 Access Control : Plugin permissions, sandbox restrictions Runtime Environment : Process isolation, resource limits Monitoring : Basic logging, security event recording Low Compliance (0-49%) \u00b6 Cryptographic Controls : Signature verification (hash-based only) Authentication : User authentication not implemented Incident Response : Formal incident response procedures needed Compliance Gaps \u00b6 Critical Gaps \u00b6 User Authentication : No user authentication system Incident Response : No formal incident response procedures Cryptographic Standards : Hash-based signatures only Medium Priority Gaps \u00b6 Enhanced Monitoring : Real-time security monitoring needed Container Sandboxing : Full isolation not implemented Secrets Management : No formal secrets management system Low Priority Gaps \u00b6 Compliance Reporting : Automated compliance reporting needed Security Training : Security awareness training program needed Third-party Audits : External security audits not conducted Compliance Roadmap \u00b6 M1 (Current) \u00b6 \u2705 Basic security controls implemented \u2705 Security review process established \u2705 Dependency auditing operational \u2705 SAST scanning integrated M2 (Next 3 months) \u00b6 \ud83d\udd04 Enhanced signature verification (RSA/ECDSA) \ud83d\udd04 Container-based sandboxing \ud83d\udd04 Real-time security monitoring \ud83d\udd04 Formal incident response procedures M3 (Next 6 months) \u00b6 \ud83d\udd04 User authentication system \ud83d\udd04 Secrets management system \ud83d\udd04 Compliance reporting automation \ud83d\udd04 External security audits M4 (Next 12 months) \u00b6 \ud83d\udd04 Full NIST SP 800-53 compliance \ud83d\udd04 SOC 2 Type II certification \ud83d\udd04 ISO 27001 certification \ud83d\udd04 Continuous compliance monitoring Compliance Evidence \u00b6 Documentation Evidence \u00b6 Security Policy : plugins/plugin_policy.yaml Threat Model : docs/security/threat-model.md Security Review Guidelines : docs/security/security-review-guidelines.md Audit Reports : reports/security/ Technical Evidence \u00b6 Code Implementation : Security controls in codebase Test Results : Security test suite results Scan Reports : SAST and dependency audit reports Configuration : Security configuration files Process Evidence \u00b6 PR Templates : Security checklist in PR template Review Process : Security review guidelines Training Materials : Security awareness documentation Incident Procedures : Security incident response procedures Compliance Monitoring \u00b6 Automated Monitoring \u00b6 Daily : Dependency vulnerability scans Weekly : SAST security scans Monthly : Compliance assessment reports Quarterly : Security control effectiveness reviews Manual Monitoring \u00b6 Monthly : Security policy review Quarterly : Compliance gap analysis Annually : External security audit As Needed : Incident response reviews References \u00b6 NIST SP 800-53 OWASP ASVS 4.0 ISO 27001:2013 SOC 2 Type II CA/Browser Forum BR","title":"Security Controls Standards Mapping"},{"location":"security/standards-mapping/#security-controls-standards-mapping","text":"","title":"Security Controls Standards Mapping"},{"location":"security/standards-mapping/#overview","text":"This document maps SecFlow security controls to industry standards and frameworks, providing traceable compliance evidence for security assessments and audits.","title":"Overview"},{"location":"security/standards-mapping/#standards-mapping-matrix","text":"Control Category Control Implementation Standard Mapping Compliance Level Integrity Signature Verification SHA256 hash verification NIST SP 800-57 Part 1 Cryptographic Key Management Partial CA/Browser Forum BR 7.1 Certificate Authority Standards Partial FIPS 140-2 Cryptographic Module Validation Not Applicable Runtime Environment Sandbox Limits CPU/Memory constraints OWASP ASVS 4.0 14.2 Runtime Environment Protection Full NIST SP 800-53 SC-7 Boundary Protection Partial ISO 27001 A.12.6 Technical Vulnerability Management Partial Data Protection Safe YAML Parsing yaml.safe_load() usage OWASP API Top 10 A8 Injection Full OWASP Top 10 A03 Injection Full NIST SP 800-53 SI-10 Information Input Validation Full Development PR Security Checklist Security review process ISO 27001 A.14 System Acquisition, Development and Maintenance Full NIST SP 800-53 SA-11 Developer Security Testing Partial OWASP SAMM Secure Development Lifecycle Partial Monitoring Audit Logging Security event logging NIST SP 800-53 AU-2 Audit Events Partial ISO 27001 A.12.4 Logging and Monitoring Partial SOC 2 Type II CC6.1 Logical and Physical Access Controls Partial Access Control Plugin Permissions Filesystem/Network restrictions NIST SP 800-53 AC-3 Access Enforcement Partial ISO 27001 A.9.1 Business Requirements of Access Control Partial OWASP ASVS 4.0 4.1 Authentication Partial","title":"Standards Mapping Matrix"},{"location":"security/standards-mapping/#detailed-standards-compliance","text":"","title":"Detailed Standards Compliance"},{"location":"security/standards-mapping/#nist-sp-800-53-compliance","text":"","title":"NIST SP 800-53 Compliance"},{"location":"security/standards-mapping/#ac-3-access-enforcement","text":"Control : Plugin permissions enforce access restrictions Implementation : Filesystem allowlists, network access controls Status : \u2705 Implemented Evidence : Plugin manifest security checklist, sandbox configuration","title":"AC-3: Access Enforcement"},{"location":"security/standards-mapping/#au-2-audit-events","text":"Control : Security events are logged and monitored Implementation : Plugin execution logging, security event recording Status : \u26a0\ufe0f Partial Implementation Evidence : Basic logging implemented, enhanced monitoring needed","title":"AU-2: Audit Events"},{"location":"security/standards-mapping/#sc-7-boundary-protection","text":"Control : System boundaries are protected Implementation : Sandbox isolation, resource limits Status : \u26a0\ufe0f Partial Implementation Evidence : Process-based sandboxing, container-based sandboxing planned","title":"SC-7: Boundary Protection"},{"location":"security/standards-mapping/#si-10-information-input-validation","text":"Control : Input validation prevents malicious data Implementation : YAML safe loading, schema validation Status : \u2705 Implemented Evidence : yaml.safe_load() usage, recipe validation framework","title":"SI-10: Information Input Validation"},{"location":"security/standards-mapping/#owasp-compliance","text":"","title":"OWASP Compliance"},{"location":"security/standards-mapping/#owasp-asvs-40","text":"14.2 Runtime Environment Protection : \u2705 Implemented Sandbox execution environment Resource limits enforcement Process isolation 4.1 Authentication : \u26a0\ufe0f Partial Implementation Plugin signature verification User authentication not implemented","title":"OWASP ASVS 4.0"},{"location":"security/standards-mapping/#owasp-api-top-10","text":"A8 Injection : \u2705 Implemented Safe YAML parsing Input validation Schema validation","title":"OWASP API Top 10"},{"location":"security/standards-mapping/#owasp-samm","text":"Secure Development Lifecycle : \u26a0\ufe0f Partial Implementation Security review process Static analysis integration Security testing framework","title":"OWASP SAMM"},{"location":"security/standards-mapping/#iso-27001-compliance","text":"","title":"ISO 27001 Compliance"},{"location":"security/standards-mapping/#a91-business-requirements-of-access-control","text":"Control : Access control based on business requirements Implementation : Plugin permission model Status : \u26a0\ufe0f Partial Implementation Evidence : Plugin security policy, permission validation","title":"A.9.1 Business Requirements of Access Control"},{"location":"security/standards-mapping/#a124-logging-and-monitoring","text":"Control : Security events are logged and monitored Implementation : Audit logging, security monitoring Status : \u26a0\ufe0f Partial Implementation Evidence : Basic logging, enhanced monitoring planned","title":"A.12.4 Logging and Monitoring"},{"location":"security/standards-mapping/#a126-technical-vulnerability-management","text":"Control : Technical vulnerabilities are managed Implementation : Dependency auditing, SAST scanning Status : \u2705 Implemented Evidence : Safety dependency audit, Bandit SAST integration","title":"A.12.6 Technical Vulnerability Management"},{"location":"security/standards-mapping/#a14-system-acquisition-development-and-maintenance","text":"Control : Secure development practices Implementation : Security review process, PR checklist Status : \u2705 Implemented Evidence : PR security template, security review guidelines","title":"A.14 System Acquisition, Development and Maintenance"},{"location":"security/standards-mapping/#compliance-assessment","text":"","title":"Compliance Assessment"},{"location":"security/standards-mapping/#current-compliance-status","text":"","title":"Current Compliance Status"},{"location":"security/standards-mapping/#high-compliance-80-100","text":"Data Protection : Safe YAML parsing, input validation Development Security : PR security checklist, review process Vulnerability Management : Dependency auditing, SAST scanning","title":"High Compliance (80-100%)"},{"location":"security/standards-mapping/#medium-compliance-50-79","text":"Access Control : Plugin permissions, sandbox restrictions Runtime Environment : Process isolation, resource limits Monitoring : Basic logging, security event recording","title":"Medium Compliance (50-79%)"},{"location":"security/standards-mapping/#low-compliance-0-49","text":"Cryptographic Controls : Signature verification (hash-based only) Authentication : User authentication not implemented Incident Response : Formal incident response procedures needed","title":"Low Compliance (0-49%)"},{"location":"security/standards-mapping/#compliance-gaps","text":"","title":"Compliance Gaps"},{"location":"security/standards-mapping/#critical-gaps","text":"User Authentication : No user authentication system Incident Response : No formal incident response procedures Cryptographic Standards : Hash-based signatures only","title":"Critical Gaps"},{"location":"security/standards-mapping/#medium-priority-gaps","text":"Enhanced Monitoring : Real-time security monitoring needed Container Sandboxing : Full isolation not implemented Secrets Management : No formal secrets management system","title":"Medium Priority Gaps"},{"location":"security/standards-mapping/#low-priority-gaps","text":"Compliance Reporting : Automated compliance reporting needed Security Training : Security awareness training program needed Third-party Audits : External security audits not conducted","title":"Low Priority Gaps"},{"location":"security/standards-mapping/#compliance-roadmap","text":"","title":"Compliance Roadmap"},{"location":"security/standards-mapping/#m1-current","text":"\u2705 Basic security controls implemented \u2705 Security review process established \u2705 Dependency auditing operational \u2705 SAST scanning integrated","title":"M1 (Current)"},{"location":"security/standards-mapping/#m2-next-3-months","text":"\ud83d\udd04 Enhanced signature verification (RSA/ECDSA) \ud83d\udd04 Container-based sandboxing \ud83d\udd04 Real-time security monitoring \ud83d\udd04 Formal incident response procedures","title":"M2 (Next 3 months)"},{"location":"security/standards-mapping/#m3-next-6-months","text":"\ud83d\udd04 User authentication system \ud83d\udd04 Secrets management system \ud83d\udd04 Compliance reporting automation \ud83d\udd04 External security audits","title":"M3 (Next 6 months)"},{"location":"security/standards-mapping/#m4-next-12-months","text":"\ud83d\udd04 Full NIST SP 800-53 compliance \ud83d\udd04 SOC 2 Type II certification \ud83d\udd04 ISO 27001 certification \ud83d\udd04 Continuous compliance monitoring","title":"M4 (Next 12 months)"},{"location":"security/standards-mapping/#compliance-evidence","text":"","title":"Compliance Evidence"},{"location":"security/standards-mapping/#documentation-evidence","text":"Security Policy : plugins/plugin_policy.yaml Threat Model : docs/security/threat-model.md Security Review Guidelines : docs/security/security-review-guidelines.md Audit Reports : reports/security/","title":"Documentation Evidence"},{"location":"security/standards-mapping/#technical-evidence","text":"Code Implementation : Security controls in codebase Test Results : Security test suite results Scan Reports : SAST and dependency audit reports Configuration : Security configuration files","title":"Technical Evidence"},{"location":"security/standards-mapping/#process-evidence","text":"PR Templates : Security checklist in PR template Review Process : Security review guidelines Training Materials : Security awareness documentation Incident Procedures : Security incident response procedures","title":"Process Evidence"},{"location":"security/standards-mapping/#compliance-monitoring","text":"","title":"Compliance Monitoring"},{"location":"security/standards-mapping/#automated-monitoring","text":"Daily : Dependency vulnerability scans Weekly : SAST security scans Monthly : Compliance assessment reports Quarterly : Security control effectiveness reviews","title":"Automated Monitoring"},{"location":"security/standards-mapping/#manual-monitoring","text":"Monthly : Security policy review Quarterly : Compliance gap analysis Annually : External security audit As Needed : Incident response reviews","title":"Manual Monitoring"},{"location":"security/standards-mapping/#references","text":"NIST SP 800-53 OWASP ASVS 4.0 ISO 27001:2013 SOC 2 Type II CA/Browser Forum BR","title":"References"},{"location":"security/threat-model/","text":"SecFlow Plugin Security Threat Model \u00b6 Overview \u00b6 This document defines the threat model for the SecFlow plugin system, identifying assets, threat actors, and primary threats to guide security control implementation. Assets Protected \u00b6 Primary Assets \u00b6 Plugin Runtime Environment : Core execution environment for plugins Plugin Loader : Component responsible for loading and validating plugins Plugin Manifests : Metadata files describing plugin capabilities and permissions YAML Workflow Recipes : User-defined workflow configurations System Resources : CPU, memory, filesystem, and network access Sensitive Data : Findings, configurations, and user data processed by plugins Secondary Assets \u00b6 Plugin Registry : Repository of approved plugins Security Policies : Plugin security configuration and constraints Audit Logs : Security event logging and monitoring data Signature Keys : Cryptographic keys for plugin verification Threat Actors \u00b6 External Threat Actors \u00b6 Malicious Plugin Developers : Developers creating plugins with malicious intent Supply Chain Compromise : Attackers compromising plugin distribution channels Script Kiddies : Low-skill attackers using automated tools Advanced Persistent Threats (APT) : Sophisticated attackers with long-term objectives Internal Threat Actors \u00b6 Insider Abuse : Authorized users misusing their privileges Compromised Accounts : Legitimate accounts taken over by attackers Social Engineering : Attackers manipulating users to bypass security controls Primary Threats \u00b6 1. Code Execution Attacks \u00b6 Arbitrary Code Execution : Malicious plugins executing unauthorized code Privilege Escalation : Plugins gaining elevated system privileges Remote Code Execution : Plugins executing code from external sources Attack Vectors: - Malicious plugin manifests with dangerous node types - YAML deserialization attacks - Plugin signature forgery - Sandbox escape techniques 2. Data Exfiltration \u00b6 Sensitive Data Theft : Plugins accessing and exfiltrating sensitive information Configuration Disclosure : Plugins revealing system configurations User Data Harvesting : Plugins collecting user information Attack Vectors: - Unauthorized filesystem access - Network communication to external servers - Memory dumps and process inspection - Log file access 3. System Compromise \u00b6 Resource Exhaustion : Plugins consuming excessive system resources Denial of Service : Plugins causing system unavailability System Modification : Plugins modifying system files or configurations Attack Vectors: - CPU and memory exhaustion attacks - Infinite loops and recursive operations - File system modification - Network flooding 4. Supply Chain Attacks \u00b6 Plugin Tampering : Modification of plugins during distribution Dependency Poisoning : Compromise of plugin dependencies Manifest Spoofing : Creation of fake plugin manifests Attack Vectors: - Man-in-the-middle attacks on plugin distribution - Compromise of plugin registry - Dependency confusion attacks - Signature forgery Threat Scenarios \u00b6 Scenario 1: Malicious Plugin Execution \u00b6 Actor : Malicious plugin developer Objective : Execute arbitrary code on the system Method : 1. Create plugin with malicious manifest 2. Bypass signature verification 3. Execute code with elevated privileges 4. Exfiltrate sensitive data Mitigations : - Plugin signature verification - Sandbox execution environment - Resource limits and monitoring - Audit logging Scenario 2: Supply Chain Compromise \u00b6 Actor : Advanced persistent threat Objective : Compromise plugin distribution Method : 1. Compromise plugin registry 2. Inject malicious code into legitimate plugins 3. Distribute compromised plugins 4. Execute malicious code on target systems Mitigations : - Cryptographic signature verification - Plugin integrity checking - Secure distribution channels - Regular security audits Scenario 3: Insider Abuse \u00b6 Actor : Authorized user with elevated privileges Objective : Misuse system access Method : 1. Create plugins with excessive permissions 2. Bypass security controls 3. Access sensitive data 4. Modify system configurations Mitigations : - Principle of least privilege - Audit logging and monitoring - Regular access reviews - Separation of duties Security Control Mapping \u00b6 Prevention Controls \u00b6 Plugin Signature Verification : Prevents unauthorized plugin execution Sandbox Execution : Isolates plugin execution environment Resource Limits : Prevents resource exhaustion attacks Input Validation : Prevents injection attacks Detection Controls \u00b6 Audit Logging : Records security events for analysis Security Monitoring : Detects suspicious plugin behavior Anomaly Detection : Identifies unusual resource usage patterns Signature Verification : Detects plugin tampering Response Controls \u00b6 Plugin Termination : Stops malicious plugin execution Resource Quotas : Limits plugin resource consumption Network Isolation : Prevents data exfiltration Incident Response : Procedures for security incidents Risk Assessment \u00b6 High Risk Threats \u00b6 Arbitrary Code Execution : High impact, medium likelihood Privilege Escalation : High impact, low likelihood Data Exfiltration : High impact, medium likelihood Medium Risk Threats \u00b6 Resource Exhaustion : Medium impact, medium likelihood Denial of Service : Medium impact, low likelihood System Modification : Medium impact, low likelihood Low Risk Threats \u00b6 Information Disclosure : Low impact, low likelihood Audit Trail Tampering : Low impact, very low likelihood Security Requirements \u00b6 Functional Requirements \u00b6 FR-1 : Plugin signature verification must be implemented FR-2 : Sandbox execution environment must isolate plugins FR-3 : Resource limits must be enforced FR-4 : Audit logging must record all security events Non-Functional Requirements \u00b6 NFR-1 : Security controls must not impact performance by more than 10% NFR-2 : Security monitoring must detect threats within 5 minutes NFR-3 : Incident response must be initiated within 15 minutes NFR-4 : Security updates must be deployed within 24 hours Threat Model Validation \u00b6 Testing Scenarios \u00b6 Penetration Testing : Simulate attack scenarios Red Team Exercises : Test security controls effectiveness Vulnerability Assessment : Identify security weaknesses Security Code Review : Analyze code for security issues Metrics and KPIs \u00b6 Mean Time to Detection (MTTD) : Time to detect security incidents Mean Time to Response (MTTR) : Time to respond to security incidents False Positive Rate : Rate of false security alerts Security Control Coverage : Percentage of threats covered by controls References \u00b6 STRIDE Threat Modeling MITRE ATT&CK Framework OWASP Threat Modeling NIST SP 800-30 Risk Assessment","title":"SecFlow Plugin Security Threat Model"},{"location":"security/threat-model/#secflow-plugin-security-threat-model","text":"","title":"SecFlow Plugin Security Threat Model"},{"location":"security/threat-model/#overview","text":"This document defines the threat model for the SecFlow plugin system, identifying assets, threat actors, and primary threats to guide security control implementation.","title":"Overview"},{"location":"security/threat-model/#assets-protected","text":"","title":"Assets Protected"},{"location":"security/threat-model/#primary-assets","text":"Plugin Runtime Environment : Core execution environment for plugins Plugin Loader : Component responsible for loading and validating plugins Plugin Manifests : Metadata files describing plugin capabilities and permissions YAML Workflow Recipes : User-defined workflow configurations System Resources : CPU, memory, filesystem, and network access Sensitive Data : Findings, configurations, and user data processed by plugins","title":"Primary Assets"},{"location":"security/threat-model/#secondary-assets","text":"Plugin Registry : Repository of approved plugins Security Policies : Plugin security configuration and constraints Audit Logs : Security event logging and monitoring data Signature Keys : Cryptographic keys for plugin verification","title":"Secondary Assets"},{"location":"security/threat-model/#threat-actors","text":"","title":"Threat Actors"},{"location":"security/threat-model/#external-threat-actors","text":"Malicious Plugin Developers : Developers creating plugins with malicious intent Supply Chain Compromise : Attackers compromising plugin distribution channels Script Kiddies : Low-skill attackers using automated tools Advanced Persistent Threats (APT) : Sophisticated attackers with long-term objectives","title":"External Threat Actors"},{"location":"security/threat-model/#internal-threat-actors","text":"Insider Abuse : Authorized users misusing their privileges Compromised Accounts : Legitimate accounts taken over by attackers Social Engineering : Attackers manipulating users to bypass security controls","title":"Internal Threat Actors"},{"location":"security/threat-model/#primary-threats","text":"","title":"Primary Threats"},{"location":"security/threat-model/#1-code-execution-attacks","text":"Arbitrary Code Execution : Malicious plugins executing unauthorized code Privilege Escalation : Plugins gaining elevated system privileges Remote Code Execution : Plugins executing code from external sources Attack Vectors: - Malicious plugin manifests with dangerous node types - YAML deserialization attacks - Plugin signature forgery - Sandbox escape techniques","title":"1. Code Execution Attacks"},{"location":"security/threat-model/#2-data-exfiltration","text":"Sensitive Data Theft : Plugins accessing and exfiltrating sensitive information Configuration Disclosure : Plugins revealing system configurations User Data Harvesting : Plugins collecting user information Attack Vectors: - Unauthorized filesystem access - Network communication to external servers - Memory dumps and process inspection - Log file access","title":"2. Data Exfiltration"},{"location":"security/threat-model/#3-system-compromise","text":"Resource Exhaustion : Plugins consuming excessive system resources Denial of Service : Plugins causing system unavailability System Modification : Plugins modifying system files or configurations Attack Vectors: - CPU and memory exhaustion attacks - Infinite loops and recursive operations - File system modification - Network flooding","title":"3. System Compromise"},{"location":"security/threat-model/#4-supply-chain-attacks","text":"Plugin Tampering : Modification of plugins during distribution Dependency Poisoning : Compromise of plugin dependencies Manifest Spoofing : Creation of fake plugin manifests Attack Vectors: - Man-in-the-middle attacks on plugin distribution - Compromise of plugin registry - Dependency confusion attacks - Signature forgery","title":"4. Supply Chain Attacks"},{"location":"security/threat-model/#threat-scenarios","text":"","title":"Threat Scenarios"},{"location":"security/threat-model/#scenario-1-malicious-plugin-execution","text":"Actor : Malicious plugin developer Objective : Execute arbitrary code on the system Method : 1. Create plugin with malicious manifest 2. Bypass signature verification 3. Execute code with elevated privileges 4. Exfiltrate sensitive data Mitigations : - Plugin signature verification - Sandbox execution environment - Resource limits and monitoring - Audit logging","title":"Scenario 1: Malicious Plugin Execution"},{"location":"security/threat-model/#scenario-2-supply-chain-compromise","text":"Actor : Advanced persistent threat Objective : Compromise plugin distribution Method : 1. Compromise plugin registry 2. Inject malicious code into legitimate plugins 3. Distribute compromised plugins 4. Execute malicious code on target systems Mitigations : - Cryptographic signature verification - Plugin integrity checking - Secure distribution channels - Regular security audits","title":"Scenario 2: Supply Chain Compromise"},{"location":"security/threat-model/#scenario-3-insider-abuse","text":"Actor : Authorized user with elevated privileges Objective : Misuse system access Method : 1. Create plugins with excessive permissions 2. Bypass security controls 3. Access sensitive data 4. Modify system configurations Mitigations : - Principle of least privilege - Audit logging and monitoring - Regular access reviews - Separation of duties","title":"Scenario 3: Insider Abuse"},{"location":"security/threat-model/#security-control-mapping","text":"","title":"Security Control Mapping"},{"location":"security/threat-model/#prevention-controls","text":"Plugin Signature Verification : Prevents unauthorized plugin execution Sandbox Execution : Isolates plugin execution environment Resource Limits : Prevents resource exhaustion attacks Input Validation : Prevents injection attacks","title":"Prevention Controls"},{"location":"security/threat-model/#detection-controls","text":"Audit Logging : Records security events for analysis Security Monitoring : Detects suspicious plugin behavior Anomaly Detection : Identifies unusual resource usage patterns Signature Verification : Detects plugin tampering","title":"Detection Controls"},{"location":"security/threat-model/#response-controls","text":"Plugin Termination : Stops malicious plugin execution Resource Quotas : Limits plugin resource consumption Network Isolation : Prevents data exfiltration Incident Response : Procedures for security incidents","title":"Response Controls"},{"location":"security/threat-model/#risk-assessment","text":"","title":"Risk Assessment"},{"location":"security/threat-model/#high-risk-threats","text":"Arbitrary Code Execution : High impact, medium likelihood Privilege Escalation : High impact, low likelihood Data Exfiltration : High impact, medium likelihood","title":"High Risk Threats"},{"location":"security/threat-model/#medium-risk-threats","text":"Resource Exhaustion : Medium impact, medium likelihood Denial of Service : Medium impact, low likelihood System Modification : Medium impact, low likelihood","title":"Medium Risk Threats"},{"location":"security/threat-model/#low-risk-threats","text":"Information Disclosure : Low impact, low likelihood Audit Trail Tampering : Low impact, very low likelihood","title":"Low Risk Threats"},{"location":"security/threat-model/#security-requirements","text":"","title":"Security Requirements"},{"location":"security/threat-model/#functional-requirements","text":"FR-1 : Plugin signature verification must be implemented FR-2 : Sandbox execution environment must isolate plugins FR-3 : Resource limits must be enforced FR-4 : Audit logging must record all security events","title":"Functional Requirements"},{"location":"security/threat-model/#non-functional-requirements","text":"NFR-1 : Security controls must not impact performance by more than 10% NFR-2 : Security monitoring must detect threats within 5 minutes NFR-3 : Incident response must be initiated within 15 minutes NFR-4 : Security updates must be deployed within 24 hours","title":"Non-Functional Requirements"},{"location":"security/threat-model/#threat-model-validation","text":"","title":"Threat Model Validation"},{"location":"security/threat-model/#testing-scenarios","text":"Penetration Testing : Simulate attack scenarios Red Team Exercises : Test security controls effectiveness Vulnerability Assessment : Identify security weaknesses Security Code Review : Analyze code for security issues","title":"Testing Scenarios"},{"location":"security/threat-model/#metrics-and-kpis","text":"Mean Time to Detection (MTTD) : Time to detect security incidents Mean Time to Response (MTTR) : Time to respond to security incidents False Positive Rate : Rate of false security alerts Security Control Coverage : Percentage of threats covered by controls","title":"Metrics and KPIs"},{"location":"security/threat-model/#references","text":"STRIDE Threat Modeling MITRE ATT&CK Framework OWASP Threat Modeling NIST SP 800-30 Risk Assessment","title":"References"}]}