{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SecFlow Documentation \u00b6 This is the comprehensive documentation portal for the SecFlow security toolkit orchestration framework. \ud83d\udcda Documentation Sections \u00b6 \ud83c\udfd7\ufe0f Architecture \u00b6 Complete technical architecture documentation covering: - Repository layout and package structure - Core packages and responsibilities - Workflow orchestration and plugin systems - Tools integration and resource management - Security models and observability \ud83d\udd0d Review & Validation \u00b6 Documentation validation and review tools: - Review guidelines and workflow - Validation checklists and reports - AI-assisted validation summaries - Mermaid diagram testing \ud83d\ude80 Quick Start \u00b6 Architecture Overview : Start with Home for the high-level system overview Executive Summary : Read the Executive Summary for key concepts Validation Portal : Check the Review & Validation section for documentation standards \ud83d\udcca Current Status \u00b6 \u2705 Mermaid Diagrams : All diagrams rendering correctly \u2705 ASCII Artifacts : Eliminated from documentation \u2705 CI Gates : Automated validation in place \u2705 Navigation : All pages accessible and properly linked Built with MkDocs | Last Updated : 2025-10-08","title":"SecFlow Documentation"},{"location":"#welcome-to-secflow-documentation","text":"This is the comprehensive documentation portal for the SecFlow security toolkit orchestration framework.","title":"Welcome to SecFlow Documentation"},{"location":"#documentation-sections","text":"","title":"\ud83d\udcda Documentation Sections"},{"location":"#architecture","text":"Complete technical architecture documentation covering: - Repository layout and package structure - Core packages and responsibilities - Workflow orchestration and plugin systems - Tools integration and resource management - Security models and observability","title":"\ud83c\udfd7\ufe0f Architecture"},{"location":"#review-validation","text":"Documentation validation and review tools: - Review guidelines and workflow - Validation checklists and reports - AI-assisted validation summaries - Mermaid diagram testing","title":"\ud83d\udd0d Review &amp; Validation"},{"location":"#quick-start","text":"Architecture Overview : Start with Home for the high-level system overview Executive Summary : Read the Executive Summary for key concepts Validation Portal : Check the Review & Validation section for documentation standards","title":"\ud83d\ude80 Quick Start"},{"location":"#current-status","text":"\u2705 Mermaid Diagrams : All diagrams rendering correctly \u2705 ASCII Artifacts : Eliminated from documentation \u2705 CI Gates : Automated validation in place \u2705 Navigation : All pages accessible and properly linked Built with MkDocs | Last Updated : 2025-10-08","title":"\ud83d\udcca Current Status"},{"location":"architecture/00-index/","text":"SecFlow \u2014 Architecture & Design Documentation Index \u00b6 Welcome to the SecFlow technical documentation suite. This collection defines the complete internal architecture of the Security Toolkit platform \u2014 covering everything from orchestration engines to data models, plugin systems, and CVE enrichment. Each document in this /docs/architecture/ folder represents a bounded architectural domain , written in deep-technical mode to serve engineers and architects implementing or extending the system. \ud83d\udcda Document Navigation \u00b6 # File Description 01 Title & Executive Summary Project mission, audience, and objectives. 02 Architecture Philosophy Core tenets, design rationale, and constraints. 03 Repository Layout Folder hierarchy, import rules, and tooling. 04 Core Packages & Responsibilities Module roles, data contracts, and dependencies. 05 Orchestration & Workflow Engine DAG-based tool chaining and execution logic. 06 Plugin System Plugin registration, lifecycle, and testing. 07 Tools Integration Model Wrappers, manifests, and sandbox execution. 08 Tool Manager & UX Design How users manage tools and workflows. 09 Resource Registry Global wordlists, payloads, templates, and configs. 10 Wordlist & Output Sharing Rules for shared and isolated resources. 11 Project Isolation & Data Sharing Workspace boundaries, access control, linking. 12 Findings Model & Schema Data models, normalization, and persistence. 13 CVE/CWE/POC Enrichment Layer External vulnerability mapping and enrichment. 14 POC Sources & Legal Guidelines Governance and sandbox policies for proof-of-concepts. 15 Garbage Collection & Retention Data lifecycle, cleanup tasks, and recovery. 16 Security Model Auth, RBAC, sandboxing, and auditing. 17 Observability, Logging & Metrics Telemetry, structured logs, and OpenTelemetry. 18 Error Handling & Recovery Centralized error strategies and resilience patterns. 19 Risk Assessment Framework NIST 5\u00d75 Matrix, CVSS, and MITRE mapping. 20 Migration & Implementation Phases Step-by-step rollout plan. 21 CI/CD & Testing Strategy Build matrix, pipelines, and test automation. 22 Developer Experience & Docs Local setup, MkDocs, and contribution workflow. 23 Future Roadmap AI triage, automation, and next-gen features. \ud83e\udded Reading Order \u00b6 While each document is standalone, recommended order for new contributors: Start with 01\u201304 to grasp architecture and structure. Proceed to 05\u201307 for orchestration and integration. Review 09\u201313 for data, findings, and enrichment logic. Study 16\u201318 for security and observability. Conclude with 19\u201323 for governance, testing, and roadmap. \ud83e\udde9 Versioning & Maintenance \u00b6 Documentation version follows codebase major versions (e.g., 1.0 = v1.x releases). Each architectural change must update its corresponding file and cross-links. Use mkdocs serve locally to preview rendered docs. Changes to interfaces or DTOs require schema diff logs. \ud83e\uddf1 Example Structure Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/\"] C[\"00-index.md\"] D[\"01-title-and-executive-summary.md\"] E[\"02-architecture-philosophy.md\"] F[\"03-repository-layout.md\"] G[\"04-core-packages-and-responsibilities.md\"] H[\"05-orchestration-and-workflow-engine.md\"] I[\"06-plugin-system.md\"] J[\"07-tools-integration-model.md\"] K[\"08-tool-manager-and-ux-design.md\"] L[\"09-resource-registry.md\"] M[\"10-wordlist-and-output-sharing.md\"] N[\"11-project-isolation-and-data-sharing.md\"] O[\"12-findings-model-and-schema.md\"] P[\"13-cve-cwe-poc-enrichment-layer.md\"] Q[\"14-poc-sources-and-legal-guidelines.md\"] R[\"15-garbage-collection-and-retention.md\"] S[\"16-security-model.md\"] T[\"17-observability-logging-and-metrics.md\"] U[\"18-error-handling-and-recovery.md\"] V[\"19-risk-assessment-framework.md\"] W[\"20-migration-and-implementation-phases.md\"] X[\"21-ci-cd-and-testing-strategy.md\"] Y[\"22-developer-experience-and-docs.md\"] Z[\"23-future-roadmap.md\"] A --> B B --> C B --> D B --> E B --> F B --> G B --> H B --> I B --> J B --> K B --> L B --> M B --> N B --> O B --> P B --> Q B --> R B --> S B --> T B --> U B --> V B --> W B --> X B --> Y B --> Z Next: Title & Executive Summary ```","title":"Home"},{"location":"architecture/00-index/#secflow-architecture-design-documentation-index","text":"Welcome to the SecFlow technical documentation suite. This collection defines the complete internal architecture of the Security Toolkit platform \u2014 covering everything from orchestration engines to data models, plugin systems, and CVE enrichment. Each document in this /docs/architecture/ folder represents a bounded architectural domain , written in deep-technical mode to serve engineers and architects implementing or extending the system.","title":"SecFlow \u2014 Architecture &amp; Design Documentation Index"},{"location":"architecture/00-index/#document-navigation","text":"# File Description 01 Title & Executive Summary Project mission, audience, and objectives. 02 Architecture Philosophy Core tenets, design rationale, and constraints. 03 Repository Layout Folder hierarchy, import rules, and tooling. 04 Core Packages & Responsibilities Module roles, data contracts, and dependencies. 05 Orchestration & Workflow Engine DAG-based tool chaining and execution logic. 06 Plugin System Plugin registration, lifecycle, and testing. 07 Tools Integration Model Wrappers, manifests, and sandbox execution. 08 Tool Manager & UX Design How users manage tools and workflows. 09 Resource Registry Global wordlists, payloads, templates, and configs. 10 Wordlist & Output Sharing Rules for shared and isolated resources. 11 Project Isolation & Data Sharing Workspace boundaries, access control, linking. 12 Findings Model & Schema Data models, normalization, and persistence. 13 CVE/CWE/POC Enrichment Layer External vulnerability mapping and enrichment. 14 POC Sources & Legal Guidelines Governance and sandbox policies for proof-of-concepts. 15 Garbage Collection & Retention Data lifecycle, cleanup tasks, and recovery. 16 Security Model Auth, RBAC, sandboxing, and auditing. 17 Observability, Logging & Metrics Telemetry, structured logs, and OpenTelemetry. 18 Error Handling & Recovery Centralized error strategies and resilience patterns. 19 Risk Assessment Framework NIST 5\u00d75 Matrix, CVSS, and MITRE mapping. 20 Migration & Implementation Phases Step-by-step rollout plan. 21 CI/CD & Testing Strategy Build matrix, pipelines, and test automation. 22 Developer Experience & Docs Local setup, MkDocs, and contribution workflow. 23 Future Roadmap AI triage, automation, and next-gen features.","title":"\ud83d\udcda Document Navigation"},{"location":"architecture/00-index/#reading-order","text":"While each document is standalone, recommended order for new contributors: Start with 01\u201304 to grasp architecture and structure. Proceed to 05\u201307 for orchestration and integration. Review 09\u201313 for data, findings, and enrichment logic. Study 16\u201318 for security and observability. Conclude with 19\u201323 for governance, testing, and roadmap.","title":"\ud83e\udded Reading Order"},{"location":"architecture/00-index/#versioning-maintenance","text":"Documentation version follows codebase major versions (e.g., 1.0 = v1.x releases). Each architectural change must update its corresponding file and cross-links. Use mkdocs serve locally to preview rendered docs. Changes to interfaces or DTOs require schema diff logs.","title":"\ud83e\udde9 Versioning &amp; Maintenance"},{"location":"architecture/00-index/#example-structure-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/\"] C[\"00-index.md\"] D[\"01-title-and-executive-summary.md\"] E[\"02-architecture-philosophy.md\"] F[\"03-repository-layout.md\"] G[\"04-core-packages-and-responsibilities.md\"] H[\"05-orchestration-and-workflow-engine.md\"] I[\"06-plugin-system.md\"] J[\"07-tools-integration-model.md\"] K[\"08-tool-manager-and-ux-design.md\"] L[\"09-resource-registry.md\"] M[\"10-wordlist-and-output-sharing.md\"] N[\"11-project-isolation-and-data-sharing.md\"] O[\"12-findings-model-and-schema.md\"] P[\"13-cve-cwe-poc-enrichment-layer.md\"] Q[\"14-poc-sources-and-legal-guidelines.md\"] R[\"15-garbage-collection-and-retention.md\"] S[\"16-security-model.md\"] T[\"17-observability-logging-and-metrics.md\"] U[\"18-error-handling-and-recovery.md\"] V[\"19-risk-assessment-framework.md\"] W[\"20-migration-and-implementation-phases.md\"] X[\"21-ci-cd-and-testing-strategy.md\"] Y[\"22-developer-experience-and-docs.md\"] Z[\"23-future-roadmap.md\"] A --> B B --> C B --> D B --> E B --> F B --> G B --> H B --> I B --> J B --> K B --> L B --> M B --> N B --> O B --> P B --> Q B --> R B --> S B --> T B --> U B --> V B --> W B --> X B --> Y B --> Z Next: Title & Executive Summary ```","title":"\ud83e\uddf1 Example Structure Diagram"},{"location":"architecture/01-title-and-executive-summary/","text":"01 \u2014 Title & Executive Summary \u00b6 \ud83e\udde9 Project Name \u00b6 SecFlow \u2014 Security Toolkit Orchestration Framework \ud83e\udded Executive Summary \u00b6 SecFlow is a modular, extensible, and automation-first security orchestration platform designed to unify multiple open-source and proprietary tools for penetration testing, vulnerability management, and risk analytics. It merges dynamic scanning , tool orchestration , data normalization , and risk intelligence into a single cohesive system \u2014 optimized for both solo pentesters and enterprise red-team workflows. Core Vision \u00b6 \"One orchestrator to chain, enrich, and analyze all security tools \u2014 safely, intelligently, and reproducibly.\" Core Differentiators \u00b6 \ud83d\udd04 End-to-end orchestration: Tools (Nuclei, Feroxbuster, Katana, ZAP, Burp, Caido, etc.) communicate via normalized JSON pipelines. \ud83e\uddf1 Hexagonal architecture: Strict separation between core logic , adapters , and interfaces . \ud83e\udde0 Data-driven intelligence: Findings automatically enriched with CVE/CWE/CVSS and MITRE mapping. \u2699\ufe0f Resource registry: Shared wordlists, templates, and payloads under a versioned resource management system. \ud83e\uddf0 AI-ready foundation: All findings and telemetry are machine-readable for later AI triage and summarization. \ud83c\udfaf Project Goals \u00b6 Category Objective Engineering Create a modular mono-repo structure enabling multiple apps (CLI, API, Worker, UI). Automation Allow users to chain discovery \u2192 scanning \u2192 enrichment workflows via declarative YAML recipes. Data Consistency Standardize tool outputs into common Finding DTOs for unified triage and metrics. Usability Provide a CLI and web UI for project management, orchestration, and analytics. Security Guarantee safe sandboxing, configuration isolation, and audit logging. Scalability Support multi-tenant projects with configurable resource scopes. \ud83e\uddee Key Capabilities Overview \u00b6 Layer Functionality Description Core-Lib Schema & DTOs Defines data models, ports, and contracts. Findings-Engine Normalization Converts raw tool outputs into structured findings. Wrappers Tool Adapters Execute, monitor, and parse tool output. Worker Orchestration Runs workflows (DAG execution, concurrency, retry policies). Web-API REST + WS Interface Exposes project, scan, and findings endpoints. UI HTMX/React Web dashboard for triage and insights. Plugins Extensibility Detectors, enrichers, or analytics extensions. \ud83e\uddf1 Architecture Summary \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Web-API / UI\"] B[\"REST / WebSocket\"] C[\"Worker Engine<br/>(Workflows, DAG, Queues)\"] D[\"Ports / DTOs\"] E[\"Core-Lib<br/>(Models, Ports, Repositories)\"] F[\"Wrappers / Plugins<br/>(Nuclei, Ferox, Katana\u2026)\"] G[\"Tool Output\"] H[\"Findings Engine<br/>(Normalization, Metrics)\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H \u2699\ufe0f Toolchain & Technology Stack \u00b6 Category Tools / Frameworks Language Python 3.11+ Frameworks FastAPI / Flask (API), HTMX / React (UI) ORM / DB SQLModel (SQLite / PostgreSQL) Task Queue Celery / RQ (Redis backend) Caching Redis Testing Pytest + Tox + Coverage CI/CD GitHub Actions + Import-Linter + Ruff + Pyright Packaging Poetry Docs MkDocs + Material theme \ud83e\udde9 Intended Audience \u00b6 Audience Needs Red-Team / Pentesters Automate repetitive workflows, triage large data, correlate findings. Security Engineers Integrate with CI/CD, enrich findings with vulnerability intelligence. Researchers Extend plugins, add enrichment sources, or create experimental scanners. Developers Contribute to wrappers, tooling, and orchestration improvements. \ud83e\udde0 Guiding Principles \u00b6 Automation-First \u2013 Everything that can be automated should be . Safe by Default \u2013 Every scan runs in isolation, respecting sandbox and audit policies. Extensible by Design \u2013 Each tool or feature lives as a plugin or wrapper. Transparent \u2013 Clear data lineage and provenance for every finding. Declarative Configuration \u2013 Tools and workflows defined in YAML/JSON, not hardcoded logic. Deterministic Execution \u2013 Same input, same results. All randomness is logged. \ud83d\udd2e Strategic Vision \u00b6 SecFlow is built not as a single pentest tool, but as a foundation for orchestrated cybersecurity automation . It aims to support: - Scalable enterprise deployments. - Cloud-native workloads (Kubernetes, containerized workers). - AI-driven vulnerability triage. - Integration with bug bounty, CI/CD, and SIEM pipelines. Next: Architecture Philosophy ```","title":"Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#01-title-executive-summary","text":"","title":"01 \u2014 Title &amp; Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#project-name","text":"SecFlow \u2014 Security Toolkit Orchestration Framework","title":"\ud83e\udde9 Project Name"},{"location":"architecture/01-title-and-executive-summary/#executive-summary","text":"SecFlow is a modular, extensible, and automation-first security orchestration platform designed to unify multiple open-source and proprietary tools for penetration testing, vulnerability management, and risk analytics. It merges dynamic scanning , tool orchestration , data normalization , and risk intelligence into a single cohesive system \u2014 optimized for both solo pentesters and enterprise red-team workflows.","title":"\ud83e\udded Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#core-vision","text":"\"One orchestrator to chain, enrich, and analyze all security tools \u2014 safely, intelligently, and reproducibly.\"","title":"Core Vision"},{"location":"architecture/01-title-and-executive-summary/#core-differentiators","text":"\ud83d\udd04 End-to-end orchestration: Tools (Nuclei, Feroxbuster, Katana, ZAP, Burp, Caido, etc.) communicate via normalized JSON pipelines. \ud83e\uddf1 Hexagonal architecture: Strict separation between core logic , adapters , and interfaces . \ud83e\udde0 Data-driven intelligence: Findings automatically enriched with CVE/CWE/CVSS and MITRE mapping. \u2699\ufe0f Resource registry: Shared wordlists, templates, and payloads under a versioned resource management system. \ud83e\uddf0 AI-ready foundation: All findings and telemetry are machine-readable for later AI triage and summarization.","title":"Core Differentiators"},{"location":"architecture/01-title-and-executive-summary/#project-goals","text":"Category Objective Engineering Create a modular mono-repo structure enabling multiple apps (CLI, API, Worker, UI). Automation Allow users to chain discovery \u2192 scanning \u2192 enrichment workflows via declarative YAML recipes. Data Consistency Standardize tool outputs into common Finding DTOs for unified triage and metrics. Usability Provide a CLI and web UI for project management, orchestration, and analytics. Security Guarantee safe sandboxing, configuration isolation, and audit logging. Scalability Support multi-tenant projects with configurable resource scopes.","title":"\ud83c\udfaf Project Goals"},{"location":"architecture/01-title-and-executive-summary/#key-capabilities-overview","text":"Layer Functionality Description Core-Lib Schema & DTOs Defines data models, ports, and contracts. Findings-Engine Normalization Converts raw tool outputs into structured findings. Wrappers Tool Adapters Execute, monitor, and parse tool output. Worker Orchestration Runs workflows (DAG execution, concurrency, retry policies). Web-API REST + WS Interface Exposes project, scan, and findings endpoints. UI HTMX/React Web dashboard for triage and insights. Plugins Extensibility Detectors, enrichers, or analytics extensions.","title":"\ud83e\uddee Key Capabilities Overview"},{"location":"architecture/01-title-and-executive-summary/#architecture-summary","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Web-API / UI\"] B[\"REST / WebSocket\"] C[\"Worker Engine<br/>(Workflows, DAG, Queues)\"] D[\"Ports / DTOs\"] E[\"Core-Lib<br/>(Models, Ports, Repositories)\"] F[\"Wrappers / Plugins<br/>(Nuclei, Ferox, Katana\u2026)\"] G[\"Tool Output\"] H[\"Findings Engine<br/>(Normalization, Metrics)\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H","title":"\ud83e\uddf1 Architecture Summary"},{"location":"architecture/01-title-and-executive-summary/#toolchain-technology-stack","text":"Category Tools / Frameworks Language Python 3.11+ Frameworks FastAPI / Flask (API), HTMX / React (UI) ORM / DB SQLModel (SQLite / PostgreSQL) Task Queue Celery / RQ (Redis backend) Caching Redis Testing Pytest + Tox + Coverage CI/CD GitHub Actions + Import-Linter + Ruff + Pyright Packaging Poetry Docs MkDocs + Material theme","title":"\u2699\ufe0f Toolchain &amp; Technology Stack"},{"location":"architecture/01-title-and-executive-summary/#intended-audience","text":"Audience Needs Red-Team / Pentesters Automate repetitive workflows, triage large data, correlate findings. Security Engineers Integrate with CI/CD, enrich findings with vulnerability intelligence. Researchers Extend plugins, add enrichment sources, or create experimental scanners. Developers Contribute to wrappers, tooling, and orchestration improvements.","title":"\ud83e\udde9 Intended Audience"},{"location":"architecture/01-title-and-executive-summary/#guiding-principles","text":"Automation-First \u2013 Everything that can be automated should be . Safe by Default \u2013 Every scan runs in isolation, respecting sandbox and audit policies. Extensible by Design \u2013 Each tool or feature lives as a plugin or wrapper. Transparent \u2013 Clear data lineage and provenance for every finding. Declarative Configuration \u2013 Tools and workflows defined in YAML/JSON, not hardcoded logic. Deterministic Execution \u2013 Same input, same results. All randomness is logged.","title":"\ud83e\udde0 Guiding Principles"},{"location":"architecture/01-title-and-executive-summary/#strategic-vision","text":"SecFlow is built not as a single pentest tool, but as a foundation for orchestrated cybersecurity automation . It aims to support: - Scalable enterprise deployments. - Cloud-native workloads (Kubernetes, containerized workers). - AI-driven vulnerability triage. - Integration with bug bounty, CI/CD, and SIEM pipelines. Next: Architecture Philosophy ```","title":"\ud83d\udd2e Strategic Vision"},{"location":"architecture/02-architecture-philosophy/","text":"02 \u2014 Architecture Philosophy \u00b6 \ud83e\udded Overview \u00b6 SecFlow's architecture follows the Hexagonal Architecture (Ports & Adapters) paradigm, enforcing a clear separation of concerns between business logic, I/O mechanisms, and user interfaces. It is designed for tool orchestration, data normalization, and future AI enrichment \u2014 with extensibility, safety, and testability at its core. \ud83e\uddf1 Architectural Tenets \u00b6 Principle Description Hexagonal Isolation All business logic lives in core modules with no framework dependencies. I/O adapters (DB, CLI, API) communicate via interfaces (Ports). Event-Driven Orchestration Workflows operate as Directed Acyclic Graphs (DAGs) executed asynchronously, allowing parallel chaining of tools. Immutable Data Flow Each execution step produces structured, immutable output for auditing and replay. Composable Pipelines Discovery \u2192 Scanning \u2192 Enrichment workflows can be combined declaratively. Strong Typing All data contracts use Pydantic models for type safety and schema validation. Declarative Configuration YAML or JSON defines tools, wordlists, templates, and workflows. Safe Execution Every external tool runs within sandbox boundaries (filesystem + network isolation). \u2699\ufe0f Core Architectural Layers \u00b6 +---------------------------------------------------------------+ | Applications | | Web-API | +---------------------------------------------------------------+ | Services Layer | | Workflow Engine | +---------------------------------------------------------------+ | Core-Lib | | DTOs | +---------------------------------------------------------------+ | Infrastructure | | Database | +---------------------------------------------------------------+ | External | | Tools | +---------------------------------------------------------------+ ```python Each layer exposes **well-defined boundaries** : - *Applications * depend only on *Services* through **ports**. - *Services * interact with *Core* logic through **domain interfaces**. - *Core-Lib * defines **pure domain logic** with zero external imports. - *Infrastructure * implements adapters to databases, cache, and external tools. --- ## \ud83e\udde9 Design Goals | Goal | Implementation Strategy | | ------|---------------------------| | **Maintainability** | Modular mono-repo with strict import boundaries (`import-linter`). | | **Scalability** | Async task execution (Celery/RQ) and worker-based orchestration. | | **Observability** | Structured logs, Prometheus metrics, and OpenTelemetry tracing. | | **Reproducibility** | Deterministic workflows with cached configuration + results. | | **Security** | Sandboxed subprocesses, limited file system access, and tokenized configuration. | | **Extensibility** | Plugin registry and manifest-based tool definitions. | --- ## \ud83e\udde9 Why Hexagonal Architecture? | Aspect | Traditional Architecture | SecFlow Approach | | --------|--------------------------|------------------| | **Dependencies** | Frameworks import core directly | Core is framework-agnostic | | **Testing** | Hard to isolate logic | Core modules unit-tested independently | | **Tool Integration** | Ad-hoc scripts | Formal wrappers with contracts | | **Maintainability** | Spaghetti imports | Controlled boundaries with Import-Linter | | **Extensibility** | Static toolset | Plugin & manifest system | --- ## \ud83e\udde9 Component Responsibility Map | Component | Responsibility | | ------------|----------------| | **Core-Lib** | Defines domain models (`Finding`, `Project`, `Resource`) and interfaces (`ToolPort`, `StoragePort`). | | **Findings-Engine** | Normalizes raw scan data into standardized Finding objects. | | **Wrappers** | Execute external tools (Nuclei, Ferox, etc.) via manifest-driven configs. | | **Worker** | Executes workflows, manages concurrency, caching, and cleanup. | | **API** | Exposes endpoints for managing projects, workflows, and triage. | | **Triage-UI** | Visual interface for findings review, filtering, and reporting. | | **Plugins** | Optional modules extending detection or enrichment logic. | | **Resource Registry** | Central management of wordlists, templates, and payloads. | --- ## \ud83e\udde9 Data Flow Model ``` mermaid %%{init : { \"theme\" : \"neutral\" } }%% flowchart TD A[\"Project\"] B[\"Workflow DAG\"] C[\"Tool Wrappers\"] D[\"Runs (execution logs)\"] E[\"Findings Engine\"] F[\"Findings (normalized)\"] G[\"Enrichment Layer\"] H[\"CVE/CWE/POC metadata\"] I[\"Triage / Metrics\"] J[\"Analytics, Dashboards\"] A --> B B --> C C --> D C --> E E --> F E --> G G --> H G --> I I --> J \ud83e\udde0 Architectural Patterns Used \u00b6 Pattern Purpose Repository Pattern Abstract data access (DB, file, memory). DTO (Data Transfer Object) Enforce schema boundaries between layers. Command Pattern Workflow node execution commands. Strategy Pattern Dynamic selection of enrichment or parsing strategies. Observer Pattern Metrics and event hooks on scan completion. Decorator Pattern Reusable middleware for retries, timeouts, and auditing. \ud83d\udd10 Security-by-Design Integration \u00b6 Every architectural decision includes a security review: - Minimal privilege principle (scoped resource access). - Tokenized secrets (no plaintext credentials). - Environment isolation (Docker/Kubernetes runtime sandboxes). - Mandatory audit trail for findings modification. \ud83e\udde0 Future-Proofing Considerations \u00b6 AI Integrations: Workflow outputs use JSON-LD style structures, allowing LLMs to reason over findings safely. Multi-Tenant Mode: Namespaced projects ensure logical and data isolation. Plugin Safety: Plugins are signed, versioned, and validated before loading. Extensible Schema: Findings model allows additional enrichment fields via metadata dicts. Next: Repository Layout","title":"Philosophy"},{"location":"architecture/02-architecture-philosophy/#02-architecture-philosophy","text":"","title":"02 \u2014 Architecture Philosophy"},{"location":"architecture/02-architecture-philosophy/#overview","text":"SecFlow's architecture follows the Hexagonal Architecture (Ports & Adapters) paradigm, enforcing a clear separation of concerns between business logic, I/O mechanisms, and user interfaces. It is designed for tool orchestration, data normalization, and future AI enrichment \u2014 with extensibility, safety, and testability at its core.","title":"\ud83e\udded Overview"},{"location":"architecture/02-architecture-philosophy/#architectural-tenets","text":"Principle Description Hexagonal Isolation All business logic lives in core modules with no framework dependencies. I/O adapters (DB, CLI, API) communicate via interfaces (Ports). Event-Driven Orchestration Workflows operate as Directed Acyclic Graphs (DAGs) executed asynchronously, allowing parallel chaining of tools. Immutable Data Flow Each execution step produces structured, immutable output for auditing and replay. Composable Pipelines Discovery \u2192 Scanning \u2192 Enrichment workflows can be combined declaratively. Strong Typing All data contracts use Pydantic models for type safety and schema validation. Declarative Configuration YAML or JSON defines tools, wordlists, templates, and workflows. Safe Execution Every external tool runs within sandbox boundaries (filesystem + network isolation).","title":"\ud83e\uddf1 Architectural Tenets"},{"location":"architecture/02-architecture-philosophy/#core-architectural-layers","text":"+---------------------------------------------------------------+ | Applications | | Web-API | +---------------------------------------------------------------+ | Services Layer | | Workflow Engine | +---------------------------------------------------------------+ | Core-Lib | | DTOs | +---------------------------------------------------------------+ | Infrastructure | | Database | +---------------------------------------------------------------+ | External | | Tools | +---------------------------------------------------------------+ ```python Each layer exposes **well-defined boundaries** : - *Applications * depend only on *Services* through **ports**. - *Services * interact with *Core* logic through **domain interfaces**. - *Core-Lib * defines **pure domain logic** with zero external imports. - *Infrastructure * implements adapters to databases, cache, and external tools. --- ## \ud83e\udde9 Design Goals | Goal | Implementation Strategy | | ------|---------------------------| | **Maintainability** | Modular mono-repo with strict import boundaries (`import-linter`). | | **Scalability** | Async task execution (Celery/RQ) and worker-based orchestration. | | **Observability** | Structured logs, Prometheus metrics, and OpenTelemetry tracing. | | **Reproducibility** | Deterministic workflows with cached configuration + results. | | **Security** | Sandboxed subprocesses, limited file system access, and tokenized configuration. | | **Extensibility** | Plugin registry and manifest-based tool definitions. | --- ## \ud83e\udde9 Why Hexagonal Architecture? | Aspect | Traditional Architecture | SecFlow Approach | | --------|--------------------------|------------------| | **Dependencies** | Frameworks import core directly | Core is framework-agnostic | | **Testing** | Hard to isolate logic | Core modules unit-tested independently | | **Tool Integration** | Ad-hoc scripts | Formal wrappers with contracts | | **Maintainability** | Spaghetti imports | Controlled boundaries with Import-Linter | | **Extensibility** | Static toolset | Plugin & manifest system | --- ## \ud83e\udde9 Component Responsibility Map | Component | Responsibility | | ------------|----------------| | **Core-Lib** | Defines domain models (`Finding`, `Project`, `Resource`) and interfaces (`ToolPort`, `StoragePort`). | | **Findings-Engine** | Normalizes raw scan data into standardized Finding objects. | | **Wrappers** | Execute external tools (Nuclei, Ferox, etc.) via manifest-driven configs. | | **Worker** | Executes workflows, manages concurrency, caching, and cleanup. | | **API** | Exposes endpoints for managing projects, workflows, and triage. | | **Triage-UI** | Visual interface for findings review, filtering, and reporting. | | **Plugins** | Optional modules extending detection or enrichment logic. | | **Resource Registry** | Central management of wordlists, templates, and payloads. | --- ## \ud83e\udde9 Data Flow Model ``` mermaid %%{init : { \"theme\" : \"neutral\" } }%% flowchart TD A[\"Project\"] B[\"Workflow DAG\"] C[\"Tool Wrappers\"] D[\"Runs (execution logs)\"] E[\"Findings Engine\"] F[\"Findings (normalized)\"] G[\"Enrichment Layer\"] H[\"CVE/CWE/POC metadata\"] I[\"Triage / Metrics\"] J[\"Analytics, Dashboards\"] A --> B B --> C C --> D C --> E E --> F E --> G G --> H G --> I I --> J","title":"\u2699\ufe0f Core Architectural Layers"},{"location":"architecture/02-architecture-philosophy/#architectural-patterns-used","text":"Pattern Purpose Repository Pattern Abstract data access (DB, file, memory). DTO (Data Transfer Object) Enforce schema boundaries between layers. Command Pattern Workflow node execution commands. Strategy Pattern Dynamic selection of enrichment or parsing strategies. Observer Pattern Metrics and event hooks on scan completion. Decorator Pattern Reusable middleware for retries, timeouts, and auditing.","title":"\ud83e\udde0 Architectural Patterns Used"},{"location":"architecture/02-architecture-philosophy/#security-by-design-integration","text":"Every architectural decision includes a security review: - Minimal privilege principle (scoped resource access). - Tokenized secrets (no plaintext credentials). - Environment isolation (Docker/Kubernetes runtime sandboxes). - Mandatory audit trail for findings modification.","title":"\ud83d\udd10 Security-by-Design Integration"},{"location":"architecture/02-architecture-philosophy/#future-proofing-considerations","text":"AI Integrations: Workflow outputs use JSON-LD style structures, allowing LLMs to reason over findings safely. Multi-Tenant Mode: Namespaced projects ensure logical and data isolation. Plugin Safety: Plugins are signed, versioned, and validated before loading. Extensible Schema: Findings model allows additional enrichment fields via metadata dicts. Next: Repository Layout","title":"\ud83e\udde0 Future-Proofing Considerations"},{"location":"architecture/03-repository-layout/","text":"03 \u2014 Repository Layout \u00b6 \ud83e\udded Overview \u00b6 The SecFlow repository is structured as a Python mono-repo workspace housing multiple interdependent packages and apps. This allows us to: - Share interfaces and DTOs across services. - Enforce architecture boundaries centrally. - Run unified CI/CD pipelines and dependency resolution through Poetry . \ud83e\uddf1 Directory Structure \u00b6 flowchart TD root[\"SecFlow/\"] root --> pkgs[\"packages/\"] pkgs --> core[\"core-lib/\"] pkgs --> wf[\"workflow-engine/\"] pkgs --> tm[\"tool-manager/\"] pkgs --> rr[\"resource-registry/\"] pkgs --> obs[\"observability/\"] pkgs --> sec[\"security/\"] root --> apps[\"apps/\"] apps --> cli[\"cli/\"] apps --> api[\"api-server/\"] apps --> web[\"web-ui/\"] root --> docs[\"docs/\"] docs --> arch[\"architecture/\"] docs --> review[\"review/\"] docs --> diagrams[\"diagrams/\"] root --> tools[\"tools/\"] tools --> scripts[\"scripts/\"] tools --> linters[\"linters/\"] root --> github[\".github/\"] github --> workflows[\"workflows/\"] \u2699\ufe0f Python Workspace Configuration \u00b6 pyproject.toml Snippet \u00b6 [tool.poetry] name = \"SecFlow\" version = \"1.0.0\" description = \"Security Toolkit Orchestration Framework\" authors = [ \"Hernan Trajtemberg <hernan.trajtemberg@domain>\" ] [tool.poetry.dependencies] python = \"^3.11\" fastapi = \"^0.115\" sqlmodel = \"^0.0.22\" celery = \"^5.3\" redis = \"^5.0\" pydantic = \"^2.7\" import-linter = \"^1.5\" ruff = \"^0.7\" pytest = \"^8.3\" [tool.poetry.group.dev.dependencies] pyright = \"*\" coverage = \"*\" [build-system] requires = [ \"poetry-core>=1.6\" ] build-backend = \"poetry.core.masonry.api\" ``` python ## \ud83e\udde9 Application Layering Each app in `/ apps /` uses internal packages exclusively via ports , ensuring loose coupling . | Layer | Directory | Import Rules | | ------- | ----------- | -------------- | | ** Core ** | ` packages / core-lib ` | No external imports | | ** Findings Engine ** | ` packages / findings-engine ` | May import core-lib | | ** Wrappers ** | ` packages / wrappers ` | May import core-lib , utils | | ** API / Worker ** | ` apps / web-api `, ` apps / worker ` | May import via ports only | | ** Plugins ** | ` packages / plugins ` | Dynamically loaded at runtime | ## \ud83e\udde9 Import-Linter Configuration ` importlinter . ini ` enforces import boundaries automatically : ``` ini [importlinter] root_package = SecFlow [contract : core-isolation] name = Core-Lib Is Independent type = forbidden source_modules = SecFlow.packages.core _ lib forbidden_modules = SecFlow.apps SecFlow . packages . wrappers SecFlow . packages . findings_engine [contract : adapters-only] name = Adapters Only Depend On Ports type = layers layers = SecFlow.packages.core _ lib SecFlow . packages . findings_engine SecFlow . packages . wrappers SecFlow . apps ``` bash If violated , the CI pipeline fails the build . ## \ud83e\udde0 Developer Workflow ### Local Development ``` bash poetry install poetry run pre-commit install poetry run pytest ``` yaml ### Run the Worker ``` bash poetry run celery -A SecFlow . apps . worker worker --loglevel = inf o ``` bash ### Run the Web API ``` bash poetry run uvicorn SecFlow . apps . web_api . main : app --reload ``` yaml ## \ud83e\udde9 Continuous Integration Pipeline GitHub Actions (` . github / workflows / ci . yml `): ``` yaml name : SecFlow CI on : [push , pull_request] jobs : build : runs-on : ubuntu-latest steps : - uses : actions / checkout @ v4 - uses : actions / setup-python @ v5 with : python-version : '3.11' - name : Install dependencies run : | pip install poetry poetry install - name : Lint & Type Check run : | poetry run ruff check . poetry run pyright - name : Run Tests run : poetry run pytest --maxfail = 1 --disable-warnings -q ``` yaml ## \ud83e\uddf0 Tooling & Developer Aids | Tool | Purpose | | ------ | --------- | | ** Ruff ** | Linting , formatting enforcement | | ** Pyright ** | Static type checking | | ** Import-Linter ** | Architecture enforcement | | ** Poetry ** | Dependency & build management | | ** Tox ** | Multi-environment testing | | ** MkDocs ** | Documentation site generation | | ** Coverage . py ** | Test coverage reports | ## \ud83e\udde9 ASCII Diagram \u2014 High-Level View ``` text + ----------------------------- + | SecFlow / | + ------------- + --------------- + | + ------------v------------- + | packages / | | core-lib , findings , etc . | + ------------ + ------------- + | + ------------v------------- + | apps / | | web-api , worker , cli , ui | + ------------ + ------------- + | + ------------v------------- + | tests / | + -------------------------- + \ud83e\udde0 Future Enhancements \u00b6 Monorepo Versioning: Each package versioned via poetry-dynamic-versioning. Documentation Pipeline: Auto-regenerate schema docs ( mkdocs build ) on merge. Pre-Commit Hooks: Validate imports and enforce ruff formatting pre-push. Code Owners: Assign maintainers per package via .github/CODEOWNERS . Next: Core Packages & Responsibilities","title":"Repository Layout"},{"location":"architecture/03-repository-layout/#03-repository-layout","text":"","title":"03 \u2014 Repository Layout"},{"location":"architecture/03-repository-layout/#overview","text":"The SecFlow repository is structured as a Python mono-repo workspace housing multiple interdependent packages and apps. This allows us to: - Share interfaces and DTOs across services. - Enforce architecture boundaries centrally. - Run unified CI/CD pipelines and dependency resolution through Poetry .","title":"\ud83e\udded Overview"},{"location":"architecture/03-repository-layout/#directory-structure","text":"flowchart TD root[\"SecFlow/\"] root --> pkgs[\"packages/\"] pkgs --> core[\"core-lib/\"] pkgs --> wf[\"workflow-engine/\"] pkgs --> tm[\"tool-manager/\"] pkgs --> rr[\"resource-registry/\"] pkgs --> obs[\"observability/\"] pkgs --> sec[\"security/\"] root --> apps[\"apps/\"] apps --> cli[\"cli/\"] apps --> api[\"api-server/\"] apps --> web[\"web-ui/\"] root --> docs[\"docs/\"] docs --> arch[\"architecture/\"] docs --> review[\"review/\"] docs --> diagrams[\"diagrams/\"] root --> tools[\"tools/\"] tools --> scripts[\"scripts/\"] tools --> linters[\"linters/\"] root --> github[\".github/\"] github --> workflows[\"workflows/\"]","title":"\ud83e\uddf1 Directory Structure"},{"location":"architecture/03-repository-layout/#python-workspace-configuration","text":"","title":"\u2699\ufe0f Python Workspace Configuration"},{"location":"architecture/03-repository-layout/#pyprojecttoml-snippet","text":"[tool.poetry] name = \"SecFlow\" version = \"1.0.0\" description = \"Security Toolkit Orchestration Framework\" authors = [ \"Hernan Trajtemberg <hernan.trajtemberg@domain>\" ] [tool.poetry.dependencies] python = \"^3.11\" fastapi = \"^0.115\" sqlmodel = \"^0.0.22\" celery = \"^5.3\" redis = \"^5.0\" pydantic = \"^2.7\" import-linter = \"^1.5\" ruff = \"^0.7\" pytest = \"^8.3\" [tool.poetry.group.dev.dependencies] pyright = \"*\" coverage = \"*\" [build-system] requires = [ \"poetry-core>=1.6\" ] build-backend = \"poetry.core.masonry.api\" ``` python ## \ud83e\udde9 Application Layering Each app in `/ apps /` uses internal packages exclusively via ports , ensuring loose coupling . | Layer | Directory | Import Rules | | ------- | ----------- | -------------- | | ** Core ** | ` packages / core-lib ` | No external imports | | ** Findings Engine ** | ` packages / findings-engine ` | May import core-lib | | ** Wrappers ** | ` packages / wrappers ` | May import core-lib , utils | | ** API / Worker ** | ` apps / web-api `, ` apps / worker ` | May import via ports only | | ** Plugins ** | ` packages / plugins ` | Dynamically loaded at runtime | ## \ud83e\udde9 Import-Linter Configuration ` importlinter . ini ` enforces import boundaries automatically : ``` ini [importlinter] root_package = SecFlow [contract : core-isolation] name = Core-Lib Is Independent type = forbidden source_modules = SecFlow.packages.core _ lib forbidden_modules = SecFlow.apps SecFlow . packages . wrappers SecFlow . packages . findings_engine [contract : adapters-only] name = Adapters Only Depend On Ports type = layers layers = SecFlow.packages.core _ lib SecFlow . packages . findings_engine SecFlow . packages . wrappers SecFlow . apps ``` bash If violated , the CI pipeline fails the build . ## \ud83e\udde0 Developer Workflow ### Local Development ``` bash poetry install poetry run pre-commit install poetry run pytest ``` yaml ### Run the Worker ``` bash poetry run celery -A SecFlow . apps . worker worker --loglevel = inf o ``` bash ### Run the Web API ``` bash poetry run uvicorn SecFlow . apps . web_api . main : app --reload ``` yaml ## \ud83e\udde9 Continuous Integration Pipeline GitHub Actions (` . github / workflows / ci . yml `): ``` yaml name : SecFlow CI on : [push , pull_request] jobs : build : runs-on : ubuntu-latest steps : - uses : actions / checkout @ v4 - uses : actions / setup-python @ v5 with : python-version : '3.11' - name : Install dependencies run : | pip install poetry poetry install - name : Lint & Type Check run : | poetry run ruff check . poetry run pyright - name : Run Tests run : poetry run pytest --maxfail = 1 --disable-warnings -q ``` yaml ## \ud83e\uddf0 Tooling & Developer Aids | Tool | Purpose | | ------ | --------- | | ** Ruff ** | Linting , formatting enforcement | | ** Pyright ** | Static type checking | | ** Import-Linter ** | Architecture enforcement | | ** Poetry ** | Dependency & build management | | ** Tox ** | Multi-environment testing | | ** MkDocs ** | Documentation site generation | | ** Coverage . py ** | Test coverage reports | ## \ud83e\udde9 ASCII Diagram \u2014 High-Level View ``` text + ----------------------------- + | SecFlow / | + ------------- + --------------- + | + ------------v------------- + | packages / | | core-lib , findings , etc . | + ------------ + ------------- + | + ------------v------------- + | apps / | | web-api , worker , cli , ui | + ------------ + ------------- + | + ------------v------------- + | tests / | + -------------------------- +","title":"pyproject.toml Snippet"},{"location":"architecture/03-repository-layout/#future-enhancements","text":"Monorepo Versioning: Each package versioned via poetry-dynamic-versioning. Documentation Pipeline: Auto-regenerate schema docs ( mkdocs build ) on merge. Pre-Commit Hooks: Validate imports and enforce ruff formatting pre-push. Code Owners: Assign maintainers per package via .github/CODEOWNERS . Next: Core Packages & Responsibilities","title":"\ud83e\udde0 Future Enhancements"},{"location":"architecture/04-core-packages-and-responsibilities/","text":"04 \u2014 Core Packages & Responsibilities \u00b6 \ud83e\udded Overview \u00b6 This document describes the core packages inside the packages/ directory and explains the roles, contracts, and relationships that define the heart of SecFlow. The architecture is intentionally hexagonal : - core-lib defines domain models and ports (interfaces). - Other packages such as findings-engine , wrappers , and storage implement those ports. - Application layers ( web-api , worker , cli ) use the ports but never touch implementations directly. \ud83e\udde9 Package Overview Table \u00b6 Package Responsibility Depends On core-lib Domain models, DTOs, Ports, and Service Interfaces. None findings-engine Normalization and enrichment of scan data. core-lib wrappers Executes external tools and parses results. core-lib , utils resources Manages reusable assets (wordlists, payloads, templates). core-lib storage Implements persistence and caching (Postgres, SQLite, Redis). core-lib plugins Houses extension modules (detectors, enrichers, analytics). core-lib utils Generic utilities: logging, validation, subprocess helpers. None \u2699\ufe0f Core-Lib \u00b6 Purpose \u00b6 core-lib is the foundation of SecFlow. It defines all domain entities, type contracts, and abstract interfaces (ports) used throughout the system. Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"core-lib/\"] B[\"models/\"] C[\"finding.py\"] D[\"project.py\"] E[\"resource.py\"] F[\"run.py\"] G[\"ports/\"] H[\"storage_port.py\"] I[\"tool_port.py\"] J[\"resource_port.py\"] K[\"findings_port.py\"] L[\"services/\"] M[\"workflow_service.py\"] N[\"triage_service.py\"] O[\"exceptions.py\"] A --> B A --> G A --> L A --> O B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K L --> M L --> N Example Model \u00b6 # core-lib/models/finding.py from pydantic import BaseModel from datetime import datetime from typing import List , Optional , Dict class Finding ( BaseModel ): id : str project_id : str detector_id : str title : str severity : str path : str evidence : Dict [ str , str ] created_at : datetime cwe : Optional [ int ] owasp : Optional [ str ] cve_ids : List [ str ] = [] enrichment : Dict [ str , any ] = {} ``` python ### Example Port ``` python # core-lib/ports/tool_port.py from typing import Protocol , List from core_lib.models.finding import Finding class ToolPort ( Protocol ): def prepare ( self , config : dict ) -> None : \"\"\"Prepare the tool with given configuration.\"\"\" pass def execute ( self ) -> List [ Finding ]: \"\"\"Execute the tool and return findings.\"\"\" pass def validate_output ( self , raw_output : str ) -> bool : \"\"\"Validate tool output format.\"\"\" pass ``` python This abstraction allows any external tool to be integrated simply by implementing ` ToolPort ` . ## \ud83e\udde0 Findings-Engine ### Purpose Responsible for normalizing , deduplicating , and enriching findings produced by wrappers . ### Example Normalization Flow ``` python def normalize ( raw_data : str , tool : str ) -> Finding : if tool == \"nuclei\" : return _normalize_nuclei_output ( raw_data ) elif tool == \"ferox\" : return _normalize_ferox_output ( raw_data ) ``` python ### Capabilities - Parse multiple output formats ( JSON , XML , plain text ) . - Deduplicate based on fingerprint ( host + path + vuln_id ) . - Attach CWE , CVSS , and severity from enrichment sources . - Store normalized data through ` StoragePort ` . ## \u2699\ufe0f Wrappers ### Purpose Wraps and executes third - party tools through a unified interface defined by ` ToolPort ` . ### Example Structure ``` text wrappers / %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"wrappers/\"] B[\"nuclei_wrapper.py\"] C[\"ferox_wrapper.py\"] D[\"zap_wrapper.py\"] E[\"base_wrapper.py\"] A --> B A --> C A --> D A --> E Example Base Class \u00b6 # wrappers/base_wrapper.py import subprocess from core_lib.models.finding import Finding class BaseWrapper : def __init__ ( self , manifest : dict ): self . manifest = manifest def run ( self , target : str ) -> list [ Finding ]: cmd = [ self . manifest [ \"binary\" ], \"-u\" , target ] result = subprocess . run ( cmd , capture_output = True , text = True ) return self . parse_output ( result . stdout ) def parse_output ( self , raw : str ) -> list [ Finding ]: raise NotImplementedError ``` python All wrappers inherit from ` BaseWrapper ` and override ` parse_output ` . ## \ud83d\udce6 Resources ### Purpose ` resources ` manages global and scoped assets : - Wordlists ( directories , subdomains , parameters ) - Templates ( Nuclei , ZAP ) - Payload sets - Custom configurations ### Example Resource Model ``` python # core-lib/models/resource.py class Resource ( BaseModel ): id : str name : str type : Literal [ \"wordlist\" , \"template\" , \"payload\" ] scope : Literal [ \"global\" , \"group\" , \"project\" ] hash : str version : str metadata : dict ``` python ## \ud83d\uddc3\ufe0f Storage ### Purpose Implements all persistence operations via the ` StoragePort ` interface . ### Example Interface ``` python # core-lib/ports/storage_port.py class StoragePort ( Protocol ): def save_finding ( self , finding : Finding ) -> None : \"\"\"Save a finding to storage.\"\"\" pass def list_findings ( self , project_id : str ) -> list [ Finding ]: \"\"\"List all findings for a project.\"\"\" pass def delete_project ( self , project_id : str ) -> None : \"\"\"Delete a project and all its findings.\"\"\" pass ``` python ### Implementation Examples - ` SQLiteStorageAdapter ` - ` PostgresStorageAdapter ` - ` RedisCacheAdapter ` All registered in the ` storage . registry ` . ## \ud83d\udd0c Plugins ### Purpose Extend SecFlow with additional detection or enrichment capabilities . ### Example Plugin Registration ``` python # plugins/registry.py from typing import Dict , Callable PLUGIN_REGISTRY : Dict [ str , Callable ] = {} def register_plugin ( name : str ): def decorator ( func ): PLUGIN_REGISTRY [ name ] = func return func return decorator ``` yaml Plugins can dynamically hook into findings processing , orchestration , or resource management . ## \ud83e\uddf0 Utils ` utils ` contains helper modules that are shared across packages but contain no business logic . ### Examples: - ` utils . subprocess_safe ` \u2013 wrapper for secure process spawning . - ` utils . hashing ` \u2013 generate resource hashes ( SHA256 ) . - ` utils . validation ` \u2013 reusable Pydantic validators . - ` utils . config ` \u2013 environment - aware configuration loader . ## \ud83d\udd17 Cross-Package Interaction Diagram ``` text +-------------+ +----------------+ +---------------+ | Wrappers | ----> | Findings - Engine | ----> | Storage | +-------------+ +----------------+ +---------------+ | ^ | | | | v | v +-----------+ +----------------+ +----------------+ | Resources | | Core - Lib | | Plugins | +-----------+ +----------------+ +----------------+ Arrows represent dependency flow, not import direction (imports always go downward). \ud83e\udde0 Best Practices \u00b6 Every package must expose a __all__ list for stable imports. Each module must define __version__ for dependency tracking. No package may import directly from /apps/ . Keep adapters stateless; use dependency injection for configuration. Always prefer Pydantic models over raw dictionaries. \ud83e\udde9 Testing Guidelines \u00b6 Package Test Type Focus core-lib Unit DTO validation and port contracts findings-engine Integration Normalization and enrichment correctness wrappers System Command execution, parsing accuracy storage Integration CRUD consistency across backends plugins Unit Hook execution and registry behavior \ud83e\udde0 Future Enhancements \u00b6 Introduce a service layer in core-lib to coordinate between repositories and orchestrators. Implement schema diffing for findings-engine to detect breaking changes. Add async wrappers for high-performance tools (e.g., Katana via aiohttp). Build auto-generated docs from port signatures ( mkdocs-gen-files ). Next: Orchestration & Workflow Engine","title":"Core Packages"},{"location":"architecture/04-core-packages-and-responsibilities/#04-core-packages-responsibilities","text":"","title":"04 \u2014 Core Packages &amp; Responsibilities"},{"location":"architecture/04-core-packages-and-responsibilities/#overview","text":"This document describes the core packages inside the packages/ directory and explains the roles, contracts, and relationships that define the heart of SecFlow. The architecture is intentionally hexagonal : - core-lib defines domain models and ports (interfaces). - Other packages such as findings-engine , wrappers , and storage implement those ports. - Application layers ( web-api , worker , cli ) use the ports but never touch implementations directly.","title":"\ud83e\udded Overview"},{"location":"architecture/04-core-packages-and-responsibilities/#package-overview-table","text":"Package Responsibility Depends On core-lib Domain models, DTOs, Ports, and Service Interfaces. None findings-engine Normalization and enrichment of scan data. core-lib wrappers Executes external tools and parses results. core-lib , utils resources Manages reusable assets (wordlists, payloads, templates). core-lib storage Implements persistence and caching (Postgres, SQLite, Redis). core-lib plugins Houses extension modules (detectors, enrichers, analytics). core-lib utils Generic utilities: logging, validation, subprocess helpers. None","title":"\ud83e\udde9 Package Overview Table"},{"location":"architecture/04-core-packages-and-responsibilities/#core-lib","text":"","title":"\u2699\ufe0f Core-Lib"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose","text":"core-lib is the foundation of SecFlow. It defines all domain entities, type contracts, and abstract interfaces (ports) used throughout the system.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"core-lib/\"] B[\"models/\"] C[\"finding.py\"] D[\"project.py\"] E[\"resource.py\"] F[\"run.py\"] G[\"ports/\"] H[\"storage_port.py\"] I[\"tool_port.py\"] J[\"resource_port.py\"] K[\"findings_port.py\"] L[\"services/\"] M[\"workflow_service.py\"] N[\"triage_service.py\"] O[\"exceptions.py\"] A --> B A --> G A --> L A --> O B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K L --> M L --> N","title":"Structure"},{"location":"architecture/04-core-packages-and-responsibilities/#example-model","text":"# core-lib/models/finding.py from pydantic import BaseModel from datetime import datetime from typing import List , Optional , Dict class Finding ( BaseModel ): id : str project_id : str detector_id : str title : str severity : str path : str evidence : Dict [ str , str ] created_at : datetime cwe : Optional [ int ] owasp : Optional [ str ] cve_ids : List [ str ] = [] enrichment : Dict [ str , any ] = {} ``` python ### Example Port ``` python # core-lib/ports/tool_port.py from typing import Protocol , List from core_lib.models.finding import Finding class ToolPort ( Protocol ): def prepare ( self , config : dict ) -> None : \"\"\"Prepare the tool with given configuration.\"\"\" pass def execute ( self ) -> List [ Finding ]: \"\"\"Execute the tool and return findings.\"\"\" pass def validate_output ( self , raw_output : str ) -> bool : \"\"\"Validate tool output format.\"\"\" pass ``` python This abstraction allows any external tool to be integrated simply by implementing ` ToolPort ` . ## \ud83e\udde0 Findings-Engine ### Purpose Responsible for normalizing , deduplicating , and enriching findings produced by wrappers . ### Example Normalization Flow ``` python def normalize ( raw_data : str , tool : str ) -> Finding : if tool == \"nuclei\" : return _normalize_nuclei_output ( raw_data ) elif tool == \"ferox\" : return _normalize_ferox_output ( raw_data ) ``` python ### Capabilities - Parse multiple output formats ( JSON , XML , plain text ) . - Deduplicate based on fingerprint ( host + path + vuln_id ) . - Attach CWE , CVSS , and severity from enrichment sources . - Store normalized data through ` StoragePort ` . ## \u2699\ufe0f Wrappers ### Purpose Wraps and executes third - party tools through a unified interface defined by ` ToolPort ` . ### Example Structure ``` text wrappers / %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"wrappers/\"] B[\"nuclei_wrapper.py\"] C[\"ferox_wrapper.py\"] D[\"zap_wrapper.py\"] E[\"base_wrapper.py\"] A --> B A --> C A --> D A --> E","title":"Example Model"},{"location":"architecture/04-core-packages-and-responsibilities/#example-base-class","text":"# wrappers/base_wrapper.py import subprocess from core_lib.models.finding import Finding class BaseWrapper : def __init__ ( self , manifest : dict ): self . manifest = manifest def run ( self , target : str ) -> list [ Finding ]: cmd = [ self . manifest [ \"binary\" ], \"-u\" , target ] result = subprocess . run ( cmd , capture_output = True , text = True ) return self . parse_output ( result . stdout ) def parse_output ( self , raw : str ) -> list [ Finding ]: raise NotImplementedError ``` python All wrappers inherit from ` BaseWrapper ` and override ` parse_output ` . ## \ud83d\udce6 Resources ### Purpose ` resources ` manages global and scoped assets : - Wordlists ( directories , subdomains , parameters ) - Templates ( Nuclei , ZAP ) - Payload sets - Custom configurations ### Example Resource Model ``` python # core-lib/models/resource.py class Resource ( BaseModel ): id : str name : str type : Literal [ \"wordlist\" , \"template\" , \"payload\" ] scope : Literal [ \"global\" , \"group\" , \"project\" ] hash : str version : str metadata : dict ``` python ## \ud83d\uddc3\ufe0f Storage ### Purpose Implements all persistence operations via the ` StoragePort ` interface . ### Example Interface ``` python # core-lib/ports/storage_port.py class StoragePort ( Protocol ): def save_finding ( self , finding : Finding ) -> None : \"\"\"Save a finding to storage.\"\"\" pass def list_findings ( self , project_id : str ) -> list [ Finding ]: \"\"\"List all findings for a project.\"\"\" pass def delete_project ( self , project_id : str ) -> None : \"\"\"Delete a project and all its findings.\"\"\" pass ``` python ### Implementation Examples - ` SQLiteStorageAdapter ` - ` PostgresStorageAdapter ` - ` RedisCacheAdapter ` All registered in the ` storage . registry ` . ## \ud83d\udd0c Plugins ### Purpose Extend SecFlow with additional detection or enrichment capabilities . ### Example Plugin Registration ``` python # plugins/registry.py from typing import Dict , Callable PLUGIN_REGISTRY : Dict [ str , Callable ] = {} def register_plugin ( name : str ): def decorator ( func ): PLUGIN_REGISTRY [ name ] = func return func return decorator ``` yaml Plugins can dynamically hook into findings processing , orchestration , or resource management . ## \ud83e\uddf0 Utils ` utils ` contains helper modules that are shared across packages but contain no business logic . ### Examples: - ` utils . subprocess_safe ` \u2013 wrapper for secure process spawning . - ` utils . hashing ` \u2013 generate resource hashes ( SHA256 ) . - ` utils . validation ` \u2013 reusable Pydantic validators . - ` utils . config ` \u2013 environment - aware configuration loader . ## \ud83d\udd17 Cross-Package Interaction Diagram ``` text +-------------+ +----------------+ +---------------+ | Wrappers | ----> | Findings - Engine | ----> | Storage | +-------------+ +----------------+ +---------------+ | ^ | | | | v | v +-----------+ +----------------+ +----------------+ | Resources | | Core - Lib | | Plugins | +-----------+ +----------------+ +----------------+ Arrows represent dependency flow, not import direction (imports always go downward).","title":"Example Base Class"},{"location":"architecture/04-core-packages-and-responsibilities/#best-practices","text":"Every package must expose a __all__ list for stable imports. Each module must define __version__ for dependency tracking. No package may import directly from /apps/ . Keep adapters stateless; use dependency injection for configuration. Always prefer Pydantic models over raw dictionaries.","title":"\ud83e\udde0 Best Practices"},{"location":"architecture/04-core-packages-and-responsibilities/#testing-guidelines","text":"Package Test Type Focus core-lib Unit DTO validation and port contracts findings-engine Integration Normalization and enrichment correctness wrappers System Command execution, parsing accuracy storage Integration CRUD consistency across backends plugins Unit Hook execution and registry behavior","title":"\ud83e\udde9 Testing Guidelines"},{"location":"architecture/04-core-packages-and-responsibilities/#future-enhancements","text":"Introduce a service layer in core-lib to coordinate between repositories and orchestrators. Implement schema diffing for findings-engine to detect breaking changes. Add async wrappers for high-performance tools (e.g., Katana via aiohttp). Build auto-generated docs from port signatures ( mkdocs-gen-files ). Next: Orchestration & Workflow Engine","title":"\ud83e\udde0 Future Enhancements"},{"location":"architecture/05-orchestration-and-workflow-engine/","text":"05 \u2014 Orchestration & Workflow Engine \u00b6 \ud83e\udded Overview \u00b6 The Orchestration Engine is the operational core of SecFlow. It executes multi-tool workflows composed of discovery , scanning , filtering , enrichment , and analysis stages. Workflows are defined declaratively using YAML recipes and executed as Directed Acyclic Graphs (DAGs) . Each node represents a tool invocation , and edges define data flow between outputs and inputs. \ud83e\uddf1 Conceptual Model \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Node A<br/>(Ferox)\"] B[\"Node B<br/>(Nuclei)\"] C[\"Node C<br/>(CVE Enr)\"] D[\"Inputs/Outputs\"] E[\"Config/Env\"] F[\"Artifact Storage\"] A --> B B --> C A --> D B --> E C --> F Each node produces one or more datasets that can be consumed by downstream nodes. The workflow engine guarantees: - Topological order of execution - Concurrent execution where possible - Automatic retries, timeouts, and logging per node \u2699\ufe0f Workflow Specification Schema \u00b6 version : \"1.0\" name : \"OWASP Top 10 Scan\" description : \"End-to-end test: discovery \u2192 scan \u2192 enrichment\" nodes : - id : \"discovery\" type : \"discovery.ferox\" config : wordlist : \"res://wordlists/dirb:latest\" threads : 50 outputs : [ \"urls\" ] - id : \"scan\" type : \"scan.nuclei\" inputs : [ \"urls\" ] config : templates : \"res://templates/owasp-top10:latest\" rate_limit : 150 outputs : [ \"findings\" ] - id : \"enrich\" type : \"enrich.cve\" inputs : [ \"findings\" ] config : sources : [ \"nvd\" , \"osv\" , \"exploitdb\" ] outputs : [ \"enriched_findings\" ] ``` text ## \ud83e\udde9 Workflow Engine Architecture ``` text +-------------------------------------------------------------+ | Worker Engine | | -------------------------------------------------------------| | - WorkflowScheduler | | - NodeExecutor | | - ResultCache | | - EventBus | | -------------------------------------------------------------| | Uses : Celery (Redis), asyncio, Pydantic validation | +-------------------------------------------------------------+ ``` text ### Key Components | Component | Description | | -----------|-------------| | **WorkflowScheduler** | Parses YAML recipes, builds DAG, submits jobs to queue. | | **NodeExecutor** | Executes nodes, manages subprocess wrappers. | | **ResultCache** | Stores intermediate results between nodes. | | **EventBus** | Publishes events (node_started, node_completed, workflow_failed). | | **WorkflowStore** | Persists workflow metadata in DB. | ## \u2699\ufe0f Python Model \u2014 DAG Representation ``` python # findings-engine/workflow_dag.py from typing import List, Dict, Any from pydantic import BaseModel class Node(BaseModel) : id : str type : str config : Dict[str, Any] = {} inputs : List[str] = [] outputs : List[str] = [] class Workflow(BaseModel) : id : str name : str description : str nodes : List[Node] ``` text ### DAG Validation Example ``` python def validate_dag(workflow: Workflow) : ids = [n.id for n in workflow.nodes] for node in workflow.nodes : for inp in node.inputs : if inp not in [out for n in workflow.nodes for out in n.outputs] : raise ValueError(f\"Unresolved input '{inp}' in node {node.id}\") ``` python ## \ud83e\udde0 Execution Flow 1. Parse & Validate YAML workflow using Pydantic schema. 2. Register DAG in database (`WorkflowStore`). 3. Submit nodes to Celery/RQ queue respecting topological order. 4. Execute wrappers through `ToolPort` interface. 5. Normalize findings via `FindingsEngine`. 6. Publish events to `EventBus`. 7. Update metrics and trigger downstream listeners (e.g., triage UI). ## \u2699\ufe0f Node Executor (Simplified) ``` python # worker/executor.py from core_lib.ports.tool_port import ToolPort class NodeExecutor : def __init__(self, node, context) : self.node = node self.context = context def run(self) : tool : ToolPort = self.context.resolve_tool(self.node.type) tool.prepare(self.node.config) results = tool.execute() self.context.store_results(self.node.outputs, results) return results ``` python ## \ud83d\udd04 Concurrency Model The orchestration engine is designed for asynchronous, multi-tool execution. | Execution Mode | Description | | ----------------|-------------| | **Sequential** | Enforced by DAG dependencies. | | **Parallel** | Independent nodes run concurrently (async or Celery workers). | | **Chained** | Output from one node auto-feeds into next via `ResultCache`. | ### Example ``` mermaid %% { init : { \"theme\" : \"neutral\" }} %% flowchart LR A[\"ferox\"] B[\"katana\"] C[\"nuclei\"] D[\"httpx\"] A --> B B --> C A --> D D --> C \ud83e\udde9 Error Handling \u00b6 Error Type Handling Strategy Tool crash / non-zero exit Retry (max=3) then mark node failed. Timeout Kill process, log event, continue DAG. Missing input dataset Block downstream nodes, mark dependency unresolved. Parser error Log raw output, fallback to generic findings schema. Each failure is logged in the workflow_runs table and visible in the UI. \ud83e\udde0 Event System \u00b6 The orchestration layer publishes real-time events to facilitate reactive behavior. Example Event Contract \u00b6 { \"event\" : \"node_completed\" , \"workflow_id\" : \"abc123\" , \"node_id\" : \"scan\" , \"duration\" : 12.3 , \"findings\" : 124 } ```py t ho n Eve nts ca n be co nsu med by : - WebSocke t clie nts i n UI (live progress) - Audi t log processors - Me tr ics collec t ors ## \ud83e\udde9 Cachi n g & Reuse - **I nter media te Da ta : ** S t ored u n der `/cache/ { work fl ow_id } / { n ode_id } .jso n ` - **Resul t Hashi n g : ** SHA 256 o f co nf ig + i n pu ts f or cache hi ts - **Warm Ru ns : ** Work fl ows ca n resume fr om cached i nter media te ou t pu ts ```py t ho n cache_key = hashlib.sha 256 (jso n .dumps( n ode.co nf ig).e n code()).hexdiges t () ``` te x t ## \ud83e\uddf1 Example DAG Execu t io n Trace ``` te x t [ 2025-10-06 12 : 01 : 02 ] Work fl ow \"OWASP Top 10 Scan\" s tarte d [ 2025-10-06 12 : 01 : 05 ] Node discovery. fer ox comple te d (urls= 356 ) [ 2025-10-06 12 : 01 : 07 ] Node sca n . nu clei comple te d ( f i n di n gs= 112 ) [ 2025-10-06 12 : 01 : 10 ] Node e nr ich.cve comple te d (e nr iched_ f i n di n gs= 112 ) [ 2025-10-06 12 : 01 : 10 ] Work fl ow comple te d success full y \ud83d\udd0c Integration with Other Components \u00b6 Component Interaction Findings Engine Receives raw output for normalization. Wrappers Execute the underlying binaries/tools. Storage Persists workflow runs, logs, results. Plugins Hooks into on_node_complete and on_workflow_complete . UI / API Subscribes to event bus for progress updates. \ud83e\udde9 Monitoring & Metrics \u00b6 Every node execution reports: - Duration (seconds) - Findings count - Exit status - CPU/memory usage - Cache hits These metrics feed Prometheus exporters and the analytics dashboards. \ud83e\udde0 Future Enhancements \u00b6 GraphQL-based workflow builder UI. Dynamic scheduling policies (priority, resource weighting). Conditional branching (if, switch nodes). AI-assisted workflow suggestions based on context and prior runs. Distributed orchestration using Celery groups/chords. Next: Plugin System","title":"Orchestration Engine"},{"location":"architecture/05-orchestration-and-workflow-engine/#05-orchestration-workflow-engine","text":"","title":"05 \u2014 Orchestration &amp; Workflow Engine"},{"location":"architecture/05-orchestration-and-workflow-engine/#overview","text":"The Orchestration Engine is the operational core of SecFlow. It executes multi-tool workflows composed of discovery , scanning , filtering , enrichment , and analysis stages. Workflows are defined declaratively using YAML recipes and executed as Directed Acyclic Graphs (DAGs) . Each node represents a tool invocation , and edges define data flow between outputs and inputs.","title":"\ud83e\udded Overview"},{"location":"architecture/05-orchestration-and-workflow-engine/#conceptual-model","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Node A<br/>(Ferox)\"] B[\"Node B<br/>(Nuclei)\"] C[\"Node C<br/>(CVE Enr)\"] D[\"Inputs/Outputs\"] E[\"Config/Env\"] F[\"Artifact Storage\"] A --> B B --> C A --> D B --> E C --> F Each node produces one or more datasets that can be consumed by downstream nodes. The workflow engine guarantees: - Topological order of execution - Concurrent execution where possible - Automatic retries, timeouts, and logging per node","title":"\ud83e\uddf1 Conceptual Model"},{"location":"architecture/05-orchestration-and-workflow-engine/#workflow-specification-schema","text":"version : \"1.0\" name : \"OWASP Top 10 Scan\" description : \"End-to-end test: discovery \u2192 scan \u2192 enrichment\" nodes : - id : \"discovery\" type : \"discovery.ferox\" config : wordlist : \"res://wordlists/dirb:latest\" threads : 50 outputs : [ \"urls\" ] - id : \"scan\" type : \"scan.nuclei\" inputs : [ \"urls\" ] config : templates : \"res://templates/owasp-top10:latest\" rate_limit : 150 outputs : [ \"findings\" ] - id : \"enrich\" type : \"enrich.cve\" inputs : [ \"findings\" ] config : sources : [ \"nvd\" , \"osv\" , \"exploitdb\" ] outputs : [ \"enriched_findings\" ] ``` text ## \ud83e\udde9 Workflow Engine Architecture ``` text +-------------------------------------------------------------+ | Worker Engine | | -------------------------------------------------------------| | - WorkflowScheduler | | - NodeExecutor | | - ResultCache | | - EventBus | | -------------------------------------------------------------| | Uses : Celery (Redis), asyncio, Pydantic validation | +-------------------------------------------------------------+ ``` text ### Key Components | Component | Description | | -----------|-------------| | **WorkflowScheduler** | Parses YAML recipes, builds DAG, submits jobs to queue. | | **NodeExecutor** | Executes nodes, manages subprocess wrappers. | | **ResultCache** | Stores intermediate results between nodes. | | **EventBus** | Publishes events (node_started, node_completed, workflow_failed). | | **WorkflowStore** | Persists workflow metadata in DB. | ## \u2699\ufe0f Python Model \u2014 DAG Representation ``` python # findings-engine/workflow_dag.py from typing import List, Dict, Any from pydantic import BaseModel class Node(BaseModel) : id : str type : str config : Dict[str, Any] = {} inputs : List[str] = [] outputs : List[str] = [] class Workflow(BaseModel) : id : str name : str description : str nodes : List[Node] ``` text ### DAG Validation Example ``` python def validate_dag(workflow: Workflow) : ids = [n.id for n in workflow.nodes] for node in workflow.nodes : for inp in node.inputs : if inp not in [out for n in workflow.nodes for out in n.outputs] : raise ValueError(f\"Unresolved input '{inp}' in node {node.id}\") ``` python ## \ud83e\udde0 Execution Flow 1. Parse & Validate YAML workflow using Pydantic schema. 2. Register DAG in database (`WorkflowStore`). 3. Submit nodes to Celery/RQ queue respecting topological order. 4. Execute wrappers through `ToolPort` interface. 5. Normalize findings via `FindingsEngine`. 6. Publish events to `EventBus`. 7. Update metrics and trigger downstream listeners (e.g., triage UI). ## \u2699\ufe0f Node Executor (Simplified) ``` python # worker/executor.py from core_lib.ports.tool_port import ToolPort class NodeExecutor : def __init__(self, node, context) : self.node = node self.context = context def run(self) : tool : ToolPort = self.context.resolve_tool(self.node.type) tool.prepare(self.node.config) results = tool.execute() self.context.store_results(self.node.outputs, results) return results ``` python ## \ud83d\udd04 Concurrency Model The orchestration engine is designed for asynchronous, multi-tool execution. | Execution Mode | Description | | ----------------|-------------| | **Sequential** | Enforced by DAG dependencies. | | **Parallel** | Independent nodes run concurrently (async or Celery workers). | | **Chained** | Output from one node auto-feeds into next via `ResultCache`. | ### Example ``` mermaid %% { init : { \"theme\" : \"neutral\" }} %% flowchart LR A[\"ferox\"] B[\"katana\"] C[\"nuclei\"] D[\"httpx\"] A --> B B --> C A --> D D --> C","title":"\u2699\ufe0f Workflow Specification Schema"},{"location":"architecture/05-orchestration-and-workflow-engine/#error-handling","text":"Error Type Handling Strategy Tool crash / non-zero exit Retry (max=3) then mark node failed. Timeout Kill process, log event, continue DAG. Missing input dataset Block downstream nodes, mark dependency unresolved. Parser error Log raw output, fallback to generic findings schema. Each failure is logged in the workflow_runs table and visible in the UI.","title":"\ud83e\udde9 Error Handling"},{"location":"architecture/05-orchestration-and-workflow-engine/#event-system","text":"The orchestration layer publishes real-time events to facilitate reactive behavior.","title":"\ud83e\udde0 Event System"},{"location":"architecture/05-orchestration-and-workflow-engine/#example-event-contract","text":"{ \"event\" : \"node_completed\" , \"workflow_id\" : \"abc123\" , \"node_id\" : \"scan\" , \"duration\" : 12.3 , \"findings\" : 124 } ```py t ho n Eve nts ca n be co nsu med by : - WebSocke t clie nts i n UI (live progress) - Audi t log processors - Me tr ics collec t ors ## \ud83e\udde9 Cachi n g & Reuse - **I nter media te Da ta : ** S t ored u n der `/cache/ { work fl ow_id } / { n ode_id } .jso n ` - **Resul t Hashi n g : ** SHA 256 o f co nf ig + i n pu ts f or cache hi ts - **Warm Ru ns : ** Work fl ows ca n resume fr om cached i nter media te ou t pu ts ```py t ho n cache_key = hashlib.sha 256 (jso n .dumps( n ode.co nf ig).e n code()).hexdiges t () ``` te x t ## \ud83e\uddf1 Example DAG Execu t io n Trace ``` te x t [ 2025-10-06 12 : 01 : 02 ] Work fl ow \"OWASP Top 10 Scan\" s tarte d [ 2025-10-06 12 : 01 : 05 ] Node discovery. fer ox comple te d (urls= 356 ) [ 2025-10-06 12 : 01 : 07 ] Node sca n . nu clei comple te d ( f i n di n gs= 112 ) [ 2025-10-06 12 : 01 : 10 ] Node e nr ich.cve comple te d (e nr iched_ f i n di n gs= 112 ) [ 2025-10-06 12 : 01 : 10 ] Work fl ow comple te d success full y","title":"Example Event Contract"},{"location":"architecture/05-orchestration-and-workflow-engine/#integration-with-other-components","text":"Component Interaction Findings Engine Receives raw output for normalization. Wrappers Execute the underlying binaries/tools. Storage Persists workflow runs, logs, results. Plugins Hooks into on_node_complete and on_workflow_complete . UI / API Subscribes to event bus for progress updates.","title":"\ud83d\udd0c Integration with Other Components"},{"location":"architecture/05-orchestration-and-workflow-engine/#monitoring-metrics","text":"Every node execution reports: - Duration (seconds) - Findings count - Exit status - CPU/memory usage - Cache hits These metrics feed Prometheus exporters and the analytics dashboards.","title":"\ud83e\udde9 Monitoring &amp; Metrics"},{"location":"architecture/05-orchestration-and-workflow-engine/#future-enhancements","text":"GraphQL-based workflow builder UI. Dynamic scheduling policies (priority, resource weighting). Conditional branching (if, switch nodes). AI-assisted workflow suggestions based on context and prior runs. Distributed orchestration using Celery groups/chords. Next: Plugin System","title":"\ud83e\udde0 Future Enhancements"},{"location":"architecture/06-plugin-system/","text":"06 \u2014 Plugin System \u00b6 \ud83e\udded Overview \u00b6 The Plugin System in SecFlow provides an extensible runtime for integrating new detection, enrichment, or analytics capabilities without modifying core code. Plugins operate under a controlled lifecycle and interact through strictly defined contracts (ports) . SecFlow supports three primary plugin categories: 1. Detectors \u2014 Introduce new scanning logic or post-processing steps. 2. Enrichers \u2014 Add vulnerability metadata (CVE, CWE, CVSS, etc.). 3. Analytics \u2014 Compute metrics, patterns, or visual insights over findings. \ud83e\udde9 Design Principles \u00b6 Principle Description Isolation Each plugin runs in its own context (thread/process sandbox). Declarative Registration Plugins register via manifests or decorators. Contract Enforcement Plugins implement strict base interfaces ( PluginBase , DetectorPlugin , etc.). Hot Reloading Plugin discovery occurs dynamically at startup. Safety Each plugin is validated, hashed, and versioned. \ud83e\uddf1 Plugin Architecture Overview \u00b6 +-------------------------------------------------------------+ | Plugin Manager | | - Registry | | - Loader | | - Validator | | - Sandbox | +-------------------------------------------------------------+ ```mermaid %%{init : { \"theme\" : \"neutral\" } }%% flowchart TD A[\"Plugin Registry\"] B[\"Detector Plugins\"] C[\"Enricher Plugins\"] D[\"Analytics Plugins\"] A --> B A --> C A --> D \u2699\ufe0f Base Interfaces \u00b6 # core-lib/ports/plugin_port.py from typing import Protocol , Any , Dict class PluginPort ( Protocol ): name : str version : str category : str # \"detector\" | \"enricher\" | \"analytics\" def initialize ( self , context : Dict [ str , Any ]) -> None : \"\"\"Initialize plugin with execution context.\"\"\" pass def execute ( self , data : Any ) -> Any : \"\"\"Execute plugin logic on input data.\"\"\" pass def teardown ( self ) -> None : \"\"\"Clean up plugin resources.\"\"\" pass ``` python All plugins must subclass or conform to ` PluginPort ` . ## \ud83e\udde9 Example Plugin Registry ``` python # plugins/registry.py from typing import Dict , Type from core_lib.ports.plugin_port import PluginPort class PluginRegistry : _registry : Dict [ str , Type [ PluginPort ]] = {} @classmethod def register ( cls , name : str , plugin : Type [ PluginPort ]): cls . _registry [ name ] = plugin print ( f \"[+] Registered plugin: { name } \" ) @classmethod def get ( cls , name : str ) -> PluginPort : return cls . _registry [ name ] @classmethod def list_plugins ( cls ): return list ( cls . _registry . keys ()) ``` python ### Registration via Decorator ``` python def register_plugin ( name : str ): def decorator ( cls ): PluginRegistry . register ( name , cls ) return cls return decorator ``` python ## \ud83e\udde0 Plugin Lifecycle 1. ** Registration : ** Discovered from manifests or decorators . 2. ** Validation : ** Signature & schema verification . 3. ** Initialization : ** Context injected ( paths , project ID , config ) . 4. ** Execution : ** Process data via ` execute () ` . 5. ** Teardown : ** Release resources , log metrics . ## \ud83e\udde9 Plugin Manifest Specification ``` yaml name : nuclei - enricher version : \"1.0.0\" category : \"enricher\" entrypoint : \"plugins.nuclei_enricher:Enricher\" dependencies : - requests - cpe config_schema : \"schemas/nuclei_enricher.json\" sandbox : true ``` python Each manifest is stored under ` / plugins / manifests / ` and validated on startup . ## \u2699\ufe0f Example \u2014 CVE Enricher Plugin ``` python # plugins/nuclei_enricher.py import requests from core_lib.models.finding import Finding from core_lib.ports.plugin_port import PluginPort @register_plugin ( \"cve_enricher\" ) class CVEEnricher ( PluginPort ): name = \"cve_enricher\" version = \"1.0.0\" category = \"enricher\" def initialize ( self , context ): self . sources = context . get ( \"sources\" , [ \"nvd\" ]) def execute ( self , finding : Finding ) -> Finding : for cve_id in finding . cve_ids : data = self . _fetch_cve_data ( cve_id ) finding . enrichment [ \"cve\" ][ cve_id ] = data return finding def _fetch_cve_data ( self , cve_id ): return requests . get ( f \"https://services.nvd.nist.gov/rest/json/cve/1.0/ { cve_id } \" ) . json () def teardown ( self ): pass ``` yaml ## \ud83d\udd10 Sandbox Model Each plugin executes inside a restricted environment : | Control | Enforcement | |---------|-------------| | ** Filesystem ** | Read - only mount or temp directory | | ** Network ** | Denied by default , opt - in per manifest | | ** Memory / CPU ** | Controlled via subprocess resource limits | | ** Timeouts ** | Enforced via execution wrapper | | ** Audit ** | Every plugin invocation logged with context | ## \ud83e\udde9 Plugin Discovery ### Directory Layout ``` python plugins / %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"plugins/\"] B[\"registry.py\"] C[\"manifests/\"] D[\"cve_enricher.yaml\"] E[\"nuclei_detector.yaml\"] F[\"detectors/\"] G[\"nuclei_detector.py\"] H[\"zap_detector.py\"] A --> B A --> C A --> F C --> D C --> E F --> G F --> H Discovery Algorithm \u00b6 def discover_plugins (): for manifest in Path ( \"plugins/manifests\" ) . glob ( \"*.yaml\" ): data = yaml . safe_load ( manifest . read_text ()) entrypoint = import_string ( data [ \"entrypoint\" ]) PluginRegistry . register ( data [ \"name\" ], entrypoint ) ``` python ## \ud83e\udde0 Plugin Telemetry Each plugin emits lifecycle events : ``` json { \"event\" : \"plugin_executed\" , \"plugin\" : \"cve_enricher\" , \"duration_ms\" : 342 , \"memory_mb\" : 42 , \"success\" : true } ``` json Telemetry is captured by the Observability subsystem ( see [ Observability , Logging & Metrics ]( 17 - observability - logging - and - metrics . md )) . ## \ud83e\udde9 Error Handling | Error | Strategy | |-------|----------| | Invalid Manifest | Skip plugin , log warning . | | Dependency ImportError | Attempt isolated install if allowed . | | Execution Timeout | Abort plugin , mark node partial - success . | | Sandbox Violation | Terminate process , revoke plugin signature . | ## \ud83e\udde0 Example End-to-End Plugin Flow ``` text [ Plugin Manifest ] \u2192 [ Registry Register ] \u2192 [ Initialize ] \u2193 \u2193 \u2193 [ Execute via Port ] \u2192 [ Telemetry + Logging ] \u2192 [ Teardown ] \ud83d\udd2e Future Enhancements \u00b6 Plugin signing (PGP signatures). Remote plugin repository (index + version resolution). In-UI plugin store with validation and ratings. Plugin telemetry aggregation dashboards. Next: Tools Integration Model","title":"Plugin System"},{"location":"architecture/06-plugin-system/#06-plugin-system","text":"","title":"06 \u2014 Plugin System"},{"location":"architecture/06-plugin-system/#overview","text":"The Plugin System in SecFlow provides an extensible runtime for integrating new detection, enrichment, or analytics capabilities without modifying core code. Plugins operate under a controlled lifecycle and interact through strictly defined contracts (ports) . SecFlow supports three primary plugin categories: 1. Detectors \u2014 Introduce new scanning logic or post-processing steps. 2. Enrichers \u2014 Add vulnerability metadata (CVE, CWE, CVSS, etc.). 3. Analytics \u2014 Compute metrics, patterns, or visual insights over findings.","title":"\ud83e\udded Overview"},{"location":"architecture/06-plugin-system/#design-principles","text":"Principle Description Isolation Each plugin runs in its own context (thread/process sandbox). Declarative Registration Plugins register via manifests or decorators. Contract Enforcement Plugins implement strict base interfaces ( PluginBase , DetectorPlugin , etc.). Hot Reloading Plugin discovery occurs dynamically at startup. Safety Each plugin is validated, hashed, and versioned.","title":"\ud83e\udde9 Design Principles"},{"location":"architecture/06-plugin-system/#plugin-architecture-overview","text":"+-------------------------------------------------------------+ | Plugin Manager | | - Registry | | - Loader | | - Validator | | - Sandbox | +-------------------------------------------------------------+ ```mermaid %%{init : { \"theme\" : \"neutral\" } }%% flowchart TD A[\"Plugin Registry\"] B[\"Detector Plugins\"] C[\"Enricher Plugins\"] D[\"Analytics Plugins\"] A --> B A --> C A --> D","title":"\ud83e\uddf1 Plugin Architecture Overview"},{"location":"architecture/06-plugin-system/#base-interfaces","text":"# core-lib/ports/plugin_port.py from typing import Protocol , Any , Dict class PluginPort ( Protocol ): name : str version : str category : str # \"detector\" | \"enricher\" | \"analytics\" def initialize ( self , context : Dict [ str , Any ]) -> None : \"\"\"Initialize plugin with execution context.\"\"\" pass def execute ( self , data : Any ) -> Any : \"\"\"Execute plugin logic on input data.\"\"\" pass def teardown ( self ) -> None : \"\"\"Clean up plugin resources.\"\"\" pass ``` python All plugins must subclass or conform to ` PluginPort ` . ## \ud83e\udde9 Example Plugin Registry ``` python # plugins/registry.py from typing import Dict , Type from core_lib.ports.plugin_port import PluginPort class PluginRegistry : _registry : Dict [ str , Type [ PluginPort ]] = {} @classmethod def register ( cls , name : str , plugin : Type [ PluginPort ]): cls . _registry [ name ] = plugin print ( f \"[+] Registered plugin: { name } \" ) @classmethod def get ( cls , name : str ) -> PluginPort : return cls . _registry [ name ] @classmethod def list_plugins ( cls ): return list ( cls . _registry . keys ()) ``` python ### Registration via Decorator ``` python def register_plugin ( name : str ): def decorator ( cls ): PluginRegistry . register ( name , cls ) return cls return decorator ``` python ## \ud83e\udde0 Plugin Lifecycle 1. ** Registration : ** Discovered from manifests or decorators . 2. ** Validation : ** Signature & schema verification . 3. ** Initialization : ** Context injected ( paths , project ID , config ) . 4. ** Execution : ** Process data via ` execute () ` . 5. ** Teardown : ** Release resources , log metrics . ## \ud83e\udde9 Plugin Manifest Specification ``` yaml name : nuclei - enricher version : \"1.0.0\" category : \"enricher\" entrypoint : \"plugins.nuclei_enricher:Enricher\" dependencies : - requests - cpe config_schema : \"schemas/nuclei_enricher.json\" sandbox : true ``` python Each manifest is stored under ` / plugins / manifests / ` and validated on startup . ## \u2699\ufe0f Example \u2014 CVE Enricher Plugin ``` python # plugins/nuclei_enricher.py import requests from core_lib.models.finding import Finding from core_lib.ports.plugin_port import PluginPort @register_plugin ( \"cve_enricher\" ) class CVEEnricher ( PluginPort ): name = \"cve_enricher\" version = \"1.0.0\" category = \"enricher\" def initialize ( self , context ): self . sources = context . get ( \"sources\" , [ \"nvd\" ]) def execute ( self , finding : Finding ) -> Finding : for cve_id in finding . cve_ids : data = self . _fetch_cve_data ( cve_id ) finding . enrichment [ \"cve\" ][ cve_id ] = data return finding def _fetch_cve_data ( self , cve_id ): return requests . get ( f \"https://services.nvd.nist.gov/rest/json/cve/1.0/ { cve_id } \" ) . json () def teardown ( self ): pass ``` yaml ## \ud83d\udd10 Sandbox Model Each plugin executes inside a restricted environment : | Control | Enforcement | |---------|-------------| | ** Filesystem ** | Read - only mount or temp directory | | ** Network ** | Denied by default , opt - in per manifest | | ** Memory / CPU ** | Controlled via subprocess resource limits | | ** Timeouts ** | Enforced via execution wrapper | | ** Audit ** | Every plugin invocation logged with context | ## \ud83e\udde9 Plugin Discovery ### Directory Layout ``` python plugins / %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"plugins/\"] B[\"registry.py\"] C[\"manifests/\"] D[\"cve_enricher.yaml\"] E[\"nuclei_detector.yaml\"] F[\"detectors/\"] G[\"nuclei_detector.py\"] H[\"zap_detector.py\"] A --> B A --> C A --> F C --> D C --> E F --> G F --> H","title":"\u2699\ufe0f Base Interfaces"},{"location":"architecture/06-plugin-system/#discovery-algorithm","text":"def discover_plugins (): for manifest in Path ( \"plugins/manifests\" ) . glob ( \"*.yaml\" ): data = yaml . safe_load ( manifest . read_text ()) entrypoint = import_string ( data [ \"entrypoint\" ]) PluginRegistry . register ( data [ \"name\" ], entrypoint ) ``` python ## \ud83e\udde0 Plugin Telemetry Each plugin emits lifecycle events : ``` json { \"event\" : \"plugin_executed\" , \"plugin\" : \"cve_enricher\" , \"duration_ms\" : 342 , \"memory_mb\" : 42 , \"success\" : true } ``` json Telemetry is captured by the Observability subsystem ( see [ Observability , Logging & Metrics ]( 17 - observability - logging - and - metrics . md )) . ## \ud83e\udde9 Error Handling | Error | Strategy | |-------|----------| | Invalid Manifest | Skip plugin , log warning . | | Dependency ImportError | Attempt isolated install if allowed . | | Execution Timeout | Abort plugin , mark node partial - success . | | Sandbox Violation | Terminate process , revoke plugin signature . | ## \ud83e\udde0 Example End-to-End Plugin Flow ``` text [ Plugin Manifest ] \u2192 [ Registry Register ] \u2192 [ Initialize ] \u2193 \u2193 \u2193 [ Execute via Port ] \u2192 [ Telemetry + Logging ] \u2192 [ Teardown ]","title":"Discovery Algorithm"},{"location":"architecture/06-plugin-system/#future-enhancements","text":"Plugin signing (PGP signatures). Remote plugin repository (index + version resolution). In-UI plugin store with validation and ratings. Plugin telemetry aggregation dashboards. Next: Tools Integration Model","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/07-tools-integration-model/","text":"07 \u2014 Tools Integration Model \u00b6 \ud83e\udded Overview \u00b6 The Tools Integration Model defines how external security tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.) are integrated and orchestrated inside SecFlow . The system is manifest-driven , allowing tools to be defined declaratively, executed in isolation, and chained dynamically via the Workflow Engine. Each integrated tool is represented as: - A manifest (describes binary, configuration, security policies, I/O schema) - A wrapper (implements the standardized execution interface) - An optional adapter plugin (translates output to normalized Findings) \ud83e\uddf1 Architectural Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry<br/>- Manifest Validator<br/>- Sandbox Executor<br/>- Output Parser\"] B[\"Tool Wrapper<br/>(e.g., Nuclei)\"] C[\"Tool Output Parser\"] D[\"Findings Engine\"] A --> B B --> C C --> D \u2699\ufe0f Tool Manifest Structure \u00b6 Every tool integrated into SecFlow must define a manifest under /wrappers/manifests/<tool>.json . Example: nuclei.json \u00b6 { \"name\" : \"nuclei\" , \"version\" : \"3.2.1\" , \"binary\" : \"nuclei\" , \"capabilities\" : [ \"scan\" ], \"outputs\" : { \"dataset\" : \"findings\" }, \"defaults\" : { \"threads\" : 25 , \"rate_limit\" : 150 , \"templates\" : \"res://templates/nuclei:latest\" }, \"configSchema\" : \"schemas/nuclei-config.json\" , \"resource_requirements\" : { \"cpu_cores\" : 2 , \"memory_mb\" : 512 , \"timeout_seconds\" : 300 }, \"security\" : { \"sandbox\" : true , \"network_access\" : true , \"file_system_access\" : \"read_only\" }, \"selftest\" : { \"args\" : [ \"-version\" ], \"expect\" : \"Nuclei\" } } ```py t ho n ## \ud83e\udde9 Wrapper I nterfa ce All t ool wrappers impleme nt t he same co ntra c t t o e nsure co ns is tent orches trat io n . ```py t ho n # core - lib/por ts / t ool_por t .py fr om t ypi n g impor t A n y , Dic t , Pro t ocol , Lis t fr om core_lib.models. f i n di n g impor t Fi n di n g class ToolPor t (Pro t ocol) : de f prepare(sel f , co nf ig : Dic t [ s tr , A n y ] ) - > No ne : \"\"\"Prepare tool with configuration.\"\"\" pass de f ru n (sel f ) - > s tr : \"\"\"Execute tool and return raw output.\"\"\" pass de f parse_ou t pu t (sel f , raw_ou t pu t : s tr ) - > Lis t [ Fi n di n g ]: \"\"\"Parse raw output into structured findings.\"\"\" pass ```py t ho n ## \ud83e\udde0 Example Impleme ntat io n : Nuclei Wrapper ```py t ho n # wrappers/ nu clei_wrapper.py impor t subprocess , jso n fr om core_lib.models. f i n di n g impor t Fi n di n g fr om core_lib.por ts . t ool_por t impor t ToolPor t class NucleiWrapper(ToolPor t ) : de f __i n i t __(sel f , co nf ig) : sel f .co nf ig = co nf ig de f prepare(sel f , co nf ig) : sel f .args = [ \"nuclei\" , \"-json\" , \"-t\" , co nf ig.ge t ( \"templates\" , \"res://templates/nuclei:latest\" ) , \"-rl\" , s tr (co nf ig.ge t ( \"rate_limit\" , 150 )) , \"-c\" , s tr (co nf ig.ge t ( \"threads\" , 25 )) ] de f ru n (sel f ) : resul t = subprocess.ru n (sel f .args , cap ture _ou t pu t =True , te x t =True , t imeou t = 300 ) re turn resul t .s t dou t de f parse_ou t pu t (sel f , raw_ou t pu t ) : f i n di n gs = [] f or li ne i n raw_ou t pu t .spli tl i nes () : tr y : da ta = jso n .loads(li ne ) f i n di n gs.appe n d( Fi n di n g( t i tle =da ta [ \"info\" ][ \"name\" ], severi t y=da ta [ \"info\" ][ \"severity\" ], pa t h=da ta [ \"matched-at\" ], de te c t or_id= \"nuclei\" , evide n ce=da ta ) ) excep t Excep t io n : co nt i nue re turn f i n di n gs ```py t ho n ## \ud83d\udd10 Sa n dbox Execu t io n Tools ru n t hrough a Sa n dbox Execu t or , e nf orci n g CPU , memory , a n d net work co nstra i nts . ```py t ho n # wrappers/execu t or.py impor t resource , subprocess , sig nal class Sa n dboxExecu t or : de f __i n i t __(sel f , cpu_limi t = 2 , mem_limi t _mb= 512 ) : sel f .cpu_limi t = cpu_limi t sel f .mem_limi t _mb = mem_limi t _mb de f execu te (sel f , args) : de f se t _limi ts () : resource.se trl imi t (resource.RLIMIT_CPU , (sel f .cpu_limi t , sel f .cpu_limi t )) resource.se trl imi t (resource.RLIMIT_AS , (sel f .mem_limi t _mb * 1024 ** 2 , ) * 2 ) proc = subprocess.Pope n (args , preexec_ fn =se t _limi ts , s t dou t =subprocess.PIPE , s t derr=subprocess.PIPE) s t dou t , s t derr = proc.commu n ica te ( t imeou t = 300 ) re turn s t dou t .decode() , s t derr.decode() , proc.re turn code ```py t ho n ## \ud83e\udde9 Tool Regis tr y ```py t ho n # wrappers/regis tr y.py fr om t ypi n g impor t Dic t , Type fr om core_lib.por ts . t ool_por t impor t ToolPor t class ToolRegis tr y : _regis tr y : Dic t [ s tr , Type [ ToolPor t ]] = {} @classme t hod de f regis ter (cls , na me : s tr , impl : Type [ ToolPor t ] ) : cls._regis tr y [ na me ] = impl pri nt ( f \"[+] Registered tool: {name}\" ) @classme t hod de f ge t (cls , na me : s tr ) - > ToolPor t : re turn cls._regis tr y [ na me ] ```py t ho n Tools regis ter via decora t ors or discovery : ```py t ho n fr om wrappers.regis tr y impor t ToolRegis tr y @ToolRegis tr y.regis ter ( \"feroxbuster\" ) class FeroxWrapper(ToolPor t ) : de f prepare(sel f , co nf ig : Dic t [ s tr , A n y ] ) - > No ne : sel f .wordlis t = co nf ig.ge t ( \"wordlist\" , \"res://wordlists/dirb:latest\" ) sel f . t hreads = co nf ig.ge t ( \"threads\" , 50 ) de f ru n (sel f ) - > s tr : cmd = [ \"feroxbuster\" , \"-u\" , sel f . tar ge t , \"-w\" , sel f .wordlis t , \"-t\" , s tr (sel f . t hreads) ] resul t = subprocess.ru n (cmd , cap ture _ou t pu t =True , te x t =True) re turn resul t .s t dou t de f parse_ou t pu t (sel f , raw_ou t pu t : s tr ) - > Lis t [ Fi n di n g ]: f i n di n gs = [] f or li ne i n raw_ou t pu t .spli t ('\\ n ') : i f li ne .s tr ip() a n d n o t li ne .s tarts wi t h('#') : f i n di n gs.appe n d(Fi n di n g( id= f \"ferox_{hash(line)}\" , url=li ne .s tr ip() , t ool= \"feroxbuster\" , severi t y= \"info\" )) re turn f i n di n gs ```py t ho n ## \ud83e\udde0 Example I nte gra t io n \u2014 Feroxbus ter ```py t ho n # wrappers/ fer ox_wrapper.py impor t subprocess fr om core_lib.models. f i n di n g impor t Fi n di n g class FeroxWrapper(ToolPor t ) : de f prepare(sel f , co nf ig) : sel f . tar ge t = co nf ig [ \"target\" ] sel f .wordlis t = co nf ig.ge t ( \"wordlist\" , \"/usr/share/wordlists/dirb/common.txt\" ) sel f .args = [ \"feroxbuster\" , \"-u\" , sel f . tar ge t , \"-w\" , sel f .wordlis t , \"-o\" , \"-\" ] de f ru n (sel f ) : resul t = subprocess.ru n (sel f .args , cap ture _ou t pu t =True , te x t =True) re turn resul t .s t dou t de f parse_ou t pu t (sel f , raw) : f i n di n gs = [] f or li ne i n raw.spli tl i nes () : i f \"200\" i n li ne or \"301\" i n li ne : f i n di n gs.appe n d( Fi n di n g( t i tle = \"Discovered Path\" , severi t y= \"info\" , pa t h=li ne .spli t () [ 0 ], de te c t or_id= \"feroxbuster\" ) ) re turn f i n di n gs ``` te x t ## \ud83e\udde9 Tool Orches trat io n The Work fl ow E n gi ne dy na mically chai ns t ool execu t io ns : - Each n ode de f i nes a t ool (by na me) , co nf igura t io n , a n d expec te d ou t pu ts . - Ou t pu ts become i n pu ts f or subseque nt n odes. ### Example : ```yaml n odes : - id : discovery t ype : discovery. fer ox co nf ig : wordlis t : res : //wordlists/dirb:latest ou t pu ts : [ \"urls\" ] - id : sca n t ype : sca n . nu clei i n pu ts : [ \"urls\" ] ou t pu ts : [ \"findings\" ] \ud83e\udde0 Tool Output Normalization \u00b6 All tool outputs are normalized into the Finding schema before storage. Field Description detector_id Tool name (e.g., nuclei, zap) title Human-readable description path Target or endpoint severity Normalized (critical/high/medium/low/info) evidence Raw tool output created_at UTC timestamp provenance Manifest + execution context \ud83e\udde9 Error Handling \u00b6 Condition Behavior Tool binary missing Raise ToolNotFoundError Sandbox violation Terminate subprocess Timeout Abort execution, mark partial-success Parse failure Skip line, log warning Non-zero exit code Log and attach stderr to run context \ud83d\udd2e Future Extensions \u00b6 Interactive tool chaining (UI-based pipeline builder) Tool profiles for project-specific setups Remote execution agents (distributed scanning) Version-aware compatibility checks Auto-updating manifests (self-test validation) Next: Tool Manager & User Experience Design","title":"Tools Integration"},{"location":"architecture/07-tools-integration-model/#07-tools-integration-model","text":"","title":"07 \u2014 Tools Integration Model"},{"location":"architecture/07-tools-integration-model/#overview","text":"The Tools Integration Model defines how external security tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.) are integrated and orchestrated inside SecFlow . The system is manifest-driven , allowing tools to be defined declaratively, executed in isolation, and chained dynamically via the Workflow Engine. Each integrated tool is represented as: - A manifest (describes binary, configuration, security policies, I/O schema) - A wrapper (implements the standardized execution interface) - An optional adapter plugin (translates output to normalized Findings)","title":"\ud83e\udded Overview"},{"location":"architecture/07-tools-integration-model/#architectural-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry<br/>- Manifest Validator<br/>- Sandbox Executor<br/>- Output Parser\"] B[\"Tool Wrapper<br/>(e.g., Nuclei)\"] C[\"Tool Output Parser\"] D[\"Findings Engine\"] A --> B B --> C C --> D","title":"\ud83e\uddf1 Architectural Overview"},{"location":"architecture/07-tools-integration-model/#tool-manifest-structure","text":"Every tool integrated into SecFlow must define a manifest under /wrappers/manifests/<tool>.json .","title":"\u2699\ufe0f Tool Manifest Structure"},{"location":"architecture/07-tools-integration-model/#example-nucleijson","text":"{ \"name\" : \"nuclei\" , \"version\" : \"3.2.1\" , \"binary\" : \"nuclei\" , \"capabilities\" : [ \"scan\" ], \"outputs\" : { \"dataset\" : \"findings\" }, \"defaults\" : { \"threads\" : 25 , \"rate_limit\" : 150 , \"templates\" : \"res://templates/nuclei:latest\" }, \"configSchema\" : \"schemas/nuclei-config.json\" , \"resource_requirements\" : { \"cpu_cores\" : 2 , \"memory_mb\" : 512 , \"timeout_seconds\" : 300 }, \"security\" : { \"sandbox\" : true , \"network_access\" : true , \"file_system_access\" : \"read_only\" }, \"selftest\" : { \"args\" : [ \"-version\" ], \"expect\" : \"Nuclei\" } } ```py t ho n ## \ud83e\udde9 Wrapper I nterfa ce All t ool wrappers impleme nt t he same co ntra c t t o e nsure co ns is tent orches trat io n . ```py t ho n # core - lib/por ts / t ool_por t .py fr om t ypi n g impor t A n y , Dic t , Pro t ocol , Lis t fr om core_lib.models. f i n di n g impor t Fi n di n g class ToolPor t (Pro t ocol) : de f prepare(sel f , co nf ig : Dic t [ s tr , A n y ] ) - > No ne : \"\"\"Prepare tool with configuration.\"\"\" pass de f ru n (sel f ) - > s tr : \"\"\"Execute tool and return raw output.\"\"\" pass de f parse_ou t pu t (sel f , raw_ou t pu t : s tr ) - > Lis t [ Fi n di n g ]: \"\"\"Parse raw output into structured findings.\"\"\" pass ```py t ho n ## \ud83e\udde0 Example Impleme ntat io n : Nuclei Wrapper ```py t ho n # wrappers/ nu clei_wrapper.py impor t subprocess , jso n fr om core_lib.models. f i n di n g impor t Fi n di n g fr om core_lib.por ts . t ool_por t impor t ToolPor t class NucleiWrapper(ToolPor t ) : de f __i n i t __(sel f , co nf ig) : sel f .co nf ig = co nf ig de f prepare(sel f , co nf ig) : sel f .args = [ \"nuclei\" , \"-json\" , \"-t\" , co nf ig.ge t ( \"templates\" , \"res://templates/nuclei:latest\" ) , \"-rl\" , s tr (co nf ig.ge t ( \"rate_limit\" , 150 )) , \"-c\" , s tr (co nf ig.ge t ( \"threads\" , 25 )) ] de f ru n (sel f ) : resul t = subprocess.ru n (sel f .args , cap ture _ou t pu t =True , te x t =True , t imeou t = 300 ) re turn resul t .s t dou t de f parse_ou t pu t (sel f , raw_ou t pu t ) : f i n di n gs = [] f or li ne i n raw_ou t pu t .spli tl i nes () : tr y : da ta = jso n .loads(li ne ) f i n di n gs.appe n d( Fi n di n g( t i tle =da ta [ \"info\" ][ \"name\" ], severi t y=da ta [ \"info\" ][ \"severity\" ], pa t h=da ta [ \"matched-at\" ], de te c t or_id= \"nuclei\" , evide n ce=da ta ) ) excep t Excep t io n : co nt i nue re turn f i n di n gs ```py t ho n ## \ud83d\udd10 Sa n dbox Execu t io n Tools ru n t hrough a Sa n dbox Execu t or , e nf orci n g CPU , memory , a n d net work co nstra i nts . ```py t ho n # wrappers/execu t or.py impor t resource , subprocess , sig nal class Sa n dboxExecu t or : de f __i n i t __(sel f , cpu_limi t = 2 , mem_limi t _mb= 512 ) : sel f .cpu_limi t = cpu_limi t sel f .mem_limi t _mb = mem_limi t _mb de f execu te (sel f , args) : de f se t _limi ts () : resource.se trl imi t (resource.RLIMIT_CPU , (sel f .cpu_limi t , sel f .cpu_limi t )) resource.se trl imi t (resource.RLIMIT_AS , (sel f .mem_limi t _mb * 1024 ** 2 , ) * 2 ) proc = subprocess.Pope n (args , preexec_ fn =se t _limi ts , s t dou t =subprocess.PIPE , s t derr=subprocess.PIPE) s t dou t , s t derr = proc.commu n ica te ( t imeou t = 300 ) re turn s t dou t .decode() , s t derr.decode() , proc.re turn code ```py t ho n ## \ud83e\udde9 Tool Regis tr y ```py t ho n # wrappers/regis tr y.py fr om t ypi n g impor t Dic t , Type fr om core_lib.por ts . t ool_por t impor t ToolPor t class ToolRegis tr y : _regis tr y : Dic t [ s tr , Type [ ToolPor t ]] = {} @classme t hod de f regis ter (cls , na me : s tr , impl : Type [ ToolPor t ] ) : cls._regis tr y [ na me ] = impl pri nt ( f \"[+] Registered tool: {name}\" ) @classme t hod de f ge t (cls , na me : s tr ) - > ToolPor t : re turn cls._regis tr y [ na me ] ```py t ho n Tools regis ter via decora t ors or discovery : ```py t ho n fr om wrappers.regis tr y impor t ToolRegis tr y @ToolRegis tr y.regis ter ( \"feroxbuster\" ) class FeroxWrapper(ToolPor t ) : de f prepare(sel f , co nf ig : Dic t [ s tr , A n y ] ) - > No ne : sel f .wordlis t = co nf ig.ge t ( \"wordlist\" , \"res://wordlists/dirb:latest\" ) sel f . t hreads = co nf ig.ge t ( \"threads\" , 50 ) de f ru n (sel f ) - > s tr : cmd = [ \"feroxbuster\" , \"-u\" , sel f . tar ge t , \"-w\" , sel f .wordlis t , \"-t\" , s tr (sel f . t hreads) ] resul t = subprocess.ru n (cmd , cap ture _ou t pu t =True , te x t =True) re turn resul t .s t dou t de f parse_ou t pu t (sel f , raw_ou t pu t : s tr ) - > Lis t [ Fi n di n g ]: f i n di n gs = [] f or li ne i n raw_ou t pu t .spli t ('\\ n ') : i f li ne .s tr ip() a n d n o t li ne .s tarts wi t h('#') : f i n di n gs.appe n d(Fi n di n g( id= f \"ferox_{hash(line)}\" , url=li ne .s tr ip() , t ool= \"feroxbuster\" , severi t y= \"info\" )) re turn f i n di n gs ```py t ho n ## \ud83e\udde0 Example I nte gra t io n \u2014 Feroxbus ter ```py t ho n # wrappers/ fer ox_wrapper.py impor t subprocess fr om core_lib.models. f i n di n g impor t Fi n di n g class FeroxWrapper(ToolPor t ) : de f prepare(sel f , co nf ig) : sel f . tar ge t = co nf ig [ \"target\" ] sel f .wordlis t = co nf ig.ge t ( \"wordlist\" , \"/usr/share/wordlists/dirb/common.txt\" ) sel f .args = [ \"feroxbuster\" , \"-u\" , sel f . tar ge t , \"-w\" , sel f .wordlis t , \"-o\" , \"-\" ] de f ru n (sel f ) : resul t = subprocess.ru n (sel f .args , cap ture _ou t pu t =True , te x t =True) re turn resul t .s t dou t de f parse_ou t pu t (sel f , raw) : f i n di n gs = [] f or li ne i n raw.spli tl i nes () : i f \"200\" i n li ne or \"301\" i n li ne : f i n di n gs.appe n d( Fi n di n g( t i tle = \"Discovered Path\" , severi t y= \"info\" , pa t h=li ne .spli t () [ 0 ], de te c t or_id= \"feroxbuster\" ) ) re turn f i n di n gs ``` te x t ## \ud83e\udde9 Tool Orches trat io n The Work fl ow E n gi ne dy na mically chai ns t ool execu t io ns : - Each n ode de f i nes a t ool (by na me) , co nf igura t io n , a n d expec te d ou t pu ts . - Ou t pu ts become i n pu ts f or subseque nt n odes. ### Example : ```yaml n odes : - id : discovery t ype : discovery. fer ox co nf ig : wordlis t : res : //wordlists/dirb:latest ou t pu ts : [ \"urls\" ] - id : sca n t ype : sca n . nu clei i n pu ts : [ \"urls\" ] ou t pu ts : [ \"findings\" ]","title":"Example: nuclei.json"},{"location":"architecture/07-tools-integration-model/#tool-output-normalization","text":"All tool outputs are normalized into the Finding schema before storage. Field Description detector_id Tool name (e.g., nuclei, zap) title Human-readable description path Target or endpoint severity Normalized (critical/high/medium/low/info) evidence Raw tool output created_at UTC timestamp provenance Manifest + execution context","title":"\ud83e\udde0 Tool Output Normalization"},{"location":"architecture/07-tools-integration-model/#error-handling","text":"Condition Behavior Tool binary missing Raise ToolNotFoundError Sandbox violation Terminate subprocess Timeout Abort execution, mark partial-success Parse failure Skip line, log warning Non-zero exit code Log and attach stderr to run context","title":"\ud83e\udde9 Error Handling"},{"location":"architecture/07-tools-integration-model/#future-extensions","text":"Interactive tool chaining (UI-based pipeline builder) Tool profiles for project-specific setups Remote execution agents (distributed scanning) Version-aware compatibility checks Auto-updating manifests (self-test validation) Next: Tool Manager & User Experience Design","title":"\ud83d\udd2e Future Extensions"},{"location":"architecture/08-tool-manager-and-ux-design/","text":"08 \u2014 Tool Manager & User Experience Design \u00b6 \ud83e\udded Overview \u00b6 The Tool Manager serves as the central orchestration and configuration interface for all integrated tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.). It acts as the human-facing control plane that bridges declarative configuration, project-level defaults, and the automated orchestration engine. The UX design ensures that both CLI users and UI users can: - Install or enable tools dynamically. - Configure tool parameters or use defaults. - Chain tools together (e.g., Ferox \u2192 Katana \u2192 Nuclei ). - Reuse shared wordlists, payloads, and templates. - View and modify execution pipelines visually or textually. \ud83e\udde9 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry (runtime manifest cache)<br/>- Config Editor (CLI + UI layer)<br/>- Execution Controller (worker interface)<br/>- Output Collector (findings normalization)\"] B[\"Tool Wrappers Layer\"] C[\"Findings Engine\"] A --> B B --> C \u2699\ufe0f Tool Manager Responsibilities \u00b6 Subsystem Responsibility Registry Keeps in-memory registry of available tools with metadata. Configurator Provides unified interface for tool configuration & validation. Executor Handles actual subprocess calls via SandboxExecutor . Cache Layer Stores tool state, results, and logs for replay. Renderer Generates UI/CLI views for configuration and results. \ud83e\uddf1 UX Goals \u00b6 Goal Description Low Friction User can run default scans without prior configuration. Transparency Full visibility into command-line arguments, logs, and results. Reusability Configurations and results can be reused across projects. Isolation Each project may have independent or shared tool setups. Extensibility Easy to add new tools through manifests. \ud83e\udde0 CLI Experience \u00b6 Example Commands \u00b6 # List available tools SecFlow tools list # Enable Feroxbuster & Nuclei SecFlow tools enable feroxbuster nuclei # Configure a tool SecFlow tools config nuclei --threads 50 --templates owasp-top10 # Run workflow interactively SecFlow run workflow.yaml # Chain tools manually SecFlow run feroxbuster --target https://example.com | SecFlow run nuclei ``` text ### CLI UX Principles - YAML/JSON inputs mirror internal manifests. - All commands are idempotent \u2014 re-runs use cached configurations. - CLI supports both single-tool and multi-node workflow modes. ## \ud83e\udde0 Web UI Experience ### Visual Overview ``` text Project: Acme Web API %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project: Acme Web API\"] B[\"Tools Panel\"] C[\"Workflow Builder\"] D[\"Execution Log\"] E[\"Results\"] A --> B A --> C A --> D A --> E Key Panels \u00b6 Panel Description Tools Panel Shows installed tools, version, manifest, health check. Workflow Builder Drag-and-drop DAG composer (Ferox \u2192 Katana \u2192 Nuclei). Execution Log Real-time stdout/stderr stream with structured logging. Results Viewer Interactive findings dashboard, linked to triage view. Example Workflow Builder Nodes \u00b6 ( Discovery ) ( Crawler ) ( Scanner ) %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Katana\"] C[\"Nuclei\"] A --> B B --> C Users can: - Configure node properties via sidebar. - Save and export workflows as YAML ( workflow-recipe.yaml ). - Re-run saved workflows directly or modify them visually. \ud83e\udde9 Configuration Persistence \u00b6 Each tool has a persistent JSON/YAML config stored under: ~/.SecFlow/config/tools/<tool>.yaml ```text ### Example ```yaml version: \"1.0\" defaults: threads: 25 rate_limit: 150 templates: res://templates/nuclei:latest profiles: aggressive: rate_limit: 300 safe: rate_limit: 50 sandbox: true ```text Profiles allow contextual configurations (e.g., \"aggressive\", \"safe\"). ## \ud83d\udd17 Tool Chaining (Runtime) Tool chaining uses the Workflow Engine, but users can also trigger ad-hoc sequences. Each output dataset is normalized as JSON Lines, allowing flexible piping. ```bash SecFlow run feroxbuster --target https://app.local \\ | SecFlow run katana --stdin \\ | SecFlow run nuclei --stdin ```text ### Data interoperability: Each tool must produce output in a structured JSON format with required fields: ```json { \"url\": \"https://example.com/login\", \"status\": 200, \"source\": \"feroxbuster\" } ```python ## \ud83e\udde9 Default Configurations & Templates When the user installs SecFlow: - Default manifests are loaded from `/resources/defaults/` - All core tools (Nuclei, Ferox, Katana) have pre-configured templates. - The user can immediately run scans without extra setup. ### Example command: ```bash SecFlow quickscan https://example.com ```python Equivalent to: - **Workflow:** Ferox \u2192 Nuclei (OWASP templates) ## \ud83e\udde0 Advanced Features | Feature | Description | |---------|-------------| | **Auto-Discovery** | Detects installed binaries and populates manifest registry automatically. | | **Tool Self-Test** | Validates binary presence and functionality via manifest-defined tests. | | **Runtime Profiles** | Switch between safe/aggressive scanning modes. | | **Execution History** | Stores past runs and parameters for reproducibility. | ## \ud83d\udd10 User Permissions Tool Manager respects project and role isolation: | Role | Capabilities | |------|-------------| | **Admin** | Install, configure, delete tools. | | **Analyst** | Execute workflows, view logs, triage findings. | | **Viewer** | Read-only access to results. | All actions are logged to the Audit Log. ## \ud83e\uddf1 Integration with Resource Registry The Tool Manager automatically pulls shared wordlists, templates, or payloads from the Resource Registry (see [09-resource-registry.md](09-resource-registry.md)). ### Example: ```yaml wordlist: res://wordlists/dirb:latest templates: res://templates/nuclei:latest ```ini ## \ud83e\udde9 Error Recovery UX If a tool fails: - The UI highlights failed nodes in red. - Logs show stderr output inline. - Users can retry individual nodes. - Failures generate a triage entry (finding type = \"tool error\"). ## \ud83e\udde0 Example CLI Session ```bash # Add Ferox with a project-local override wordlist secflow tools add ferox \\ --version 2.10.0 \\ --scope project \\ --config '{ \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64 }' # Run a two-node workflow directly from CLI (discovery \u2192 nuclei) secflow run \\ --project acme-web \\ --recipe workflows/discovery-to-nuclei.yaml \\ --var target=https://app.acme.com ```bash ### API Response Example ```json { \"tool\": \"feroxbuster\", \"version\": \"2.10.0\", \"scope\": \"project\", \"effective_config\": { \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64, \"rate_limit\": 0 }, \"status\": \"installed\", \"selftest\": { \"ok\": true, \"elapsed_ms\": 143 } } ```json ## \ud83d\udd2e Future Enhancements - UI-based \"Workflow Marketplace\" for community templates. - AI-assisted tool parameter tuning based on context. - Live terminal dashboard with interactive progress visualization. - Integration with Burp/OWASP ZAP APIs for direct import. --- **Next:** [Resource Registry](09-resource-registry.md) ```","title":"Tool Manager & UX"},{"location":"architecture/08-tool-manager-and-ux-design/#08-tool-manager-user-experience-design","text":"","title":"08 \u2014 Tool Manager &amp; User Experience Design"},{"location":"architecture/08-tool-manager-and-ux-design/#overview","text":"The Tool Manager serves as the central orchestration and configuration interface for all integrated tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.). It acts as the human-facing control plane that bridges declarative configuration, project-level defaults, and the automated orchestration engine. The UX design ensures that both CLI users and UI users can: - Install or enable tools dynamically. - Configure tool parameters or use defaults. - Chain tools together (e.g., Ferox \u2192 Katana \u2192 Nuclei ). - Reuse shared wordlists, payloads, and templates. - View and modify execution pipelines visually or textually.","title":"\ud83e\udded Overview"},{"location":"architecture/08-tool-manager-and-ux-design/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry (runtime manifest cache)<br/>- Config Editor (CLI + UI layer)<br/>- Execution Controller (worker interface)<br/>- Output Collector (findings normalization)\"] B[\"Tool Wrappers Layer\"] C[\"Findings Engine\"] A --> B B --> C","title":"\ud83e\udde9 Architecture Overview"},{"location":"architecture/08-tool-manager-and-ux-design/#tool-manager-responsibilities","text":"Subsystem Responsibility Registry Keeps in-memory registry of available tools with metadata. Configurator Provides unified interface for tool configuration & validation. Executor Handles actual subprocess calls via SandboxExecutor . Cache Layer Stores tool state, results, and logs for replay. Renderer Generates UI/CLI views for configuration and results.","title":"\u2699\ufe0f Tool Manager Responsibilities"},{"location":"architecture/08-tool-manager-and-ux-design/#ux-goals","text":"Goal Description Low Friction User can run default scans without prior configuration. Transparency Full visibility into command-line arguments, logs, and results. Reusability Configurations and results can be reused across projects. Isolation Each project may have independent or shared tool setups. Extensibility Easy to add new tools through manifests.","title":"\ud83e\uddf1 UX Goals"},{"location":"architecture/08-tool-manager-and-ux-design/#cli-experience","text":"","title":"\ud83e\udde0 CLI Experience"},{"location":"architecture/08-tool-manager-and-ux-design/#example-commands","text":"# List available tools SecFlow tools list # Enable Feroxbuster & Nuclei SecFlow tools enable feroxbuster nuclei # Configure a tool SecFlow tools config nuclei --threads 50 --templates owasp-top10 # Run workflow interactively SecFlow run workflow.yaml # Chain tools manually SecFlow run feroxbuster --target https://example.com | SecFlow run nuclei ``` text ### CLI UX Principles - YAML/JSON inputs mirror internal manifests. - All commands are idempotent \u2014 re-runs use cached configurations. - CLI supports both single-tool and multi-node workflow modes. ## \ud83e\udde0 Web UI Experience ### Visual Overview ``` text Project: Acme Web API %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project: Acme Web API\"] B[\"Tools Panel\"] C[\"Workflow Builder\"] D[\"Execution Log\"] E[\"Results\"] A --> B A --> C A --> D A --> E","title":"Example Commands"},{"location":"architecture/08-tool-manager-and-ux-design/#key-panels","text":"Panel Description Tools Panel Shows installed tools, version, manifest, health check. Workflow Builder Drag-and-drop DAG composer (Ferox \u2192 Katana \u2192 Nuclei). Execution Log Real-time stdout/stderr stream with structured logging. Results Viewer Interactive findings dashboard, linked to triage view.","title":"Key Panels"},{"location":"architecture/08-tool-manager-and-ux-design/#example-workflow-builder-nodes","text":"( Discovery ) ( Crawler ) ( Scanner ) %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Katana\"] C[\"Nuclei\"] A --> B B --> C Users can: - Configure node properties via sidebar. - Save and export workflows as YAML ( workflow-recipe.yaml ). - Re-run saved workflows directly or modify them visually.","title":"Example Workflow Builder Nodes"},{"location":"architecture/08-tool-manager-and-ux-design/#configuration-persistence","text":"Each tool has a persistent JSON/YAML config stored under: ~/.SecFlow/config/tools/<tool>.yaml ```text ### Example ```yaml version: \"1.0\" defaults: threads: 25 rate_limit: 150 templates: res://templates/nuclei:latest profiles: aggressive: rate_limit: 300 safe: rate_limit: 50 sandbox: true ```text Profiles allow contextual configurations (e.g., \"aggressive\", \"safe\"). ## \ud83d\udd17 Tool Chaining (Runtime) Tool chaining uses the Workflow Engine, but users can also trigger ad-hoc sequences. Each output dataset is normalized as JSON Lines, allowing flexible piping. ```bash SecFlow run feroxbuster --target https://app.local \\ | SecFlow run katana --stdin \\ | SecFlow run nuclei --stdin ```text ### Data interoperability: Each tool must produce output in a structured JSON format with required fields: ```json { \"url\": \"https://example.com/login\", \"status\": 200, \"source\": \"feroxbuster\" } ```python ## \ud83e\udde9 Default Configurations & Templates When the user installs SecFlow: - Default manifests are loaded from `/resources/defaults/` - All core tools (Nuclei, Ferox, Katana) have pre-configured templates. - The user can immediately run scans without extra setup. ### Example command: ```bash SecFlow quickscan https://example.com ```python Equivalent to: - **Workflow:** Ferox \u2192 Nuclei (OWASP templates) ## \ud83e\udde0 Advanced Features | Feature | Description | |---------|-------------| | **Auto-Discovery** | Detects installed binaries and populates manifest registry automatically. | | **Tool Self-Test** | Validates binary presence and functionality via manifest-defined tests. | | **Runtime Profiles** | Switch between safe/aggressive scanning modes. | | **Execution History** | Stores past runs and parameters for reproducibility. | ## \ud83d\udd10 User Permissions Tool Manager respects project and role isolation: | Role | Capabilities | |------|-------------| | **Admin** | Install, configure, delete tools. | | **Analyst** | Execute workflows, view logs, triage findings. | | **Viewer** | Read-only access to results. | All actions are logged to the Audit Log. ## \ud83e\uddf1 Integration with Resource Registry The Tool Manager automatically pulls shared wordlists, templates, or payloads from the Resource Registry (see [09-resource-registry.md](09-resource-registry.md)). ### Example: ```yaml wordlist: res://wordlists/dirb:latest templates: res://templates/nuclei:latest ```ini ## \ud83e\udde9 Error Recovery UX If a tool fails: - The UI highlights failed nodes in red. - Logs show stderr output inline. - Users can retry individual nodes. - Failures generate a triage entry (finding type = \"tool error\"). ## \ud83e\udde0 Example CLI Session ```bash # Add Ferox with a project-local override wordlist secflow tools add ferox \\ --version 2.10.0 \\ --scope project \\ --config '{ \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64 }' # Run a two-node workflow directly from CLI (discovery \u2192 nuclei) secflow run \\ --project acme-web \\ --recipe workflows/discovery-to-nuclei.yaml \\ --var target=https://app.acme.com ```bash ### API Response Example ```json { \"tool\": \"feroxbuster\", \"version\": \"2.10.0\", \"scope\": \"project\", \"effective_config\": { \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64, \"rate_limit\": 0 }, \"status\": \"installed\", \"selftest\": { \"ok\": true, \"elapsed_ms\": 143 } } ```json ## \ud83d\udd2e Future Enhancements - UI-based \"Workflow Marketplace\" for community templates. - AI-assisted tool parameter tuning based on context. - Live terminal dashboard with interactive progress visualization. - Integration with Burp/OWASP ZAP APIs for direct import. --- **Next:** [Resource Registry](09-resource-registry.md) ```","title":"\ud83e\udde9 Configuration Persistence"},{"location":"architecture/09-resource-registry/","text":"09 \u2014 Resource Registry \u00b6 \ud83e\udded Overview \u00b6 The Resource Registry is SecFlow's shared data layer for managing reusable assets such as: Wordlists Templates (Nuclei, ZAP, custom YAML) Headers and payload sets Configurations and schema files CVE/CWE enrichment databases This registry provides unified access to all resources, manages versioning and scope isolation, and acts as the single source of truth for both CLI and API workflows. \ud83e\uddf1 Design Goals \u00b6 Goal Description Centralized One consistent repository for all reusable data. Scoped Resources may be global, group-specific, or project-specific. Versioned Immutable artifacts with hash-based versioning. Queryable Searchable by tags, type, or metadata. Cacheable Local caching for fast re-use and offline execution. Auditable Every modification is logged in the audit trail. \ud83e\udde9 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource API<br/>- Resource Manager<br/>- Indexer / Search Engine<br/>- Version Resolver<br/>- Storage Adapter (File, DB, Remote)\"] B[\"Resource Blob\"] C[\"Local Cache\"] A --> B B --> C \u2699\ufe0f Resource Model \u00b6 # core-lib/models/resource.py from typing import Literal , Optional from datetime import datetime from pydantic import BaseModel class Resource ( BaseModel ): id : str name : str type : Literal [ \"wordlist\" , \"templates\" , \"headers\" , \"payloads\" , \"configs\" , \"schema\" ] version : str hash : str scope : Literal [ \"global\" , \"group\" , \"project\" ] owner : Optional [ str ] blob_uri : str metadata : dict created_at : datetime updated_at : datetime usage_count : int last_used : Optional [ datetime ] ``` yaml ## \ud83e\udde9 Scope Hierarchy | Level | Description | Example Path | |-------|-------------|--------------| | ** Global ** | Available across all users and projects . | ` res : // wordlists / common . txt ` | | ** Group ** | Shared among specific teams . | ` res : // group / redteam / headers . json ` | | ** Project ** | Private to a specific pentest or client project . | ` res : // project / acme / templates / custom . yaml ` | ### Precedence Rules 1. Run - level override 2. Node - level override 3. Project default 4. Group default 5. Global default ### Example: If project - A has a custom wordlist ` res : // project / acme / dirb . yaml ` , it overrides ` res : // wordlists / dirb : latest ` . ## \ud83e\udde9 Example Registry Entry ``` yaml id : \"res://wordlists/dirb:1.2.0\" name : \"Dirbuster Common\" type : \"wordlist\" version : \"1.2.0\" hash : \"sha256:ea13f3b4c8d9e2f1a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" scope : \"global\" metadata : tags : [ \"web\" , \"discovery\" ] size : 24312 license : \"GPL-3.0\" source : \"https://github.com/digination/dirbuster\" created_at : \"2025-08-01T12:32:00Z\" updated_at : \"2025-08-15T09:21:00Z\" ``` python ## \ud83e\uddf1 Storage Backends The Resource Registry supports multiple backends : | Backend | Usage | Notes | |---------|-------|-------| | ** Local Filesystem ** | Default for developer use . | Fast , no dependencies . | | ** SQLite / Postgres ** | Production database . | Metadata stored in DB , blob on disk . | | ** S3 / MinIO ** | Remote multi - tenant storage . | Ideal for distributed environments . | | ** Git - backed Repository ** | Version - controlled registry . | Enables audit and rollback . | ## \ud83e\udde9 Resource Manager Interface ``` python # core-lib/ports/resource_port.py from typing import List from .models import Resource class ResourcePort ( Protocol ): def register ( self , resource : Resource ) -> None : \"\"\"Register a new resource in the registry.\"\"\" pass def list ( self , scope : str ) -> List [ Resource ]: \"\"\"List resources within a scope.\"\"\" pass def get ( self , id : str ) -> Resource : \"\"\"Get a resource by ID.\"\"\" pass def resolve ( self , name : str , version : str ) -> Resource : \"\"\"Resolve a resource by name and version.\"\"\" pass def increment_usage ( self , id : str ) -> None : \"\"\"Increment usage counter for a resource.\"\"\" pass ``` python ### Example Adapter Implementation ``` python # storage/resource_repository.py import json , os from core_lib.models.resource import Resource class FileResourceRepo : def __init__ ( self , base_path = \"~/.SecFlow/resources\" ): self . base = os . path . expanduser ( base_path ) def register ( self , res : Resource ): path = os . path . join ( self . base , f \" { res . id . replace ( 'res://' , '' ) } .json\" ) os . makedirs ( os . path . dirname ( path ), exist_ok = True ) with open ( path , \"w\" ) as f : f . write ( res . model_dump_json ()) def get ( self , id ): path = os . path . join ( self . base , f \" { id . replace ( 'res://' , '' ) } .json\" ) return Resource . parse_file ( path ) ``` yaml ## \ud83e\udde9 Resource Fetching & Caching When a workflow references ` res : // wordlists / dirb : latest ` : 1. The registry resolves the resource ( global / group / project ) . 2. The manager checks the local cache . 3. If missing or outdated , it downloads or loads the file blob . 4. The reference is then injected into the tool configuration . ``` python resolved = ResourceManager . resolve ( \"res://wordlists/dirb:latest\" ) path = CacheManager . fetch ( resolved ) ``` yaml ## \ud83e\udde0 Resource Versioning Resources are immutable once published ; updates produce new versions . | Operation | Behavior | |-----------|----------| | ** publish ** | Adds new resource version , preserves old one . | | ** retire ** | Marks resource as deprecated ( kept for history ) . | | ** promote ** | Moves a project resource to group / global scope . | ### Version Reference Syntax ``` text res : // wordlists / dirb : 1.2.0 res : // templates / nuclei : latest ``` text ## \ud83e\udde9 Registry CLI Commands ``` bash # List all resources SecFlow resources list # Show details for a resource SecFlow resources show res : // wordlists / dirb : latest # Add new resource SecFlow resources add wordlist ./ custom . txt -- scope project # Promote resource SecFlow resources promote res : // project / acme / dirb : latest -- to global ``` text ## \ud83e\udde0 Integration with Tool Manager Tools reference resources via symbolic URIs . At runtime , the registry automatically resolves URIs into local paths . ### Example Nuclei config: ``` yaml templates : res : // templates / nuclei : latest wordlist : res : // wordlists / dirb : latest \ud83d\udd12 Security & Integrity \u00b6 Mechanism Description Hash Validation SHA-256 hashes validated before use. Signed Resources Optional GPG signing for remote sources. Access Control Scoped permissions (Admin / Analyst / Viewer). Audit Trail Every fetch and publish event logged. \ud83e\udde9 Garbage Collection Policy \u00b6 When a resource is deleted: - Orphaned blob files are queued for cleanup by the GarbageCollector . - The audit log retains metadata for traceability. - Cached copies remain valid until TTL expiration. \ud83e\udde0 Example Use Case \u00b6 Global Wordlist Shared by Tools \u00b6 All tools can reference res://wordlists/dirb:latest : - Feroxbuster uses it for directory brute-force. - Katana can reuse it for crawl hints. - Nuclei leverages it for parameter fuzzing. Each tool can still override with its own wordlist if required. \ud83d\udd2e Future Enhancements \u00b6 Remote Resource Index API (REST/GraphQL). Smart caching with usage-based prefetch. Resource metrics: \"most used\", \"last updated\", etc. Tag-based search and semantic discovery. Signed update feeds from verified community sources. Next: Wordlist & Output Sharing Rules","title":"Resource Registry"},{"location":"architecture/09-resource-registry/#09-resource-registry","text":"","title":"09 \u2014 Resource Registry"},{"location":"architecture/09-resource-registry/#overview","text":"The Resource Registry is SecFlow's shared data layer for managing reusable assets such as: Wordlists Templates (Nuclei, ZAP, custom YAML) Headers and payload sets Configurations and schema files CVE/CWE enrichment databases This registry provides unified access to all resources, manages versioning and scope isolation, and acts as the single source of truth for both CLI and API workflows.","title":"\ud83e\udded Overview"},{"location":"architecture/09-resource-registry/#design-goals","text":"Goal Description Centralized One consistent repository for all reusable data. Scoped Resources may be global, group-specific, or project-specific. Versioned Immutable artifacts with hash-based versioning. Queryable Searchable by tags, type, or metadata. Cacheable Local caching for fast re-use and offline execution. Auditable Every modification is logged in the audit trail.","title":"\ud83e\uddf1 Design Goals"},{"location":"architecture/09-resource-registry/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource API<br/>- Resource Manager<br/>- Indexer / Search Engine<br/>- Version Resolver<br/>- Storage Adapter (File, DB, Remote)\"] B[\"Resource Blob\"] C[\"Local Cache\"] A --> B B --> C","title":"\ud83e\udde9 Architecture Overview"},{"location":"architecture/09-resource-registry/#resource-model","text":"# core-lib/models/resource.py from typing import Literal , Optional from datetime import datetime from pydantic import BaseModel class Resource ( BaseModel ): id : str name : str type : Literal [ \"wordlist\" , \"templates\" , \"headers\" , \"payloads\" , \"configs\" , \"schema\" ] version : str hash : str scope : Literal [ \"global\" , \"group\" , \"project\" ] owner : Optional [ str ] blob_uri : str metadata : dict created_at : datetime updated_at : datetime usage_count : int last_used : Optional [ datetime ] ``` yaml ## \ud83e\udde9 Scope Hierarchy | Level | Description | Example Path | |-------|-------------|--------------| | ** Global ** | Available across all users and projects . | ` res : // wordlists / common . txt ` | | ** Group ** | Shared among specific teams . | ` res : // group / redteam / headers . json ` | | ** Project ** | Private to a specific pentest or client project . | ` res : // project / acme / templates / custom . yaml ` | ### Precedence Rules 1. Run - level override 2. Node - level override 3. Project default 4. Group default 5. Global default ### Example: If project - A has a custom wordlist ` res : // project / acme / dirb . yaml ` , it overrides ` res : // wordlists / dirb : latest ` . ## \ud83e\udde9 Example Registry Entry ``` yaml id : \"res://wordlists/dirb:1.2.0\" name : \"Dirbuster Common\" type : \"wordlist\" version : \"1.2.0\" hash : \"sha256:ea13f3b4c8d9e2f1a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" scope : \"global\" metadata : tags : [ \"web\" , \"discovery\" ] size : 24312 license : \"GPL-3.0\" source : \"https://github.com/digination/dirbuster\" created_at : \"2025-08-01T12:32:00Z\" updated_at : \"2025-08-15T09:21:00Z\" ``` python ## \ud83e\uddf1 Storage Backends The Resource Registry supports multiple backends : | Backend | Usage | Notes | |---------|-------|-------| | ** Local Filesystem ** | Default for developer use . | Fast , no dependencies . | | ** SQLite / Postgres ** | Production database . | Metadata stored in DB , blob on disk . | | ** S3 / MinIO ** | Remote multi - tenant storage . | Ideal for distributed environments . | | ** Git - backed Repository ** | Version - controlled registry . | Enables audit and rollback . | ## \ud83e\udde9 Resource Manager Interface ``` python # core-lib/ports/resource_port.py from typing import List from .models import Resource class ResourcePort ( Protocol ): def register ( self , resource : Resource ) -> None : \"\"\"Register a new resource in the registry.\"\"\" pass def list ( self , scope : str ) -> List [ Resource ]: \"\"\"List resources within a scope.\"\"\" pass def get ( self , id : str ) -> Resource : \"\"\"Get a resource by ID.\"\"\" pass def resolve ( self , name : str , version : str ) -> Resource : \"\"\"Resolve a resource by name and version.\"\"\" pass def increment_usage ( self , id : str ) -> None : \"\"\"Increment usage counter for a resource.\"\"\" pass ``` python ### Example Adapter Implementation ``` python # storage/resource_repository.py import json , os from core_lib.models.resource import Resource class FileResourceRepo : def __init__ ( self , base_path = \"~/.SecFlow/resources\" ): self . base = os . path . expanduser ( base_path ) def register ( self , res : Resource ): path = os . path . join ( self . base , f \" { res . id . replace ( 'res://' , '' ) } .json\" ) os . makedirs ( os . path . dirname ( path ), exist_ok = True ) with open ( path , \"w\" ) as f : f . write ( res . model_dump_json ()) def get ( self , id ): path = os . path . join ( self . base , f \" { id . replace ( 'res://' , '' ) } .json\" ) return Resource . parse_file ( path ) ``` yaml ## \ud83e\udde9 Resource Fetching & Caching When a workflow references ` res : // wordlists / dirb : latest ` : 1. The registry resolves the resource ( global / group / project ) . 2. The manager checks the local cache . 3. If missing or outdated , it downloads or loads the file blob . 4. The reference is then injected into the tool configuration . ``` python resolved = ResourceManager . resolve ( \"res://wordlists/dirb:latest\" ) path = CacheManager . fetch ( resolved ) ``` yaml ## \ud83e\udde0 Resource Versioning Resources are immutable once published ; updates produce new versions . | Operation | Behavior | |-----------|----------| | ** publish ** | Adds new resource version , preserves old one . | | ** retire ** | Marks resource as deprecated ( kept for history ) . | | ** promote ** | Moves a project resource to group / global scope . | ### Version Reference Syntax ``` text res : // wordlists / dirb : 1.2.0 res : // templates / nuclei : latest ``` text ## \ud83e\udde9 Registry CLI Commands ``` bash # List all resources SecFlow resources list # Show details for a resource SecFlow resources show res : // wordlists / dirb : latest # Add new resource SecFlow resources add wordlist ./ custom . txt -- scope project # Promote resource SecFlow resources promote res : // project / acme / dirb : latest -- to global ``` text ## \ud83e\udde0 Integration with Tool Manager Tools reference resources via symbolic URIs . At runtime , the registry automatically resolves URIs into local paths . ### Example Nuclei config: ``` yaml templates : res : // templates / nuclei : latest wordlist : res : // wordlists / dirb : latest","title":"\u2699\ufe0f Resource Model"},{"location":"architecture/09-resource-registry/#security-integrity","text":"Mechanism Description Hash Validation SHA-256 hashes validated before use. Signed Resources Optional GPG signing for remote sources. Access Control Scoped permissions (Admin / Analyst / Viewer). Audit Trail Every fetch and publish event logged.","title":"\ud83d\udd12 Security &amp; Integrity"},{"location":"architecture/09-resource-registry/#garbage-collection-policy","text":"When a resource is deleted: - Orphaned blob files are queued for cleanup by the GarbageCollector . - The audit log retains metadata for traceability. - Cached copies remain valid until TTL expiration.","title":"\ud83e\udde9 Garbage Collection Policy"},{"location":"architecture/09-resource-registry/#example-use-case","text":"","title":"\ud83e\udde0 Example Use Case"},{"location":"architecture/09-resource-registry/#global-wordlist-shared-by-tools","text":"All tools can reference res://wordlists/dirb:latest : - Feroxbuster uses it for directory brute-force. - Katana can reuse it for crawl hints. - Nuclei leverages it for parameter fuzzing. Each tool can still override with its own wordlist if required.","title":"Global Wordlist Shared by Tools"},{"location":"architecture/09-resource-registry/#future-enhancements","text":"Remote Resource Index API (REST/GraphQL). Smart caching with usage-based prefetch. Resource metrics: \"most used\", \"last updated\", etc. Tag-based search and semantic discovery. Signed update feeds from verified community sources. Next: Wordlist & Output Sharing Rules","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/10-wordlist-and-output-sharing/","text":"10 \u2014 Wordlist & Output Sharing Rules \u00b6 \ud83e\udded Overview \u00b6 One of SecFlow's key strengths is inter-tool resource sharing \u2014 allowing different scanners and discovery tools to reuse wordlists , templates , and output datasets through a unified and versioned Resource Registry. This enables workflows like: %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster<br/>(directory discovery)\"] B[\"Katana<br/>(crawler + parameter discovery)\"] C[\"Nuclei<br/>(template-driven vulnerability scan)\"] A --> B B --> C Each step consumes outputs and produces standardized findings that can be reused in subsequent stages \u2014 without manual conversions or reconfiguration. \ud83e\uddf1 Design Objectives \u00b6 Objective Description Shared Wordlists All tools can reference the same library of wordlists. Isolated Overrides Each tool may override global wordlists with project-specific ones. Consistent Output Contracts Every wrapper emits normalized JSON output. Dataset Interoperability Discovery results from one tool can feed another automatically. Performance Efficiency Cached resources and deduplication prevent redundant I/O. \u2699\ufe0f Wordlist Management Flow \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource Registry\"] B[\"Wordlist Resolver\"] C[\"Global Cache\"] D[\"Tool Wrapper Config\"] E[\"Execution Context\"] A --> B B --> C B --> D D --> E Each execution context resolves wordlists from the Resource Registry , fetching them locally if needed. \ud83e\udde9 Shared Wordlist Schema \u00b6 id : res://wordlists/dirb:1.2.0 type : wordlist scope : global metadata : description : \"Common web directory wordlist\" format : \"text\" size : 24312 ``` yaml ### Usage Example ``` yaml tools : feroxbuster : wordlist : res://wordlists/dirb:latest nuclei : templates : res://templates/owasp-top10:latest ``` yaml At runtime, SecFlow resolves these resources to absolute paths and injects them into each wrapper. ## \ud83e\udde0 Multi-Scope Selection Logic ### 1. Run-Level Override Supplied directly in the workflow recipe or CLI argument. * *Example :** ``` bash SecFlow run feroxbuster --wordlist ./private.txt ``` bash ### 2. Node-Level Configuration Declared inside workflow YAML. * *Example :** ``` yaml nodes : - id : ferox type : discovery.ferox config : wordlist : res://wordlists/dirb:1.0 ``` yaml ### 3. Project Default Stored under `~/.SecFlow/projects/<id>/config.yaml`. ### 4. Group Default Shared among organizational units or red-team groups. ### 5. Global Default Fallback resource available for all users. ## \ud83e\udde0 Tool-Specific Overrides Each wrapper can define preferred wordlists and formats in its manifest. ### Example (feroxbuster.json): ``` json { \"defaults\" : { \"wordlist\" : \"res://wordlists/dirb:latest\" }, \"accepted_formats\" : [ \"txt\" , \"lst\" ] } ``` json ### Nuclei Example: ``` json { \"defaults\" : { \"templates\" : \"res://templates/nuclei:latest\" }, \"accepted_formats\" : [ \"yaml\" ] } ``` json The Tool Manager dynamically validates that the provided wordlist matches the expected format before execution. ## \ud83e\udde9 Output Standardization All tools output to a shared JSON Lines dataset (`.jsonl`). ### Example \u2014 Ferox Output ``` json { \"url\" : \"https://target.com/login\" , \"status\" : 200 , \"source\" : \"feroxbuster\" } ``` json ### Example \u2014 Nuclei Output ``` json { \"id\" : \"CVE-2024-12345\" , \"template\" : \"sql-injection\" , \"severity\" : \"high\" , \"matched-at\" : \"https://target.com/login\" } ``` json These outputs are normalized into the Finding schema by the Findings Engine. ## \ud83e\udde0 Chained Data Exchange ``` mermaid %% { init : { \"theme\" : \"neutral\" }} %% flowchart LR A[\"Feroxbuster\"] B[\"Output (urls.jsonl)\"] C[\"Katana\"] D[\"Nuclei\"] A --> B C --> B B --> D Each wrapper declares output channels (urls, parameters, endpoints, etc.). The Workflow Engine automatically maps compatible output types to input fields of the next node. Example Workflow Excerpt \u00b6 nodes : - id : ferox type : discovery.ferox outputs : [ \"urls\" ] - id : katana type : crawler.katana inputs : [ \"urls\" ] outputs : [ \"urls\" , \"params\" ] - id : nuclei type : scanner.nuclei inputs : [ \"urls\" , \"params\" ] outputs : [ \"findings\" ] ``` yaml ## \ud83e\udde9 Cross-Tool Resource Sharing Rules | Rule | Description | | ------|-------------| | **Wordlists** | All tools can access any registered wordlist; wrappers define which formats they accept. | | **Templates** | Shared globally between Nuclei, ZAP, or other scanners. | | **Headers** | Reusable header sets (e.g., API tokens) can be applied per project. | | **Payloads** | Payload libraries are versioned and accessible to all fuzzers. | | **Findings Outputs** | Findings may be exported or reused as seed data for enrichment tools. | ## \ud83e\udde0 Example \u2014 Shared Output Dataset ### Scenario: A user runs Ferox \u2192 Nuclei chain. Nuclei reuses the output from Ferox as its input dataset. ``` bash SecFlow run feroxbuster --target https://api.company.com \\ | SecFlow run nuclei --stdin ``` bash ### Result: 1. Ferox writes discovered URLs to `/runs/<uuid>/ferox/urls.jsonl` 2. The Workflow Engine pipes this dataset to the Nuclei node. 3. Nuclei scans each URL using selected templates. 4. Normalized findings are saved under `/runs/<uuid>/nuclei/findings.jsonl` ## \ud83e\udde9 Shared Dataset Metadata ``` yaml dataset : id : \"runs/2025-10-06T12:31Z-ferox-urls\" type : \"urls\" source : \"feroxbuster\" size : 243 created_at : \"2025-10-06T12:31Z\" ``` yaml This metadata is referenced by downstream nodes to ensure deterministic workflows. ## \ud83d\udd10 Data Isolation & Sharing Between Projects SecFlow supports granular sharing control for multi-project setups : | Mode | Description | | ------|-------------| | **Isolated** | Each project keeps separate resources and findings. | | **Shared Group** | Projects under the same group share wordlists and results. | | **Selective** | User manually links resources or outputs between projects. | ### Example: ``` yaml project : name : acme-api sharing : with : [ \"internal-api\" , \"dev-api\" ] resources : [ \"wordlists\" , \"templates\" ] outputs : [ \"urls\" , \"parameters\" ] ``` yaml ## \ud83e\udde9 Cache & Deduplication Wordlists and tool outputs are hash-indexed and cached for reuse. ### Cache Key Formula ``` python cache_key = sha256(resource_id + version + scope) ``` python This guarantees consistent retrieval and avoids redundant downloads. ## \ud83e\udde0 Example End-to-End Flow ``` text Project : acme-api %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Shared wordlists (global)\"] B[\"Custom payloads (project)\"] C[\"Workflow: Ferox \u2192 Katana \u2192 Nuclei\"] D[\"URLs discovered \u2192 shared dataset\"] E[\"Parameters extracted \u2192 temp dataset\"] F[\"Findings enriched \u2192 persisted\"] G[\"Cached resources \u2192 reused in next run\"] A --> C B --> C C --> D C --> E C --> F C --> G \ud83e\udde9 Validation & Conflict Resolution \u00b6 Conflict Resolution Same resource name, different scope Higher precedence scope wins (project > group > global). Resource deleted mid-run Cached version remains until TTL expires. Output schema mismatch Adapter plugin transforms data. Duplicate URLs or findings De-duplication engine merges entries by hash. \ud83d\udd2e Future Enhancements \u00b6 Distributed cache synchronization (Redis or MinIO). Tag-based linking of related outputs (e.g., same target domain). Automatic scope inference based on project classification. Resource \"diff\" view between runs. AI suggestion for optimal wordlists or template selection. Next: Project Isolation & Data Sharing Controls","title":"Wordlists & Sharing"},{"location":"architecture/10-wordlist-and-output-sharing/#10-wordlist-output-sharing-rules","text":"","title":"10 \u2014 Wordlist &amp; Output Sharing Rules"},{"location":"architecture/10-wordlist-and-output-sharing/#overview","text":"One of SecFlow's key strengths is inter-tool resource sharing \u2014 allowing different scanners and discovery tools to reuse wordlists , templates , and output datasets through a unified and versioned Resource Registry. This enables workflows like: %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster<br/>(directory discovery)\"] B[\"Katana<br/>(crawler + parameter discovery)\"] C[\"Nuclei<br/>(template-driven vulnerability scan)\"] A --> B B --> C Each step consumes outputs and produces standardized findings that can be reused in subsequent stages \u2014 without manual conversions or reconfiguration.","title":"\ud83e\udded Overview"},{"location":"architecture/10-wordlist-and-output-sharing/#design-objectives","text":"Objective Description Shared Wordlists All tools can reference the same library of wordlists. Isolated Overrides Each tool may override global wordlists with project-specific ones. Consistent Output Contracts Every wrapper emits normalized JSON output. Dataset Interoperability Discovery results from one tool can feed another automatically. Performance Efficiency Cached resources and deduplication prevent redundant I/O.","title":"\ud83e\uddf1 Design Objectives"},{"location":"architecture/10-wordlist-and-output-sharing/#wordlist-management-flow","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource Registry\"] B[\"Wordlist Resolver\"] C[\"Global Cache\"] D[\"Tool Wrapper Config\"] E[\"Execution Context\"] A --> B B --> C B --> D D --> E Each execution context resolves wordlists from the Resource Registry , fetching them locally if needed.","title":"\u2699\ufe0f Wordlist Management Flow"},{"location":"architecture/10-wordlist-and-output-sharing/#shared-wordlist-schema","text":"id : res://wordlists/dirb:1.2.0 type : wordlist scope : global metadata : description : \"Common web directory wordlist\" format : \"text\" size : 24312 ``` yaml ### Usage Example ``` yaml tools : feroxbuster : wordlist : res://wordlists/dirb:latest nuclei : templates : res://templates/owasp-top10:latest ``` yaml At runtime, SecFlow resolves these resources to absolute paths and injects them into each wrapper. ## \ud83e\udde0 Multi-Scope Selection Logic ### 1. Run-Level Override Supplied directly in the workflow recipe or CLI argument. * *Example :** ``` bash SecFlow run feroxbuster --wordlist ./private.txt ``` bash ### 2. Node-Level Configuration Declared inside workflow YAML. * *Example :** ``` yaml nodes : - id : ferox type : discovery.ferox config : wordlist : res://wordlists/dirb:1.0 ``` yaml ### 3. Project Default Stored under `~/.SecFlow/projects/<id>/config.yaml`. ### 4. Group Default Shared among organizational units or red-team groups. ### 5. Global Default Fallback resource available for all users. ## \ud83e\udde0 Tool-Specific Overrides Each wrapper can define preferred wordlists and formats in its manifest. ### Example (feroxbuster.json): ``` json { \"defaults\" : { \"wordlist\" : \"res://wordlists/dirb:latest\" }, \"accepted_formats\" : [ \"txt\" , \"lst\" ] } ``` json ### Nuclei Example: ``` json { \"defaults\" : { \"templates\" : \"res://templates/nuclei:latest\" }, \"accepted_formats\" : [ \"yaml\" ] } ``` json The Tool Manager dynamically validates that the provided wordlist matches the expected format before execution. ## \ud83e\udde9 Output Standardization All tools output to a shared JSON Lines dataset (`.jsonl`). ### Example \u2014 Ferox Output ``` json { \"url\" : \"https://target.com/login\" , \"status\" : 200 , \"source\" : \"feroxbuster\" } ``` json ### Example \u2014 Nuclei Output ``` json { \"id\" : \"CVE-2024-12345\" , \"template\" : \"sql-injection\" , \"severity\" : \"high\" , \"matched-at\" : \"https://target.com/login\" } ``` json These outputs are normalized into the Finding schema by the Findings Engine. ## \ud83e\udde0 Chained Data Exchange ``` mermaid %% { init : { \"theme\" : \"neutral\" }} %% flowchart LR A[\"Feroxbuster\"] B[\"Output (urls.jsonl)\"] C[\"Katana\"] D[\"Nuclei\"] A --> B C --> B B --> D Each wrapper declares output channels (urls, parameters, endpoints, etc.). The Workflow Engine automatically maps compatible output types to input fields of the next node.","title":"\ud83e\udde9 Shared Wordlist Schema"},{"location":"architecture/10-wordlist-and-output-sharing/#example-workflow-excerpt","text":"nodes : - id : ferox type : discovery.ferox outputs : [ \"urls\" ] - id : katana type : crawler.katana inputs : [ \"urls\" ] outputs : [ \"urls\" , \"params\" ] - id : nuclei type : scanner.nuclei inputs : [ \"urls\" , \"params\" ] outputs : [ \"findings\" ] ``` yaml ## \ud83e\udde9 Cross-Tool Resource Sharing Rules | Rule | Description | | ------|-------------| | **Wordlists** | All tools can access any registered wordlist; wrappers define which formats they accept. | | **Templates** | Shared globally between Nuclei, ZAP, or other scanners. | | **Headers** | Reusable header sets (e.g., API tokens) can be applied per project. | | **Payloads** | Payload libraries are versioned and accessible to all fuzzers. | | **Findings Outputs** | Findings may be exported or reused as seed data for enrichment tools. | ## \ud83e\udde0 Example \u2014 Shared Output Dataset ### Scenario: A user runs Ferox \u2192 Nuclei chain. Nuclei reuses the output from Ferox as its input dataset. ``` bash SecFlow run feroxbuster --target https://api.company.com \\ | SecFlow run nuclei --stdin ``` bash ### Result: 1. Ferox writes discovered URLs to `/runs/<uuid>/ferox/urls.jsonl` 2. The Workflow Engine pipes this dataset to the Nuclei node. 3. Nuclei scans each URL using selected templates. 4. Normalized findings are saved under `/runs/<uuid>/nuclei/findings.jsonl` ## \ud83e\udde9 Shared Dataset Metadata ``` yaml dataset : id : \"runs/2025-10-06T12:31Z-ferox-urls\" type : \"urls\" source : \"feroxbuster\" size : 243 created_at : \"2025-10-06T12:31Z\" ``` yaml This metadata is referenced by downstream nodes to ensure deterministic workflows. ## \ud83d\udd10 Data Isolation & Sharing Between Projects SecFlow supports granular sharing control for multi-project setups : | Mode | Description | | ------|-------------| | **Isolated** | Each project keeps separate resources and findings. | | **Shared Group** | Projects under the same group share wordlists and results. | | **Selective** | User manually links resources or outputs between projects. | ### Example: ``` yaml project : name : acme-api sharing : with : [ \"internal-api\" , \"dev-api\" ] resources : [ \"wordlists\" , \"templates\" ] outputs : [ \"urls\" , \"parameters\" ] ``` yaml ## \ud83e\udde9 Cache & Deduplication Wordlists and tool outputs are hash-indexed and cached for reuse. ### Cache Key Formula ``` python cache_key = sha256(resource_id + version + scope) ``` python This guarantees consistent retrieval and avoids redundant downloads. ## \ud83e\udde0 Example End-to-End Flow ``` text Project : acme-api %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Shared wordlists (global)\"] B[\"Custom payloads (project)\"] C[\"Workflow: Ferox \u2192 Katana \u2192 Nuclei\"] D[\"URLs discovered \u2192 shared dataset\"] E[\"Parameters extracted \u2192 temp dataset\"] F[\"Findings enriched \u2192 persisted\"] G[\"Cached resources \u2192 reused in next run\"] A --> C B --> C C --> D C --> E C --> F C --> G","title":"Example Workflow Excerpt"},{"location":"architecture/10-wordlist-and-output-sharing/#validation-conflict-resolution","text":"Conflict Resolution Same resource name, different scope Higher precedence scope wins (project > group > global). Resource deleted mid-run Cached version remains until TTL expires. Output schema mismatch Adapter plugin transforms data. Duplicate URLs or findings De-duplication engine merges entries by hash.","title":"\ud83e\udde9 Validation &amp; Conflict Resolution"},{"location":"architecture/10-wordlist-and-output-sharing/#future-enhancements","text":"Distributed cache synchronization (Redis or MinIO). Tag-based linking of related outputs (e.g., same target domain). Automatic scope inference based on project classification. Resource \"diff\" view between runs. AI suggestion for optimal wordlists or template selection. Next: Project Isolation & Data Sharing Controls","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/11-project-isolation-and-data-sharing/","text":"11 \u2014 Project Isolation & Data Sharing Controls \u00b6 \ud83e\udded Overview \u00b6 The Project Isolation & Sharing subsystem defines how multiple SecFlow projects coexist securely within the same environment. Each project has its own namespace , storage domain , and execution context , but can optionally share data or resources under controlled policies. SecFlow's isolation model is designed to: - Protect sensitive customer or target data. - Enable reuse of discovery outputs between related engagements. - Support team-based collaboration while maintaining auditability. \ud83e\uddf1 Design Principles \u00b6 Principle Description Default Isolation Every project operates in its own workspace and database namespace. Explicit Sharing No cross-project access occurs unless explicitly declared. Scoped Resources Shared assets must be tagged and versioned. Least Privilege Access Users only see or manage projects they are assigned to. Full Traceability Every cross-project data access is logged and auditable. \u2699\ufe0f Architectural Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project Manager<br/>- Workspace Manager (namespacing, FS isolation)<br/>- Data Access Layer (scope resolution)<br/>- Sharing Policy Engine (group + project-level rules)<br/>- Audit Log (cross-project actions)\"] \ud83e\udde9 Project Data Model \u00b6 # core-lib/models/project.py from typing import List , Optional from datetime import datetime from pydantic import BaseModel class Project ( BaseModel ): id : str name : str owner : str group : Optional [ str ] description : Optional [ str ] created_at : datetime updated_at : datetime sharing : Optional [ dict ] = { \"enabled\" : False , \"with\" : [], \"resources\" : [], \"outputs\" : [] } ``` text ## \ud83e\udde9 Workspace Isolation Each project is backed by its own filesystem and database schema . ### Example Directory Layout ``` mermaid %% { init : { \"theme\" : \"neutral\" }} %% flowchart TD A [ \"~/.SecFlow/projects/\" ] B [ \"acme-api/\" ] C [ \"config.yaml\" ] D [ \"runs/\" ] E [ \"findings/\" ] F [ \"cache/\" ] G [ \"finance-portal/\" ] H [ \"config.yaml\" ] I [ \"runs/\" ] J [ \"findings/\" ] K [ \"cache/\" ] A --> B A --> G B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K Database Schema Isolation \u00b6 Each project gets a dedicated schema: public.findings_acme_api public.findings_finance_portal ```yaml This allows multiple concurrent engagements with strict data boundaries. ## \ud83e\udde0 Data Sharing Configuration ### Example : Controlled Cross-Project Sharing ``` yaml project : name : \"acme-api\" sharing : enabled : true with : - \"internal-api\" - \"qa-staging\" resources : - \"wordlists\" - \"templates\" outputs : - \"urls\" - \"parameters\" ``` python In this configuration : - The `acme-api` project shares wordlists and templates with two sibling projects. - The outputs (e.g., discovered URLs) are made available for enrichment or scanning reuse. ## \ud83e\udde9 Sharing Policy Engine ### Logic Flow ``` text User Request \u2193 Policy Check (Project A \u2192 Project B) \u2193 Scope Validation \u2193 Access Decision (allow | deny) ``` yaml ### Policy Structure | Field | Description | | -------|-------------| | **resource_type** | What type of data is being shared (wordlist, output, finding). | | **scope** | Allowed scope (project, group, global). | | **mode** | Access type (read-only, read-write, clone). | | **ttl** | Time-to-live for shared access. | ### Example Policy Definition ``` yaml policies : - resource_type : \"outputs\" scope : \"group\" mode : \"read-only\" ttl : 30d ``` yaml ## \ud83e\udde9 Isolation Enforcement Mechanisms | Layer | Enforcement | | -------|-------------| | **Filesystem** | Each project path is sandboxed under `~/.SecFlow/projects/<id>`. | | **Database** | Separate schema per project (namespaced tables). | | **Cache** | Project-specific cache directories. | | **Runtime Context** | Workers run with `PROJECT_ID` environment variable. | | **Authorization** | API tokens include `project_id` scope claim. | ## \ud83e\udde0 Access Token Scoping API tokens encode the project scope : ``` json { \"sub\" : \"hernan.trajtemberg\" , \"project_id\" : \"acme-api\" , \"roles\" : [ \"analyst\" ], \"exp\" : 1759870400 } ``` python Tokens can be project-scoped or group-scoped. The access control middleware rejects out-of-scope operations. ## \ud83e\udde9 Resource Linking Between Projects Projects can import shared assets from another project's registry. ### Example Command ``` bash SecFlow projects link internal-api --resources wordlists templates ``` text ### Example Output ``` text Linked resources : \u2714 wordlists (3) \u2714 templates (5) ``` text Linked resources are marked in metadata : ``` yaml linked_from : \"project:internal-api\" mode : \"read-only\" ``` python ## \ud83e\uddf1 Output Sharing Outputs (datasets or findings) can also be shared for cross-project correlation or enrichment. ### Example Workflow: ``` text Project A \u2192 Discovery + Scan \u2193 Shared Outputs (URLs, endpoints) \u2193 Project B \u2192 Enrichment / Retesting ``` text ### Sharing Command ``` bash SecFlow share outputs acme-api --with finance-portal --types urls parameters ``` text The receiving project's engine imports the shared dataset as a read-only reference. ## \ud83e\udde9 Audit Logging Every cross-project access event is logged. ### Example Log Entry ``` json { \"event\" : \"resource_access\" , \"actor\" : \"hernan.trajtemberg\" , \"source_project\" : \"acme-api\" , \"target_project\" : \"internal-api\" , \"resource\" : \"wordlists\" , \"timestamp\" : \"2025-10-06T11:42:21Z\" , \"action\" : \"read\" } ``` python ## \ud83e\udde0 Isolation Example Scenarios ### 1. Strict Isolation (Default) Each project operates completely independently. Useful for sensitive pentests or regulated environments. ### 2. Group-Level Sharing Multiple analysts share discovery data across projects within the same team. ### 3. Selective Sharing A red-team lead selectively shares Nuclei findings between \"internal-api\" and \"staging-api\". ## \ud83d\udd10 Security Considerations | Risk | Mitigation | | ------|------------| | Unauthorized access to shared data | Token-scoped enforcement and audit logging | | Resource version drift | Immutable hashes + version pinning | | Data leakage across clients | No implicit sharing; explicit only | | Lateral movement between project schemas | Database role isolation | | Policy misconfiguration | Policy validation + test harness | ## \ud83e\udde9 Example Policy Validation Script ``` python def validate_policy(policy) : assert policy[\"mode\"] in (\"read-only\", \"read-write\", \"clone\") assert policy[\"scope\"] in (\"project\", \"group\", \"global\") if policy[\"ttl\"] : assert re.match(r\"^\\d+[dh]$\", policy[\"ttl\"]) \ud83d\udd2e Future Enhancements \u00b6 Group-level \"Shared Intelligence Pool\" for recurring findings. Automatic synchronization of enrichment data across related projects. AI-based data deduplication and anomaly detection. Visual dependency graph of shared resources in UI. Temporal sharing policies (\"auto-expire after 30 days\"). Next: Findings Model & Schema Normalization","title":"Project Isolation & Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#11-project-isolation-data-sharing-controls","text":"","title":"11 \u2014 Project Isolation &amp; Data Sharing Controls"},{"location":"architecture/11-project-isolation-and-data-sharing/#overview","text":"The Project Isolation & Sharing subsystem defines how multiple SecFlow projects coexist securely within the same environment. Each project has its own namespace , storage domain , and execution context , but can optionally share data or resources under controlled policies. SecFlow's isolation model is designed to: - Protect sensitive customer or target data. - Enable reuse of discovery outputs between related engagements. - Support team-based collaboration while maintaining auditability.","title":"\ud83e\udded Overview"},{"location":"architecture/11-project-isolation-and-data-sharing/#design-principles","text":"Principle Description Default Isolation Every project operates in its own workspace and database namespace. Explicit Sharing No cross-project access occurs unless explicitly declared. Scoped Resources Shared assets must be tagged and versioned. Least Privilege Access Users only see or manage projects they are assigned to. Full Traceability Every cross-project data access is logged and auditable.","title":"\ud83e\uddf1 Design Principles"},{"location":"architecture/11-project-isolation-and-data-sharing/#architectural-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project Manager<br/>- Workspace Manager (namespacing, FS isolation)<br/>- Data Access Layer (scope resolution)<br/>- Sharing Policy Engine (group + project-level rules)<br/>- Audit Log (cross-project actions)\"]","title":"\u2699\ufe0f Architectural Overview"},{"location":"architecture/11-project-isolation-and-data-sharing/#project-data-model","text":"# core-lib/models/project.py from typing import List , Optional from datetime import datetime from pydantic import BaseModel class Project ( BaseModel ): id : str name : str owner : str group : Optional [ str ] description : Optional [ str ] created_at : datetime updated_at : datetime sharing : Optional [ dict ] = { \"enabled\" : False , \"with\" : [], \"resources\" : [], \"outputs\" : [] } ``` text ## \ud83e\udde9 Workspace Isolation Each project is backed by its own filesystem and database schema . ### Example Directory Layout ``` mermaid %% { init : { \"theme\" : \"neutral\" }} %% flowchart TD A [ \"~/.SecFlow/projects/\" ] B [ \"acme-api/\" ] C [ \"config.yaml\" ] D [ \"runs/\" ] E [ \"findings/\" ] F [ \"cache/\" ] G [ \"finance-portal/\" ] H [ \"config.yaml\" ] I [ \"runs/\" ] J [ \"findings/\" ] K [ \"cache/\" ] A --> B A --> G B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K","title":"\ud83e\udde9 Project Data Model"},{"location":"architecture/11-project-isolation-and-data-sharing/#database-schema-isolation","text":"Each project gets a dedicated schema: public.findings_acme_api public.findings_finance_portal ```yaml This allows multiple concurrent engagements with strict data boundaries. ## \ud83e\udde0 Data Sharing Configuration ### Example : Controlled Cross-Project Sharing ``` yaml project : name : \"acme-api\" sharing : enabled : true with : - \"internal-api\" - \"qa-staging\" resources : - \"wordlists\" - \"templates\" outputs : - \"urls\" - \"parameters\" ``` python In this configuration : - The `acme-api` project shares wordlists and templates with two sibling projects. - The outputs (e.g., discovered URLs) are made available for enrichment or scanning reuse. ## \ud83e\udde9 Sharing Policy Engine ### Logic Flow ``` text User Request \u2193 Policy Check (Project A \u2192 Project B) \u2193 Scope Validation \u2193 Access Decision (allow | deny) ``` yaml ### Policy Structure | Field | Description | | -------|-------------| | **resource_type** | What type of data is being shared (wordlist, output, finding). | | **scope** | Allowed scope (project, group, global). | | **mode** | Access type (read-only, read-write, clone). | | **ttl** | Time-to-live for shared access. | ### Example Policy Definition ``` yaml policies : - resource_type : \"outputs\" scope : \"group\" mode : \"read-only\" ttl : 30d ``` yaml ## \ud83e\udde9 Isolation Enforcement Mechanisms | Layer | Enforcement | | -------|-------------| | **Filesystem** | Each project path is sandboxed under `~/.SecFlow/projects/<id>`. | | **Database** | Separate schema per project (namespaced tables). | | **Cache** | Project-specific cache directories. | | **Runtime Context** | Workers run with `PROJECT_ID` environment variable. | | **Authorization** | API tokens include `project_id` scope claim. | ## \ud83e\udde0 Access Token Scoping API tokens encode the project scope : ``` json { \"sub\" : \"hernan.trajtemberg\" , \"project_id\" : \"acme-api\" , \"roles\" : [ \"analyst\" ], \"exp\" : 1759870400 } ``` python Tokens can be project-scoped or group-scoped. The access control middleware rejects out-of-scope operations. ## \ud83e\udde9 Resource Linking Between Projects Projects can import shared assets from another project's registry. ### Example Command ``` bash SecFlow projects link internal-api --resources wordlists templates ``` text ### Example Output ``` text Linked resources : \u2714 wordlists (3) \u2714 templates (5) ``` text Linked resources are marked in metadata : ``` yaml linked_from : \"project:internal-api\" mode : \"read-only\" ``` python ## \ud83e\uddf1 Output Sharing Outputs (datasets or findings) can also be shared for cross-project correlation or enrichment. ### Example Workflow: ``` text Project A \u2192 Discovery + Scan \u2193 Shared Outputs (URLs, endpoints) \u2193 Project B \u2192 Enrichment / Retesting ``` text ### Sharing Command ``` bash SecFlow share outputs acme-api --with finance-portal --types urls parameters ``` text The receiving project's engine imports the shared dataset as a read-only reference. ## \ud83e\udde9 Audit Logging Every cross-project access event is logged. ### Example Log Entry ``` json { \"event\" : \"resource_access\" , \"actor\" : \"hernan.trajtemberg\" , \"source_project\" : \"acme-api\" , \"target_project\" : \"internal-api\" , \"resource\" : \"wordlists\" , \"timestamp\" : \"2025-10-06T11:42:21Z\" , \"action\" : \"read\" } ``` python ## \ud83e\udde0 Isolation Example Scenarios ### 1. Strict Isolation (Default) Each project operates completely independently. Useful for sensitive pentests or regulated environments. ### 2. Group-Level Sharing Multiple analysts share discovery data across projects within the same team. ### 3. Selective Sharing A red-team lead selectively shares Nuclei findings between \"internal-api\" and \"staging-api\". ## \ud83d\udd10 Security Considerations | Risk | Mitigation | | ------|------------| | Unauthorized access to shared data | Token-scoped enforcement and audit logging | | Resource version drift | Immutable hashes + version pinning | | Data leakage across clients | No implicit sharing; explicit only | | Lateral movement between project schemas | Database role isolation | | Policy misconfiguration | Policy validation + test harness | ## \ud83e\udde9 Example Policy Validation Script ``` python def validate_policy(policy) : assert policy[\"mode\"] in (\"read-only\", \"read-write\", \"clone\") assert policy[\"scope\"] in (\"project\", \"group\", \"global\") if policy[\"ttl\"] : assert re.match(r\"^\\d+[dh]$\", policy[\"ttl\"])","title":"Database Schema Isolation"},{"location":"architecture/11-project-isolation-and-data-sharing/#future-enhancements","text":"Group-level \"Shared Intelligence Pool\" for recurring findings. Automatic synchronization of enrichment data across related projects. AI-based data deduplication and anomaly detection. Visual dependency graph of shared resources in UI. Temporal sharing policies (\"auto-expire after 30 days\"). Next: Findings Model & Schema Normalization","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/12-findings-model-and-schema/","text":"12 \u2014 Findings Model & Schema Normalization \u00b6 \ud83e\udded Overview \u00b6 The Findings Schema is the canonical data contract for SecFlow \u2014 it defines how vulnerabilities, anomalies, and tool outputs are normalized, enriched, and persisted. Every discovery, scan, or enrichment step in the workflow produces Findings , which are standardized across tools (Nuclei, Ferox, ZAP, Caido, custom detectors). The schema ensures: - Cross-tool consistency - Automated CVE/CWE/OWASP correlation - Risk scoring alignment (CVSS/NIST) - Deterministic enrichment for analytics and triage \ud83e\uddf1 Data Flow Summary \u00b6 [ Tool Output ] \u2193 [ Findings Engine ] \u2193 [ Normalization Layer ] \u2193 [ Enrichment Layer (CVE/CWE/OWASP) ] \u2193 [ Persistent Store + Cache + Analytics ] ```text --- ## \u2699\ufe0f Findings Core Model ```python from pydantic import BaseModel, Field from typing import Optional, List, Dict from uuid import UUID from datetime import datetime class Finding(BaseModel): id: UUID project_id: UUID run_id: Optional[UUID] detector_id: str = Field(description=\"canonical tool or plugin id, e.g. scan.nuclei\") title: str severity: str # enum: info, low, medium, high, critical resource: str # canonical URL or identifier evidence: Dict[str, object] = {} cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] cvss: Optional[float] mitre_attack: List[str] = [] poc_links: List[str] = [] created_at: datetime provenance: Dict[str, object] = {} ```python ## \ud83e\udde9 Normalization Process Each wrapper or plugin output goes through the Findings Normalizer, which performs: 1. Schema validation 2. Severity normalization 3. Field mapping 4. Evidence compression 5. Deduplication ### Example Normalized Finding ```json { \"id\": \"7a3d9f8e-1f83-4a55-9a6c-5ea9e1a3b8d2\", \"project_id\": \"b0b8b6e0-7c6a-43e8-8b2f-8a0d0e8b0f3c\", \"run_id\": \"2a2b3c4d-5e6f-7081-92a3-b4c5d6e7f809\", \"detector_id\": \"scan.nuclei\", \"title\": \"X-Frame-Options header missing\", \"severity\": \"low\", \"resource\": \"https://app.acme.com/\", \"evidence\": { \"request_id\": \"r-01HXXQ5AKP0NWS\", \"template\": \"misconfig/headers/xfo-missing.yaml\" }, \"cwe\": 693, \"owasp\": \"A05\", \"cve_ids\": [], \"cvss\": 3.7, \"mitre_attack\": [\"T1190\"], \"poc_links\": [], \"created_at\": \"2025-10-07T08:22:31Z\", \"provenance\": { \"tool_version\": \"nuclei 3.1.0\", \"host\": \"runner-12\" } } ```json ```python def normalize(raw: dict, source: str) -> Finding: severity_map = {\"info\": \"info\", \"low\": \"low\", \"medium\": \"medium\", \"high\": \"high\", \"critical\": \"critical\"} return Finding( id=str(uuid4()), project_id=raw.get(\"project_id\"), detector_id=source, title=raw.get(\"info\", {}).get(\"name\") or raw.get(\"title\", \"Unnamed Finding\"), severity=severity_map.get(raw.get(\"severity\", \"info\")), path=raw.get(\"matched-at\") or raw.get(\"url\"), evidence=raw, created_at=datetime.utcnow(), provenance={\"tool\": source, \"version\": raw.get(\"version\", \"unknown\")} ) ```python ## \ud83e\udde9 Normalization Rules by Source | Tool | Input Format | Mapping | |------|--------------|---------| | **Nuclei** | JSON lines | `info.name` \u2192 `title`, `info.severity` \u2192 `severity`, `matched-at` \u2192 `path` | | **Feroxbuster** | Text | `URL` \u2192 `path`, `status` \u2192 `evidence.status` | | **ZAP/Burp** | XML/JSON | `PluginId` \u2192 `cwe`, `RiskDesc` \u2192 `severity` | | **Caido** | SQLite | `Vulnerability.name` \u2192 `title`, `score` \u2192 `cvss_score` | | **Custom Detectors** | Python dict | Arbitrary fields normalized via schema mapping | Normalization is performed by `findings-engine` using source-specific adapters. ## \ud83e\udde0 Severity Mapping | Raw Severity | Normalized | CVSS Equivalent | |--------------|------------|-----------------| | informational | info | 0.0\u20133.9 | | low | low | 4.0\u20136.9 | | medium | medium | 6.0\u20137.4 | | high | high | 7.5\u20138.9 | | critical | critical | 9.0\u201310.0 | ## \ud83e\udde0 Deduplication Strategy Findings are hashed on a deterministic fingerprint: ```python hash_input = f\"{finding.path}:{finding.title}:{finding.detector_id}\" finding_hash = hashlib.sha256(hash_input.encode()).hexdigest() ```python If the hash already exists in the same project and run scope, the finding is merged rather than duplicated. ## \ud83e\udde9 Enrichment Metadata Structure ```python finding.enrichment = { \"cwe\": {\"id\": 89, \"name\": \"SQL Injection\"}, \"owasp\": {\"category\": \"A03\", \"name\": \"Injection\"}, \"CVSS\": {\"score\": 9.1, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\"}, \"mitre\": {\"technique\": \"T1505\", \"tactic\": \"Persistence\"}, \"poc\": {\"source\": \"exploitdb\", \"link\": \"https://www.exploit-db.com/exploits/52341\"} } ```python This metadata is populated during the Enrichment Phase (CVE/CWE/CVSS sync). ## \ud83e\udde9 CVE / CWE / OWASP Mapping ### CVE \u2192 CWE - NVD API provides `cve.affects.vendor.vendor_data` \u2192 `cwe.id` - Mapping stored in local SQLite cache. ### CWE \u2192 OWASP | CWE | OWASP Category | |-----|----------------| | CWE-79 | A03: Injection | | CWE-89 | A03: Injection | | CWE-287 | A07: Identification and Authentication Failures | | CWE-601 | A10: Server-Side Request Forgery (SSRF) | ### Example Mapping Resolver ```python def resolve_owasp(cwe_id): mapping = {\"79\": \"A03\", \"89\": \"A03\", \"287\": \"A07\", \"601\": \"A10\"} return mapping.get(str(cwe_id)) ```python ## \ud83e\udde9 Confidence & Risk Scoring Confidence combines tool reliability, correlation consistency, and enrichment coverage. ### Formula ```python confidence = (tool_weight * 0.5) + (enrichment_score * 0.3) + (cross_source_count * 0.2) ```python ### Risk Score Calculation ```python risk_score = CVSS * confidence ```python This allows probabilistic triage prioritization. ## \ud83e\udde0 Evidence Normalization Evidence is stored in compact, structured form for indexing: ```json { \"request\": { \"method\": \"POST\", \"url\": \"https://target.com/login\", \"headers\": {\"Content-Type\": \"application/json\"}, \"body\": \"{\\\"username\\\":\\\"admin\\\"}\" }, \"response\": { \"status\": 500, \"headers\": {\"Server\": \"Apache\"}, \"body_snippet\": \"SQL syntax error\" } } ```json Large payloads are truncated or compressed to avoid storage overhead. ## \ud83e\udde9 Finding Status Lifecycle | Status | Meaning | Managed By | |--------|---------|------------| | open | Newly discovered issue | Scanner | | triaged | Analyst reviewed | Analyst | | resolved | Fixed or confirmed | Analyst | | false_positive | Invalid finding | Analyst | | archived | Expired or obsolete | System (GC) | Each status change triggers an audit log event and optional webhook notification. ## \ud83e\uddf1 Storage Layer Integration Findings are persisted via the `StoragePort` interface: ```python class FindingsRepository(Protocol): def save(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list(self, project_id: str) -> List[Finding]: \"\"\"List findings for a project.\"\"\" pass def get(self, id: str) -> Finding: \"\"\"Get a finding by ID.\"\"\" pass ```python ### Supported backends: - SQLite (default local mode) - PostgreSQL (production multi-project) - JSON (testing or demo mode) ## \ud83e\udde9 Findings Export Schema SecFlow exports findings in structured formats for interoperability: | Format | Command | |--------|---------| | JSON | `SecFlow export findings --format json` | | CSV | `SecFlow export findings --format csv` | | HTML | `SecFlow report findings --template summary.html` | | SARIF | `SecFlow export findings --format sarif` | ### Example JSON export: ```json { \"project\": \"acme-api\", \"findings\": [ { \"id\": \"a2b4f3c8-d9e2-4f1a-5b6c-7d8e9f0a1b2c\", \"title\": \"SQL Injection\", \"severity\": \"critical\", \"cwe\": 89, \"cvss_score\": 9.1, \"path\": \"/login\", \"created_at\": \"2025-10-06T10:15:00Z\" } ] } \ud83e\udde0 Indexing & Analytics \u00b6 Each finding is indexed in the analytics database: Primary Index: project_id + severity + cwe Full-Text Search: title , path , evidence.body_snippet Aggregations: count by severity, top affected endpoints, common CWE classes. The metrics system (see 06-plugin-system.md & 17-observability-logging-and-metrics.md ) uses these indexes to generate dashboards. \ud83d\udd2e Future Enhancements \u00b6 Graph-based finding correlation (attack paths). AI-driven risk clustering (\"find similar vulnerabilities\"). Contextual auto-triage (OWASP/NIST mapping feedback loops). Delta reports between runs ( run_id diff). Live sync to vulnerability management platforms (DefectDojo, VulnDB). Next: CVE/CWE/POC Enrichment Layer","title":"Findings Schema"},{"location":"architecture/12-findings-model-and-schema/#12-findings-model-schema-normalization","text":"","title":"12 \u2014 Findings Model &amp; Schema Normalization"},{"location":"architecture/12-findings-model-and-schema/#overview","text":"The Findings Schema is the canonical data contract for SecFlow \u2014 it defines how vulnerabilities, anomalies, and tool outputs are normalized, enriched, and persisted. Every discovery, scan, or enrichment step in the workflow produces Findings , which are standardized across tools (Nuclei, Ferox, ZAP, Caido, custom detectors). The schema ensures: - Cross-tool consistency - Automated CVE/CWE/OWASP correlation - Risk scoring alignment (CVSS/NIST) - Deterministic enrichment for analytics and triage","title":"\ud83e\udded Overview"},{"location":"architecture/12-findings-model-and-schema/#data-flow-summary","text":"[ Tool Output ] \u2193 [ Findings Engine ] \u2193 [ Normalization Layer ] \u2193 [ Enrichment Layer (CVE/CWE/OWASP) ] \u2193 [ Persistent Store + Cache + Analytics ] ```text --- ## \u2699\ufe0f Findings Core Model ```python from pydantic import BaseModel, Field from typing import Optional, List, Dict from uuid import UUID from datetime import datetime class Finding(BaseModel): id: UUID project_id: UUID run_id: Optional[UUID] detector_id: str = Field(description=\"canonical tool or plugin id, e.g. scan.nuclei\") title: str severity: str # enum: info, low, medium, high, critical resource: str # canonical URL or identifier evidence: Dict[str, object] = {} cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] cvss: Optional[float] mitre_attack: List[str] = [] poc_links: List[str] = [] created_at: datetime provenance: Dict[str, object] = {} ```python ## \ud83e\udde9 Normalization Process Each wrapper or plugin output goes through the Findings Normalizer, which performs: 1. Schema validation 2. Severity normalization 3. Field mapping 4. Evidence compression 5. Deduplication ### Example Normalized Finding ```json { \"id\": \"7a3d9f8e-1f83-4a55-9a6c-5ea9e1a3b8d2\", \"project_id\": \"b0b8b6e0-7c6a-43e8-8b2f-8a0d0e8b0f3c\", \"run_id\": \"2a2b3c4d-5e6f-7081-92a3-b4c5d6e7f809\", \"detector_id\": \"scan.nuclei\", \"title\": \"X-Frame-Options header missing\", \"severity\": \"low\", \"resource\": \"https://app.acme.com/\", \"evidence\": { \"request_id\": \"r-01HXXQ5AKP0NWS\", \"template\": \"misconfig/headers/xfo-missing.yaml\" }, \"cwe\": 693, \"owasp\": \"A05\", \"cve_ids\": [], \"cvss\": 3.7, \"mitre_attack\": [\"T1190\"], \"poc_links\": [], \"created_at\": \"2025-10-07T08:22:31Z\", \"provenance\": { \"tool_version\": \"nuclei 3.1.0\", \"host\": \"runner-12\" } } ```json ```python def normalize(raw: dict, source: str) -> Finding: severity_map = {\"info\": \"info\", \"low\": \"low\", \"medium\": \"medium\", \"high\": \"high\", \"critical\": \"critical\"} return Finding( id=str(uuid4()), project_id=raw.get(\"project_id\"), detector_id=source, title=raw.get(\"info\", {}).get(\"name\") or raw.get(\"title\", \"Unnamed Finding\"), severity=severity_map.get(raw.get(\"severity\", \"info\")), path=raw.get(\"matched-at\") or raw.get(\"url\"), evidence=raw, created_at=datetime.utcnow(), provenance={\"tool\": source, \"version\": raw.get(\"version\", \"unknown\")} ) ```python ## \ud83e\udde9 Normalization Rules by Source | Tool | Input Format | Mapping | |------|--------------|---------| | **Nuclei** | JSON lines | `info.name` \u2192 `title`, `info.severity` \u2192 `severity`, `matched-at` \u2192 `path` | | **Feroxbuster** | Text | `URL` \u2192 `path`, `status` \u2192 `evidence.status` | | **ZAP/Burp** | XML/JSON | `PluginId` \u2192 `cwe`, `RiskDesc` \u2192 `severity` | | **Caido** | SQLite | `Vulnerability.name` \u2192 `title`, `score` \u2192 `cvss_score` | | **Custom Detectors** | Python dict | Arbitrary fields normalized via schema mapping | Normalization is performed by `findings-engine` using source-specific adapters. ## \ud83e\udde0 Severity Mapping | Raw Severity | Normalized | CVSS Equivalent | |--------------|------------|-----------------| | informational | info | 0.0\u20133.9 | | low | low | 4.0\u20136.9 | | medium | medium | 6.0\u20137.4 | | high | high | 7.5\u20138.9 | | critical | critical | 9.0\u201310.0 | ## \ud83e\udde0 Deduplication Strategy Findings are hashed on a deterministic fingerprint: ```python hash_input = f\"{finding.path}:{finding.title}:{finding.detector_id}\" finding_hash = hashlib.sha256(hash_input.encode()).hexdigest() ```python If the hash already exists in the same project and run scope, the finding is merged rather than duplicated. ## \ud83e\udde9 Enrichment Metadata Structure ```python finding.enrichment = { \"cwe\": {\"id\": 89, \"name\": \"SQL Injection\"}, \"owasp\": {\"category\": \"A03\", \"name\": \"Injection\"}, \"CVSS\": {\"score\": 9.1, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\"}, \"mitre\": {\"technique\": \"T1505\", \"tactic\": \"Persistence\"}, \"poc\": {\"source\": \"exploitdb\", \"link\": \"https://www.exploit-db.com/exploits/52341\"} } ```python This metadata is populated during the Enrichment Phase (CVE/CWE/CVSS sync). ## \ud83e\udde9 CVE / CWE / OWASP Mapping ### CVE \u2192 CWE - NVD API provides `cve.affects.vendor.vendor_data` \u2192 `cwe.id` - Mapping stored in local SQLite cache. ### CWE \u2192 OWASP | CWE | OWASP Category | |-----|----------------| | CWE-79 | A03: Injection | | CWE-89 | A03: Injection | | CWE-287 | A07: Identification and Authentication Failures | | CWE-601 | A10: Server-Side Request Forgery (SSRF) | ### Example Mapping Resolver ```python def resolve_owasp(cwe_id): mapping = {\"79\": \"A03\", \"89\": \"A03\", \"287\": \"A07\", \"601\": \"A10\"} return mapping.get(str(cwe_id)) ```python ## \ud83e\udde9 Confidence & Risk Scoring Confidence combines tool reliability, correlation consistency, and enrichment coverage. ### Formula ```python confidence = (tool_weight * 0.5) + (enrichment_score * 0.3) + (cross_source_count * 0.2) ```python ### Risk Score Calculation ```python risk_score = CVSS * confidence ```python This allows probabilistic triage prioritization. ## \ud83e\udde0 Evidence Normalization Evidence is stored in compact, structured form for indexing: ```json { \"request\": { \"method\": \"POST\", \"url\": \"https://target.com/login\", \"headers\": {\"Content-Type\": \"application/json\"}, \"body\": \"{\\\"username\\\":\\\"admin\\\"}\" }, \"response\": { \"status\": 500, \"headers\": {\"Server\": \"Apache\"}, \"body_snippet\": \"SQL syntax error\" } } ```json Large payloads are truncated or compressed to avoid storage overhead. ## \ud83e\udde9 Finding Status Lifecycle | Status | Meaning | Managed By | |--------|---------|------------| | open | Newly discovered issue | Scanner | | triaged | Analyst reviewed | Analyst | | resolved | Fixed or confirmed | Analyst | | false_positive | Invalid finding | Analyst | | archived | Expired or obsolete | System (GC) | Each status change triggers an audit log event and optional webhook notification. ## \ud83e\uddf1 Storage Layer Integration Findings are persisted via the `StoragePort` interface: ```python class FindingsRepository(Protocol): def save(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list(self, project_id: str) -> List[Finding]: \"\"\"List findings for a project.\"\"\" pass def get(self, id: str) -> Finding: \"\"\"Get a finding by ID.\"\"\" pass ```python ### Supported backends: - SQLite (default local mode) - PostgreSQL (production multi-project) - JSON (testing or demo mode) ## \ud83e\udde9 Findings Export Schema SecFlow exports findings in structured formats for interoperability: | Format | Command | |--------|---------| | JSON | `SecFlow export findings --format json` | | CSV | `SecFlow export findings --format csv` | | HTML | `SecFlow report findings --template summary.html` | | SARIF | `SecFlow export findings --format sarif` | ### Example JSON export: ```json { \"project\": \"acme-api\", \"findings\": [ { \"id\": \"a2b4f3c8-d9e2-4f1a-5b6c-7d8e9f0a1b2c\", \"title\": \"SQL Injection\", \"severity\": \"critical\", \"cwe\": 89, \"cvss_score\": 9.1, \"path\": \"/login\", \"created_at\": \"2025-10-06T10:15:00Z\" } ] }","title":"\ud83e\uddf1 Data Flow Summary"},{"location":"architecture/12-findings-model-and-schema/#indexing-analytics","text":"Each finding is indexed in the analytics database: Primary Index: project_id + severity + cwe Full-Text Search: title , path , evidence.body_snippet Aggregations: count by severity, top affected endpoints, common CWE classes. The metrics system (see 06-plugin-system.md & 17-observability-logging-and-metrics.md ) uses these indexes to generate dashboards.","title":"\ud83e\udde0 Indexing &amp; Analytics"},{"location":"architecture/12-findings-model-and-schema/#future-enhancements","text":"Graph-based finding correlation (attack paths). AI-driven risk clustering (\"find similar vulnerabilities\"). Contextual auto-triage (OWASP/NIST mapping feedback loops). Delta reports between runs ( run_id diff). Live sync to vulnerability management platforms (DefectDojo, VulnDB). Next: CVE/CWE/POC Enrichment Layer","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/","text":"13 \u2014 CVE/CWE/POC Enrichment Layer \u00b6 \ud83e\udded Overview \u00b6 The Enrichment Layer bridges the gap between raw scanner results and full vulnerability intelligence. It performs automated correlation between findings and vulnerability databases such as: CVE/NVD \u2014 canonical vulnerability records CWE \u2014 weakness categorization CVSS \u2014 scoring and severity models Exploit-DB , Vulners , Metasploit , OSV \u2014 PoC and exploit references MITRE ATT&CK \u2014 behavioral mapping SecFlow treats enrichment as a pluggable knowledge graph layer that can be extended via simple JSON/YAML definitions or API connectors. \u2699\ufe0f Enrichment Pipeline \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Normalized Finding\"] B[\"CPE Identifier Extraction\"] C[\"CVE Resolver (NVD, OSV, Vulners)\"] D[\"CWE / OWASP Mapping\"] E[\"CVSS Calculation\"] F[\"POC + Exploit Lookup\"] G[\"MITRE ATT&CK Correlation\"] H[\"Final Enriched Finding\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H \ud83e\udde9 CPE & Component Extraction \u00b6 The enrichment process begins by fingerprinting software components. def extract_cpe ( finding : Finding ) -> Optional [ str ]: headers = finding . evidence . get ( \"response\" , {}) . get ( \"headers\" , {}) banner = headers . get ( \"Server\" ) or headers . get ( \"X-Powered-By\" ) return cpe_guess ( banner ) if banner else None ``` python ### Example results: - ` Server : Apache / 2.4.54 ` \u2192 ` cpe : / a : apache : http_server : 2.4.54 ` - ` X - Powered - By : Express ` \u2192 ` cpe : / a : npmjs : express : 4.18.2 ` ## \ud83e\udde9 CVE Resolution Engine The CVE Resolver queries multiple backends in a failover chain : | Source | Endpoint | Rate Limit | Cache TTL | |--------|----------|------------|-----------| | ** NVD ** | ` https : // services . nvd . nist . gov / rest / json / cves / 2.0 ` | 1000 / day | 24 h | | ** OSV . dev ** | ` https : // api . osv . dev / v1 / query ` | Unlimited | 12 h | | ** Vulners API ** | ` https : // vulners . com / api / v3 / search / lucene / ` | 2000 / day | 24 h | ``` python class CVEResolver : def resolve ( self , cpe : str ) -> List [ dict ]: cached = self . cache . get ( cpe ) if cached : return cached results = [] for backend in self . backends : try : results . extend ( backend . query ( cpe )) except Exception : continue self . cache . set ( cpe , results ) return results ``` python Results are merged and normalized into a unified CVE format . ## \ud83e\udde0 CVE Normalized Model ``` python class CVEEntry ( BaseModel ): cve_id : str description : str published : datetime cvss_score : float cvss_vector : str cwe_ids : List [ int ] references : List [ str ] exploit_refs : List [ str ] source : str ``` python Each finding may be associated with multiple CVE entries . ## \ud83e\udde9 CWE / OWASP / MITRE Mapping Once CVEs are linked , weaknesses and behavioral context are resolved . | Source | Purpose | Mapping Strategy | |--------|---------|------------------| | ** CWE ** | Weakness classification | CVE \u2192 CWE via NVD JSON | | ** OWASP ** | Application risk class | CWE \u2192 OWASP Top 10 map | | ** MITRE ATT & CK ** | Adversary tactics / techniques | CWE \u2192 ATT & CK TID correlation | ``` python def map_cwe_to_owasp ( cwe_id : int ) -> str : mapping = { 79 : \"A03: Injection\" , 89 : \"A03: Injection\" , 787 : \"A05: Buffer Overflow\" , 601 : \"A10: SSRF\" } return mapping . get ( cwe_id , \"N/A\" ) ``` python ### MITRE correlation example: ``` python mitre_map = { \"CWE-79\" : \"T1059.007 (Cross-Site Scripting)\" , \"CWE-89\" : \"T1505.003 (SQL Injection)\" } ``` python ## \ud83e\udde9 CVSS Calculation If a finding lacks explicit CVSS scoring , SecFlow derives one via heuristics : ``` python def derive_cvss ( cwe_id : int , context : dict ) -> float : # basic fallback estimation if cwe_id in ( 79 , 89 ): return 9.0 elif cwe_id in ( 200 , 201 ): return 7.5 return 5.0 ``` python ### Final score combines: ``` python base = CVSS temporal = exploit_availability * 0.2 environmental = exposure_factor * 0.3 final = ( base + temporal + environmental ) / 1.5 ``` python ## \ud83e\udde0 PoC & Exploit Correlation ### Data Sources | Source | Access | Notes | |--------|--------|-------| | ** Exploit - DB ** | Public dump | Weekly sync | | ** Vulners ** | API | Indexed by CVE | | ** Metasploit ** | Local metadata | Optional | | ** GitHub PoC ** | OSV + GitHub GraphQL | Filter by repo tags | | ** SecurityFocus ( legacy ) ** | Offline mirror | Static references | ### Example Resolver ``` python def resolve_poc ( cve_id : str ) -> list : sources = [ exploitdb , vulners , githubpoc ] results = [] for s in sources : results . extend ( s . search ( cve_id )) return list ( set ( results )) ``` python ## \ud83e\udde9 PoC Safety Governance Because PoCs can contain malicious payloads , SecFlow enforces strict isolation . | Policy | Enforcement | |--------|-------------| | ** Read - only storage ** | PoCs stored as text blobs , no exec permission | | ** Sandbox validation ** | Hash - check before use | | ** Legal disclaimer ** | Must be accepted before PoC download | | ** Runtime restriction ** | Execution allowed only in ` -- sandbox ` mode | ### Example governance guard: ``` python def safe_open_poc ( poc_path : Path ): if not user . accepted_disclaimer : raise PermissionError ( \"PoC execution disabled until disclaimer accepted.\" ) subprocess . run ([ \"sandbox\" , \"python3\" , poc_path ]) ``` python ## \ud83e\udde9 Caching & Synchronization Each enrichment source maintains a versioned local cache : | Component | Backend | Format | TTL | |-----------|---------|--------|-----| | ** CVE ** | NVD JSON | SQLite | 24 h | | ** CWE ** | MITRE XML | JSON | 7 d | | ** PoC ** | Exploit - DB | FS / JSON | 14 d | ### Example cache adapter: ``` python class LocalCache : def get ( self , key : str ) -> Optional [ Any ]: \"\"\"Get value from cache by key.\"\"\" pass def set ( self , key : str , value : Any , ttl : int ) -> None : \"\"\"Set value in cache with TTL.\"\"\" pass def purge_expired ( self ) -> None : \"\"\"Remove expired entries from cache.\"\"\" pass ``` python ## \ud83e\udde9 API Exposure The enrichment system provides a unified query interface : | Endpoint | Method | Description | |----------|--------|-------------| | ` / api / v1 / enrich / cve ` | POST | Enrich a finding by CPE / CVE | | ` / api / v1 / enrich / cwe ` | POST | Map CWE to OWASP | | ` / api / v1 / enrich / poc ` | GET | Retrieve PoC links | | ` / api / v1 / enrich / status ` | GET | Show cache health | ### Example response: ``` json { \"finding_id\" : \"1234\" , \"cve\" : [ \"CVE-2024-12345\" ], \"CVSS\" : 9.8 , \"cwe\" : 89 , \"owasp\" : \"A03: Injection\" , \"poc_links\" : [ \"https://exploit-db.com/exploits/52341\" ] } ``` json ## \ud83e\udde9 Enrichment Rules & Priority 1. Local cache first 2. API sources ( NVD / OSV ) second 3. Third - party mirrors ( Vulners , Exploit - DB ) last Each backend includes retry and circuit - breaker logic via Tenacity . ## \ud83e\udde9 Parallel Enrichment The enrichment worker uses async pipelines for batch enrichment : ``` python async def enrich_findings_batch ( findings : list ): async with aiohttp . ClientSession () as session : tasks = [ enrich_one ( f , session ) for f in findings ] return await asyncio . gather ( * tasks ) ``` python Each finding may take 0.1 \u2013 1.5 seconds depending on CVE count ; concurrency keeps throughput high . ## \ud83e\udde0 Example Enrichment Output ``` json { \"finding_id\" : \"abcd-1234\" , \"cpe\" : \"cpe:/a:apache:http_server:2.4.54\" , \"cve_ids\" : [ \"CVE-2023-25690\" ], \"cwe\" : 89 , \"owasp\" : \"A03: Injection\" , \"cvss_score\" : 9.8 , \"poc_links\" : [ \"https://exploit-db.com/exploits/52341\" ], \"mitre_tid\" : \"T1505.003\" , \"last_enriched\" : \"2025-10-06T09:43:00Z\" } \ud83d\udd12 Security & Compliance \u00b6 All enrichment data is public-source only. No proprietary or restricted databases. Caching system automatically purges expired CVE entries. Full audit trail for every enrichment event. \ud83d\udd2e Future Enhancements \u00b6 Integrate VulnCheck, CISA KEV, and EPSS for exploit prediction. Introduce AI-based CVE clustering for semantic matching. Enrichment correlation graph for cross-project analysis. Automated risk re-scoring based on KEV updates. Next: POC Sources, Safety & Legal Guidelines","title":"Enrichment Layer"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#13-cvecwepoc-enrichment-layer","text":"","title":"13 \u2014 CVE/CWE/POC Enrichment Layer"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#overview","text":"The Enrichment Layer bridges the gap between raw scanner results and full vulnerability intelligence. It performs automated correlation between findings and vulnerability databases such as: CVE/NVD \u2014 canonical vulnerability records CWE \u2014 weakness categorization CVSS \u2014 scoring and severity models Exploit-DB , Vulners , Metasploit , OSV \u2014 PoC and exploit references MITRE ATT&CK \u2014 behavioral mapping SecFlow treats enrichment as a pluggable knowledge graph layer that can be extended via simple JSON/YAML definitions or API connectors.","title":"\ud83e\udded Overview"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#enrichment-pipeline","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Normalized Finding\"] B[\"CPE Identifier Extraction\"] C[\"CVE Resolver (NVD, OSV, Vulners)\"] D[\"CWE / OWASP Mapping\"] E[\"CVSS Calculation\"] F[\"POC + Exploit Lookup\"] G[\"MITRE ATT&CK Correlation\"] H[\"Final Enriched Finding\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H","title":"\u2699\ufe0f Enrichment Pipeline"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cpe-component-extraction","text":"The enrichment process begins by fingerprinting software components. def extract_cpe ( finding : Finding ) -> Optional [ str ]: headers = finding . evidence . get ( \"response\" , {}) . get ( \"headers\" , {}) banner = headers . get ( \"Server\" ) or headers . get ( \"X-Powered-By\" ) return cpe_guess ( banner ) if banner else None ``` python ### Example results: - ` Server : Apache / 2.4.54 ` \u2192 ` cpe : / a : apache : http_server : 2.4.54 ` - ` X - Powered - By : Express ` \u2192 ` cpe : / a : npmjs : express : 4.18.2 ` ## \ud83e\udde9 CVE Resolution Engine The CVE Resolver queries multiple backends in a failover chain : | Source | Endpoint | Rate Limit | Cache TTL | |--------|----------|------------|-----------| | ** NVD ** | ` https : // services . nvd . nist . gov / rest / json / cves / 2.0 ` | 1000 / day | 24 h | | ** OSV . dev ** | ` https : // api . osv . dev / v1 / query ` | Unlimited | 12 h | | ** Vulners API ** | ` https : // vulners . com / api / v3 / search / lucene / ` | 2000 / day | 24 h | ``` python class CVEResolver : def resolve ( self , cpe : str ) -> List [ dict ]: cached = self . cache . get ( cpe ) if cached : return cached results = [] for backend in self . backends : try : results . extend ( backend . query ( cpe )) except Exception : continue self . cache . set ( cpe , results ) return results ``` python Results are merged and normalized into a unified CVE format . ## \ud83e\udde0 CVE Normalized Model ``` python class CVEEntry ( BaseModel ): cve_id : str description : str published : datetime cvss_score : float cvss_vector : str cwe_ids : List [ int ] references : List [ str ] exploit_refs : List [ str ] source : str ``` python Each finding may be associated with multiple CVE entries . ## \ud83e\udde9 CWE / OWASP / MITRE Mapping Once CVEs are linked , weaknesses and behavioral context are resolved . | Source | Purpose | Mapping Strategy | |--------|---------|------------------| | ** CWE ** | Weakness classification | CVE \u2192 CWE via NVD JSON | | ** OWASP ** | Application risk class | CWE \u2192 OWASP Top 10 map | | ** MITRE ATT & CK ** | Adversary tactics / techniques | CWE \u2192 ATT & CK TID correlation | ``` python def map_cwe_to_owasp ( cwe_id : int ) -> str : mapping = { 79 : \"A03: Injection\" , 89 : \"A03: Injection\" , 787 : \"A05: Buffer Overflow\" , 601 : \"A10: SSRF\" } return mapping . get ( cwe_id , \"N/A\" ) ``` python ### MITRE correlation example: ``` python mitre_map = { \"CWE-79\" : \"T1059.007 (Cross-Site Scripting)\" , \"CWE-89\" : \"T1505.003 (SQL Injection)\" } ``` python ## \ud83e\udde9 CVSS Calculation If a finding lacks explicit CVSS scoring , SecFlow derives one via heuristics : ``` python def derive_cvss ( cwe_id : int , context : dict ) -> float : # basic fallback estimation if cwe_id in ( 79 , 89 ): return 9.0 elif cwe_id in ( 200 , 201 ): return 7.5 return 5.0 ``` python ### Final score combines: ``` python base = CVSS temporal = exploit_availability * 0.2 environmental = exposure_factor * 0.3 final = ( base + temporal + environmental ) / 1.5 ``` python ## \ud83e\udde0 PoC & Exploit Correlation ### Data Sources | Source | Access | Notes | |--------|--------|-------| | ** Exploit - DB ** | Public dump | Weekly sync | | ** Vulners ** | API | Indexed by CVE | | ** Metasploit ** | Local metadata | Optional | | ** GitHub PoC ** | OSV + GitHub GraphQL | Filter by repo tags | | ** SecurityFocus ( legacy ) ** | Offline mirror | Static references | ### Example Resolver ``` python def resolve_poc ( cve_id : str ) -> list : sources = [ exploitdb , vulners , githubpoc ] results = [] for s in sources : results . extend ( s . search ( cve_id )) return list ( set ( results )) ``` python ## \ud83e\udde9 PoC Safety Governance Because PoCs can contain malicious payloads , SecFlow enforces strict isolation . | Policy | Enforcement | |--------|-------------| | ** Read - only storage ** | PoCs stored as text blobs , no exec permission | | ** Sandbox validation ** | Hash - check before use | | ** Legal disclaimer ** | Must be accepted before PoC download | | ** Runtime restriction ** | Execution allowed only in ` -- sandbox ` mode | ### Example governance guard: ``` python def safe_open_poc ( poc_path : Path ): if not user . accepted_disclaimer : raise PermissionError ( \"PoC execution disabled until disclaimer accepted.\" ) subprocess . run ([ \"sandbox\" , \"python3\" , poc_path ]) ``` python ## \ud83e\udde9 Caching & Synchronization Each enrichment source maintains a versioned local cache : | Component | Backend | Format | TTL | |-----------|---------|--------|-----| | ** CVE ** | NVD JSON | SQLite | 24 h | | ** CWE ** | MITRE XML | JSON | 7 d | | ** PoC ** | Exploit - DB | FS / JSON | 14 d | ### Example cache adapter: ``` python class LocalCache : def get ( self , key : str ) -> Optional [ Any ]: \"\"\"Get value from cache by key.\"\"\" pass def set ( self , key : str , value : Any , ttl : int ) -> None : \"\"\"Set value in cache with TTL.\"\"\" pass def purge_expired ( self ) -> None : \"\"\"Remove expired entries from cache.\"\"\" pass ``` python ## \ud83e\udde9 API Exposure The enrichment system provides a unified query interface : | Endpoint | Method | Description | |----------|--------|-------------| | ` / api / v1 / enrich / cve ` | POST | Enrich a finding by CPE / CVE | | ` / api / v1 / enrich / cwe ` | POST | Map CWE to OWASP | | ` / api / v1 / enrich / poc ` | GET | Retrieve PoC links | | ` / api / v1 / enrich / status ` | GET | Show cache health | ### Example response: ``` json { \"finding_id\" : \"1234\" , \"cve\" : [ \"CVE-2024-12345\" ], \"CVSS\" : 9.8 , \"cwe\" : 89 , \"owasp\" : \"A03: Injection\" , \"poc_links\" : [ \"https://exploit-db.com/exploits/52341\" ] } ``` json ## \ud83e\udde9 Enrichment Rules & Priority 1. Local cache first 2. API sources ( NVD / OSV ) second 3. Third - party mirrors ( Vulners , Exploit - DB ) last Each backend includes retry and circuit - breaker logic via Tenacity . ## \ud83e\udde9 Parallel Enrichment The enrichment worker uses async pipelines for batch enrichment : ``` python async def enrich_findings_batch ( findings : list ): async with aiohttp . ClientSession () as session : tasks = [ enrich_one ( f , session ) for f in findings ] return await asyncio . gather ( * tasks ) ``` python Each finding may take 0.1 \u2013 1.5 seconds depending on CVE count ; concurrency keeps throughput high . ## \ud83e\udde0 Example Enrichment Output ``` json { \"finding_id\" : \"abcd-1234\" , \"cpe\" : \"cpe:/a:apache:http_server:2.4.54\" , \"cve_ids\" : [ \"CVE-2023-25690\" ], \"cwe\" : 89 , \"owasp\" : \"A03: Injection\" , \"cvss_score\" : 9.8 , \"poc_links\" : [ \"https://exploit-db.com/exploits/52341\" ], \"mitre_tid\" : \"T1505.003\" , \"last_enriched\" : \"2025-10-06T09:43:00Z\" }","title":"\ud83e\udde9 CPE &amp; Component Extraction"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#security-compliance","text":"All enrichment data is public-source only. No proprietary or restricted databases. Caching system automatically purges expired CVE entries. Full audit trail for every enrichment event.","title":"\ud83d\udd12 Security &amp; Compliance"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#future-enhancements","text":"Integrate VulnCheck, CISA KEV, and EPSS for exploit prediction. Introduce AI-based CVE clustering for semantic matching. Enrichment correlation graph for cross-project analysis. Automated risk re-scoring based on KEV updates. Next: POC Sources, Safety & Legal Guidelines","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/14-poc-sources-and-legal-guidelines/","text":"14 \u2014 PoC Governance, Safety, and Legal Framework \u00b6 \ud83e\udded Overview \u00b6 Proof-of-Concept (PoC) execution is the most sensitive aspect of the SecFlow platform. While enrichment and discovery involve data collection, PoC modules may trigger actual exploits \u2014 often on controlled test targets or lab environments. This section defines: - The security model for PoC management and execution. - Legal and ethical boundaries for researchers. - Technical enforcement of sandbox isolation. - Policy hierarchy for PoC lifecycle, provenance, and deletion. \u2696\ufe0f Legal & Ethical Framework \u00b6 1. Authorized Testing Only \u00b6 PoC execution is strictly limited to: - Assets owned or explicitly authorized by the user. - Lab or offline sandbox targets. - Non-production systems unless a signed authorization exists. 2. Compliance with International Norms \u00b6 SecFlow aligns with: - ISO/IEC 29147 \u2014 Vulnerability Disclosure. - ISO/IEC 30111 \u2014 Vulnerability Handling. - NIST SP 800-115 \u2014 Technical Guide to Information Security Testing. - CISA KEV and CVE\u00ae Program standards. 3. Researcher Agreement \u00b6 Before using any exploit-related features: - Users must accept the Responsible Use Agreement (RUA) . - Every PoC download or execution is logged. - No redistribution of PoC code outside permitted licensing (e.g., GPL, BSD). \ud83e\uddf1 PoC Lifecycle Management \u00b6 Stage Description Acquisition PoC fetched from trusted sources (Exploit-DB, Vulners, GitHub PoC, OSV). Verification Signature/hash validated; metadata normalized. Classification Marked as local, remote, DoS, or privilege escalation. Sandbox Execution Run only in isolated containers or VMs. Audit Logging Execution parameters, user, and timestamp recorded. Retention & Purge Expired PoCs automatically archived or deleted. \ud83e\udde9 Storage & Metadata \u00b6 PoCs are stored as immutable artifacts under the internal resource store: %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/resources/poc/\"] B[\"CVE-2024-12345/\"] C[\"exploit.py\"] D[\"metadata.json\"] E[\"README.md\"] F[\"CVE-2023-98765/\"] A --> B A --> F B --> C B --> D B --> E Example metadata.json \u00b6 { \"cve\" : \"CVE-2024-12345\" , \"source\" : \"exploit-db\" , \"url\" : \"https://www.exploit-db.com/exploits/52341\" , \"verified\" : true , \"language\" : \"python\" , \"type\" : \"rce\" , \"hash\" : \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\" , \"sandbox_only\" : true , \"license\" : \"GPLv2\" , \"date_fetched\" : \"2025-10-06T10:20:00Z\" } ```jso n ## \ud83d\udd12 Sa n dbox Execu t io n Archi te c ture ### 1. Isola t io n Model All PoC execu t io ns occur wi t hi n t he SecFlow Sa n dbox Ru nt ime , impleme nte d as : - Docker co nta i ner wi t h res tr ic te d capabili t ies (` -- cap - drop=ALL`). - Read - o nl y mou nt o f PoC f iles. - No net work egress u nless explici tl y allowed. - Time & memory quo tas e nf orced by cgroups. ### 2. Ru nt ime Diagram ```yaml + -------------------------------------------------------+ | SecFlow Sa n dbox Ru nt ime | | ------------------------------------------------------- | | - Namespace Isola t io n (PID , NET , MNT) | | - Read - o nl y FS f or /poc | | - IPC + SYS_ADMIN disabled | | - AppArmor / SELi nu x pro f iles | | - Seccomp f il ters (de n y da n gerous syscalls) | + -------------------------------------------------------+ \u2191 | PoC ar t i fa c t + parame ters | [ SecFlow Worker ] \u2192 [ Sa n dbox Orches trat or ] \u2192 [ Co nta i ner Ru nt ime ] ``` te x t ### 3. Sample Sa n dbox I n voca t io n ```bash SecFlow sa n dbox ru n poc CVE -2024-12345 -- tar ge t h tt ps : //staging.example.com ```yaml U n der t he hood : ```bash subprocess.ru n ( [ \"docker\" , \"run\" , \"--rm\" , \"--network\" , \"none\" , \"--memory\" , \"512m\" , \"--cpus\" , \"1\" , \"-v\" , \"/pocstore/CVE-2024-12345:/poc:ro\" , \"SecFlow-sandbox:latest\" , \"python3\" , \"/poc/exploit.py\" , \"--target\" , \"https://staging.example.com\" ] ) ```yaml ## \u2699\ufe0f PoC Execu t io n Policy | Policy | E nf orceme nt | | -------- | ------------- | | **Sa n dbox O nl y** | No PoC ru ns o n hos t sys te m. | | **Read - o nl y Filesys te m** | Preve nts code modi f ica t io n or persis ten ce. | | **No Ne t work by De fault ** | All ou t bou n d co nne c t io ns blocked. | | **User Au t horiza t io n ** | Each ru n sig ne d wi t h user ide nt i t y & t imes ta mp. | | **Loggi n g & Replay** | S t dou t /s t derr cap ture d i n audi t logs. | | **Time - Bou n d Execu t io n ** | Hard kill i f ru nt ime exceeds ` t imeou t _seco n ds`. | ## \ud83e\udde0 Policy Co nf igura t io n Example ```yaml # ~/.SecFlow/policies/poc.yaml sa n dbox : image : SecFlow - sa n dbox : la test max_cpu : 1 max_memory_mb : 512 t imeou t _seco n ds : 300 allow_ net work : false allow_ f ilesys te m_wri te : false complia n ce : require_disclaimer : true require_projec t _au t horiza t io n : true au t o_veri f y_hashes : true ```py t ho n ## \ud83e\udde9 Gover nan ce Loggi n g Every PoC - rela te d eve nt is appe n ded t o a ta mper - resis tant audi t log : | Field | Descrip t io n | | ------- | ------------- | | eve nt _id | UUID o f t he audi t e ntr y | | user_id | Execu t i n g user | | t imes ta mp | UTC ISO -8601 t ime | | ac t io n | e.g. , \"sandbox_run\" , \"download\" , \"verify\" | | cve_id | Rela te d CVE | | t ool | Source or wrapper (e.g. , \"exploitdb\" ) | | sa n dbox_id | Co nta i ner ide nt i f ier | | hash | SHA 256 o f PoC code | ### Example Log E ntr y ```jso n { \"event_id\" : \"a1f3b7c2-9f11-45d0-bc97-6c9472cbdcb2\" , \"user_id\" : \"hernan\" , \"action\" : \"sandbox_run\" , \"cve_id\" : \"CVE-2024-12345\" , \"timestamp\" : \"2025-10-06T10:43:00Z\" , \"sandbox_id\" : \"sandbox-83214\" , \"hash\" : \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\" , \"tool\" : \"exploitdb\" } ``` te x t ## \ud83e\udde0 Legal No t ice E nf orceme nt Be f ore a n y PoC i ntera c t io n , users mus t sig n a legal disclaimer : ```py t ho n Respo ns ible Use No t ice : You ack n owledge t ha t PoC exploi tat io n is t o be per f ormed exclusively o n sys te ms you ow n or are explici tl y au t horized t o test . SecFlow is n o t liable f or a n y misuse or damages resul t i n g fr om u naut horized use. ``` te x t The sys te m s t ores a n accep tan ce hash : ```yaml ~/.SecFlow/.disclaimer_accep te d \ud83e\udde9 Cross-Project PoC Access \u00b6 To prevent accidental disclosure: - PoC artifacts are scoped per project by default. - Shared PoCs require explicit admin approval. - Access controlled via role-based permissions (RBAC). Role Access Admin Global & project PoCs Analyst Project PoCs only Viewer Read-only (metadata only) \ud83e\uddf1 PoC Verification Workflow \u00b6 graph TD A[Download PoC] --> B[Verify Hash & Signature] B --> C{Trusted Source?} C -->|Yes| D[Mark Verified] C -->|No| E[Quarantine] E --> F[Manual Review Required] D --> G[Available for Sandbox Run] \ud83d\udd12 Quarantine Mechanism \u00b6 Unknown or tampered PoCs are moved to: ~/.SecFlow/resources/poc/quarantine/ Each is tagged with a quarantine reason. Admins can review and promote back to verified status. def quarantine_poc ( poc_id : str , reason : str ): shutil . move ( f \"/pocstore/ { poc_id } \" , \"/pocstore/quarantine/\" ) write_log ( f \"PoC { poc_id } quarantined: { reason } \" ) \ud83e\udde0 Example Execution Trace \u00b6 [PoC: CVE-2024-12345] Verified source: ExploitDB [Sandbox] Starting container SecFlow-sandbox:latest [Sandbox] CPU quota: 1 core, Memory: 512MB [Sandbox] Network: disabled [Output] [+] Exploit successful: remote command executed [Cleanup] PoC container destroyed after 120s ```text ## \ud83d\udd2e Future Enhancements - AppArmor enforcement policies with dynamic runtime profiling. - PoC provenance blockchain for cryptographic integrity. - AI-assisted PoC safety classification (RCE, DoS, PrivEsc). - Multi-tenant isolation for collaborative workspaces. - Live replay of PoC runs for training and documentation. --- **Next:** [Garbage Collection & Data Retention Policy](15-garbage-collection-and-retention.md) ```","title":"PoC Sources & Legal"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#14-poc-governance-safety-and-legal-framework","text":"","title":"14 \u2014 PoC Governance, Safety, and Legal Framework"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#overview","text":"Proof-of-Concept (PoC) execution is the most sensitive aspect of the SecFlow platform. While enrichment and discovery involve data collection, PoC modules may trigger actual exploits \u2014 often on controlled test targets or lab environments. This section defines: - The security model for PoC management and execution. - Legal and ethical boundaries for researchers. - Technical enforcement of sandbox isolation. - Policy hierarchy for PoC lifecycle, provenance, and deletion.","title":"\ud83e\udded Overview"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#legal-ethical-framework","text":"","title":"\u2696\ufe0f Legal &amp; Ethical Framework"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#1-authorized-testing-only","text":"PoC execution is strictly limited to: - Assets owned or explicitly authorized by the user. - Lab or offline sandbox targets. - Non-production systems unless a signed authorization exists.","title":"1. Authorized Testing Only"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#2-compliance-with-international-norms","text":"SecFlow aligns with: - ISO/IEC 29147 \u2014 Vulnerability Disclosure. - ISO/IEC 30111 \u2014 Vulnerability Handling. - NIST SP 800-115 \u2014 Technical Guide to Information Security Testing. - CISA KEV and CVE\u00ae Program standards.","title":"2. Compliance with International Norms"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#3-researcher-agreement","text":"Before using any exploit-related features: - Users must accept the Responsible Use Agreement (RUA) . - Every PoC download or execution is logged. - No redistribution of PoC code outside permitted licensing (e.g., GPL, BSD).","title":"3. Researcher Agreement"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#poc-lifecycle-management","text":"Stage Description Acquisition PoC fetched from trusted sources (Exploit-DB, Vulners, GitHub PoC, OSV). Verification Signature/hash validated; metadata normalized. Classification Marked as local, remote, DoS, or privilege escalation. Sandbox Execution Run only in isolated containers or VMs. Audit Logging Execution parameters, user, and timestamp recorded. Retention & Purge Expired PoCs automatically archived or deleted.","title":"\ud83e\uddf1 PoC Lifecycle Management"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#storage-metadata","text":"PoCs are stored as immutable artifacts under the internal resource store: %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/resources/poc/\"] B[\"CVE-2024-12345/\"] C[\"exploit.py\"] D[\"metadata.json\"] E[\"README.md\"] F[\"CVE-2023-98765/\"] A --> B A --> F B --> C B --> D B --> E","title":"\ud83e\udde9 Storage &amp; Metadata"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#example-metadatajson","text":"{ \"cve\" : \"CVE-2024-12345\" , \"source\" : \"exploit-db\" , \"url\" : \"https://www.exploit-db.com/exploits/52341\" , \"verified\" : true , \"language\" : \"python\" , \"type\" : \"rce\" , \"hash\" : \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\" , \"sandbox_only\" : true , \"license\" : \"GPLv2\" , \"date_fetched\" : \"2025-10-06T10:20:00Z\" } ```jso n ## \ud83d\udd12 Sa n dbox Execu t io n Archi te c ture ### 1. Isola t io n Model All PoC execu t io ns occur wi t hi n t he SecFlow Sa n dbox Ru nt ime , impleme nte d as : - Docker co nta i ner wi t h res tr ic te d capabili t ies (` -- cap - drop=ALL`). - Read - o nl y mou nt o f PoC f iles. - No net work egress u nless explici tl y allowed. - Time & memory quo tas e nf orced by cgroups. ### 2. Ru nt ime Diagram ```yaml + -------------------------------------------------------+ | SecFlow Sa n dbox Ru nt ime | | ------------------------------------------------------- | | - Namespace Isola t io n (PID , NET , MNT) | | - Read - o nl y FS f or /poc | | - IPC + SYS_ADMIN disabled | | - AppArmor / SELi nu x pro f iles | | - Seccomp f il ters (de n y da n gerous syscalls) | + -------------------------------------------------------+ \u2191 | PoC ar t i fa c t + parame ters | [ SecFlow Worker ] \u2192 [ Sa n dbox Orches trat or ] \u2192 [ Co nta i ner Ru nt ime ] ``` te x t ### 3. Sample Sa n dbox I n voca t io n ```bash SecFlow sa n dbox ru n poc CVE -2024-12345 -- tar ge t h tt ps : //staging.example.com ```yaml U n der t he hood : ```bash subprocess.ru n ( [ \"docker\" , \"run\" , \"--rm\" , \"--network\" , \"none\" , \"--memory\" , \"512m\" , \"--cpus\" , \"1\" , \"-v\" , \"/pocstore/CVE-2024-12345:/poc:ro\" , \"SecFlow-sandbox:latest\" , \"python3\" , \"/poc/exploit.py\" , \"--target\" , \"https://staging.example.com\" ] ) ```yaml ## \u2699\ufe0f PoC Execu t io n Policy | Policy | E nf orceme nt | | -------- | ------------- | | **Sa n dbox O nl y** | No PoC ru ns o n hos t sys te m. | | **Read - o nl y Filesys te m** | Preve nts code modi f ica t io n or persis ten ce. | | **No Ne t work by De fault ** | All ou t bou n d co nne c t io ns blocked. | | **User Au t horiza t io n ** | Each ru n sig ne d wi t h user ide nt i t y & t imes ta mp. | | **Loggi n g & Replay** | S t dou t /s t derr cap ture d i n audi t logs. | | **Time - Bou n d Execu t io n ** | Hard kill i f ru nt ime exceeds ` t imeou t _seco n ds`. | ## \ud83e\udde0 Policy Co nf igura t io n Example ```yaml # ~/.SecFlow/policies/poc.yaml sa n dbox : image : SecFlow - sa n dbox : la test max_cpu : 1 max_memory_mb : 512 t imeou t _seco n ds : 300 allow_ net work : false allow_ f ilesys te m_wri te : false complia n ce : require_disclaimer : true require_projec t _au t horiza t io n : true au t o_veri f y_hashes : true ```py t ho n ## \ud83e\udde9 Gover nan ce Loggi n g Every PoC - rela te d eve nt is appe n ded t o a ta mper - resis tant audi t log : | Field | Descrip t io n | | ------- | ------------- | | eve nt _id | UUID o f t he audi t e ntr y | | user_id | Execu t i n g user | | t imes ta mp | UTC ISO -8601 t ime | | ac t io n | e.g. , \"sandbox_run\" , \"download\" , \"verify\" | | cve_id | Rela te d CVE | | t ool | Source or wrapper (e.g. , \"exploitdb\" ) | | sa n dbox_id | Co nta i ner ide nt i f ier | | hash | SHA 256 o f PoC code | ### Example Log E ntr y ```jso n { \"event_id\" : \"a1f3b7c2-9f11-45d0-bc97-6c9472cbdcb2\" , \"user_id\" : \"hernan\" , \"action\" : \"sandbox_run\" , \"cve_id\" : \"CVE-2024-12345\" , \"timestamp\" : \"2025-10-06T10:43:00Z\" , \"sandbox_id\" : \"sandbox-83214\" , \"hash\" : \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\" , \"tool\" : \"exploitdb\" } ``` te x t ## \ud83e\udde0 Legal No t ice E nf orceme nt Be f ore a n y PoC i ntera c t io n , users mus t sig n a legal disclaimer : ```py t ho n Respo ns ible Use No t ice : You ack n owledge t ha t PoC exploi tat io n is t o be per f ormed exclusively o n sys te ms you ow n or are explici tl y au t horized t o test . SecFlow is n o t liable f or a n y misuse or damages resul t i n g fr om u naut horized use. ``` te x t The sys te m s t ores a n accep tan ce hash : ```yaml ~/.SecFlow/.disclaimer_accep te d","title":"Example metadata.json"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#cross-project-poc-access","text":"To prevent accidental disclosure: - PoC artifacts are scoped per project by default. - Shared PoCs require explicit admin approval. - Access controlled via role-based permissions (RBAC). Role Access Admin Global & project PoCs Analyst Project PoCs only Viewer Read-only (metadata only)","title":"\ud83e\udde9 Cross-Project PoC Access"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#poc-verification-workflow","text":"graph TD A[Download PoC] --> B[Verify Hash & Signature] B --> C{Trusted Source?} C -->|Yes| D[Mark Verified] C -->|No| E[Quarantine] E --> F[Manual Review Required] D --> G[Available for Sandbox Run]","title":"\ud83e\uddf1 PoC Verification Workflow"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#quarantine-mechanism","text":"Unknown or tampered PoCs are moved to: ~/.SecFlow/resources/poc/quarantine/ Each is tagged with a quarantine reason. Admins can review and promote back to verified status. def quarantine_poc ( poc_id : str , reason : str ): shutil . move ( f \"/pocstore/ { poc_id } \" , \"/pocstore/quarantine/\" ) write_log ( f \"PoC { poc_id } quarantined: { reason } \" )","title":"\ud83d\udd12 Quarantine Mechanism"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#example-execution-trace","text":"[PoC: CVE-2024-12345] Verified source: ExploitDB [Sandbox] Starting container SecFlow-sandbox:latest [Sandbox] CPU quota: 1 core, Memory: 512MB [Sandbox] Network: disabled [Output] [+] Exploit successful: remote command executed [Cleanup] PoC container destroyed after 120s ```text ## \ud83d\udd2e Future Enhancements - AppArmor enforcement policies with dynamic runtime profiling. - PoC provenance blockchain for cryptographic integrity. - AI-assisted PoC safety classification (RCE, DoS, PrivEsc). - Multi-tenant isolation for collaborative workspaces. - Live replay of PoC runs for training and documentation. --- **Next:** [Garbage Collection & Data Retention Policy](15-garbage-collection-and-retention.md) ```","title":"\ud83e\udde0 Example Execution Trace"},{"location":"architecture/15-garbage-collection-and-retention/","text":"15 \u2014 Garbage Collection & Data Retention Policy \u00b6 \ud83e\udded Overview \u00b6 The Garbage Collection (GC) subsystem ensures long-term maintainability and performance of the SecFlow platform. It automates the cleanup of: - Expired runs and temporary results - Deleted project data (findings, resources, logs) - Old cache entries and transient files - Obsolete resources (e.g., old templates or wordlists) GC operates as an asynchronous background service within the worker app and supports both soft-delete and hard-delete modes. \u2699\ufe0f Core Objectives \u00b6 Objective Description Consistency Prevent orphaned or dangling records (runs, findings). Recoverability Allow temporary undo (via soft delete + TTL). Auditability Maintain logs of every GC event for compliance. Efficiency Keep database and disk footprint minimal. Isolation Per-project retention scopes (independent cleanup). \ud83e\udde9 Architecture Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Worker App<br/>- GC Scheduler (Celery Beat)<br/>- GC Worker (Async Cleanup Tasks)<br/>- Retention Policy Manager<br/>- Audit Trail Logger\"] B[\"Persistent Storage Layer<br/>- DB Tables: projects, runs, findings, logs<br/>- FS Paths: /resources, /cache, /artifacts\"] A --> B \ud83e\udde9 Retention Model \u00b6 Each project defines its retention profile in ~/.SecFlow/projects/<id>/config.yaml : retention : findings_ttl_days : 180 runs_ttl_days : 90 cache_ttl_days : 30 artifacts_ttl_days : 180 soft_delete_ttl_days : 14 auto_cleanup : true ``` yaml ## \ud83e\uddf1 Data Lifecycle | Stage | Description | | -------|-------------| | **Active** | Data used by ongoing projects or workflows. | | **Soft Deleted** | Marked for deletion but restorable (`flag : deleted=true`). | | **Expired** | TTL exceeded; scheduled for cleanup. | | **Hard Deleted** | Permanently removed after grace period. | ## \ud83e\udde0 Database-Level Soft Delete ``` python class BaseModel(SQLModel) : id : UUID created_at : datetime updated_at : datetime deleted : bool = False deleted_at : Optional[datetime] = None ``` python When a record is soft-deleted : ``` python def soft_delete(obj) : obj.deleted = True obj.deleted_at = datetime.utcnow() session.add(obj) session.commit() ``` python Recovery : ``` python def restore(obj) : obj.deleted = False obj.deleted_at = None session.commit() ``` text ## \ud83e\udde9 File System Garbage Collector ### Directory Structure ``` text /data/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/\"] B[\"projects/\"] C[\"<project_id>/\"] D[\"findings/\"] E[\"runs/\"] F[\"logs/\"] G[\"artifacts/\"] H[\"cache/\"] I[\"tmp/\"] A --> B A --> H A --> I B --> C C --> D C --> E C --> F C --> G The GC worker traverses these trees periodically: def sweep_directory ( base_path : Path , older_than : timedelta ): now = datetime . utcnow () for p in base_path . rglob ( \"*\" ): if p . is_file () and ( now - datetime . fromtimestamp ( p . stat () . st_mtime )) > older_than : p . unlink () ``` python ## \ud83e\udde0 GC Task Scheduling ### Celery Task Definition ``` python @app . task ( name = \"gc.cleanup_expired\" ) def cleanup_expired (): sweep_projects () sweep_cache () sweep_artifacts () ``` text ### Scheduler Configuration ``` python CELERY_BEAT_SCHEDULE = { \"cleanup-every-6h\" : { \"task\" : \"gc.cleanup_expired\" , \"schedule\" : crontab ( hour = \"*/6\" ), }, } ``` text GC tasks can be triggered manually : ``` bash SecFlow gc run -- project acme - api ``` python ## \ud83e\udde9 Retention Policy Evaluation ### Example Policy Rules | Rule | Condition | Action | |------|-----------|--------| | Inactive Runs | Run ended > 90 days ago | Delete run logs | | Soft - Deleted Findings | Deleted > 14 days ago | Purge permanently | | Cache Expired | Cache entry older than 30 days | Remove | | Unused Artifacts | Artifact not accessed for 180 days | Archive or delete | ### Policy Engine Snippet ``` python def evaluate_retention ( entity , policy ): if entity . deleted and expired ( entity . deleted_at , policy . soft_delete_ttl_days ): hard_delete ( entity ) elif expired ( entity . updated_at , policy . findings_ttl_days ): soft_delete ( entity ) ``` text ## \ud83e\udde9 Audit Logging for GC Each GC operation generates an audit record : ``` json { \"event\" : \"gc_delete\" , \"type\" : \"finding\" , \"target_id\" : \"f123-45ac\" , \"project_id\" : \"p001\" , \"timestamp\" : \"2025-10-06T09:30:00Z\" , \"user\" : \"system\" , \"ttl_rule\" : \"soft_delete_ttl_days=14\" } ``` text Stored in : ``` text ~/. SecFlow / audit / gc . log ``` python ## \ud83e\uddf1 Orphan Detection ### SQL Example ``` sql SELECT f . id FROM findings f LEFT JOIN runs r ON f . run_id = r . id WHERE r . id IS NULL ; ``` python Any orphaned findings or artifacts ( without associated runs / projects ) are purged automatically . ## \ud83e\udde9 Cache Lifecycle Caches ( e . g . , CVE data , scan results , tool logs ) use a standardized interface : ``` python class CacheEntry ( BaseModel ): key : str value : bytes expires_at : datetime def purge_expired (): session . query ( CacheEntry ) . filter ( CacheEntry . expires_at < datetime . utcnow ()) . delete () ``` text ## \ud83e\udde0 Manual Cleanup Command Users can trigger GC manually via CLI : ``` bash # Run full cleanup (all projects) SecFlow gc run # Run cleanup for one project SecFlow gc run -- project acme - api # Preview what will be deleted SecFlow gc dry - run ``` text ### Example output: ``` text [ GC ] Found 12 expired runs , 4 orphaned findings , 6 stale cache entries [ GC ] Total reclaimed : 1.2 GB ``` text ## \ud83d\udd10 Security Considerations - All deletions ( soft or hard ) are logged . - Data is never removed without audit trace . - System prevents GC while a project is locked or running . - Manual GC requires Admin role . ## \ud83d\udd04 GC Metrics & Observability | Metric | Description | |--------|-------------| | gc_runs_total | Number of GC cycles executed | | gc_files_removed_total | Number of files deleted | | gc_bytes_reclaimed_total | Storage reclaimed in bytes | | gc_duration_seconds | Time per GC cycle | | gc_errors_total | Failed cleanup operations | Exposed via Prometheus at ` / metrics ` . ## \ud83e\udde0 Example GC Cycle Log ``` text [ GC ] Cycle started at 2025 - 10 - 06 T09 : 00 : 00 Z [ GC ] Processed 3 projects [ GC ] Deleted 15 findings ( soft ) [ GC ] Purged 10 runs ( hard ) [ GC ] Reclaimed 1.8 GB disk space [ GC ] Cycle completed in 42.3 s \ud83d\udd2e Future Enhancements \u00b6 Incremental snapshot pruning \u2014 keep only latest versions of runs. Policy-as-Code \u2014 customizable YAML rulesets. AI-based cleanup predictions \u2014 identify stale datasets dynamically. Storage quota enforcement per user/project. UI dashboard for retention and GC insights. Next: Security Model (RBAC, Authentication, Sandboxing)","title":"Garbage Collection"},{"location":"architecture/15-garbage-collection-and-retention/#15-garbage-collection-data-retention-policy","text":"","title":"15 \u2014 Garbage Collection &amp; Data Retention Policy"},{"location":"architecture/15-garbage-collection-and-retention/#overview","text":"The Garbage Collection (GC) subsystem ensures long-term maintainability and performance of the SecFlow platform. It automates the cleanup of: - Expired runs and temporary results - Deleted project data (findings, resources, logs) - Old cache entries and transient files - Obsolete resources (e.g., old templates or wordlists) GC operates as an asynchronous background service within the worker app and supports both soft-delete and hard-delete modes.","title":"\ud83e\udded Overview"},{"location":"architecture/15-garbage-collection-and-retention/#core-objectives","text":"Objective Description Consistency Prevent orphaned or dangling records (runs, findings). Recoverability Allow temporary undo (via soft delete + TTL). Auditability Maintain logs of every GC event for compliance. Efficiency Keep database and disk footprint minimal. Isolation Per-project retention scopes (independent cleanup).","title":"\u2699\ufe0f Core Objectives"},{"location":"architecture/15-garbage-collection-and-retention/#architecture-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Worker App<br/>- GC Scheduler (Celery Beat)<br/>- GC Worker (Async Cleanup Tasks)<br/>- Retention Policy Manager<br/>- Audit Trail Logger\"] B[\"Persistent Storage Layer<br/>- DB Tables: projects, runs, findings, logs<br/>- FS Paths: /resources, /cache, /artifacts\"] A --> B","title":"\ud83e\udde9 Architecture Diagram"},{"location":"architecture/15-garbage-collection-and-retention/#retention-model","text":"Each project defines its retention profile in ~/.SecFlow/projects/<id>/config.yaml : retention : findings_ttl_days : 180 runs_ttl_days : 90 cache_ttl_days : 30 artifacts_ttl_days : 180 soft_delete_ttl_days : 14 auto_cleanup : true ``` yaml ## \ud83e\uddf1 Data Lifecycle | Stage | Description | | -------|-------------| | **Active** | Data used by ongoing projects or workflows. | | **Soft Deleted** | Marked for deletion but restorable (`flag : deleted=true`). | | **Expired** | TTL exceeded; scheduled for cleanup. | | **Hard Deleted** | Permanently removed after grace period. | ## \ud83e\udde0 Database-Level Soft Delete ``` python class BaseModel(SQLModel) : id : UUID created_at : datetime updated_at : datetime deleted : bool = False deleted_at : Optional[datetime] = None ``` python When a record is soft-deleted : ``` python def soft_delete(obj) : obj.deleted = True obj.deleted_at = datetime.utcnow() session.add(obj) session.commit() ``` python Recovery : ``` python def restore(obj) : obj.deleted = False obj.deleted_at = None session.commit() ``` text ## \ud83e\udde9 File System Garbage Collector ### Directory Structure ``` text /data/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/\"] B[\"projects/\"] C[\"<project_id>/\"] D[\"findings/\"] E[\"runs/\"] F[\"logs/\"] G[\"artifacts/\"] H[\"cache/\"] I[\"tmp/\"] A --> B A --> H A --> I B --> C C --> D C --> E C --> F C --> G The GC worker traverses these trees periodically: def sweep_directory ( base_path : Path , older_than : timedelta ): now = datetime . utcnow () for p in base_path . rglob ( \"*\" ): if p . is_file () and ( now - datetime . fromtimestamp ( p . stat () . st_mtime )) > older_than : p . unlink () ``` python ## \ud83e\udde0 GC Task Scheduling ### Celery Task Definition ``` python @app . task ( name = \"gc.cleanup_expired\" ) def cleanup_expired (): sweep_projects () sweep_cache () sweep_artifacts () ``` text ### Scheduler Configuration ``` python CELERY_BEAT_SCHEDULE = { \"cleanup-every-6h\" : { \"task\" : \"gc.cleanup_expired\" , \"schedule\" : crontab ( hour = \"*/6\" ), }, } ``` text GC tasks can be triggered manually : ``` bash SecFlow gc run -- project acme - api ``` python ## \ud83e\udde9 Retention Policy Evaluation ### Example Policy Rules | Rule | Condition | Action | |------|-----------|--------| | Inactive Runs | Run ended > 90 days ago | Delete run logs | | Soft - Deleted Findings | Deleted > 14 days ago | Purge permanently | | Cache Expired | Cache entry older than 30 days | Remove | | Unused Artifacts | Artifact not accessed for 180 days | Archive or delete | ### Policy Engine Snippet ``` python def evaluate_retention ( entity , policy ): if entity . deleted and expired ( entity . deleted_at , policy . soft_delete_ttl_days ): hard_delete ( entity ) elif expired ( entity . updated_at , policy . findings_ttl_days ): soft_delete ( entity ) ``` text ## \ud83e\udde9 Audit Logging for GC Each GC operation generates an audit record : ``` json { \"event\" : \"gc_delete\" , \"type\" : \"finding\" , \"target_id\" : \"f123-45ac\" , \"project_id\" : \"p001\" , \"timestamp\" : \"2025-10-06T09:30:00Z\" , \"user\" : \"system\" , \"ttl_rule\" : \"soft_delete_ttl_days=14\" } ``` text Stored in : ``` text ~/. SecFlow / audit / gc . log ``` python ## \ud83e\uddf1 Orphan Detection ### SQL Example ``` sql SELECT f . id FROM findings f LEFT JOIN runs r ON f . run_id = r . id WHERE r . id IS NULL ; ``` python Any orphaned findings or artifacts ( without associated runs / projects ) are purged automatically . ## \ud83e\udde9 Cache Lifecycle Caches ( e . g . , CVE data , scan results , tool logs ) use a standardized interface : ``` python class CacheEntry ( BaseModel ): key : str value : bytes expires_at : datetime def purge_expired (): session . query ( CacheEntry ) . filter ( CacheEntry . expires_at < datetime . utcnow ()) . delete () ``` text ## \ud83e\udde0 Manual Cleanup Command Users can trigger GC manually via CLI : ``` bash # Run full cleanup (all projects) SecFlow gc run # Run cleanup for one project SecFlow gc run -- project acme - api # Preview what will be deleted SecFlow gc dry - run ``` text ### Example output: ``` text [ GC ] Found 12 expired runs , 4 orphaned findings , 6 stale cache entries [ GC ] Total reclaimed : 1.2 GB ``` text ## \ud83d\udd10 Security Considerations - All deletions ( soft or hard ) are logged . - Data is never removed without audit trace . - System prevents GC while a project is locked or running . - Manual GC requires Admin role . ## \ud83d\udd04 GC Metrics & Observability | Metric | Description | |--------|-------------| | gc_runs_total | Number of GC cycles executed | | gc_files_removed_total | Number of files deleted | | gc_bytes_reclaimed_total | Storage reclaimed in bytes | | gc_duration_seconds | Time per GC cycle | | gc_errors_total | Failed cleanup operations | Exposed via Prometheus at ` / metrics ` . ## \ud83e\udde0 Example GC Cycle Log ``` text [ GC ] Cycle started at 2025 - 10 - 06 T09 : 00 : 00 Z [ GC ] Processed 3 projects [ GC ] Deleted 15 findings ( soft ) [ GC ] Purged 10 runs ( hard ) [ GC ] Reclaimed 1.8 GB disk space [ GC ] Cycle completed in 42.3 s","title":"\ud83e\udde9 Retention Model"},{"location":"architecture/15-garbage-collection-and-retention/#future-enhancements","text":"Incremental snapshot pruning \u2014 keep only latest versions of runs. Policy-as-Code \u2014 customizable YAML rulesets. AI-based cleanup predictions \u2014 identify stale datasets dynamically. Storage quota enforcement per user/project. UI dashboard for retention and GC insights. Next: Security Model (RBAC, Authentication, Sandboxing)","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/16-security-model/","text":"16 \u2014 Security Model (RBAC, Authentication, and Sandboxing) \u00b6 \ud83e\udded Overview \u00b6 Security is a first-class design goal in the SecFlow architecture. Every user action, process execution, and data mutation is subject to authentication, authorization, and sandboxing. This model ensures: - Strict access separation between users, projects, and data. - Controlled execution of potentially dangerous tools or PoCs. - Immutable auditability of all operations. - Zero-trust posture inside the orchestration environment. \ud83e\udde9 Layers of the Security Model \u00b6 Layer Scope Mechanism Authentication User identity verification JWT / OAuth2 tokens Authorization (RBAC) Access control Role-based & scope-aware Execution Sandboxing Process isolation Containerized runners Secrets Management API keys, credentials Encrypted vault Audit Logging Accountability Immutable append-only logs \ud83e\udde0 Authentication Architecture \u00b6 SecFlow supports two auth models: 1. Local Auth (default): For standalone or on-premise deployments. 2. Federated Auth (optional): OAuth2/OpenID Connect integration (GitHub, Google, Azure AD). Token Model \u00b6 { \"sub\" : \"hernan\" , \"role\" : \"admin\" , \"exp\" : 1738783200 , \"projects\" : [ \"proj-01\" , \"proj-02\" ] } ```jso n ### Toke n Flow Diagram ``` te x t [ User Logi n ] \u2192 [ Au t h Provider ] \u2192 [ JWT Issued ] \u2192 [ API Ga te way ] \u2192 [ SecFlow Web/API ] ``` te x t Each reques t t o `/api/*` must include: ```text Authorization: Bearer <token> ```text Tokens are verified by the API middleware using RS256 signature validation. ## \ud83e\udde9 Role-Based Access Control (RBAC) Roles define the scope of capabilities across the platform. | Role | Permissions | |------|-------------| | **Admin** | Full control \u2014 manage tools, users, projects, retention, policies. | | **Analyst** | Execute workflows, triage findings, view reports, limited editing. | | **Viewer** | Read-only access to results and dashboards. | | **Automation (Service)** | Used by background tasks (limited scoped tokens). | ### Example permission matrix: | Action | Admin | Analyst | Viewer | Service | |--------|-------|---------|--------|---------| | Run workflow | \u2705 | \u2705 | \u274c | \u2705 | | Modify tool config | \u2705 | \u274c | \u274c | \u274c | | View findings | \u2705 | \u2705 | \u2705 | \u2705 | | Delete project | \u2705 | \u274c | \u274c | \u274c | | Access PoCs | \u2705 | \u2705 | \u274c | \u274c | | Run GC tasks | \u2705 | \u274c | \u274c | \u2705 | ## \u2699\ufe0f Policy Enforcement Every endpoint and command passes through an Access Policy Filter: ```python def authorize(action: str, user: User, project: Optional[str] = None): if not user.has_permission(action, project): raise HTTPException(403, detail=f\"Forbidden: {action}\") ```python ### Example route decorator: ```python @app.get(\"/api/v1/findings\") @require_role([\"admin\", \"analyst\", \"viewer\"]) def list_findings(): def authorize(self, action: str, user: User, project: Optional[str] = None) -> bool: \"\"\"Check if user is authorized for action.\"\"\" return user.has_permission(action, project) ```python ## \ud83e\uddf1 Secrets Management ### Secret Types - API tokens for external tools (e.g., Shodan, Vulners) - Private SSH keys for remote scans - Encrypted credentials for authenticated targets ### Storage Backend All secrets are stored in the SecFlow Vault, an encrypted JSON database backed by Fernet (AES-256). ```python class SecretVault: def __init__(self, keyfile: Path): self.key = load_key(keyfile) def store(self, id: str, data: dict): enc = fernet_encrypt(json.dumps(data), self.key) write_file(f\"/vault/{id}.enc\", enc) ```python ### Secrets CLI ```bash SecFlow secrets add nuclei_api --value \"TOKEN123\" SecFlow secrets list SecFlow secrets remove nuclei_api ```yaml All access is scoped by user and project context. ## \ud83d\udd12 Execution Sandboxing All scanner and PoC executions run inside restricted containers or subprocess jails. ### Isolation Techniques | Mechanism | Purpose | |-----------|---------| | Namespaces (PID, NET, MNT) | Process isolation | | Seccomp Filters | Syscall restriction | | cgroups v2 | CPU/memory limits | | No-root UID mapping | Drops privileges | | AppArmor profiles | File access control | | Read-only FS | Prevents persistence | ### Example ```bash docker run --rm --cap-drop=ALL \\ --security-opt=no-new-privileges \\ --read-only -m 512m --cpus=1 \\ SecFlow-runner:latest nuclei -t /templates ```python ## \ud83e\udde9 Network & Data Security | Channel | Encryption | Notes | |---------|------------|-------| | API <-> UI | HTTPS (TLS 1.3) | Strict transport enforced | | Worker <-> API | Mutual TLS | Each worker has its own cert | | File Sync | AES-256 encrypted | Optional compression | | Database | At-rest encryption | SQLite: SEE, Postgres: TDE | | Audit Logs | Signed + timestamped | Prevents tampering | ## \ud83e\udde0 Secure Inter-Process Communication Internal apps communicate via ZeroMQ or Redis queues over TLS. Each message includes a signed envelope: ```json { \"msg_id\": \"uuid\", \"issuer\": \"worker-1\", \"signature\": \"HMAC-SHA256\", \"payload\": {\"data\": \"encrypted_content\"} } ```json This ensures authenticity and non-repudiation. ## \ud83e\udde9 Security Hooks & Middleware SecFlow injects middleware for: - Request validation (pydantic schemas) - JWT token expiry verification - Role validation (fast-path lookup) - Audit log writing after each mutating operation ### Example: ```python @app.middleware(\"http\") async def audit_request(request, call_next): response = await call_next(request) if request.method in (\"POST\", \"DELETE\", \"PATCH\"): log_audit(request, response) return response ```yaml ## \ud83d\udd10 Audit Trail & Tamper Resistance ### Log Format ```json { \"timestamp\": \"2025-10-06T09:40:00Z\", \"user\": \"hernan\", \"action\": \"workflow_run\", \"project\": \"api-audit\", \"status\": \"success\", \"hash\": \"sha256:0a3b1c2d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2\" } ```json ### Storage & Verification - Logs stored as JSON lines under `/audit/` - Each file signed with an HMAC chain: ```python H_i = HMAC(H_prev + log_i, key) ```yaml Immutable and verifiable chain-of-trust. ## \ud83e\udde0 Security Monitoring | Metric | Description | |--------|-------------| | auth_failures_total | Failed login attempts | | sandbox_executions_total | Containers spawned | | policy_violations_total | Unauthorized actions | | vault_accesses_total | Secret retrievals | | audit_events_total | Log entries recorded | ## \u2699\ufe0f Compliance Framework Alignment SecFlow's security architecture aligns with: | Framework | Compliance Area | |-----------|-----------------| | **NIST SP 800-53** | Access control, auditing, system protection | | **ISO/IEC 27001** | Information security management | | **OWASP SAMM** | Secure software development lifecycle | | **MITRE ATT&CK** | Mapping detection behaviors | | **GDPR Art. 32** | Data confidentiality and integrity | ## \ud83d\udd12 Key Rotation & Secrets Expiry - Secrets have explicit TTLs (default: 180 days). - Vault rotation command: ```bash SecFlow vault rotate ```text Rotation regenerates the encryption key and re-encrypts all entries. ## \ud83e\udde9 Example Access Workflow ```text [Analyst] \u2192 [Login via OIDC] \u2192 [JWT Issued] \u2192 [UI/API Gateway] \u2192 [RBAC Policy Check] \u2192 [Worker Executes Workflow in Sandbox] \u2192 [Findings Logged + Audit Entry Created] \ud83d\udd2e Future Enhancements \u00b6 Hardware-backed encryption (YubiKey / TPM) for Vault keys. FIPS-compliant container sandbox. Behavior-based anomaly detection on audit logs. Single Sign-On (SSO) with just-in-time role provisioning. Runtime policy engine (OPA / Rego integration). Next: Observability, Logging & Metrics","title":"Security Model"},{"location":"architecture/16-security-model/#16-security-model-rbac-authentication-and-sandboxing","text":"","title":"16 \u2014 Security Model (RBAC, Authentication, and Sandboxing)"},{"location":"architecture/16-security-model/#overview","text":"Security is a first-class design goal in the SecFlow architecture. Every user action, process execution, and data mutation is subject to authentication, authorization, and sandboxing. This model ensures: - Strict access separation between users, projects, and data. - Controlled execution of potentially dangerous tools or PoCs. - Immutable auditability of all operations. - Zero-trust posture inside the orchestration environment.","title":"\ud83e\udded Overview"},{"location":"architecture/16-security-model/#layers-of-the-security-model","text":"Layer Scope Mechanism Authentication User identity verification JWT / OAuth2 tokens Authorization (RBAC) Access control Role-based & scope-aware Execution Sandboxing Process isolation Containerized runners Secrets Management API keys, credentials Encrypted vault Audit Logging Accountability Immutable append-only logs","title":"\ud83e\udde9 Layers of the Security Model"},{"location":"architecture/16-security-model/#authentication-architecture","text":"SecFlow supports two auth models: 1. Local Auth (default): For standalone or on-premise deployments. 2. Federated Auth (optional): OAuth2/OpenID Connect integration (GitHub, Google, Azure AD).","title":"\ud83e\udde0 Authentication Architecture"},{"location":"architecture/16-security-model/#token-model","text":"{ \"sub\" : \"hernan\" , \"role\" : \"admin\" , \"exp\" : 1738783200 , \"projects\" : [ \"proj-01\" , \"proj-02\" ] } ```jso n ### Toke n Flow Diagram ``` te x t [ User Logi n ] \u2192 [ Au t h Provider ] \u2192 [ JWT Issued ] \u2192 [ API Ga te way ] \u2192 [ SecFlow Web/API ] ``` te x t Each reques t t o `/api/*` must include: ```text Authorization: Bearer <token> ```text Tokens are verified by the API middleware using RS256 signature validation. ## \ud83e\udde9 Role-Based Access Control (RBAC) Roles define the scope of capabilities across the platform. | Role | Permissions | |------|-------------| | **Admin** | Full control \u2014 manage tools, users, projects, retention, policies. | | **Analyst** | Execute workflows, triage findings, view reports, limited editing. | | **Viewer** | Read-only access to results and dashboards. | | **Automation (Service)** | Used by background tasks (limited scoped tokens). | ### Example permission matrix: | Action | Admin | Analyst | Viewer | Service | |--------|-------|---------|--------|---------| | Run workflow | \u2705 | \u2705 | \u274c | \u2705 | | Modify tool config | \u2705 | \u274c | \u274c | \u274c | | View findings | \u2705 | \u2705 | \u2705 | \u2705 | | Delete project | \u2705 | \u274c | \u274c | \u274c | | Access PoCs | \u2705 | \u2705 | \u274c | \u274c | | Run GC tasks | \u2705 | \u274c | \u274c | \u2705 | ## \u2699\ufe0f Policy Enforcement Every endpoint and command passes through an Access Policy Filter: ```python def authorize(action: str, user: User, project: Optional[str] = None): if not user.has_permission(action, project): raise HTTPException(403, detail=f\"Forbidden: {action}\") ```python ### Example route decorator: ```python @app.get(\"/api/v1/findings\") @require_role([\"admin\", \"analyst\", \"viewer\"]) def list_findings(): def authorize(self, action: str, user: User, project: Optional[str] = None) -> bool: \"\"\"Check if user is authorized for action.\"\"\" return user.has_permission(action, project) ```python ## \ud83e\uddf1 Secrets Management ### Secret Types - API tokens for external tools (e.g., Shodan, Vulners) - Private SSH keys for remote scans - Encrypted credentials for authenticated targets ### Storage Backend All secrets are stored in the SecFlow Vault, an encrypted JSON database backed by Fernet (AES-256). ```python class SecretVault: def __init__(self, keyfile: Path): self.key = load_key(keyfile) def store(self, id: str, data: dict): enc = fernet_encrypt(json.dumps(data), self.key) write_file(f\"/vault/{id}.enc\", enc) ```python ### Secrets CLI ```bash SecFlow secrets add nuclei_api --value \"TOKEN123\" SecFlow secrets list SecFlow secrets remove nuclei_api ```yaml All access is scoped by user and project context. ## \ud83d\udd12 Execution Sandboxing All scanner and PoC executions run inside restricted containers or subprocess jails. ### Isolation Techniques | Mechanism | Purpose | |-----------|---------| | Namespaces (PID, NET, MNT) | Process isolation | | Seccomp Filters | Syscall restriction | | cgroups v2 | CPU/memory limits | | No-root UID mapping | Drops privileges | | AppArmor profiles | File access control | | Read-only FS | Prevents persistence | ### Example ```bash docker run --rm --cap-drop=ALL \\ --security-opt=no-new-privileges \\ --read-only -m 512m --cpus=1 \\ SecFlow-runner:latest nuclei -t /templates ```python ## \ud83e\udde9 Network & Data Security | Channel | Encryption | Notes | |---------|------------|-------| | API <-> UI | HTTPS (TLS 1.3) | Strict transport enforced | | Worker <-> API | Mutual TLS | Each worker has its own cert | | File Sync | AES-256 encrypted | Optional compression | | Database | At-rest encryption | SQLite: SEE, Postgres: TDE | | Audit Logs | Signed + timestamped | Prevents tampering | ## \ud83e\udde0 Secure Inter-Process Communication Internal apps communicate via ZeroMQ or Redis queues over TLS. Each message includes a signed envelope: ```json { \"msg_id\": \"uuid\", \"issuer\": \"worker-1\", \"signature\": \"HMAC-SHA256\", \"payload\": {\"data\": \"encrypted_content\"} } ```json This ensures authenticity and non-repudiation. ## \ud83e\udde9 Security Hooks & Middleware SecFlow injects middleware for: - Request validation (pydantic schemas) - JWT token expiry verification - Role validation (fast-path lookup) - Audit log writing after each mutating operation ### Example: ```python @app.middleware(\"http\") async def audit_request(request, call_next): response = await call_next(request) if request.method in (\"POST\", \"DELETE\", \"PATCH\"): log_audit(request, response) return response ```yaml ## \ud83d\udd10 Audit Trail & Tamper Resistance ### Log Format ```json { \"timestamp\": \"2025-10-06T09:40:00Z\", \"user\": \"hernan\", \"action\": \"workflow_run\", \"project\": \"api-audit\", \"status\": \"success\", \"hash\": \"sha256:0a3b1c2d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2\" } ```json ### Storage & Verification - Logs stored as JSON lines under `/audit/` - Each file signed with an HMAC chain: ```python H_i = HMAC(H_prev + log_i, key) ```yaml Immutable and verifiable chain-of-trust. ## \ud83e\udde0 Security Monitoring | Metric | Description | |--------|-------------| | auth_failures_total | Failed login attempts | | sandbox_executions_total | Containers spawned | | policy_violations_total | Unauthorized actions | | vault_accesses_total | Secret retrievals | | audit_events_total | Log entries recorded | ## \u2699\ufe0f Compliance Framework Alignment SecFlow's security architecture aligns with: | Framework | Compliance Area | |-----------|-----------------| | **NIST SP 800-53** | Access control, auditing, system protection | | **ISO/IEC 27001** | Information security management | | **OWASP SAMM** | Secure software development lifecycle | | **MITRE ATT&CK** | Mapping detection behaviors | | **GDPR Art. 32** | Data confidentiality and integrity | ## \ud83d\udd12 Key Rotation & Secrets Expiry - Secrets have explicit TTLs (default: 180 days). - Vault rotation command: ```bash SecFlow vault rotate ```text Rotation regenerates the encryption key and re-encrypts all entries. ## \ud83e\udde9 Example Access Workflow ```text [Analyst] \u2192 [Login via OIDC] \u2192 [JWT Issued] \u2192 [UI/API Gateway] \u2192 [RBAC Policy Check] \u2192 [Worker Executes Workflow in Sandbox] \u2192 [Findings Logged + Audit Entry Created]","title":"Token Model"},{"location":"architecture/16-security-model/#future-enhancements","text":"Hardware-backed encryption (YubiKey / TPM) for Vault keys. FIPS-compliant container sandbox. Behavior-based anomaly detection on audit logs. Single Sign-On (SSO) with just-in-time role provisioning. Runtime policy engine (OPA / Rego integration). Next: Observability, Logging & Metrics","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/17-observability-logging-and-metrics/","text":"17 \u2014 Observability, Logging, Metrics & Tracing \u00b6 \ud83e\udded Overview \u00b6 The Observability Layer provides full transparency into the runtime behavior of SecFlow. It enables developers, analysts, and operators to: - Track system health and performance metrics. - Trace workflow execution across tools and workers. - Monitor logs and alerts from all layers (API, Worker, Sandbox). - Correlate findings, runs, and audit trails with unique identifiers. The observability system is standards-based , built upon: - OpenTelemetry (OTel) for tracing and context propagation. - Prometheus for metrics export. - JSON structured logging for event correlation. - Grafana dashboards for visualization. \ud83e\uddf1 Observability Architecture Overview \u00b6 +--------------------------------------------------------------+ | SecFlow Stack | | API Layer \u2192 Structured Logging, Metrics, Tracing | | Worker \u2192 Task Metrics, Workflow Spans, Sandbox Logs | | Sandbox \u2192 Runtime Metrics, Security Telemetry | | Database \u2192 Query Timing, Connection Pool Stats | -------------------------------------------------------------- | Exporters \u2192 Prometheus (Metrics), OTLP (Traces), Loki (Logs)| +--------------------------------------------------------------+ ```yaml --- ## \ud83e\udde9 Logging Subsystem ### Logging Design Goals | Goal | Implementation | | -------|----------------| | **Machine-readable** | JSON structured format with standard fields | | **Correlated across systems** | `trace_id` and `span_id` included | | **Context-aware** | User, project, workflow metadata embedded | | **Immutable** | Append-only, timestamped, HMAC-signed if required | ### Log Record Example ``` json { \"timestamp\" : \"2025-10-06T09:45:32Z\" , \"level\" : \"INFO\" , \"service\" : \"worker\" , \"trace_id\" : \"cbd82b67-4a2b-4db6-9a90-1c3ed1b7e203\" , \"span_id\" : \"4f7c2b91\" , \"project\" : \"acme-api\" , \"workflow_id\" : \"wf-abc123\" , \"message\" : \"Nuclei scan completed successfully\" , \"duration_ms\" : 34215 } ``` text ### Logging Stack - * *Python Logging + Structlog** \u2014 base structured logs. - * *OpenTelemetry LoggingHandler** \u2014 trace context propagation. - * *Loki Exporter** \u2014 for central log aggregation (optional). ### Configuration snippet: ``` json LOGGING = { \"version\" : 1, \"formatters\" : { \"json\" : { \"()\" : \"pythonjsonlogger.jsonlogger.JsonFormatter\" }} , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , \"formatter\" : \"json\" }, } , \"root\" : { \"level\" : \"INFO\" , \"handlers\" : [ \"console\" ]} , } ``` json ## \u2699\ufe0f Log Levels & Policies | Level | Description | | -------|-------------| | **DEBUG** | Developer-only context, disabled by default in production. | | **INFO** | System lifecycle and status messages. | | **WARNING** | Recoverable issues, retryable errors. | | **ERROR** | Failures in user-triggered operations. | | **CRITICAL** | Irrecoverable errors (sandbox isolation breach, DB corruption). | Retention policy for logs follows the GC subsystem (see [15-garbage-collection-and-retention.md](15-garbage-collection-and-retention.md)). ## \ud83e\udde0 Trace Propagation & Distributed Context SecFlow uses OpenTelemetry (OTel) for distributed tracing. Every API request, task dispatch, and plugin call generates spans linked under one root trace. ### Example Trace Structure ``` text TraceID : 5b2e4f21c9a344f9 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"api.request (GET /api/v1/workflows)\"] B[\"worker.dispatch\"] C[\"tool.nuclei.run\"] D[\"sandbox.spawn\"] E[\"sandbox.cleanup\"] F[\"tool.ferox.run\"] G[\"findings.persist\"] H[\"api.response\"] A --> B A --> H B --> C B --> F B --> G C --> D C --> E Code Example \u00b6 from opentelemetry import trace tracer = trace . get_tracer ( \"SecFlow.worker\" ) with tracer . start_as_current_span ( \"workflow.execute\" ) as span : span . set_attribute ( \"workflow.id\" , workflow . id ) run_workflow ( workflow ) ``` text All traces are exported through OTLP gRPC to the observability backend ( e . g . , Tempo , Jaeger ) . ## \ud83d\udcca Metrics System SecFlow exposes runtime metrics through Prometheus - compatible endpoints . ### Default Endpoint ``` text / metrics ``` text ### Example Metrics | Metric | Type | Description | |--------|------|-------------| | secflow_requests_total | Counter | Total API requests handled | | secflow_active_workflows | Gauge | Currently running workflows | | secflow_findings_generated_total | Counter | Findings created | | secflow_task_duration_seconds | Histogram | Time taken by async tasks | | secflow_gc_bytes_reclaimed_total | Counter | GC reclaimed bytes | | secflow_sandbox_executions_total | Counter | Number of sandbox runs | | secflow_tool_failures_total | Counter | Failed tool executions | | secflow_worker_queue_depth | Gauge | Pending Celery tasks | | secflow_cve_enrichment_latency_seconds | Histogram | Time per CVE query | ### Prometheus Export Example ``` python from prometheus_client import Counter , Gauge findings_total = Counter ( \"secflow_findings_generated_total\" , \"Number of findings created\" ) active_workflows = Gauge ( \"secflow_active_workflows\" , \"Currently running workflows\" ) ``` yaml ## \ud83d\udd0d Example Grafana Dashboard Panels | Panel | Visualization | Query | |-------|---------------|-------| | Workflow Throughput | Time series | ` rate ( secflow_requests_total [ 5 m ]) ` | | Average Scan Duration | Histogram | ` histogram_quantile ( 0.9 , rate ( secflow_task_duration_seconds_bucket [ 5 m ])) ` | | Findings per Project | Bar chart | ` sum by ( project )( secflow_findings_generated_total ) ` | | GC Efficiency | SingleStat | ` rate ( secflow_gc_bytes_reclaimed_total [ 1 h ]) ` | | Sandbox Failures | Table | ` secflow_tool_failures_total ` | ## \ud83e\udde9 Error Correlation & Incident Debugging Every finding , workflow , and audit entry includes a trace ID . Errors can be traced back to exact processes and spans . ### Example correlation: ``` text Finding \u2192 Workflow ID : wf - abc123 \u2192 Trace ID : cbd82b67a1e4f9d2c8b5e6f7a3d4c9e2b1f8a5c6d7e9f2a3b4c5d6e7f8a9b2c3d4e5f6 \u2192 Logs : worker . log \u2192 Span : tool . nuclei . run ``` python This guarantees reproducibility and fast RCA ( root cause analysis ) . ## \ud83e\udde0 Alerting & Health Checks ### Health Endpoints | Endpoint | Description | |----------|-------------| | ` / healthz ` | Liveness probe ( basic app status ) | | ` / readyz ` | Readiness probe ( DB + cache + queue connectivity ) | ### Example Output ``` json { \"status\" : \"ok\" , \"services\" : { \"database\" : \"up\" , \"cache\" : \"up\" , \"worker\" : \"idle\" } } ``` yaml ### Alerts (Prometheus Rules) ``` yaml groups : - name : secflow_alerts rules : - alert : HighErrorRate expr : rate ( secflow_tool_failures_total [ 5 m ]) > 5 for : 10 m labels : { severity : warning } annotations : summary : \"Tool failure rate too high\" ``` python ## \ud83d\udd12 Security of Observability Data | Concern | Mitigation | |---------|------------| | Sensitive logs | Field redaction ( password , token , secret ) | | Trace integrity | HMAC signing of exported spans | | Log tampering | Append - only JSONL + rotation | | Metrics abuse | Authenticated ` / metrics ` endpoint ( basic token or mutual TLS ) | ### Example redaction middleware: ``` python def sanitize ( data : dict ) -> dict : for key in data . keys (): if \"token\" in key . lower () or \"password\" in key . lower (): data [ key ] = \"[REDACTED]\" return data ``` text ## \ud83e\uddf1 Correlation Example: End-to-End Trace ``` bash [ TRACE 5 b2e4f21c9a344f9 ] %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"/api/v1/workflows/start\"] B[\"worker.queue.dispatch (duration=12ms)\"] C[\"tool.ferox.run (duration=4.1s)\"] D[\"tool.nuclei.run (duration=9.3s)\"] E[\"enrich.cve (duration=1.4s)\"] F[\"findings.persist (duration=320ms)\"] G[\"audit.log (duration=7ms)\"] A --> B A --> C A --> D A --> E A --> F A --> G \ud83e\udde9 Integration with CI/CD and Testing \u00b6 During CI runs: - Unit and integration tests export OTel traces for regression analysis. - Performance tests measure task durations and error rates. Example CI configuration: \u00b6 env : OTEL_EXPORTER_OTLP_ENDPOINT : \"http://otel-collector:4317\" PROMETHEUS_MULTIPROC_DIR : \"/tmp/metrics\" \ud83d\udd2e Future Enhancements \u00b6 Distributed tracing for multi-cluster deployments. Real-time log streaming to the web UI. AI-assisted anomaly detection for workflow performance. Adaptive sampling for trace volume reduction. On-demand debug mode via CLI flag ( --trace verbose ). Next: Error Handling & Recovery","title":"Observability"},{"location":"architecture/17-observability-logging-and-metrics/#17-observability-logging-metrics-tracing","text":"","title":"17 \u2014 Observability, Logging, Metrics &amp; Tracing"},{"location":"architecture/17-observability-logging-and-metrics/#overview","text":"The Observability Layer provides full transparency into the runtime behavior of SecFlow. It enables developers, analysts, and operators to: - Track system health and performance metrics. - Trace workflow execution across tools and workers. - Monitor logs and alerts from all layers (API, Worker, Sandbox). - Correlate findings, runs, and audit trails with unique identifiers. The observability system is standards-based , built upon: - OpenTelemetry (OTel) for tracing and context propagation. - Prometheus for metrics export. - JSON structured logging for event correlation. - Grafana dashboards for visualization.","title":"\ud83e\udded Overview"},{"location":"architecture/17-observability-logging-and-metrics/#observability-architecture-overview","text":"+--------------------------------------------------------------+ | SecFlow Stack | | API Layer \u2192 Structured Logging, Metrics, Tracing | | Worker \u2192 Task Metrics, Workflow Spans, Sandbox Logs | | Sandbox \u2192 Runtime Metrics, Security Telemetry | | Database \u2192 Query Timing, Connection Pool Stats | -------------------------------------------------------------- | Exporters \u2192 Prometheus (Metrics), OTLP (Traces), Loki (Logs)| +--------------------------------------------------------------+ ```yaml --- ## \ud83e\udde9 Logging Subsystem ### Logging Design Goals | Goal | Implementation | | -------|----------------| | **Machine-readable** | JSON structured format with standard fields | | **Correlated across systems** | `trace_id` and `span_id` included | | **Context-aware** | User, project, workflow metadata embedded | | **Immutable** | Append-only, timestamped, HMAC-signed if required | ### Log Record Example ``` json { \"timestamp\" : \"2025-10-06T09:45:32Z\" , \"level\" : \"INFO\" , \"service\" : \"worker\" , \"trace_id\" : \"cbd82b67-4a2b-4db6-9a90-1c3ed1b7e203\" , \"span_id\" : \"4f7c2b91\" , \"project\" : \"acme-api\" , \"workflow_id\" : \"wf-abc123\" , \"message\" : \"Nuclei scan completed successfully\" , \"duration_ms\" : 34215 } ``` text ### Logging Stack - * *Python Logging + Structlog** \u2014 base structured logs. - * *OpenTelemetry LoggingHandler** \u2014 trace context propagation. - * *Loki Exporter** \u2014 for central log aggregation (optional). ### Configuration snippet: ``` json LOGGING = { \"version\" : 1, \"formatters\" : { \"json\" : { \"()\" : \"pythonjsonlogger.jsonlogger.JsonFormatter\" }} , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , \"formatter\" : \"json\" }, } , \"root\" : { \"level\" : \"INFO\" , \"handlers\" : [ \"console\" ]} , } ``` json ## \u2699\ufe0f Log Levels & Policies | Level | Description | | -------|-------------| | **DEBUG** | Developer-only context, disabled by default in production. | | **INFO** | System lifecycle and status messages. | | **WARNING** | Recoverable issues, retryable errors. | | **ERROR** | Failures in user-triggered operations. | | **CRITICAL** | Irrecoverable errors (sandbox isolation breach, DB corruption). | Retention policy for logs follows the GC subsystem (see [15-garbage-collection-and-retention.md](15-garbage-collection-and-retention.md)). ## \ud83e\udde0 Trace Propagation & Distributed Context SecFlow uses OpenTelemetry (OTel) for distributed tracing. Every API request, task dispatch, and plugin call generates spans linked under one root trace. ### Example Trace Structure ``` text TraceID : 5b2e4f21c9a344f9 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"api.request (GET /api/v1/workflows)\"] B[\"worker.dispatch\"] C[\"tool.nuclei.run\"] D[\"sandbox.spawn\"] E[\"sandbox.cleanup\"] F[\"tool.ferox.run\"] G[\"findings.persist\"] H[\"api.response\"] A --> B A --> H B --> C B --> F B --> G C --> D C --> E","title":"\ud83e\uddf1 Observability Architecture Overview"},{"location":"architecture/17-observability-logging-and-metrics/#code-example","text":"from opentelemetry import trace tracer = trace . get_tracer ( \"SecFlow.worker\" ) with tracer . start_as_current_span ( \"workflow.execute\" ) as span : span . set_attribute ( \"workflow.id\" , workflow . id ) run_workflow ( workflow ) ``` text All traces are exported through OTLP gRPC to the observability backend ( e . g . , Tempo , Jaeger ) . ## \ud83d\udcca Metrics System SecFlow exposes runtime metrics through Prometheus - compatible endpoints . ### Default Endpoint ``` text / metrics ``` text ### Example Metrics | Metric | Type | Description | |--------|------|-------------| | secflow_requests_total | Counter | Total API requests handled | | secflow_active_workflows | Gauge | Currently running workflows | | secflow_findings_generated_total | Counter | Findings created | | secflow_task_duration_seconds | Histogram | Time taken by async tasks | | secflow_gc_bytes_reclaimed_total | Counter | GC reclaimed bytes | | secflow_sandbox_executions_total | Counter | Number of sandbox runs | | secflow_tool_failures_total | Counter | Failed tool executions | | secflow_worker_queue_depth | Gauge | Pending Celery tasks | | secflow_cve_enrichment_latency_seconds | Histogram | Time per CVE query | ### Prometheus Export Example ``` python from prometheus_client import Counter , Gauge findings_total = Counter ( \"secflow_findings_generated_total\" , \"Number of findings created\" ) active_workflows = Gauge ( \"secflow_active_workflows\" , \"Currently running workflows\" ) ``` yaml ## \ud83d\udd0d Example Grafana Dashboard Panels | Panel | Visualization | Query | |-------|---------------|-------| | Workflow Throughput | Time series | ` rate ( secflow_requests_total [ 5 m ]) ` | | Average Scan Duration | Histogram | ` histogram_quantile ( 0.9 , rate ( secflow_task_duration_seconds_bucket [ 5 m ])) ` | | Findings per Project | Bar chart | ` sum by ( project )( secflow_findings_generated_total ) ` | | GC Efficiency | SingleStat | ` rate ( secflow_gc_bytes_reclaimed_total [ 1 h ]) ` | | Sandbox Failures | Table | ` secflow_tool_failures_total ` | ## \ud83e\udde9 Error Correlation & Incident Debugging Every finding , workflow , and audit entry includes a trace ID . Errors can be traced back to exact processes and spans . ### Example correlation: ``` text Finding \u2192 Workflow ID : wf - abc123 \u2192 Trace ID : cbd82b67a1e4f9d2c8b5e6f7a3d4c9e2b1f8a5c6d7e9f2a3b4c5d6e7f8a9b2c3d4e5f6 \u2192 Logs : worker . log \u2192 Span : tool . nuclei . run ``` python This guarantees reproducibility and fast RCA ( root cause analysis ) . ## \ud83e\udde0 Alerting & Health Checks ### Health Endpoints | Endpoint | Description | |----------|-------------| | ` / healthz ` | Liveness probe ( basic app status ) | | ` / readyz ` | Readiness probe ( DB + cache + queue connectivity ) | ### Example Output ``` json { \"status\" : \"ok\" , \"services\" : { \"database\" : \"up\" , \"cache\" : \"up\" , \"worker\" : \"idle\" } } ``` yaml ### Alerts (Prometheus Rules) ``` yaml groups : - name : secflow_alerts rules : - alert : HighErrorRate expr : rate ( secflow_tool_failures_total [ 5 m ]) > 5 for : 10 m labels : { severity : warning } annotations : summary : \"Tool failure rate too high\" ``` python ## \ud83d\udd12 Security of Observability Data | Concern | Mitigation | |---------|------------| | Sensitive logs | Field redaction ( password , token , secret ) | | Trace integrity | HMAC signing of exported spans | | Log tampering | Append - only JSONL + rotation | | Metrics abuse | Authenticated ` / metrics ` endpoint ( basic token or mutual TLS ) | ### Example redaction middleware: ``` python def sanitize ( data : dict ) -> dict : for key in data . keys (): if \"token\" in key . lower () or \"password\" in key . lower (): data [ key ] = \"[REDACTED]\" return data ``` text ## \ud83e\uddf1 Correlation Example: End-to-End Trace ``` bash [ TRACE 5 b2e4f21c9a344f9 ] %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"/api/v1/workflows/start\"] B[\"worker.queue.dispatch (duration=12ms)\"] C[\"tool.ferox.run (duration=4.1s)\"] D[\"tool.nuclei.run (duration=9.3s)\"] E[\"enrich.cve (duration=1.4s)\"] F[\"findings.persist (duration=320ms)\"] G[\"audit.log (duration=7ms)\"] A --> B A --> C A --> D A --> E A --> F A --> G","title":"Code Example"},{"location":"architecture/17-observability-logging-and-metrics/#integration-with-cicd-and-testing","text":"During CI runs: - Unit and integration tests export OTel traces for regression analysis. - Performance tests measure task durations and error rates.","title":"\ud83e\udde9 Integration with CI/CD and Testing"},{"location":"architecture/17-observability-logging-and-metrics/#example-ci-configuration","text":"env : OTEL_EXPORTER_OTLP_ENDPOINT : \"http://otel-collector:4317\" PROMETHEUS_MULTIPROC_DIR : \"/tmp/metrics\"","title":"Example CI configuration:"},{"location":"architecture/17-observability-logging-and-metrics/#future-enhancements","text":"Distributed tracing for multi-cluster deployments. Real-time log streaming to the web UI. AI-assisted anomaly detection for workflow performance. Adaptive sampling for trace volume reduction. On-demand debug mode via CLI flag ( --trace verbose ). Next: Error Handling & Recovery","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/18-error-handling-and-recovery/","text":"18 \u2014 Error Handling, Fault Tolerance & Recovery Architecture \u00b6 \ud83e\udded Overview \u00b6 SecFlow is designed to operate in unpredictable environments \u2014 remote APIs, external security tools, local sandboxes, and asynchronous task queues. This chapter defines the resilience layer of the platform, ensuring that the system can detect, contain, recover, and self-heal from errors without compromising data integrity or user experience. \ud83e\uddf1 Core Resilience Principles \u00b6 Principle Description Isolation Failures in one tool or workflow should not cascade. Retryability Transient failures should automatically reattempt. Idempotency Repeated executions must produce consistent outcomes. Observability Every error must be traceable with full context. Graceful Degradation System continues partially even under partial failure. \u2699\ufe0f Error Taxonomy \u00b6 Errors are classified to determine handling strategy: Type Example Recovery Strategy Transient Network timeout, rate-limit, temporary unavailability Retry with exponential backoff Permanent Invalid config, missing file, malformed schema Fail fast, log, require user fix External Tool Nuclei crash, Feroxbuster exit code \u2260 0 Capture stdout/stderr, mark node failed Internal Logic Python exception, schema mismatch Rollback transaction, log critical Security Violation Sandbox breakout, unauthorized access Immediate isolation + alert User Abort Manual workflow stop Graceful cancellation, cleanup resources \ud83e\udde9 Error Handling Architecture \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Engine<br/>- Retry Controller<br/>- Error Propagation Manager<br/>- Compensation Handlers<br/>- Dead Letter Queue\"] B[\"Observability & Audit Log\"] A --> B \ud83e\udde0 Exception Handling Model \u00b6 All SecFlow components use a unified exception hierarchy: class SecFlowError ( Exception ): \"\"\"Base class for all SecFlow exceptions.\"\"\" class TransientError ( SecFlowError ): \"\"\"Recoverable error, eligible for retry.\"\"\" class PermanentError ( SecFlowError ): \"\"\"Non-recoverable error, must be logged and halted.\"\"\" class SecurityError ( SecFlowError ): \"\"\"Unauthorized or unsafe action detected.\"\"\" ``` python Every operation that might fail is wrapped in a retry - safe decorator . ## \ud83d\udd01 Retry Logic & Tenacity Integration SecFlow uses the Tenacity library for intelligent retries . ``` python from tenacity import retry , wait_exponential , stop_after_attempt @retry ( wait = wait_exponential ( multiplier = 1 , min = 2 , max = 30 ), stop = stop_after_attempt ( 5 ), retry_error_callback = lambda r : log_error ( r ) ) def run_tool ( tool_name , args ): return subprocess . run ( args , check = True ) ``` python ### Retry Rules | Context | Max Retries | Delay Type | |---------|-------------|------------| | API HTTP Requests | 5 | Exponential | | CVE Enrichment Queries | 3 | Linear | | Worker Tasks | 3 | Exponential | | File System Operations | 2 | Immediate | | PoC Sandbox Launch | 1 | No retry ( for safety ) | ## \ud83e\uddf1 Circuit Breaker Pattern SecFlow prevents repeated failures from overloading systems via circuit breakers . ### Implementation Example ``` python class CircuitBreaker : def __init__ ( self , threshold = 5 , timeout = 60 ): self . failures = 0 self . opened_at = None self . threshold = threshold self . timeout = timeout def record_failure ( self ): self . failures += 1 if self . failures >= self . threshold : self . opened_at = datetime . utcnow () def can_execute ( self ): if not self . opened_at : return True return ( datetime . utcnow () - self . opened_at ) . seconds > self . timeout ``` python Used for : - Remote API ( NVD , OSV , Exploit - DB ) - File I / O saturation - Tool wrappers under repeated crashes ## \ud83e\udde9 Dead Letter Queue (DLQ) Failed tasks that exceed retry limits are pushed into the DLQ for manual review . ``` python @app . task ( bind = True , max_retries = 3 ) def run_scan ( self , task_id ): try : run_workflow ( task_id ) except Exception as e : if self . request . retries == self . max_retries : enqueue_dlq ( task_id , str ( e )) raise self . retry ( exc = e ) ``` text ### DLQ entries include: - Task ID - Workflow ID - Exception message - Retry count - Timestamp ### Example DLQ record: ``` json { \"task\" : \"wf-1234-node-nuclei\" , \"error\" : \"Connection timeout to target\" , \"retries\" : 3 , \"timestamp\" : \"2025-10-06T10:22:00Z\" } ``` text ## \ud83e\udde0 Self-Healing Workflows SecFlow supports automatic node rehydration \u2014 failed steps in a workflow can be restarted independently without restarting the entire pipeline . ### Rehydration Process 1. Detect failed node . 2. Mark upstream outputs as valid . 3. Restart failed node only . 4. Merge results into workflow graph . ### CLI example: ``` bash SecFlow workflow resume -- node nuclei -- project acme - api ``` python ## \ud83e\udde9 Transactional Integrity Database operations are wrapped in ACID transactions using SQLModel context managers : ``` python from sqlmodel import Session def save_finding ( finding ): with Session ( engine ) as session : try : session . add ( finding ) session . commit () except Exception : session . rollback () raise ``` text All cross - project mutations ( findings , triage , cache ) are transactional . ## \ud83e\udde0 Error Event Logging & Correlation Each exception generates an audit entry : ``` json { \"event\" : \"error\" , \"component\" : \"worker\" , \"type\" : \"TransientError\" , \"workflow_id\" : \"wf-abc123\" , \"trace_id\" : \"b73f0b7c-47f9-4bb3-9b9c-ffacbd1d6a67\" , \"message\" : \"Feroxbuster timeout\" , \"retries\" : 3 } ``` yaml Errors are correlated with : - Workflow Trace ID - Finding UUID ( if relevant ) - User and project context This allows full replay and debugging via observability dashboards . ## \u2699\ufe0f Graceful Degradation If a subsystem fails ( e . g . , enrichment API offline ): - Workflows continue with reduced functionality . - Missing data marked as ` \"partial\" : true ` . - Users notified in the triage panel : ``` text \u26a0 CVE enrichment service temporarily unavailable \u2014 retry later . ``` python ## \ud83e\udde9 Alerting & Notification Hooks - Integration with Prometheus Alertmanager for system errors . - Optional Slack / Email webhook for high - severity failures . - Rate - limited notifications to avoid alert fatigue . ### Example alert webhook payload: ``` json { \"severity\" : \"critical\" , \"component\" : \"sandbox\" , \"message\" : \"PoC execution timeout\" , \"project\" : \"api-audit\" , \"trace_id\" : \"a2c134b5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" } ``` python ## \ud83e\uddf1 Recovery Strategies | Context | Recovery Action | |---------|-----------------| | Sandbox crash | Auto - restart with clean container | | API outage | Retry with backoff + circuit breaker | | Tool misconfiguration | Disable tool temporarily , notify user | | Cache corruption | Rebuild from source | | Disk full | Trigger GC and alert | | Worker crash | Celery task re - queued | | DB lock contention | Exponential backoff retry | ## \ud83e\udde0 Example Error Lifecycle ``` text [ Error Detected ] \u2192 [ Retry 1 / 3 ] \u2192 [ Retry 2 / 3 ] \u2192 [ DLQ ] \u2192 [ Alert sent to Slack ] \u2192 [ Analyst re - runs workflow node ] \u2192 [ Recovered ] ``` text ## \ud83d\udd12 Security Implications - Sensitive stack traces are redacted before exposure . - Error details logged internally only . - External responses use generic safe messages : ``` json { \"error\" : \"Internal processing issue, please retry later.\" } \ud83d\udd2e Future Enhancements \u00b6 Adaptive retry policies based on machine learning. AI-powered incident summarization for triage. Transactional outbox pattern for guaranteed task delivery. Fine-grained chaos testing integrated in CI/CD. Next: Risk Assessment & Scoring Framework","title":"Error Handling"},{"location":"architecture/18-error-handling-and-recovery/#18-error-handling-fault-tolerance-recovery-architecture","text":"","title":"18 \u2014 Error Handling, Fault Tolerance &amp; Recovery Architecture"},{"location":"architecture/18-error-handling-and-recovery/#overview","text":"SecFlow is designed to operate in unpredictable environments \u2014 remote APIs, external security tools, local sandboxes, and asynchronous task queues. This chapter defines the resilience layer of the platform, ensuring that the system can detect, contain, recover, and self-heal from errors without compromising data integrity or user experience.","title":"\ud83e\udded Overview"},{"location":"architecture/18-error-handling-and-recovery/#core-resilience-principles","text":"Principle Description Isolation Failures in one tool or workflow should not cascade. Retryability Transient failures should automatically reattempt. Idempotency Repeated executions must produce consistent outcomes. Observability Every error must be traceable with full context. Graceful Degradation System continues partially even under partial failure.","title":"\ud83e\uddf1 Core Resilience Principles"},{"location":"architecture/18-error-handling-and-recovery/#error-taxonomy","text":"Errors are classified to determine handling strategy: Type Example Recovery Strategy Transient Network timeout, rate-limit, temporary unavailability Retry with exponential backoff Permanent Invalid config, missing file, malformed schema Fail fast, log, require user fix External Tool Nuclei crash, Feroxbuster exit code \u2260 0 Capture stdout/stderr, mark node failed Internal Logic Python exception, schema mismatch Rollback transaction, log critical Security Violation Sandbox breakout, unauthorized access Immediate isolation + alert User Abort Manual workflow stop Graceful cancellation, cleanup resources","title":"\u2699\ufe0f Error Taxonomy"},{"location":"architecture/18-error-handling-and-recovery/#error-handling-architecture","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Engine<br/>- Retry Controller<br/>- Error Propagation Manager<br/>- Compensation Handlers<br/>- Dead Letter Queue\"] B[\"Observability & Audit Log\"] A --> B","title":"\ud83e\udde9 Error Handling Architecture"},{"location":"architecture/18-error-handling-and-recovery/#exception-handling-model","text":"All SecFlow components use a unified exception hierarchy: class SecFlowError ( Exception ): \"\"\"Base class for all SecFlow exceptions.\"\"\" class TransientError ( SecFlowError ): \"\"\"Recoverable error, eligible for retry.\"\"\" class PermanentError ( SecFlowError ): \"\"\"Non-recoverable error, must be logged and halted.\"\"\" class SecurityError ( SecFlowError ): \"\"\"Unauthorized or unsafe action detected.\"\"\" ``` python Every operation that might fail is wrapped in a retry - safe decorator . ## \ud83d\udd01 Retry Logic & Tenacity Integration SecFlow uses the Tenacity library for intelligent retries . ``` python from tenacity import retry , wait_exponential , stop_after_attempt @retry ( wait = wait_exponential ( multiplier = 1 , min = 2 , max = 30 ), stop = stop_after_attempt ( 5 ), retry_error_callback = lambda r : log_error ( r ) ) def run_tool ( tool_name , args ): return subprocess . run ( args , check = True ) ``` python ### Retry Rules | Context | Max Retries | Delay Type | |---------|-------------|------------| | API HTTP Requests | 5 | Exponential | | CVE Enrichment Queries | 3 | Linear | | Worker Tasks | 3 | Exponential | | File System Operations | 2 | Immediate | | PoC Sandbox Launch | 1 | No retry ( for safety ) | ## \ud83e\uddf1 Circuit Breaker Pattern SecFlow prevents repeated failures from overloading systems via circuit breakers . ### Implementation Example ``` python class CircuitBreaker : def __init__ ( self , threshold = 5 , timeout = 60 ): self . failures = 0 self . opened_at = None self . threshold = threshold self . timeout = timeout def record_failure ( self ): self . failures += 1 if self . failures >= self . threshold : self . opened_at = datetime . utcnow () def can_execute ( self ): if not self . opened_at : return True return ( datetime . utcnow () - self . opened_at ) . seconds > self . timeout ``` python Used for : - Remote API ( NVD , OSV , Exploit - DB ) - File I / O saturation - Tool wrappers under repeated crashes ## \ud83e\udde9 Dead Letter Queue (DLQ) Failed tasks that exceed retry limits are pushed into the DLQ for manual review . ``` python @app . task ( bind = True , max_retries = 3 ) def run_scan ( self , task_id ): try : run_workflow ( task_id ) except Exception as e : if self . request . retries == self . max_retries : enqueue_dlq ( task_id , str ( e )) raise self . retry ( exc = e ) ``` text ### DLQ entries include: - Task ID - Workflow ID - Exception message - Retry count - Timestamp ### Example DLQ record: ``` json { \"task\" : \"wf-1234-node-nuclei\" , \"error\" : \"Connection timeout to target\" , \"retries\" : 3 , \"timestamp\" : \"2025-10-06T10:22:00Z\" } ``` text ## \ud83e\udde0 Self-Healing Workflows SecFlow supports automatic node rehydration \u2014 failed steps in a workflow can be restarted independently without restarting the entire pipeline . ### Rehydration Process 1. Detect failed node . 2. Mark upstream outputs as valid . 3. Restart failed node only . 4. Merge results into workflow graph . ### CLI example: ``` bash SecFlow workflow resume -- node nuclei -- project acme - api ``` python ## \ud83e\udde9 Transactional Integrity Database operations are wrapped in ACID transactions using SQLModel context managers : ``` python from sqlmodel import Session def save_finding ( finding ): with Session ( engine ) as session : try : session . add ( finding ) session . commit () except Exception : session . rollback () raise ``` text All cross - project mutations ( findings , triage , cache ) are transactional . ## \ud83e\udde0 Error Event Logging & Correlation Each exception generates an audit entry : ``` json { \"event\" : \"error\" , \"component\" : \"worker\" , \"type\" : \"TransientError\" , \"workflow_id\" : \"wf-abc123\" , \"trace_id\" : \"b73f0b7c-47f9-4bb3-9b9c-ffacbd1d6a67\" , \"message\" : \"Feroxbuster timeout\" , \"retries\" : 3 } ``` yaml Errors are correlated with : - Workflow Trace ID - Finding UUID ( if relevant ) - User and project context This allows full replay and debugging via observability dashboards . ## \u2699\ufe0f Graceful Degradation If a subsystem fails ( e . g . , enrichment API offline ): - Workflows continue with reduced functionality . - Missing data marked as ` \"partial\" : true ` . - Users notified in the triage panel : ``` text \u26a0 CVE enrichment service temporarily unavailable \u2014 retry later . ``` python ## \ud83e\udde9 Alerting & Notification Hooks - Integration with Prometheus Alertmanager for system errors . - Optional Slack / Email webhook for high - severity failures . - Rate - limited notifications to avoid alert fatigue . ### Example alert webhook payload: ``` json { \"severity\" : \"critical\" , \"component\" : \"sandbox\" , \"message\" : \"PoC execution timeout\" , \"project\" : \"api-audit\" , \"trace_id\" : \"a2c134b5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" } ``` python ## \ud83e\uddf1 Recovery Strategies | Context | Recovery Action | |---------|-----------------| | Sandbox crash | Auto - restart with clean container | | API outage | Retry with backoff + circuit breaker | | Tool misconfiguration | Disable tool temporarily , notify user | | Cache corruption | Rebuild from source | | Disk full | Trigger GC and alert | | Worker crash | Celery task re - queued | | DB lock contention | Exponential backoff retry | ## \ud83e\udde0 Example Error Lifecycle ``` text [ Error Detected ] \u2192 [ Retry 1 / 3 ] \u2192 [ Retry 2 / 3 ] \u2192 [ DLQ ] \u2192 [ Alert sent to Slack ] \u2192 [ Analyst re - runs workflow node ] \u2192 [ Recovered ] ``` text ## \ud83d\udd12 Security Implications - Sensitive stack traces are redacted before exposure . - Error details logged internally only . - External responses use generic safe messages : ``` json { \"error\" : \"Internal processing issue, please retry later.\" }","title":"\ud83e\udde0 Exception Handling Model"},{"location":"architecture/18-error-handling-and-recovery/#future-enhancements","text":"Adaptive retry policies based on machine learning. AI-powered incident summarization for triage. Transactional outbox pattern for guaranteed task delivery. Fine-grained chaos testing integrated in CI/CD. Next: Risk Assessment & Scoring Framework","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/19-risk-assessment-framework/","text":"19 \u2014 Risk Assessment, Scoring & Prioritization Framework \u00b6 \ud83e\udded Overview \u00b6 The Risk Assessment Framework in SecFlow provides a consistent and transparent way to evaluate and prioritize vulnerabilities across multiple projects and tools. It merges four complementary standards: NIST 5\u00d75 Matrix \u2014 contextual risk evaluation (Impact \u00d7 Likelihood). CVSS v3.1 \u2014 standardized vulnerability severity scoring. CWE / OWASP Mapping \u2014 weakness classification. MITRE ATT&CK Correlation \u2014 adversarial behavior context. This layered model supports both quantitative (numeric scores) and qualitative (High/Critical/etc.) analysis for triage and reporting. \ud83e\uddf1 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"SecFlow Risk Engine<br/>- CVSS Normalizer (from findings or enrichment)<br/>- NIST 5\u00d75 Contextual Matrix<br/>- MITRE ATT&CK Mapper<br/>- Risk Aggregator & Scorer<br/>- Project Risk Dashboard\"] \u2699\ufe0f Core Objectives \u00b6 Goal Description Standardization Consistent risk scoring across tools and projects. Transparency Traceable logic behind every score. Extensibility Pluggable scoring policies (per organization). Automation Auto-scoring during enrichment and triage. Context-Awareness Includes exploitability, exposure, and asset criticality. \ud83e\uddf1 Scoring Pipeline \u00b6 Finding \u2193 CVSS Vector Parsing \u2193 CWE / OWASP Classification \u2193 Exploit & Exposure Context \u2193 NIST Risk Matrix Evaluation \u2193 Final Risk Score (0\u2013100) + Risk Tier ```text --- ## \ud83e\udde0 CVSS Normalization SecFlow ingests CVSS vectors either from NVD (via enrichment) or tool metadata. ### Example: ```text CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H ```text Converted into internal representation: ```json { \"base_score\": 9.8, \"impact_subscore\": 5.9, \"exploitability_subscore\": 3.9, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\" } ```json If missing, heuristic fallback is applied based on CWE ID or OWASP category (see [13-cve-cwe-poc-enrichment-layer.md](13-cve-cwe-poc-enrichment-layer.md)). ## \ud83e\udde9 CWE / OWASP Mapping | CWE ID | OWASP Category | Default Impact | Default Likelihood | |--------|----------------|----------------|-------------------| | 79 | A03: Injection | High | High | | 89 | A03: Injection | Very High | High | | 200 | A01: Broken Access Control | High | Medium | | 601 | A10: SSRF | Medium | Medium | | 787 | A05: Buffer Overflow | Critical | Medium | | 352 | A08: CSRF | Medium | High | Mappings are maintained in `/resources/mappings/cwe_owasp.json`. ## \ud83e\udde0 MITRE ATT&CK Mapping Findings enriched with ATT&CK technique IDs (TIDs) during correlation (see [13-cve-cwe-poc-enrichment-layer.md](13-cve-cwe-poc-enrichment-layer.md)) are leveraged to infer attack chain context. | MITRE Technique ID | Tactic | Effect | |-------------------|--------|--------| | T1059.007 | Execution | Cross-Site Scripting | | T1505.003 | Persistence | SQL Injection | | T1071.001 | Command & Control | Web Protocols | | T1190 | Initial Access | Exploit Public-Facing App | This mapping helps classify whether the finding affects initial compromise, lateral movement, or data exfiltration. ## \u2699\ufe0f NIST 5\u00d75 Risk Matrix ### Definition | Impact \u2193 / Likelihood \u2192 | Very Low | Low | Medium | High | Very High | |------------------------|----------|-----|--------|------|-----------| | **Very High** | Medium | High | High | Critical | Critical | | **High** | Low | Medium | High | High | Critical | | **Medium** | Low | Low | Medium | High | High | | **Low** | Low | Low | Low | Medium | High | | **Very Low** | Low | Low | Low | Low | Medium | ### Mapping to Severity | Result | Score Range | Label | |--------|-------------|-------| | Critical | 90\u2013100 | \ud83d\udd25 | | High | 70\u201389 | \u26a0\ufe0f | | Medium | 40\u201369 | \u2696\ufe0f | | Low | 20\u201339 | \ud83e\udde9 | | Informational | 0\u201319 | \u2139\ufe0f | ## \ud83e\udde9 Likelihood Factors Likelihood is dynamically computed using multiple context sources: | Factor | Description | Weight | |--------|-------------|--------| | Exploit Availability | Known PoC, KEV presence | +0.3 | | Network Exposure | Publicly reachable target | +0.25 | | Authentication Required | Lowers likelihood if true | -0.15 | | Complexity | Tool-derived complexity | \u00b10.1 | | Detection Confidence | Based on finding engine | \u00b10.2 | ### Pseudo-code: ```python def likelihood_score(finding): score = 0.3 if finding.poc_available else 0 if finding.exposure == \"internet\": score += 0.25 if finding.auth_required: score -= 0.15 if finding.complexity == \"low\": score += 0.1 return min(max(score, 0), 1) ```python ## \ud83e\udde0 Impact Factors Impact combines technical and business context: | Factor | Example | Weight | |--------|---------|--------| | Confidentiality | Data exposure | +0.3 | | Integrity | Tampering possible | +0.3 | | Availability | Service crash, DoS | +0.2 | | Privilege Escalation | Root/system access | +0.2 | | Asset Criticality | System importance | +0.4 | Final impact = weighted sum normalized to 1.0. ## \u2699\ufe0f Combined Risk Formula Final quantitative risk score (0\u2013100): ```python risk_score = ((CVSS_base / 10) * 0.6 + impact_factor * 0.25 + likelihood_factor * 0.15) * 100 ```python Rounded to nearest integer. ### Example | Metric | Value | |--------|-------| | CVSS Base | 9.8 | | Impact Factor | 0.8 | | Likelihood Factor | 0.7 | | Final Score | `((0.98*0.6)+(0.8*0.25)+(0.7*0.15))*100 = 89.7` \u2192 **High** | ## \ud83e\udde0 Contextual Adjustments Certain contexts modify final risk score: | Context | Adjustment | |---------|------------| | Active exploit in wild (CISA KEV) | +10 | | Proof-of-concept verified | +5 | | Patched version available | -5 | | Internal-only system | -10 | | Compensating controls present | -15 | Scores are capped at 100 and floored at 0. ## \ud83e\udde9 Aggregated Risk Dashboard Each project's analytics tab visualizes: | Metric | Description | |--------|-------------| | Average CVSS per project | | | Top 10 findings by risk score | | | Risk evolution over time | | | Distribution by OWASP category | | | ATT&CK tactics heatmap | | ### Example chart: ```mermaid %%{init: {\"theme\":\"neutral\"}}%% xychart-beta title \"Risk Trend (Score over Time)\" x-axis [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"] y-axis \"Risk Score\" 0 --> 100 bar [85, 70, 45, 60, 80] \u2699\ufe0f Risk Normalization Across Tools \u00b6 Different tools report different severity models (e.g., \"Critical\", \"High\", \"Medium\"). SecFlow converts all to internal numeric ranges: Tool Critical High Medium Low Nuclei 90\u2013100 70\u201389 40\u201369 20\u201339 ZAP 85\u2013100 65\u201384 35\u201364 15\u201334 Burp 90\u2013100 75\u201389 45\u201374 25\u201344 All are harmonized via the risk formula to produce consistent prioritization. \ud83e\udde0 Risk Aggregation & Reporting \u00b6 Project-level risk is computed as weighted mean: def project_risk ( findings ): weights = [ f . cvss_score * f . impact_weight for f in findings ] return sum ( weights ) / len ( weights ) ``` python Analytics engine stores snapshots in ` / analytics / risk_snapshots / ` . ## \ud83e\udde9 Risk API Endpoints | Endpoint | Method | Description | |----------|--------|-------------| | ` / api / v1 / risk / score / { finding_id } ` | GET | Returns risk vector and classification | | ` / api / v1 / risk / project / { id } ` | GET | Aggregated project risk summary | | ` / api / v1 / risk / export ` | POST | Export risk data to JSON / CSV | | ` / api / v1 / risk / heatmap ` | GET | Generates OWASP \u00d7 ATT & CK matrix | ### Example Response: ``` json { \"finding_id\" : \"abcd-123\" , \"score\" : 89.7 , \"severity\" : \"High\" , \"CVSS\" : 9.8 , \"impact_factor\" : 0.8 , \"likelihood_factor\" : 0.7 , \"nist_matrix\" : \"High/High \u2192 Critical\" , \"owasp\" : \"A03: Injection\" , \"mitre_tid\" : \"T1505.003\" } ``` json ## \ud83d\udd12 Auditability & Traceability Every risk computation is versioned and auditable : - Stored with enrichment metadata hash . - Recomputed automatically if CVSS source data updates . ### Log entry example: ``` json { \"event\" : \"risk_recalc\" , \"finding_id\" : \"abcd-123\" , \"old_score\" : 78 , \"new_score\" : 89.7 , \"reason\" : \"CISA KEV inclusion\" } \ud83d\udd2e Future Enhancements \u00b6 Integration with EPSS (Exploit Prediction Scoring System). ML-based contextual risk forecasting. Auto-adjustment based on exploit telemetry feeds. Risk-driven workflow prioritization for automated scanning. AI-assistant suggestions for mitigations. Next: Migration & Implementation Phases","title":"Risk Assessment"},{"location":"architecture/19-risk-assessment-framework/#19-risk-assessment-scoring-prioritization-framework","text":"","title":"19 \u2014 Risk Assessment, Scoring &amp; Prioritization Framework"},{"location":"architecture/19-risk-assessment-framework/#overview","text":"The Risk Assessment Framework in SecFlow provides a consistent and transparent way to evaluate and prioritize vulnerabilities across multiple projects and tools. It merges four complementary standards: NIST 5\u00d75 Matrix \u2014 contextual risk evaluation (Impact \u00d7 Likelihood). CVSS v3.1 \u2014 standardized vulnerability severity scoring. CWE / OWASP Mapping \u2014 weakness classification. MITRE ATT&CK Correlation \u2014 adversarial behavior context. This layered model supports both quantitative (numeric scores) and qualitative (High/Critical/etc.) analysis for triage and reporting.","title":"\ud83e\udded Overview"},{"location":"architecture/19-risk-assessment-framework/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"SecFlow Risk Engine<br/>- CVSS Normalizer (from findings or enrichment)<br/>- NIST 5\u00d75 Contextual Matrix<br/>- MITRE ATT&CK Mapper<br/>- Risk Aggregator & Scorer<br/>- Project Risk Dashboard\"]","title":"\ud83e\uddf1 Architecture Overview"},{"location":"architecture/19-risk-assessment-framework/#core-objectives","text":"Goal Description Standardization Consistent risk scoring across tools and projects. Transparency Traceable logic behind every score. Extensibility Pluggable scoring policies (per organization). Automation Auto-scoring during enrichment and triage. Context-Awareness Includes exploitability, exposure, and asset criticality.","title":"\u2699\ufe0f Core Objectives"},{"location":"architecture/19-risk-assessment-framework/#scoring-pipeline","text":"Finding \u2193 CVSS Vector Parsing \u2193 CWE / OWASP Classification \u2193 Exploit & Exposure Context \u2193 NIST Risk Matrix Evaluation \u2193 Final Risk Score (0\u2013100) + Risk Tier ```text --- ## \ud83e\udde0 CVSS Normalization SecFlow ingests CVSS vectors either from NVD (via enrichment) or tool metadata. ### Example: ```text CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H ```text Converted into internal representation: ```json { \"base_score\": 9.8, \"impact_subscore\": 5.9, \"exploitability_subscore\": 3.9, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\" } ```json If missing, heuristic fallback is applied based on CWE ID or OWASP category (see [13-cve-cwe-poc-enrichment-layer.md](13-cve-cwe-poc-enrichment-layer.md)). ## \ud83e\udde9 CWE / OWASP Mapping | CWE ID | OWASP Category | Default Impact | Default Likelihood | |--------|----------------|----------------|-------------------| | 79 | A03: Injection | High | High | | 89 | A03: Injection | Very High | High | | 200 | A01: Broken Access Control | High | Medium | | 601 | A10: SSRF | Medium | Medium | | 787 | A05: Buffer Overflow | Critical | Medium | | 352 | A08: CSRF | Medium | High | Mappings are maintained in `/resources/mappings/cwe_owasp.json`. ## \ud83e\udde0 MITRE ATT&CK Mapping Findings enriched with ATT&CK technique IDs (TIDs) during correlation (see [13-cve-cwe-poc-enrichment-layer.md](13-cve-cwe-poc-enrichment-layer.md)) are leveraged to infer attack chain context. | MITRE Technique ID | Tactic | Effect | |-------------------|--------|--------| | T1059.007 | Execution | Cross-Site Scripting | | T1505.003 | Persistence | SQL Injection | | T1071.001 | Command & Control | Web Protocols | | T1190 | Initial Access | Exploit Public-Facing App | This mapping helps classify whether the finding affects initial compromise, lateral movement, or data exfiltration. ## \u2699\ufe0f NIST 5\u00d75 Risk Matrix ### Definition | Impact \u2193 / Likelihood \u2192 | Very Low | Low | Medium | High | Very High | |------------------------|----------|-----|--------|------|-----------| | **Very High** | Medium | High | High | Critical | Critical | | **High** | Low | Medium | High | High | Critical | | **Medium** | Low | Low | Medium | High | High | | **Low** | Low | Low | Low | Medium | High | | **Very Low** | Low | Low | Low | Low | Medium | ### Mapping to Severity | Result | Score Range | Label | |--------|-------------|-------| | Critical | 90\u2013100 | \ud83d\udd25 | | High | 70\u201389 | \u26a0\ufe0f | | Medium | 40\u201369 | \u2696\ufe0f | | Low | 20\u201339 | \ud83e\udde9 | | Informational | 0\u201319 | \u2139\ufe0f | ## \ud83e\udde9 Likelihood Factors Likelihood is dynamically computed using multiple context sources: | Factor | Description | Weight | |--------|-------------|--------| | Exploit Availability | Known PoC, KEV presence | +0.3 | | Network Exposure | Publicly reachable target | +0.25 | | Authentication Required | Lowers likelihood if true | -0.15 | | Complexity | Tool-derived complexity | \u00b10.1 | | Detection Confidence | Based on finding engine | \u00b10.2 | ### Pseudo-code: ```python def likelihood_score(finding): score = 0.3 if finding.poc_available else 0 if finding.exposure == \"internet\": score += 0.25 if finding.auth_required: score -= 0.15 if finding.complexity == \"low\": score += 0.1 return min(max(score, 0), 1) ```python ## \ud83e\udde0 Impact Factors Impact combines technical and business context: | Factor | Example | Weight | |--------|---------|--------| | Confidentiality | Data exposure | +0.3 | | Integrity | Tampering possible | +0.3 | | Availability | Service crash, DoS | +0.2 | | Privilege Escalation | Root/system access | +0.2 | | Asset Criticality | System importance | +0.4 | Final impact = weighted sum normalized to 1.0. ## \u2699\ufe0f Combined Risk Formula Final quantitative risk score (0\u2013100): ```python risk_score = ((CVSS_base / 10) * 0.6 + impact_factor * 0.25 + likelihood_factor * 0.15) * 100 ```python Rounded to nearest integer. ### Example | Metric | Value | |--------|-------| | CVSS Base | 9.8 | | Impact Factor | 0.8 | | Likelihood Factor | 0.7 | | Final Score | `((0.98*0.6)+(0.8*0.25)+(0.7*0.15))*100 = 89.7` \u2192 **High** | ## \ud83e\udde0 Contextual Adjustments Certain contexts modify final risk score: | Context | Adjustment | |---------|------------| | Active exploit in wild (CISA KEV) | +10 | | Proof-of-concept verified | +5 | | Patched version available | -5 | | Internal-only system | -10 | | Compensating controls present | -15 | Scores are capped at 100 and floored at 0. ## \ud83e\udde9 Aggregated Risk Dashboard Each project's analytics tab visualizes: | Metric | Description | |--------|-------------| | Average CVSS per project | | | Top 10 findings by risk score | | | Risk evolution over time | | | Distribution by OWASP category | | | ATT&CK tactics heatmap | | ### Example chart: ```mermaid %%{init: {\"theme\":\"neutral\"}}%% xychart-beta title \"Risk Trend (Score over Time)\" x-axis [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"] y-axis \"Risk Score\" 0 --> 100 bar [85, 70, 45, 60, 80]","title":"\ud83e\uddf1 Scoring Pipeline"},{"location":"architecture/19-risk-assessment-framework/#risk-normalization-across-tools","text":"Different tools report different severity models (e.g., \"Critical\", \"High\", \"Medium\"). SecFlow converts all to internal numeric ranges: Tool Critical High Medium Low Nuclei 90\u2013100 70\u201389 40\u201369 20\u201339 ZAP 85\u2013100 65\u201384 35\u201364 15\u201334 Burp 90\u2013100 75\u201389 45\u201374 25\u201344 All are harmonized via the risk formula to produce consistent prioritization.","title":"\u2699\ufe0f Risk Normalization Across Tools"},{"location":"architecture/19-risk-assessment-framework/#risk-aggregation-reporting","text":"Project-level risk is computed as weighted mean: def project_risk ( findings ): weights = [ f . cvss_score * f . impact_weight for f in findings ] return sum ( weights ) / len ( weights ) ``` python Analytics engine stores snapshots in ` / analytics / risk_snapshots / ` . ## \ud83e\udde9 Risk API Endpoints | Endpoint | Method | Description | |----------|--------|-------------| | ` / api / v1 / risk / score / { finding_id } ` | GET | Returns risk vector and classification | | ` / api / v1 / risk / project / { id } ` | GET | Aggregated project risk summary | | ` / api / v1 / risk / export ` | POST | Export risk data to JSON / CSV | | ` / api / v1 / risk / heatmap ` | GET | Generates OWASP \u00d7 ATT & CK matrix | ### Example Response: ``` json { \"finding_id\" : \"abcd-123\" , \"score\" : 89.7 , \"severity\" : \"High\" , \"CVSS\" : 9.8 , \"impact_factor\" : 0.8 , \"likelihood_factor\" : 0.7 , \"nist_matrix\" : \"High/High \u2192 Critical\" , \"owasp\" : \"A03: Injection\" , \"mitre_tid\" : \"T1505.003\" } ``` json ## \ud83d\udd12 Auditability & Traceability Every risk computation is versioned and auditable : - Stored with enrichment metadata hash . - Recomputed automatically if CVSS source data updates . ### Log entry example: ``` json { \"event\" : \"risk_recalc\" , \"finding_id\" : \"abcd-123\" , \"old_score\" : 78 , \"new_score\" : 89.7 , \"reason\" : \"CISA KEV inclusion\" }","title":"\ud83e\udde0 Risk Aggregation &amp; Reporting"},{"location":"architecture/19-risk-assessment-framework/#future-enhancements","text":"Integration with EPSS (Exploit Prediction Scoring System). ML-based contextual risk forecasting. Auto-adjustment based on exploit telemetry feeds. Risk-driven workflow prioritization for automated scanning. AI-assistant suggestions for mitigations. Next: Migration & Implementation Phases","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/20-migration-and-implementation-phases/","text":"20 \u2014 Migration & Implementation Phases \u00b6 \ud83e\udded Overview \u00b6 The migration plan defines a controlled, four-phase rollout of SecFlow's refactored architecture \u2014 from scaffolding and core model setup to the full orchestration of tools, UI, and enrichment. This plan emphasizes: - Incremental refactoring (no big-bang rewrite) - Zero architectural drift (guardrails first) - Progressive integration (tools, plugins, UI) - Data isolation and cleanup (garbage collector enabled) \ud83d\udce6 Phase 0 \u2014 Foundation & Guardrails (Week 1) \u00b6 Objective \u00b6 Establish the new repository structure and enforce architectural discipline before any migration work. Tasks \u00b6 Create mono-repo scaffold under /src/SecFlow/ Add dev environment setup via Makefile , pyproject.toml , and Poetry Enable static analysis tooling : ruff for linting pyright for typing import-linter for import boundaries Setup unit testing scaffold: /tests/core , /tests/wrappers , /tests/plugins Define .github/workflows/ci.yml for matrix builds (Python 3.10\u20133.12) Establish base docs/architecture/ folder for ongoing documentation Expected Deliverables \u00b6 Working mono-repo with guardrails CI passing with lint/type/test checks Developer guide for environment setup ( docs/dev-setup.md ) Example Command \u00b6 make init make lint make test ``` python ## \ud83e\uddf1 Phase 1 \u2014 Core Models & Data Persistence (Week 2) ### Objective Move fundamental entities ( Projects, Findings, Resources, Runs ) into modular core-lib and storage packages. ### Tasks - Create ` core-lib/ ` package: - Models for Project, Finding, Resource, Run - Pydantic schemas for DTOs - Create ` storage/ ` package: - Database adapters for SQLite ( local ) and PostgreSQL ( production ) - Repository interfaces ( IProjectRepo, IFindingsRepo, etc. ) - Alembic or SQLModel migrations - Implement CRUD API endpoints: - ` /api/v1/projects ` - ` /api/v1/findings ` - ` /api/v1/resources ` - Add test fixtures for sample data ### Expected Deliverables - Persistent data layer - Core models validated by schema - Functional CRUD endpoints - 80 %+ test coverage on models and repos ### Example Model ``` python class Project ( BaseModel ) : id: UUID name: str owner: str description: Optional [ str ] created_at: datetime updated_at: datetime ``` python ## \u2699\ufe0f Phase 2 \u2014 Findings Engine, Wrappers & Workflow (Week 3) ### Objective Integrate scanning tools ( Nuclei, Feroxbuster, Katana, etc. ) into the new workflow engine and plugin registry. ### Tasks - Implement ` findings-engine/ ` : - Normalization layer for all scanners - Parser adapters for each tool - Implement ` wrappers/ ` : - NucleiWrapper, FeroxWrapper, ZAPWrapper - Each using standardized manifest + sandbox - Create ` plugins/ ` package: - Detection and enrichment plugins ( e.g., CVEMapper, RiskScorer ) - Build workflow engine with DAG executor: - YAML recipe parsing - Input/output data mapping - Caching and persistence - Integrate tool registry UI in web frontend ### Expected Deliverables - Tool registry and manifest system - Workflow DAG execution engine - Normalized findings output ( JSON schema compliant ) - Risk engine integration ( Phase 1 of enrichment ) ### Example Wrapper Interface ``` python class ToolWrapper ( Protocol ) : def prepare ( self, config: Dict [ str, Any ]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run ( self ) -> ToolOutput: \"\"\"Execute tool and return output.\"\"\" pass def parse_output ( self, raw: str ) -> List [ Finding ] : \"\"\"Parse raw output into findings.\"\"\" pass ``` python ## \ud83c\udf10 Phase 3 \u2014 API, Worker, and Triage UI (Week 4) ### Objective Deliver full orchestration capability through APIs, background workers, and a lightweight HTMX/React UI. ### Tasks - Create ` web-api/ ` : - REST endpoints for workflows, findings, triage - WebSocket for live updates - Create ` worker/ ` : - Celery/asyncio-based job processor - Queues for workflow nodes and enrichment - Create ` triage-ui/ ` : - Interactive HTMX dashboard for findings triage - Tabs: \"Projects\" , \"Findings\" , \"Tools\" , \"Metrics\" - Implement user auth & RBAC - JWT + role middleware - Add audit logging for all changes - Integrate observability stack ( Prometheus, OpenTelemetry ) ### Expected Deliverables - Full end-to-end scan \u2192 finding \u2192 triage pipeline - Live progress dashboard - Role-based access and logging - Metrics export for dashboards ### Example Endpoint ``` python @app.post ( \"/api/v1/workflows/run\" ) async def run_workflow ( workflow_id: str ) : job_id = await worker.enqueue ( workflow_id ) return { \"status\" : \"queued\" , \"job_id\" : job_id } ``` yaml ## \ud83e\uddf9 Phase 4 \u2014 Garbage Collection, AI, and Advanced Analytics (Week 5+) ### Objective Introduce garbage collection, retention policy enforcement, and AI-assisted triage. ### Tasks - Implement GarbageCollector service: - Sweep orphaned runs and findings - Archive logs >30 days - Introduce CVE/CWE/PoC Enrichment - Integration with NVD, OSV, ExploitDB - Deploy AI assistant for : - Finding summaries - Risk triage automation - Workflow suggestions - Add cross-project analytics dashboard - Implement export formats ( PDF, CSV, JSON ) ### Expected Deliverables - Fully production-ready orchestration platform - Retention-safe data lifecycle - AI triage beta enabled - Analytics module complete ## \ud83d\udcc8 Migration Timeline Overview | Week | Phase | Key Deliverables | | ------ | ------- | ------------------ | | 1 | Phase 0 \u2014 Scaffold | Repo, linting, CI/CD, guardrails | | 2 | Phase 1 \u2014 Core | Models, DB, CRUD API | | 3 | Phase 2 \u2014 Engine | Wrappers, Plugins, Workflow | | 4 | Phase 3 \u2014 API/UI | Worker, Triage UI, Auth | | 5 + | Phase 4 \u2014 AI/GC | Retention, Enrichment, Analytics | ## \ud83d\ude80 Deployment Strategy - Branch-per-phase workflow ( ` feature/phase-1-core ` , etc. ) - Pre-merge CI enforcement for all phases - Feature flags for new modules - Nightly build for cross-validation - Docker-compose dev stack for quick testing ### Example Command ``` bash docker compose up -d pytest --maxfail = 1 --disable-warnings \ud83e\udde0 Key Success Metrics \u00b6 Metric Target Test Coverage >90% for core-lib & storage CI Lint Pass Rate 100% Workflow Execution Latency <300ms/node avg Risk Score Accuracy \u00b15% of reference Mean Time to Deploy <10 min via CI/CD \ud83d\udd2e Next Steps \u00b6 After migration completion: - Freeze legacy JSON-store modules. - Enable CVE/PoC enrichment. - Integrate AI triage assistant. - Conduct internal red-team testing. - Prepare open-source release structure. Next: CI/CD & Testing Strategy","title":"Migration Plan"},{"location":"architecture/20-migration-and-implementation-phases/#20-migration-implementation-phases","text":"","title":"20 \u2014 Migration &amp; Implementation Phases"},{"location":"architecture/20-migration-and-implementation-phases/#overview","text":"The migration plan defines a controlled, four-phase rollout of SecFlow's refactored architecture \u2014 from scaffolding and core model setup to the full orchestration of tools, UI, and enrichment. This plan emphasizes: - Incremental refactoring (no big-bang rewrite) - Zero architectural drift (guardrails first) - Progressive integration (tools, plugins, UI) - Data isolation and cleanup (garbage collector enabled)","title":"\ud83e\udded Overview"},{"location":"architecture/20-migration-and-implementation-phases/#phase-0-foundation-guardrails-week-1","text":"","title":"\ud83d\udce6 Phase 0 \u2014 Foundation &amp; Guardrails (Week 1)"},{"location":"architecture/20-migration-and-implementation-phases/#objective","text":"Establish the new repository structure and enforce architectural discipline before any migration work.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks","text":"Create mono-repo scaffold under /src/SecFlow/ Add dev environment setup via Makefile , pyproject.toml , and Poetry Enable static analysis tooling : ruff for linting pyright for typing import-linter for import boundaries Setup unit testing scaffold: /tests/core , /tests/wrappers , /tests/plugins Define .github/workflows/ci.yml for matrix builds (Python 3.10\u20133.12) Establish base docs/architecture/ folder for ongoing documentation","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables","text":"Working mono-repo with guardrails CI passing with lint/type/test checks Developer guide for environment setup ( docs/dev-setup.md )","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-command","text":"make init make lint make test ``` python ## \ud83e\uddf1 Phase 1 \u2014 Core Models & Data Persistence (Week 2) ### Objective Move fundamental entities ( Projects, Findings, Resources, Runs ) into modular core-lib and storage packages. ### Tasks - Create ` core-lib/ ` package: - Models for Project, Finding, Resource, Run - Pydantic schemas for DTOs - Create ` storage/ ` package: - Database adapters for SQLite ( local ) and PostgreSQL ( production ) - Repository interfaces ( IProjectRepo, IFindingsRepo, etc. ) - Alembic or SQLModel migrations - Implement CRUD API endpoints: - ` /api/v1/projects ` - ` /api/v1/findings ` - ` /api/v1/resources ` - Add test fixtures for sample data ### Expected Deliverables - Persistent data layer - Core models validated by schema - Functional CRUD endpoints - 80 %+ test coverage on models and repos ### Example Model ``` python class Project ( BaseModel ) : id: UUID name: str owner: str description: Optional [ str ] created_at: datetime updated_at: datetime ``` python ## \u2699\ufe0f Phase 2 \u2014 Findings Engine, Wrappers & Workflow (Week 3) ### Objective Integrate scanning tools ( Nuclei, Feroxbuster, Katana, etc. ) into the new workflow engine and plugin registry. ### Tasks - Implement ` findings-engine/ ` : - Normalization layer for all scanners - Parser adapters for each tool - Implement ` wrappers/ ` : - NucleiWrapper, FeroxWrapper, ZAPWrapper - Each using standardized manifest + sandbox - Create ` plugins/ ` package: - Detection and enrichment plugins ( e.g., CVEMapper, RiskScorer ) - Build workflow engine with DAG executor: - YAML recipe parsing - Input/output data mapping - Caching and persistence - Integrate tool registry UI in web frontend ### Expected Deliverables - Tool registry and manifest system - Workflow DAG execution engine - Normalized findings output ( JSON schema compliant ) - Risk engine integration ( Phase 1 of enrichment ) ### Example Wrapper Interface ``` python class ToolWrapper ( Protocol ) : def prepare ( self, config: Dict [ str, Any ]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run ( self ) -> ToolOutput: \"\"\"Execute tool and return output.\"\"\" pass def parse_output ( self, raw: str ) -> List [ Finding ] : \"\"\"Parse raw output into findings.\"\"\" pass ``` python ## \ud83c\udf10 Phase 3 \u2014 API, Worker, and Triage UI (Week 4) ### Objective Deliver full orchestration capability through APIs, background workers, and a lightweight HTMX/React UI. ### Tasks - Create ` web-api/ ` : - REST endpoints for workflows, findings, triage - WebSocket for live updates - Create ` worker/ ` : - Celery/asyncio-based job processor - Queues for workflow nodes and enrichment - Create ` triage-ui/ ` : - Interactive HTMX dashboard for findings triage - Tabs: \"Projects\" , \"Findings\" , \"Tools\" , \"Metrics\" - Implement user auth & RBAC - JWT + role middleware - Add audit logging for all changes - Integrate observability stack ( Prometheus, OpenTelemetry ) ### Expected Deliverables - Full end-to-end scan \u2192 finding \u2192 triage pipeline - Live progress dashboard - Role-based access and logging - Metrics export for dashboards ### Example Endpoint ``` python @app.post ( \"/api/v1/workflows/run\" ) async def run_workflow ( workflow_id: str ) : job_id = await worker.enqueue ( workflow_id ) return { \"status\" : \"queued\" , \"job_id\" : job_id } ``` yaml ## \ud83e\uddf9 Phase 4 \u2014 Garbage Collection, AI, and Advanced Analytics (Week 5+) ### Objective Introduce garbage collection, retention policy enforcement, and AI-assisted triage. ### Tasks - Implement GarbageCollector service: - Sweep orphaned runs and findings - Archive logs >30 days - Introduce CVE/CWE/PoC Enrichment - Integration with NVD, OSV, ExploitDB - Deploy AI assistant for : - Finding summaries - Risk triage automation - Workflow suggestions - Add cross-project analytics dashboard - Implement export formats ( PDF, CSV, JSON ) ### Expected Deliverables - Fully production-ready orchestration platform - Retention-safe data lifecycle - AI triage beta enabled - Analytics module complete ## \ud83d\udcc8 Migration Timeline Overview | Week | Phase | Key Deliverables | | ------ | ------- | ------------------ | | 1 | Phase 0 \u2014 Scaffold | Repo, linting, CI/CD, guardrails | | 2 | Phase 1 \u2014 Core | Models, DB, CRUD API | | 3 | Phase 2 \u2014 Engine | Wrappers, Plugins, Workflow | | 4 | Phase 3 \u2014 API/UI | Worker, Triage UI, Auth | | 5 + | Phase 4 \u2014 AI/GC | Retention, Enrichment, Analytics | ## \ud83d\ude80 Deployment Strategy - Branch-per-phase workflow ( ` feature/phase-1-core ` , etc. ) - Pre-merge CI enforcement for all phases - Feature flags for new modules - Nightly build for cross-validation - Docker-compose dev stack for quick testing ### Example Command ``` bash docker compose up -d pytest --maxfail = 1 --disable-warnings","title":"Example Command"},{"location":"architecture/20-migration-and-implementation-phases/#key-success-metrics","text":"Metric Target Test Coverage >90% for core-lib & storage CI Lint Pass Rate 100% Workflow Execution Latency <300ms/node avg Risk Score Accuracy \u00b15% of reference Mean Time to Deploy <10 min via CI/CD","title":"\ud83e\udde0 Key Success Metrics"},{"location":"architecture/20-migration-and-implementation-phases/#next-steps","text":"After migration completion: - Freeze legacy JSON-store modules. - Enable CVE/PoC enrichment. - Integrate AI triage assistant. - Conduct internal red-team testing. - Prepare open-source release structure. Next: CI/CD & Testing Strategy","title":"\ud83d\udd2e Next Steps"},{"location":"architecture/21-ci-cd-and-testing-strategy/","text":"21 \u2014 CI/CD and Testing Strategy \u00b6 \ud83e\udded Overview \u00b6 The CI/CD and testing strategy ensures SecFlow maintains high reliability, reproducibility, and security compliance across all modules. It defines automated pipelines for build validation, testing, linting, packaging, and deployment to staging and production. SecFlow's architecture enables modular CI pipelines for packages (core-lib, storage, wrappers, plugins) and apps (web-api, worker, triage-ui). \u2699\ufe0f CI/CD Architecture Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Developer\"] B[\"Commit / PR \u2192 GitHub Repository\"] C[\"GitHub Actions CI Workflow<br/>- Lint (Ruff)<br/>- Type Check (Pyright)<br/>- Test (Pytest Matrix)<br/>- Build (Poetry / Docker)\"] D[\"Artifacts Published \u2192 Container Registry\"] E[\"CD Pipeline (Staging \u2192 Production)\"] A --> B B --> C C --> D D --> E \ud83e\uddf1 CI Pipeline Structure \u00b6 Files \u00b6 .github/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"workflows/\"] B[\"ci.yml<br/>(Main build & test pipeline)\"] C[\"lint.yml<br/>(Fast linting PR checks)\"] D[\"deploy.yml<br/>(CD pipeline to staging/prod)\"] E[\"nightly.yml<br/>(Nightly validation builds)\"] F[\"security-scan.yml<br/>(Dependency & container scanning)\"] A --> B A --> C A --> D A --> E A --> F Environments \u00b6 dev \u2192 local or containerized build staging \u2192 auto-deployed for QA validation production \u2192 manual approval required \ud83e\uddea Test Taxonomy \u00b6 Level Scope Example Unit Tests Function-level logic validation Testing CVSS normalization, config parsing Integration Tests Module interoperability NucleiWrapper + FindingsEngine Functional Tests End-to-end system behavior Workflow execution pipeline Regression Tests Legacy feature coverage Old project import/export Performance Tests Latency and scalability Parallel scan runs Security Tests Dependency & vulnerability checks Pip-audit, Trivy \ud83e\udde9 Test Framework Stack \u00b6 Tool Purpose pytest Primary test runner pytest-asyncio Async tests for API and worker pytest-cov Coverage reports tox Matrix execution (Python 3.10\u20133.12) httpx HTTP API test client sqlite-memory Fast ephemeral DB backend for testing faker Generate synthetic test data pytest-docker Integration tests for containerized tools \ud83e\uddf1 Test Folder Structure \u00b6 tests/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"tests/\"] B[\"core/\"] C[\"test_models.py\"] D[\"test_utils.py\"] E[\"test_ports.py\"] F[\"wrappers/\"] G[\"test_nuclei_wrapper.py\"] H[\"test_ferox_wrapper.py\"] I[\"test_zap_wrapper.py\"] J[\"plugins/\"] K[\"test_cve_enrichment.py\"] L[\"test_risk_scoring.py\"] M[\"api/\"] N[\"test_projects_api.py\"] O[\"test_findings_api.py\"] P[\"test_workflow_api.py\"] Q[\"e2e/\"] R[\"test_workflow_dag_execution.py\"] A --> B A --> F A --> J A --> M A --> Q B --> C B --> D B --> E F --> G F --> H F --> I J --> K J --> L M --> N M --> O M --> P Q --> R \ud83e\uddee CI Matrix Configuration Example \u00b6 # .github/workflows/ci.yml name : CI on : push : branches : [ main ] pull_request : branches : [ main ] jobs : build-test : runs-on : ubuntu-latest strategy : matrix : python-version : [ \"3.10\" , \"3.11\" , \"3.12\" ] database : [ \"sqlite\" , \"postgres\" ] steps : - uses : actions/checkout@v4 - uses : actions/setup-python@v5 with : python-version : ${{ matrix.python-version }} - run : pip install poetry - run : poetry install - run : make lint - run : make test DB=${{ matrix.database }} - run : pytest --cov=src --cov-report=xml ``` yaml ## \ud83e\udde0 Deployment Pipeline ### Staging Pipeline (Continuous Deployment) - Triggered on merge to main - Deploys to staging environment automatically - Runs post-deploy smoke tests : - ` /healthz` endpoint - Workflow execution sanity test ### Production Pipeline - Requires manual approval (`workflow_dispatch`) - Signs Docker images before publishing - Deploys to Kubernetes or Docker Swarm cluster - Monitors deployment via Prometheus metrics ### Example job snippet: ``` yaml - name : Deploy to Staging run : | docker-compose -f docker-compose.staging.yml up -d pytest tests/e2e/ --maxfail=1 ``` yaml ## \ud83e\uddf0 Build Artifacts & Packages | Type | Output | Destination | | ------|--------|-------------| | Python Wheels | `dist/*.whl` | PyPI private index | | Docker Images | `SecFlow-api`, `SecFlow-worker` | Container registry | | Reports | `coverage.xml`, `lint.txt`, `typecheck.json` | GitHub artifacts | | Documentation | mkdocs `site/` | GitHub Pages | ## \ud83e\udde0 Quality Gates | Check | Tool | Threshold | | -------|------|-----------| | Linting | Ruff | No errors | | Type Checking | Pyright | 100% coverage | | Test Coverage | Pytest + Coverage | > 90% | | Dependency Scan | Pip-audit / Trivy | 0 Critical | | Build Size | Docker | < 400 MB per image | Failed gates block merges automatically. ## \ud83e\uddea Continuous Security Testing - * *Dependency Auditing:** via pip-audit and Safety - * *Container Scanning:** via Trivy in CI - * *Secrets Detection:** via gitleaks pre-commit hook - * *Infrastructure Scan:** via tfsec (for IaC configs) ``` bash pip install pip-audit safety gitleaks trivy make security-scan ``` text ## \ud83d\udd04 Regression & Replay Testing Each workflow run can be recorded and replayed for regression tests. This ensures stability across version upgrades. ### Example: ``` bash pytest tests/e2e/test_workflow_dag_execution.py --record pytest --replay last-run ``` bash Replay data is stored under `/tests/artifacts/replays/`. ## \ud83e\uddf0 Local Developer Testing Developers can run lightweight tests locally : ``` bash make test pytest -k \"not e2e\" ``` bash With Docker-enabled integration tests : ``` bash make test-docker ``` text ## \ud83d\udcca Metrics & Reporting After each CI build : - Coverage report published to Codecov - Lint/type results annotated in GitHub PR - Performance metrics logged to Prometheus ### Example coverage badge: ``` text [ ![Coverage](https://img.shields.io/badge/coverage-93%25-brightgreen)](https://codecov.io/gh/SecFlow) ``` text ## \ud83e\uddf1 Disaster Recovery & Rollback Every deployment is versioned : - Docker image tags = `vX.Y.Z-buildhash` ### Rollback command : ``` bash docker pull SecFlow-api : v1.3.2 docker compose up -d --no-build Database snapshots every 6h during staging deployment \ud83d\udd2e Future Enhancements \u00b6 Integration with GitHub Advanced Security (code scanning) Dynamic Test Selection (test impacted code only) Chaos Testing on worker queue reliability Parallelized build matrix using GitHub Actions caching Next: Developer Experience & Documentation Plan","title":"CI/CD & Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#21-cicd-and-testing-strategy","text":"","title":"21 \u2014 CI/CD and Testing Strategy"},{"location":"architecture/21-ci-cd-and-testing-strategy/#overview","text":"The CI/CD and testing strategy ensures SecFlow maintains high reliability, reproducibility, and security compliance across all modules. It defines automated pipelines for build validation, testing, linting, packaging, and deployment to staging and production. SecFlow's architecture enables modular CI pipelines for packages (core-lib, storage, wrappers, plugins) and apps (web-api, worker, triage-ui).","title":"\ud83e\udded Overview"},{"location":"architecture/21-ci-cd-and-testing-strategy/#cicd-architecture-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Developer\"] B[\"Commit / PR \u2192 GitHub Repository\"] C[\"GitHub Actions CI Workflow<br/>- Lint (Ruff)<br/>- Type Check (Pyright)<br/>- Test (Pytest Matrix)<br/>- Build (Poetry / Docker)\"] D[\"Artifacts Published \u2192 Container Registry\"] E[\"CD Pipeline (Staging \u2192 Production)\"] A --> B B --> C C --> D D --> E","title":"\u2699\ufe0f CI/CD Architecture Diagram"},{"location":"architecture/21-ci-cd-and-testing-strategy/#ci-pipeline-structure","text":"","title":"\ud83e\uddf1 CI Pipeline Structure"},{"location":"architecture/21-ci-cd-and-testing-strategy/#files","text":".github/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"workflows/\"] B[\"ci.yml<br/>(Main build & test pipeline)\"] C[\"lint.yml<br/>(Fast linting PR checks)\"] D[\"deploy.yml<br/>(CD pipeline to staging/prod)\"] E[\"nightly.yml<br/>(Nightly validation builds)\"] F[\"security-scan.yml<br/>(Dependency & container scanning)\"] A --> B A --> C A --> D A --> E A --> F","title":"Files"},{"location":"architecture/21-ci-cd-and-testing-strategy/#environments","text":"dev \u2192 local or containerized build staging \u2192 auto-deployed for QA validation production \u2192 manual approval required","title":"Environments"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-taxonomy","text":"Level Scope Example Unit Tests Function-level logic validation Testing CVSS normalization, config parsing Integration Tests Module interoperability NucleiWrapper + FindingsEngine Functional Tests End-to-end system behavior Workflow execution pipeline Regression Tests Legacy feature coverage Old project import/export Performance Tests Latency and scalability Parallel scan runs Security Tests Dependency & vulnerability checks Pip-audit, Trivy","title":"\ud83e\uddea Test Taxonomy"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-framework-stack","text":"Tool Purpose pytest Primary test runner pytest-asyncio Async tests for API and worker pytest-cov Coverage reports tox Matrix execution (Python 3.10\u20133.12) httpx HTTP API test client sqlite-memory Fast ephemeral DB backend for testing faker Generate synthetic test data pytest-docker Integration tests for containerized tools","title":"\ud83e\udde9 Test Framework Stack"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-folder-structure","text":"tests/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"tests/\"] B[\"core/\"] C[\"test_models.py\"] D[\"test_utils.py\"] E[\"test_ports.py\"] F[\"wrappers/\"] G[\"test_nuclei_wrapper.py\"] H[\"test_ferox_wrapper.py\"] I[\"test_zap_wrapper.py\"] J[\"plugins/\"] K[\"test_cve_enrichment.py\"] L[\"test_risk_scoring.py\"] M[\"api/\"] N[\"test_projects_api.py\"] O[\"test_findings_api.py\"] P[\"test_workflow_api.py\"] Q[\"e2e/\"] R[\"test_workflow_dag_execution.py\"] A --> B A --> F A --> J A --> M A --> Q B --> C B --> D B --> E F --> G F --> H F --> I J --> K J --> L M --> N M --> O M --> P Q --> R","title":"\ud83e\uddf1 Test Folder Structure"},{"location":"architecture/21-ci-cd-and-testing-strategy/#ci-matrix-configuration-example","text":"# .github/workflows/ci.yml name : CI on : push : branches : [ main ] pull_request : branches : [ main ] jobs : build-test : runs-on : ubuntu-latest strategy : matrix : python-version : [ \"3.10\" , \"3.11\" , \"3.12\" ] database : [ \"sqlite\" , \"postgres\" ] steps : - uses : actions/checkout@v4 - uses : actions/setup-python@v5 with : python-version : ${{ matrix.python-version }} - run : pip install poetry - run : poetry install - run : make lint - run : make test DB=${{ matrix.database }} - run : pytest --cov=src --cov-report=xml ``` yaml ## \ud83e\udde0 Deployment Pipeline ### Staging Pipeline (Continuous Deployment) - Triggered on merge to main - Deploys to staging environment automatically - Runs post-deploy smoke tests : - ` /healthz` endpoint - Workflow execution sanity test ### Production Pipeline - Requires manual approval (`workflow_dispatch`) - Signs Docker images before publishing - Deploys to Kubernetes or Docker Swarm cluster - Monitors deployment via Prometheus metrics ### Example job snippet: ``` yaml - name : Deploy to Staging run : | docker-compose -f docker-compose.staging.yml up -d pytest tests/e2e/ --maxfail=1 ``` yaml ## \ud83e\uddf0 Build Artifacts & Packages | Type | Output | Destination | | ------|--------|-------------| | Python Wheels | `dist/*.whl` | PyPI private index | | Docker Images | `SecFlow-api`, `SecFlow-worker` | Container registry | | Reports | `coverage.xml`, `lint.txt`, `typecheck.json` | GitHub artifacts | | Documentation | mkdocs `site/` | GitHub Pages | ## \ud83e\udde0 Quality Gates | Check | Tool | Threshold | | -------|------|-----------| | Linting | Ruff | No errors | | Type Checking | Pyright | 100% coverage | | Test Coverage | Pytest + Coverage | > 90% | | Dependency Scan | Pip-audit / Trivy | 0 Critical | | Build Size | Docker | < 400 MB per image | Failed gates block merges automatically. ## \ud83e\uddea Continuous Security Testing - * *Dependency Auditing:** via pip-audit and Safety - * *Container Scanning:** via Trivy in CI - * *Secrets Detection:** via gitleaks pre-commit hook - * *Infrastructure Scan:** via tfsec (for IaC configs) ``` bash pip install pip-audit safety gitleaks trivy make security-scan ``` text ## \ud83d\udd04 Regression & Replay Testing Each workflow run can be recorded and replayed for regression tests. This ensures stability across version upgrades. ### Example: ``` bash pytest tests/e2e/test_workflow_dag_execution.py --record pytest --replay last-run ``` bash Replay data is stored under `/tests/artifacts/replays/`. ## \ud83e\uddf0 Local Developer Testing Developers can run lightweight tests locally : ``` bash make test pytest -k \"not e2e\" ``` bash With Docker-enabled integration tests : ``` bash make test-docker ``` text ## \ud83d\udcca Metrics & Reporting After each CI build : - Coverage report published to Codecov - Lint/type results annotated in GitHub PR - Performance metrics logged to Prometheus ### Example coverage badge: ``` text [ ![Coverage](https://img.shields.io/badge/coverage-93%25-brightgreen)](https://codecov.io/gh/SecFlow) ``` text ## \ud83e\uddf1 Disaster Recovery & Rollback Every deployment is versioned : - Docker image tags = `vX.Y.Z-buildhash` ### Rollback command : ``` bash docker pull SecFlow-api : v1.3.2 docker compose up -d --no-build Database snapshots every 6h during staging deployment","title":"\ud83e\uddee CI Matrix Configuration Example"},{"location":"architecture/21-ci-cd-and-testing-strategy/#future-enhancements","text":"Integration with GitHub Advanced Security (code scanning) Dynamic Test Selection (test impacted code only) Chaos Testing on worker queue reliability Parallelized build matrix using GitHub Actions caching Next: Developer Experience & Documentation Plan","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/22-developer-experience-and-docs/","text":"22 \u2014 Developer Experience & Documentation Plan \u00b6 \ud83e\udded Overview \u00b6 SecFlow prioritizes an efficient, reliable, and enjoyable developer experience (DX) that promotes rapid iteration without compromising architectural integrity. This section outlines the local environment setup, development workflow, CLI tools, documentation strategy, and contribution guidelines. \ud83e\udde9 Core DX Principles \u00b6 Principle Description Fast Feedback Every command ( make test , make dev ) provides results in < 5s. Safe by Default Guardrails enforce architectural discipline (ruff, pyright, import-linter). Visible Architecture Auto-generated MkDocs diagrams and interlinked specs. One-Command Onboarding New developers can start coding in minutes. DevOps Parity Local, CI, and production environments behave identically. \u2699\ufe0f Local Environment Setup \u00b6 Prerequisites \u00b6 Python \u22653.11 Poetry Docker + Docker Compose Node.js \u226518 (for triage-ui builds) Setup Commands \u00b6 git clone https://github.com/SecFlow/security-toolkit.git cd security-toolkit make init make up ``` yaml ` make init ` performs: - Poetry virtualenv setup - Dependency installation - Database migration ( SQLite dev DB ) - Git pre-commit hooks ( ruff, pyright, pytest ) - Environment validation ( ` make check ` ) ## \ud83e\uddf1 Developer Makefile Commands | Command | Description | | --------- | ------------- | | ` make up ` | Start local stack ( API, worker, UI ) | | ` make down ` | Stop containers and cleanup | | ` make dev ` | Launch dev server with autoreload | | ` make test ` | Run all tests | | ` make lint ` | Run lint + type check | | ` make docs ` | Build MkDocs documentation | | ` make check ` | Validate dependencies and environment | | ` make clean ` | Remove caches and build artifacts | ### Example: ``` bash make dev # http://localhost:8080 ``` text ## \ud83e\uddf0 Developer CLI \u2014 secflowctl SecFlow provides an integrated command-line interface for developers and operators. ### Example Commands ``` bash secflowctl project list secflowctl scan start nuclei --project mytest secflowctl workflow run owasp-top10.yaml secflowctl plugin list secflowctl risk report --format table ``` text ### CLI Structure ``` mermaid %% { init: { \"theme\" : \"neutral\" }} %% flowchart TD A [ \"secflowctl/\" ] B [ \"__main__.py\" ] C [ \"commands/\" ] D [ \"project.py\" ] E [ \"scan.py\" ] F [ \"workflow.py\" ] G [ \"plugin.py\" ] H [ \"risk.py\" ] I [ \"utils/\" ] J [ \"formatting.py\" ] A --> B A --> C A --> I C --> D C --> E C --> F C --> G C --> H I --> J CLI Design Features \u00b6 Rich TUI (Textual) output for interactive sessions Tab autocompletion JSON/YAML output modes Direct API calls or local orchestration \ud83e\udded Development Workflow \u00b6 Branching Model \u00b6 main \u2192 stable production branch develop \u2192 integration branch feature/* \u2192 new features or refactors fix/* \u2192 bug fixes release/* \u2192 versioned release candidates ```text ### Pull Request Requirements - 1 approving review - All CI checks passed (lint, test, type, security scan) - Linked issue ID - Updated changelog entry ### Commit Style (Conventional Commits) ```text feat(workflow): add nuclei plugin support fix(storage): handle null resource hash docs(readme): update setup instructions ```text ## \ud83d\udcd8 Documentation System (MkDocs) ### MkDocs Project Layout ```bash docs/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/<br/>(Deep technical docs)\"] C[\"api/<br/>(OpenAPI spec & usage)\"] D[\"dev/<br/>(Developer onboarding)\"] E[\"operations/<br/>(Deployment & monitoring)\"] F[\"plugins/<br/>(Plugin development guide)\"] G[\"index.md<br/>(Landing page)\"] A --> B A --> C A --> D A --> E A --> F A --> G Build Command \u00b6 make docs # Builds into site/ ``` bash ### Features - Material for MkDocs theme - Auto-generated architecture diagrams via Mermaid - Built-in search and code highlighting - Versioned docs ( via mike ) for each release - Plugin-based navigation for \"core\" , \"apps\" , \"plugins\" , \"API\" ### Example mkdocs.yml: ``` yaml site_name: \"SecFlow Developer Docs\" theme: name: material features: - navigation.sections - navigation.instant markdown_extensions: - toc: permalink: true - admonition - pymdownx.superfences plugins: - search - mermaid2 - awesome-pages ``` yaml ## \ud83e\udde0 Architecture Visualization Architecture diagrams are auto-generated from the codebase using diagrams + pydeps. ### Example script: ``` bash make diagram ``` yaml Output: ` /docs/architecture/assets/architecture.svg ` ### Example generated image (ASCII simplified): ``` yaml +-------------+ | web-api | +------+------+----+ | | +-------v--+ +----v--------+ | worker | | triage-ui | +----------+ +-------------+ | | +----v---------v----+ | core-lib / engine | +--------------------+ ``` yaml ## \ud83e\udde9 Developer Onboarding Flow | Step | Description | | ------ | ------------- | | 1 . Clone Repository | ` git clone ` and ` make init ` | | 2 . Run Local Stack | ` make up ` \u2192 visit localhost:8080 | | 3 . Explore CLI | ` secflowctl help ` | | 4 . Read Docs | ` make docs ` \u2192 open site/index.html | | 5 . Add Feature | Create ` feature/my-feature ` branch | | 6 . Submit PR | Push to GitHub, run CI, get review | | 7 . Merge & Deploy | Auto-deployed to staging | ## \ud83e\uddf0 Tooling Summary | Category | Tool | Purpose | | ---------- | ------ | --------- | | Package Management | Poetry | Dependency control | | Linting | Ruff | Code style & hygiene | | Typing | Pyright | Static type enforcement | | Testing | Pytest | Unit & integration tests | | Docs | MkDocs | Documentation | | Visualization | Diagrams | Auto-generate architecture maps | | Security | Gitleaks, Safety | Prevent secrets & vulns | | Formatting | Black | Consistent code format | ## \ud83e\udde9 Developer Guidelines ### Code Style - Follow PEP8 + Ruff config - Enforce docstrings for public classes/functions - Avoid circular imports ( use ports ) - Use dependency injection where possible ### Commit Rules - Keep commits atomic ( 1 logical change ) - Use descriptive messages - Reference related issue ( #123) ### Code Review Expectations - Small PRs ( < 500 LOC preferred ) - Include before/after screenshots for UI changes - Add unit tests for every new feature ## \ud83e\udde0 Local Testing Shortcuts | Scenario | Command | | ---------- | --------- | | Run single test | ` pytest tests/core/test_models.py::test_project_model ` | | Run tests with coverage | ` pytest --cov = src --cov-report = html ` | | Run async API tests | ` pytest tests/api -k \"async\" ` | | Skip slow tests | ` pytest -m \"not slow\" ` | | Lint before commit | ` pre-commit run --all-files ` | ## \ud83d\udcd8 Developer Documentation Contributions Docs are written in Markdown under ` docs/ ` ### Always include: - Code examples - Usage samples - Config references ### Build locally via: ``` bash mkdocs serve ``` bash ### For architecture updates: ``` bash make diagram && make docs \ud83d\udd2e Future DX Enhancements \u00b6 VS Code Dev Containers for instant onboarding CLI Autoupdate System for secflowctl MkDocs AI Search Plugin (semantic search) Interactive Architecture Map (Mermaid + Live API) Unified Dev Dashboard combining logs, metrics, and CI build state Next: Future Roadmap","title":"Dev Experience & Docs"},{"location":"architecture/22-developer-experience-and-docs/#22-developer-experience-documentation-plan","text":"","title":"22 \u2014 Developer Experience &amp; Documentation Plan"},{"location":"architecture/22-developer-experience-and-docs/#overview","text":"SecFlow prioritizes an efficient, reliable, and enjoyable developer experience (DX) that promotes rapid iteration without compromising architectural integrity. This section outlines the local environment setup, development workflow, CLI tools, documentation strategy, and contribution guidelines.","title":"\ud83e\udded Overview"},{"location":"architecture/22-developer-experience-and-docs/#core-dx-principles","text":"Principle Description Fast Feedback Every command ( make test , make dev ) provides results in < 5s. Safe by Default Guardrails enforce architectural discipline (ruff, pyright, import-linter). Visible Architecture Auto-generated MkDocs diagrams and interlinked specs. One-Command Onboarding New developers can start coding in minutes. DevOps Parity Local, CI, and production environments behave identically.","title":"\ud83e\udde9 Core DX Principles"},{"location":"architecture/22-developer-experience-and-docs/#local-environment-setup","text":"","title":"\u2699\ufe0f Local Environment Setup"},{"location":"architecture/22-developer-experience-and-docs/#prerequisites","text":"Python \u22653.11 Poetry Docker + Docker Compose Node.js \u226518 (for triage-ui builds)","title":"Prerequisites"},{"location":"architecture/22-developer-experience-and-docs/#setup-commands","text":"git clone https://github.com/SecFlow/security-toolkit.git cd security-toolkit make init make up ``` yaml ` make init ` performs: - Poetry virtualenv setup - Dependency installation - Database migration ( SQLite dev DB ) - Git pre-commit hooks ( ruff, pyright, pytest ) - Environment validation ( ` make check ` ) ## \ud83e\uddf1 Developer Makefile Commands | Command | Description | | --------- | ------------- | | ` make up ` | Start local stack ( API, worker, UI ) | | ` make down ` | Stop containers and cleanup | | ` make dev ` | Launch dev server with autoreload | | ` make test ` | Run all tests | | ` make lint ` | Run lint + type check | | ` make docs ` | Build MkDocs documentation | | ` make check ` | Validate dependencies and environment | | ` make clean ` | Remove caches and build artifacts | ### Example: ``` bash make dev # http://localhost:8080 ``` text ## \ud83e\uddf0 Developer CLI \u2014 secflowctl SecFlow provides an integrated command-line interface for developers and operators. ### Example Commands ``` bash secflowctl project list secflowctl scan start nuclei --project mytest secflowctl workflow run owasp-top10.yaml secflowctl plugin list secflowctl risk report --format table ``` text ### CLI Structure ``` mermaid %% { init: { \"theme\" : \"neutral\" }} %% flowchart TD A [ \"secflowctl/\" ] B [ \"__main__.py\" ] C [ \"commands/\" ] D [ \"project.py\" ] E [ \"scan.py\" ] F [ \"workflow.py\" ] G [ \"plugin.py\" ] H [ \"risk.py\" ] I [ \"utils/\" ] J [ \"formatting.py\" ] A --> B A --> C A --> I C --> D C --> E C --> F C --> G C --> H I --> J","title":"Setup Commands"},{"location":"architecture/22-developer-experience-and-docs/#cli-design-features","text":"Rich TUI (Textual) output for interactive sessions Tab autocompletion JSON/YAML output modes Direct API calls or local orchestration","title":"CLI Design Features"},{"location":"architecture/22-developer-experience-and-docs/#development-workflow","text":"","title":"\ud83e\udded Development Workflow"},{"location":"architecture/22-developer-experience-and-docs/#branching-model","text":"main \u2192 stable production branch develop \u2192 integration branch feature/* \u2192 new features or refactors fix/* \u2192 bug fixes release/* \u2192 versioned release candidates ```text ### Pull Request Requirements - 1 approving review - All CI checks passed (lint, test, type, security scan) - Linked issue ID - Updated changelog entry ### Commit Style (Conventional Commits) ```text feat(workflow): add nuclei plugin support fix(storage): handle null resource hash docs(readme): update setup instructions ```text ## \ud83d\udcd8 Documentation System (MkDocs) ### MkDocs Project Layout ```bash docs/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/<br/>(Deep technical docs)\"] C[\"api/<br/>(OpenAPI spec & usage)\"] D[\"dev/<br/>(Developer onboarding)\"] E[\"operations/<br/>(Deployment & monitoring)\"] F[\"plugins/<br/>(Plugin development guide)\"] G[\"index.md<br/>(Landing page)\"] A --> B A --> C A --> D A --> E A --> F A --> G","title":"Branching Model"},{"location":"architecture/22-developer-experience-and-docs/#build-command","text":"make docs # Builds into site/ ``` bash ### Features - Material for MkDocs theme - Auto-generated architecture diagrams via Mermaid - Built-in search and code highlighting - Versioned docs ( via mike ) for each release - Plugin-based navigation for \"core\" , \"apps\" , \"plugins\" , \"API\" ### Example mkdocs.yml: ``` yaml site_name: \"SecFlow Developer Docs\" theme: name: material features: - navigation.sections - navigation.instant markdown_extensions: - toc: permalink: true - admonition - pymdownx.superfences plugins: - search - mermaid2 - awesome-pages ``` yaml ## \ud83e\udde0 Architecture Visualization Architecture diagrams are auto-generated from the codebase using diagrams + pydeps. ### Example script: ``` bash make diagram ``` yaml Output: ` /docs/architecture/assets/architecture.svg ` ### Example generated image (ASCII simplified): ``` yaml +-------------+ | web-api | +------+------+----+ | | +-------v--+ +----v--------+ | worker | | triage-ui | +----------+ +-------------+ | | +----v---------v----+ | core-lib / engine | +--------------------+ ``` yaml ## \ud83e\udde9 Developer Onboarding Flow | Step | Description | | ------ | ------------- | | 1 . Clone Repository | ` git clone ` and ` make init ` | | 2 . Run Local Stack | ` make up ` \u2192 visit localhost:8080 | | 3 . Explore CLI | ` secflowctl help ` | | 4 . Read Docs | ` make docs ` \u2192 open site/index.html | | 5 . Add Feature | Create ` feature/my-feature ` branch | | 6 . Submit PR | Push to GitHub, run CI, get review | | 7 . Merge & Deploy | Auto-deployed to staging | ## \ud83e\uddf0 Tooling Summary | Category | Tool | Purpose | | ---------- | ------ | --------- | | Package Management | Poetry | Dependency control | | Linting | Ruff | Code style & hygiene | | Typing | Pyright | Static type enforcement | | Testing | Pytest | Unit & integration tests | | Docs | MkDocs | Documentation | | Visualization | Diagrams | Auto-generate architecture maps | | Security | Gitleaks, Safety | Prevent secrets & vulns | | Formatting | Black | Consistent code format | ## \ud83e\udde9 Developer Guidelines ### Code Style - Follow PEP8 + Ruff config - Enforce docstrings for public classes/functions - Avoid circular imports ( use ports ) - Use dependency injection where possible ### Commit Rules - Keep commits atomic ( 1 logical change ) - Use descriptive messages - Reference related issue ( #123) ### Code Review Expectations - Small PRs ( < 500 LOC preferred ) - Include before/after screenshots for UI changes - Add unit tests for every new feature ## \ud83e\udde0 Local Testing Shortcuts | Scenario | Command | | ---------- | --------- | | Run single test | ` pytest tests/core/test_models.py::test_project_model ` | | Run tests with coverage | ` pytest --cov = src --cov-report = html ` | | Run async API tests | ` pytest tests/api -k \"async\" ` | | Skip slow tests | ` pytest -m \"not slow\" ` | | Lint before commit | ` pre-commit run --all-files ` | ## \ud83d\udcd8 Developer Documentation Contributions Docs are written in Markdown under ` docs/ ` ### Always include: - Code examples - Usage samples - Config references ### Build locally via: ``` bash mkdocs serve ``` bash ### For architecture updates: ``` bash make diagram && make docs","title":"Build Command"},{"location":"architecture/22-developer-experience-and-docs/#future-dx-enhancements","text":"VS Code Dev Containers for instant onboarding CLI Autoupdate System for secflowctl MkDocs AI Search Plugin (semantic search) Interactive Architecture Map (Mermaid + Live API) Unified Dev Dashboard combining logs, metrics, and CI build state Next: Future Roadmap","title":"\ud83d\udd2e Future DX Enhancements"},{"location":"architecture/23-future-roadmap/","text":"23 \u2014 Future Roadmap & Evolution Strategy \u00b6 \ud83e\udded Overview \u00b6 This roadmap defines the strategic evolution of SecFlow from a modular pentesting orchestration toolkit to a next-generation autonomous security platform . It describes the planned capabilities, milestones, and technical direction for the next 12\u201324 months \u2014 integrating AI, multi-tenant architectures, collaborative triage, and intelligent automation. \ud83e\uddf1 Phase Overview (Long-Term Vision) \u00b6 Phase Focus Area Description Phase 1 (Current) Foundation Mono-repo, hexagonal core, plugin registry, workflow orchestration Phase 2 (6\u201312 months) Intelligence & AI AI triage, enrichment automation, anomaly detection Phase 3 (12\u201318 months) Collaboration Multi-tenant architecture, role-based analytics, red/blue team integration Phase 4 (18\u201324 months) Autonomy Self-optimizing workflows, adaptive scanners, predictive risk modeling \u2699\ufe0f Phase 2 \u2014 Intelligence & AI Integration \u00b6 The AI Integration Phase introduces cognitive automation into all major subsystems of SecFlow. 1. AI-Assisted Triage \u00b6 LLM-based summarization of findings Context inference (vulnerability relevance, duplication detection) Recommendation engine for next actions and prioritization 2. AI-Driven Risk Enrichment \u00b6 Predict missing CVSS scores and exploit likelihood (ML regression models) Ingest external threat feeds (CISA KEV, ExploitDB updates) Leverage EPSS (Exploit Prediction Scoring System) for dynamic risk adjustments 3. Automated Pattern Discovery \u00b6 Cluster recurring issues across multiple projects Identify \"weak controls\" using unsupervised learning Integrate MITRE ATT&CK correlation heatmaps Example architecture: \u00b6 +--------------------------------------------------+ | AI Engine | | - LLM-based Triage Assistant | | - ML Risk Predictor (EPSS + CVSS Hybrid) | | - Anomaly Detector (Outlier Findings) | | - Natural Language Query Interface | +--------------------------------------------------+ ```text ### 4. Conversational Analysis Interface A secure chat layer connected to `core-lib` enabling queries like : ``` text show me all high-risk CVEs in projects using nginx summarize findings related to broken authentication predict which components will likely fail next pentest \ud83c\udf10 Phase 3 \u2014 Collaboration & Multi-Tenant Platform \u00b6 This phase focuses on scaling SecFlow for enterprise teams, MSSPs, and collaborative pentest environments. 1. Multi-Tenant Architecture \u00b6 Isolated tenants with shared infrastructure RBAC-based access and audit segregation Namespace-aware project storage ( tenant_id/project_id pattern) 2. Centralized Insights Hub \u00b6 Aggregate findings and metrics across tenants Role-based dashboards (CISO, Pentester, Developer) Global intelligence layer \u2014 vulnerability trends, exploit timelines 3. Cross-Project Correlation \u00b6 Federated search across tenants Shared resource registry (optional per policy) Vulnerability fingerprinting and reuse detection 4. Collaboration Tools \u00b6 Comment threads and annotations on findings Assignments and status tracking Shared remediation checklists \ud83e\udd16 Phase 4 \u2014 Autonomous Security Orchestration \u00b6 The final stage evolves SecFlow into an autonomous orchestration and reasoning engine . 1. Adaptive Scanning \u00b6 Automatically adjusts scanning parameters based on previous findings Integrates feedback loop from risk trends Uses Reinforcement Learning (RL) to optimize coverage vs time 2. Self-Healing Workflows \u00b6 Automatically retries failed nodes Re-schedules scans based on impact or SLA breach Learns ideal tool chaining patterns 3. Predictive Risk Forecasting \u00b6 Time-series models predicting vulnerability resurgence Integration with incident response systems for proactive alerts 4. Security Knowledge Graph \u00b6 Unifies all findings, CVEs, CWEs, ATT&CK tactics, and tool metadata Graph queries (Cypher/SPARQL) for advanced relationships Enables semantic enrichment and AI reasoning \ud83e\uddf0 Supporting Infrastructure Evolution \u00b6 Subsystem Upgrade Path Description Database Postgres \u2192 TimescaleDB Time-series analytics and trend computation Cache Layer Redis \u2192 Redis Cluster High-throughput async workflows Task Queue Celery \u2192 Prefect 3 Enhanced DAG orchestration and observability API Gateway FastAPI \u2192 APIStar (optional microgateway) Modular service boundaries UI Stack HTMX \u2192 React + GraphQL Hybrid Real-time triage dashboards \ud83e\udde0 Advanced Integrations \u00b6 Integration Purpose Caido & ZAP Direct import of active scan results GitHub Advanced Security Repo-based vulnerability correlation ExploitDB / Vulners API Continuous POC enrichment CISA KEV Sync Real-time critical vulnerability flagging AI Plugin System Extensible via OpenAI function-style agents \ud83d\udcca Planned Metrics & Analytics Expansion \u00b6 Area Metric Use Case Risk Intelligence Mean CVSS delta over time Track improvement rate Workflow Analytics Avg. scan node latency Optimize orchestration Tool Effectiveness Finding yield per tool ROI assessment Anomaly Detection Outlier frequency Identify misbehaving scans Team Collaboration Comment-to-remediation ratio Operational efficiency \ud83d\udd12 Future Security Enhancements \u00b6 Hardware attestation for sensitive tool execution (YubiHSM/TPM) Encrypted local storage using Fernet or AWS KMS Multi-factor API access tokens Zero-trust plugin sandboxing (seccomp profiles) Supply chain integrity checks (sigstore/cosign) \ud83c\udf0d Open Source & Community Roadmap \u00b6 SecFlow will release a Community Edition (CE) focusing on: - Lightweight orchestration - Built-in nuclei/ferox wrappers - Project & finding dashboards - Local SQLite backend Planned for mid-2026 under an Apache 2.0 license with optional enterprise modules. Community contributions roadmap: \u00b6 Plugin SDK Workflow Recipe Gallery Integration templates (for Burp, ZAP, Caido) AI-Triage contribution guide \ud83d\udcc5 Timeline Summary \u00b6 Quarter Milestone Highlights Q4 2025 Phase 1 \u2013 Finalize core platform API + Worker stable release Q1 2026 Phase 2 \u2013 AI & enrichment rollout LLM triage + CVE prediction Q2 2026 Phase 3 \u2013 Multi-tenant & collaboration Team dashboards, tenant isolation Q3 2026 Phase 4 \u2013 Autonomous orchestration Adaptive scans, AI reasoning graph Q4 2026 Community Edition release OSS launch, plugin marketplace \ud83e\udde9 Success Metrics & KPIs \u00b6 KPI Target Workflow execution success rate \u2265 98% Mean enrichment latency < 500 ms AI triage accuracy \u2265 90% vs manual MTTR (Mean Time to Risk Report) < 10 min Plugin SDK adoption 50+ community plugins Platform uptime (SLA) 99.9% \ud83d\udd2e Long-Term Vision \u00b6 \"From manual scanning to autonomous, continuous, and intelligent security operations.\" SecFlow's long-term mission is to become the AI-native, self-orchestrating red-team assistant , capable of: - Running its own scan campaigns, - Adjusting priorities dynamically, - Explaining vulnerabilities in human language, - Suggesting fixes, - And predicting attack surfaces before exploitation occurs. Next: Final Consensus Summary","title":"Future Roadmap"},{"location":"architecture/23-future-roadmap/#23-future-roadmap-evolution-strategy","text":"","title":"23 \u2014 Future Roadmap &amp; Evolution Strategy"},{"location":"architecture/23-future-roadmap/#overview","text":"This roadmap defines the strategic evolution of SecFlow from a modular pentesting orchestration toolkit to a next-generation autonomous security platform . It describes the planned capabilities, milestones, and technical direction for the next 12\u201324 months \u2014 integrating AI, multi-tenant architectures, collaborative triage, and intelligent automation.","title":"\ud83e\udded Overview"},{"location":"architecture/23-future-roadmap/#phase-overview-long-term-vision","text":"Phase Focus Area Description Phase 1 (Current) Foundation Mono-repo, hexagonal core, plugin registry, workflow orchestration Phase 2 (6\u201312 months) Intelligence & AI AI triage, enrichment automation, anomaly detection Phase 3 (12\u201318 months) Collaboration Multi-tenant architecture, role-based analytics, red/blue team integration Phase 4 (18\u201324 months) Autonomy Self-optimizing workflows, adaptive scanners, predictive risk modeling","title":"\ud83e\uddf1 Phase Overview (Long-Term Vision)"},{"location":"architecture/23-future-roadmap/#phase-2-intelligence-ai-integration","text":"The AI Integration Phase introduces cognitive automation into all major subsystems of SecFlow.","title":"\u2699\ufe0f Phase 2 \u2014 Intelligence &amp; AI Integration"},{"location":"architecture/23-future-roadmap/#1-ai-assisted-triage","text":"LLM-based summarization of findings Context inference (vulnerability relevance, duplication detection) Recommendation engine for next actions and prioritization","title":"1. AI-Assisted Triage"},{"location":"architecture/23-future-roadmap/#2-ai-driven-risk-enrichment","text":"Predict missing CVSS scores and exploit likelihood (ML regression models) Ingest external threat feeds (CISA KEV, ExploitDB updates) Leverage EPSS (Exploit Prediction Scoring System) for dynamic risk adjustments","title":"2. AI-Driven Risk Enrichment"},{"location":"architecture/23-future-roadmap/#3-automated-pattern-discovery","text":"Cluster recurring issues across multiple projects Identify \"weak controls\" using unsupervised learning Integrate MITRE ATT&CK correlation heatmaps","title":"3. Automated Pattern Discovery"},{"location":"architecture/23-future-roadmap/#example-architecture","text":"+--------------------------------------------------+ | AI Engine | | - LLM-based Triage Assistant | | - ML Risk Predictor (EPSS + CVSS Hybrid) | | - Anomaly Detector (Outlier Findings) | | - Natural Language Query Interface | +--------------------------------------------------+ ```text ### 4. Conversational Analysis Interface A secure chat layer connected to `core-lib` enabling queries like : ``` text show me all high-risk CVEs in projects using nginx summarize findings related to broken authentication predict which components will likely fail next pentest","title":"Example architecture:"},{"location":"architecture/23-future-roadmap/#phase-3-collaboration-multi-tenant-platform","text":"This phase focuses on scaling SecFlow for enterprise teams, MSSPs, and collaborative pentest environments.","title":"\ud83c\udf10 Phase 3 \u2014 Collaboration &amp; Multi-Tenant Platform"},{"location":"architecture/23-future-roadmap/#1-multi-tenant-architecture","text":"Isolated tenants with shared infrastructure RBAC-based access and audit segregation Namespace-aware project storage ( tenant_id/project_id pattern)","title":"1. Multi-Tenant Architecture"},{"location":"architecture/23-future-roadmap/#2-centralized-insights-hub","text":"Aggregate findings and metrics across tenants Role-based dashboards (CISO, Pentester, Developer) Global intelligence layer \u2014 vulnerability trends, exploit timelines","title":"2. Centralized Insights Hub"},{"location":"architecture/23-future-roadmap/#3-cross-project-correlation","text":"Federated search across tenants Shared resource registry (optional per policy) Vulnerability fingerprinting and reuse detection","title":"3. Cross-Project Correlation"},{"location":"architecture/23-future-roadmap/#4-collaboration-tools","text":"Comment threads and annotations on findings Assignments and status tracking Shared remediation checklists","title":"4. Collaboration Tools"},{"location":"architecture/23-future-roadmap/#phase-4-autonomous-security-orchestration","text":"The final stage evolves SecFlow into an autonomous orchestration and reasoning engine .","title":"\ud83e\udd16 Phase 4 \u2014 Autonomous Security Orchestration"},{"location":"architecture/23-future-roadmap/#1-adaptive-scanning","text":"Automatically adjusts scanning parameters based on previous findings Integrates feedback loop from risk trends Uses Reinforcement Learning (RL) to optimize coverage vs time","title":"1. Adaptive Scanning"},{"location":"architecture/23-future-roadmap/#2-self-healing-workflows","text":"Automatically retries failed nodes Re-schedules scans based on impact or SLA breach Learns ideal tool chaining patterns","title":"2. Self-Healing Workflows"},{"location":"architecture/23-future-roadmap/#3-predictive-risk-forecasting","text":"Time-series models predicting vulnerability resurgence Integration with incident response systems for proactive alerts","title":"3. Predictive Risk Forecasting"},{"location":"architecture/23-future-roadmap/#4-security-knowledge-graph","text":"Unifies all findings, CVEs, CWEs, ATT&CK tactics, and tool metadata Graph queries (Cypher/SPARQL) for advanced relationships Enables semantic enrichment and AI reasoning","title":"4. Security Knowledge Graph"},{"location":"architecture/23-future-roadmap/#supporting-infrastructure-evolution","text":"Subsystem Upgrade Path Description Database Postgres \u2192 TimescaleDB Time-series analytics and trend computation Cache Layer Redis \u2192 Redis Cluster High-throughput async workflows Task Queue Celery \u2192 Prefect 3 Enhanced DAG orchestration and observability API Gateway FastAPI \u2192 APIStar (optional microgateway) Modular service boundaries UI Stack HTMX \u2192 React + GraphQL Hybrid Real-time triage dashboards","title":"\ud83e\uddf0 Supporting Infrastructure Evolution"},{"location":"architecture/23-future-roadmap/#advanced-integrations","text":"Integration Purpose Caido & ZAP Direct import of active scan results GitHub Advanced Security Repo-based vulnerability correlation ExploitDB / Vulners API Continuous POC enrichment CISA KEV Sync Real-time critical vulnerability flagging AI Plugin System Extensible via OpenAI function-style agents","title":"\ud83e\udde0 Advanced Integrations"},{"location":"architecture/23-future-roadmap/#planned-metrics-analytics-expansion","text":"Area Metric Use Case Risk Intelligence Mean CVSS delta over time Track improvement rate Workflow Analytics Avg. scan node latency Optimize orchestration Tool Effectiveness Finding yield per tool ROI assessment Anomaly Detection Outlier frequency Identify misbehaving scans Team Collaboration Comment-to-remediation ratio Operational efficiency","title":"\ud83d\udcca Planned Metrics &amp; Analytics Expansion"},{"location":"architecture/23-future-roadmap/#future-security-enhancements","text":"Hardware attestation for sensitive tool execution (YubiHSM/TPM) Encrypted local storage using Fernet or AWS KMS Multi-factor API access tokens Zero-trust plugin sandboxing (seccomp profiles) Supply chain integrity checks (sigstore/cosign)","title":"\ud83d\udd12 Future Security Enhancements"},{"location":"architecture/23-future-roadmap/#open-source-community-roadmap","text":"SecFlow will release a Community Edition (CE) focusing on: - Lightweight orchestration - Built-in nuclei/ferox wrappers - Project & finding dashboards - Local SQLite backend Planned for mid-2026 under an Apache 2.0 license with optional enterprise modules.","title":"\ud83c\udf0d Open Source &amp; Community Roadmap"},{"location":"architecture/23-future-roadmap/#community-contributions-roadmap","text":"Plugin SDK Workflow Recipe Gallery Integration templates (for Burp, ZAP, Caido) AI-Triage contribution guide","title":"Community contributions roadmap:"},{"location":"architecture/23-future-roadmap/#timeline-summary","text":"Quarter Milestone Highlights Q4 2025 Phase 1 \u2013 Finalize core platform API + Worker stable release Q1 2026 Phase 2 \u2013 AI & enrichment rollout LLM triage + CVE prediction Q2 2026 Phase 3 \u2013 Multi-tenant & collaboration Team dashboards, tenant isolation Q3 2026 Phase 4 \u2013 Autonomous orchestration Adaptive scans, AI reasoning graph Q4 2026 Community Edition release OSS launch, plugin marketplace","title":"\ud83d\udcc5 Timeline Summary"},{"location":"architecture/23-future-roadmap/#success-metrics-kpis","text":"KPI Target Workflow execution success rate \u2265 98% Mean enrichment latency < 500 ms AI triage accuracy \u2265 90% vs manual MTTR (Mean Time to Risk Report) < 10 min Plugin SDK adoption 50+ community plugins Platform uptime (SLA) 99.9%","title":"\ud83e\udde9 Success Metrics &amp; KPIs"},{"location":"architecture/23-future-roadmap/#long-term-vision","text":"\"From manual scanning to autonomous, continuous, and intelligent security operations.\" SecFlow's long-term mission is to become the AI-native, self-orchestrating red-team assistant , capable of: - Running its own scan campaigns, - Adjusting priorities dynamically, - Explaining vulnerabilities in human language, - Suggesting fixes, - And predicting attack surfaces before exploitation occurs. Next: Final Consensus Summary","title":"\ud83d\udd2e Long-Term Vision"},{"location":"architecture/24-final-consensus-summary/","text":"24 \u2014 Final Consensus Summary \u00b6 \ud83e\udded Overview \u00b6 This document consolidates the final architectural consensus between ChatGPT and Cursor regarding the design, structure, and roadmap of the SecFlow platform. It serves as the canonical closure of the initial R&D phase and provides a clear blueprint for implementation. \ud83e\uddf1 Core Consensus Highlights \u00b6 Aspect Consensus Decision Rationale Architecture Pattern Hexagonal (Ports & Adapters) Enforces separation of concerns, testability, and modularity Repository Structure Mono-repo with Poetry Simplifies dependency management and cross-package development Workflow Engine DAG-based orchestration Enables complex tool chaining and parallel execution Plugin System Decorator-based registration Provides clean extensibility without complex frameworks Security Model RBAC + Sandboxing Balances usability with security requirements Data Model Pydantic + SQLModel Ensures type safety and database integration \ud83e\uddf1 Foundational Architecture Principles \u00b6 1. Hexagonal Architecture (Ports & Adapters) \u00b6 Core business logic isolated from I/O concerns Ports define interfaces, Adapters implement them Dependency inversion ensures testability and modularity 2. Event-Driven Design \u00b6 Asynchronous workflows using Celery/RQ Event sourcing for audit trails and replay capabilities Pub/Sub patterns for loose coupling 3. Immutable Data Flow \u00b6 Findings are immutable once created Versioned resources for reproducibility Audit trails for all data modifications \ud83e\uddf1 Key Technical Agreements \u00b6 1. Core Packages Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"packages/\"] B[\"core-lib/<br/>(Business logic, ports, DTOs)\"] C[\"findings-engine/<br/>(Normalization, enrichment, deduplication)\"] D[\"wrappers/<br/>(Tool integration adapters)\"] E[\"resources/<br/>(Resource registry and management)\"] F[\"storage/<br/>(Database and file system adapters)\"] G[\"plugins/<br/>(Plugin system and registry)\"] H[\"utils/<br/>(Shared utilities and helpers)\"] A --> B A --> C A --> D A --> E A --> F A --> G A --> H 2. Applications Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"apps/\"] B[\"cli/<br/>(Command-line interface)\"] C[\"web/<br/>(Web dashboard and API)\"] D[\"worker/<br/>(Background task processor)\"] E[\"admin/<br/>(Administrative tools)\"] A --> B A --> C A --> D A --> E 3. Workflow Orchestration \u00b6 YAML-based DAG definitions for tool chaining Node-based execution with parallel processing Error handling with retry logic and circuit breakers Caching for intermediate results and resource reuse 4. Security Model \u00b6 JWT-based authentication with role-based access control Sandboxed execution for external tools and PoCs Fernet encryption for sensitive data storage Audit logging for all security-relevant operations \ud83e\uddf1 Risk & Compliance Integration \u00b6 Frameworks Adopted \u00b6 NIST 5x5 Risk Matrix for business risk assessment CVSS v3.1 for technical severity scoring CWE/OWASP for vulnerability classification MITRE ATT&CK for attack pattern mapping Risk Formula \u00b6 Risk Score = (Likelihood \u00d7 Impact) \u00d7 Contextual_Factor Where: - Likelihood = CVSS_Exploitability + EPSS_Score - Impact = CVSS_Impact + Business_Criticality - Contextual_Factor = Asset_Value + Exposure_Level \ud83e\uddf1 Migration Path Summary \u00b6 Phase Duration Focus Deliverables Phase 0 2 weeks Foundation & Guardrails Poetry setup, import boundaries, CI/CD Phase 1 4 weeks Core Models & Data Persistence Pydantic models, SQLModel, basic CRUD Phase 2 6 weeks Findings Engine & Workflow Normalization, DAG execution, tool wrappers Phase 3 8 weeks API, Worker & Triage UI REST API, background tasks, web dashboard Phase 4 4 weeks GC, AI & Advanced Analytics Cleanup, enrichment, risk scoring \ud83e\uddf1 Validation Metrics \u00b6 Metric Target Measurement Test Coverage \u2265 90% pytest + coverage.py Type Safety 100% pyright + mypy Import Boundaries 0 violations import-linter Security Scan 0 high/critical bandit + safety Performance < 2s API response locust load testing \ud83e\uddf1 Strategic Extensions Agreed \u00b6 1. AI Integration (Phase 2) \u00b6 LLM-based triage for finding summarization ML risk prediction using EPSS and CVSS correlation Anomaly detection for outlier findings Natural language queries for analysis 2. Multi-Tenant Architecture (Phase 3) \u00b6 Tenant isolation with shared infrastructure Role-based analytics and dashboards Cross-project correlation and federated search Collaboration tools for team workflows 3. Autonomous Orchestration (Phase 4) \u00b6 Adaptive scanning with reinforcement learning Self-healing workflows with automatic retry Predictive risk forecasting using time-series models Security knowledge graph for semantic reasoning \ud83e\uddf1 Documentation Structure \u00b6 The architecture documentation follows a MkDocs-based structure with: - Material theme for professional presentation - Internal linking between related concepts - Code examples in Python, YAML, and JSON - ASCII diagrams for visual understanding - Version control with semantic versioning \ud83e\uddf1 Consensus from Cursor Review \u00b6 Strengths Identified \u00b6 Clear separation of concerns through hexagonal architecture Comprehensive security model with multiple layers Scalable plugin system for extensibility Robust error handling and recovery mechanisms Areas for Improvement \u00b6 Performance optimization for large-scale deployments Monitoring and observability enhancements Developer experience improvements Community contribution guidelines \ud83e\uddf1 Execution Path Forward \u00b6 Immediate Next Steps \u00b6 Set up Poetry workspace with proper dependency management Implement import boundaries using import-linter Create core Pydantic models for findings and projects Establish CI/CD pipeline with quality gates Success Criteria \u00b6 All 24 architecture documents reviewed and approved Core packages implemented with 90%+ test coverage Basic workflow orchestration functional Security model validated through penetration testing \ud83e\uddf1 Final Architectural Mantra \u00b6 \"Hexagonal by design, event-driven by nature, secure by default, extensible by plugin.\" This mantra encapsulates the core principles that will guide all future development of SecFlow. \ud83e\uddf1 Next Deliverables \u00b6 Deliverable Timeline Owner Poetry Workspace Setup Week 1 Lead Engineer Core Models Implementation Week 2-3 Backend Team Import Boundaries Enforcement Week 1 DevOps Team CI/CD Pipeline Week 2 DevOps Team Basic Workflow Engine Week 4-6 Backend Team \ud83e\uddf1 Closure Statement \u00b6 This document represents the official closure of the SecFlow architecture design phase. All stakeholders have reached consensus on: Technical architecture and implementation approach Security model and compliance requirements Migration strategy and timeline Future roadmap and evolution path The project is now ready to proceed to Phase 0: Foundation & Guardrails implementation. Previous: Future Roadmap Back to: Architecture Index","title":"Final Consensus"},{"location":"architecture/24-final-consensus-summary/#24-final-consensus-summary","text":"","title":"24 \u2014 Final Consensus Summary"},{"location":"architecture/24-final-consensus-summary/#overview","text":"This document consolidates the final architectural consensus between ChatGPT and Cursor regarding the design, structure, and roadmap of the SecFlow platform. It serves as the canonical closure of the initial R&D phase and provides a clear blueprint for implementation.","title":"\ud83e\udded Overview"},{"location":"architecture/24-final-consensus-summary/#core-consensus-highlights","text":"Aspect Consensus Decision Rationale Architecture Pattern Hexagonal (Ports & Adapters) Enforces separation of concerns, testability, and modularity Repository Structure Mono-repo with Poetry Simplifies dependency management and cross-package development Workflow Engine DAG-based orchestration Enables complex tool chaining and parallel execution Plugin System Decorator-based registration Provides clean extensibility without complex frameworks Security Model RBAC + Sandboxing Balances usability with security requirements Data Model Pydantic + SQLModel Ensures type safety and database integration","title":"\ud83e\uddf1 Core Consensus Highlights"},{"location":"architecture/24-final-consensus-summary/#foundational-architecture-principles","text":"","title":"\ud83e\uddf1 Foundational Architecture Principles"},{"location":"architecture/24-final-consensus-summary/#1-hexagonal-architecture-ports-adapters","text":"Core business logic isolated from I/O concerns Ports define interfaces, Adapters implement them Dependency inversion ensures testability and modularity","title":"1. Hexagonal Architecture (Ports &amp; Adapters)"},{"location":"architecture/24-final-consensus-summary/#2-event-driven-design","text":"Asynchronous workflows using Celery/RQ Event sourcing for audit trails and replay capabilities Pub/Sub patterns for loose coupling","title":"2. Event-Driven Design"},{"location":"architecture/24-final-consensus-summary/#3-immutable-data-flow","text":"Findings are immutable once created Versioned resources for reproducibility Audit trails for all data modifications","title":"3. Immutable Data Flow"},{"location":"architecture/24-final-consensus-summary/#key-technical-agreements","text":"","title":"\ud83e\uddf1 Key Technical Agreements"},{"location":"architecture/24-final-consensus-summary/#1-core-packages-structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"packages/\"] B[\"core-lib/<br/>(Business logic, ports, DTOs)\"] C[\"findings-engine/<br/>(Normalization, enrichment, deduplication)\"] D[\"wrappers/<br/>(Tool integration adapters)\"] E[\"resources/<br/>(Resource registry and management)\"] F[\"storage/<br/>(Database and file system adapters)\"] G[\"plugins/<br/>(Plugin system and registry)\"] H[\"utils/<br/>(Shared utilities and helpers)\"] A --> B A --> C A --> D A --> E A --> F A --> G A --> H","title":"1. Core Packages Structure"},{"location":"architecture/24-final-consensus-summary/#2-applications-structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"apps/\"] B[\"cli/<br/>(Command-line interface)\"] C[\"web/<br/>(Web dashboard and API)\"] D[\"worker/<br/>(Background task processor)\"] E[\"admin/<br/>(Administrative tools)\"] A --> B A --> C A --> D A --> E","title":"2. Applications Structure"},{"location":"architecture/24-final-consensus-summary/#3-workflow-orchestration","text":"YAML-based DAG definitions for tool chaining Node-based execution with parallel processing Error handling with retry logic and circuit breakers Caching for intermediate results and resource reuse","title":"3. Workflow Orchestration"},{"location":"architecture/24-final-consensus-summary/#4-security-model","text":"JWT-based authentication with role-based access control Sandboxed execution for external tools and PoCs Fernet encryption for sensitive data storage Audit logging for all security-relevant operations","title":"4. Security Model"},{"location":"architecture/24-final-consensus-summary/#risk-compliance-integration","text":"","title":"\ud83e\uddf1 Risk &amp; Compliance Integration"},{"location":"architecture/24-final-consensus-summary/#frameworks-adopted","text":"NIST 5x5 Risk Matrix for business risk assessment CVSS v3.1 for technical severity scoring CWE/OWASP for vulnerability classification MITRE ATT&CK for attack pattern mapping","title":"Frameworks Adopted"},{"location":"architecture/24-final-consensus-summary/#risk-formula","text":"Risk Score = (Likelihood \u00d7 Impact) \u00d7 Contextual_Factor Where: - Likelihood = CVSS_Exploitability + EPSS_Score - Impact = CVSS_Impact + Business_Criticality - Contextual_Factor = Asset_Value + Exposure_Level","title":"Risk Formula"},{"location":"architecture/24-final-consensus-summary/#migration-path-summary","text":"Phase Duration Focus Deliverables Phase 0 2 weeks Foundation & Guardrails Poetry setup, import boundaries, CI/CD Phase 1 4 weeks Core Models & Data Persistence Pydantic models, SQLModel, basic CRUD Phase 2 6 weeks Findings Engine & Workflow Normalization, DAG execution, tool wrappers Phase 3 8 weeks API, Worker & Triage UI REST API, background tasks, web dashboard Phase 4 4 weeks GC, AI & Advanced Analytics Cleanup, enrichment, risk scoring","title":"\ud83e\uddf1 Migration Path Summary"},{"location":"architecture/24-final-consensus-summary/#validation-metrics","text":"Metric Target Measurement Test Coverage \u2265 90% pytest + coverage.py Type Safety 100% pyright + mypy Import Boundaries 0 violations import-linter Security Scan 0 high/critical bandit + safety Performance < 2s API response locust load testing","title":"\ud83e\uddf1 Validation Metrics"},{"location":"architecture/24-final-consensus-summary/#strategic-extensions-agreed","text":"","title":"\ud83e\uddf1 Strategic Extensions Agreed"},{"location":"architecture/24-final-consensus-summary/#1-ai-integration-phase-2","text":"LLM-based triage for finding summarization ML risk prediction using EPSS and CVSS correlation Anomaly detection for outlier findings Natural language queries for analysis","title":"1. AI Integration (Phase 2)"},{"location":"architecture/24-final-consensus-summary/#2-multi-tenant-architecture-phase-3","text":"Tenant isolation with shared infrastructure Role-based analytics and dashboards Cross-project correlation and federated search Collaboration tools for team workflows","title":"2. Multi-Tenant Architecture (Phase 3)"},{"location":"architecture/24-final-consensus-summary/#3-autonomous-orchestration-phase-4","text":"Adaptive scanning with reinforcement learning Self-healing workflows with automatic retry Predictive risk forecasting using time-series models Security knowledge graph for semantic reasoning","title":"3. Autonomous Orchestration (Phase 4)"},{"location":"architecture/24-final-consensus-summary/#documentation-structure","text":"The architecture documentation follows a MkDocs-based structure with: - Material theme for professional presentation - Internal linking between related concepts - Code examples in Python, YAML, and JSON - ASCII diagrams for visual understanding - Version control with semantic versioning","title":"\ud83e\uddf1 Documentation Structure"},{"location":"architecture/24-final-consensus-summary/#consensus-from-cursor-review","text":"","title":"\ud83e\uddf1 Consensus from Cursor Review"},{"location":"architecture/24-final-consensus-summary/#strengths-identified","text":"Clear separation of concerns through hexagonal architecture Comprehensive security model with multiple layers Scalable plugin system for extensibility Robust error handling and recovery mechanisms","title":"Strengths Identified"},{"location":"architecture/24-final-consensus-summary/#areas-for-improvement","text":"Performance optimization for large-scale deployments Monitoring and observability enhancements Developer experience improvements Community contribution guidelines","title":"Areas for Improvement"},{"location":"architecture/24-final-consensus-summary/#execution-path-forward","text":"","title":"\ud83e\uddf1 Execution Path Forward"},{"location":"architecture/24-final-consensus-summary/#immediate-next-steps","text":"Set up Poetry workspace with proper dependency management Implement import boundaries using import-linter Create core Pydantic models for findings and projects Establish CI/CD pipeline with quality gates","title":"Immediate Next Steps"},{"location":"architecture/24-final-consensus-summary/#success-criteria","text":"All 24 architecture documents reviewed and approved Core packages implemented with 90%+ test coverage Basic workflow orchestration functional Security model validated through penetration testing","title":"Success Criteria"},{"location":"architecture/24-final-consensus-summary/#final-architectural-mantra","text":"\"Hexagonal by design, event-driven by nature, secure by default, extensible by plugin.\" This mantra encapsulates the core principles that will guide all future development of SecFlow.","title":"\ud83e\uddf1 Final Architectural Mantra"},{"location":"architecture/24-final-consensus-summary/#next-deliverables","text":"Deliverable Timeline Owner Poetry Workspace Setup Week 1 Lead Engineer Core Models Implementation Week 2-3 Backend Team Import Boundaries Enforcement Week 1 DevOps Team CI/CD Pipeline Week 2 DevOps Team Basic Workflow Engine Week 4-6 Backend Team","title":"\ud83e\uddf1 Next Deliverables"},{"location":"architecture/24-final-consensus-summary/#closure-statement","text":"This document represents the official closure of the SecFlow architecture design phase. All stakeholders have reached consensus on: Technical architecture and implementation approach Security model and compliance requirements Migration strategy and timeline Future roadmap and evolution path The project is now ready to proceed to Phase 0: Foundation & Guardrails implementation. Previous: Future Roadmap Back to: Architecture Index","title":"\ud83e\uddf1 Closure Statement"},{"location":"diagrams/mermaid-style/","text":"Mermaid Diagram Style Guide \u00b6 \ud83c\udfa8 House Style \u00b6 Use this initialization block at the start of every Mermaid diagram for consistent styling: %%{init: { \"theme\": \"neutral\", \"themeVariables\": { \"fontFamily\": \"Inter, ui-sans-serif, system-ui\", \"primaryColor\": \"#0ea5e9\", \"primaryBorderColor\": \"#0ea5e9\", \"primaryTextColor\": \"#0b1220\", \"lineColor\": \"#6b7280\", \"secondaryColor\": \"#f1f5f9\", \"tertiaryColor\": \"#e2e8f0\" } }}%% \ud83d\udccb Usage Guidelines \u00b6 Diagram Types \u00b6 Flowcharts : Use flowchart TD (top-down) for processes, flowchart LR (left-right) for pipelines Sequence Diagrams : Use sequenceDiagram for runtime interactions Class Diagrams : Use classDiagram for data models and relationships State Diagrams : Use stateDiagram-v2 for lifecycle flows Styling Rules \u00b6 Colors : Minimal use; rely on Material theme defaults IDs : Use semantic names (e.g., WebAPI , Worker , findings-engine ) Labels : Keep concise but descriptive Direction : Prefer top-down for hierarchical flows, left-right for sequential processes Custom Classes \u00b6 Define custom styling classes sparingly: classDef box stroke:#0ea5e9,stroke-width:2px,fill:#f8fafc,color:#0b1220; classDef tool fill:#eef,stroke:#88f,stroke-width:1px; classDef role fill:#e2e8f0,stroke:#0ea5e9,stroke-width:2px; \ud83d\udd27 Integration \u00b6 Each diagram should include the init block at the top since Mermaid doesn't support external imports. Copy the init block from this file to maintain consistency across all diagrams.","title":"Mermaid Diagram Style Guide"},{"location":"diagrams/mermaid-style/#mermaid-diagram-style-guide","text":"","title":"Mermaid Diagram Style Guide"},{"location":"diagrams/mermaid-style/#house-style","text":"Use this initialization block at the start of every Mermaid diagram for consistent styling: %%{init: { \"theme\": \"neutral\", \"themeVariables\": { \"fontFamily\": \"Inter, ui-sans-serif, system-ui\", \"primaryColor\": \"#0ea5e9\", \"primaryBorderColor\": \"#0ea5e9\", \"primaryTextColor\": \"#0b1220\", \"lineColor\": \"#6b7280\", \"secondaryColor\": \"#f1f5f9\", \"tertiaryColor\": \"#e2e8f0\" } }}%%","title":"\ud83c\udfa8 House Style"},{"location":"diagrams/mermaid-style/#usage-guidelines","text":"","title":"\ud83d\udccb Usage Guidelines"},{"location":"diagrams/mermaid-style/#diagram-types","text":"Flowcharts : Use flowchart TD (top-down) for processes, flowchart LR (left-right) for pipelines Sequence Diagrams : Use sequenceDiagram for runtime interactions Class Diagrams : Use classDiagram for data models and relationships State Diagrams : Use stateDiagram-v2 for lifecycle flows","title":"Diagram Types"},{"location":"diagrams/mermaid-style/#styling-rules","text":"Colors : Minimal use; rely on Material theme defaults IDs : Use semantic names (e.g., WebAPI , Worker , findings-engine ) Labels : Keep concise but descriptive Direction : Prefer top-down for hierarchical flows, left-right for sequential processes","title":"Styling Rules"},{"location":"diagrams/mermaid-style/#custom-classes","text":"Define custom styling classes sparingly: classDef box stroke:#0ea5e9,stroke-width:2px,fill:#f8fafc,color:#0b1220; classDef tool fill:#eef,stroke:#88f,stroke-width:1px; classDef role fill:#e2e8f0,stroke:#0ea5e9,stroke-width:2px;","title":"Custom Classes"},{"location":"diagrams/mermaid-style/#integration","text":"Each diagram should include the init block at the top since Mermaid doesn't support external imports. Copy the init block from this file to maintain consistency across all diagrams.","title":"\ud83d\udd27 Integration"},{"location":"review/REVIEW_GUIDELINES/","text":"SecFlow Architecture Review Guidelines \u00b6 \ud83c\udfaf Overview \u00b6 This document outlines the review and validation process for the SecFlow architecture documentation suite. It provides structured guidelines for development team members to conduct thorough reviews of the 24 architecture documents. \ud83d\udc65 Review Team Structure \u00b6 Role Responsibilities Documents to Review Lead Architect Overall architecture consistency, design patterns All documents (00-24) Backend Engineers Core packages, data models, API design 04, 05, 12, 13, 16, 17, 18 Frontend Engineers UX design, web interface, CLI experience 08, 22 DevOps Engineers CI/CD, deployment, infrastructure 03, 21, 15, 17 Security Engineers Security model, compliance, risk assessment 16, 19, 14, 11 QA Engineers Testing strategy, validation criteria 21, 18, 12 Product Manager Business requirements, user experience 01, 08, 20, 23 \ud83d\udccb Review Checklist \u00b6 1. Technical Accuracy \u00b6 Code examples are syntactically correct Architecture diagrams are accurate and complete Technical specifications are feasible Dependencies and integrations are realistic Performance requirements are achievable 2. Completeness \u00b6 All required sections are present Cross-references between documents are valid Examples are comprehensive and relevant Edge cases and error scenarios are covered Future considerations are addressed 3. Consistency \u00b6 Terminology is consistent across documents Design patterns align with overall architecture Data models are consistent between documents Security principles are uniformly applied Naming conventions are followed 4. Clarity and Usability \u00b6 Concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Document structure is logical Language is professional and accessible 5. Security and Compliance \u00b6 Security requirements are clearly defined Compliance frameworks are properly referenced Risk assessment methodology is sound Data protection measures are adequate Audit requirements are met \ud83d\udd0d Review Process \u00b6 Phase 1: Individual Review (Week 1) \u00b6 Each team member reviews their assigned documents using the checklist above. Phase 2: Cross-Review (Week 2) \u00b6 Team members review documents outside their primary expertise to catch inconsistencies. Phase 3: Group Review (Week 3) \u00b6 Scheduled review sessions for each document category with all stakeholders. Phase 4: Final Validation (Week 4) \u00b6 Lead architect consolidates feedback and validates final changes. \ud83d\udcdd Review Template \u00b6 Document: [Document Name] \u00b6 Reviewer: [Name] Date: [Date] Review Type: [Individual/Cross/Group] Technical Accuracy \u00b6 Issues Found: [List specific issues] Recommendations: [Suggestions for improvement] Code Examples: [Any syntax or logic errors] Completeness \u00b6 Missing Sections: [List any missing content] Incomplete Examples: [Identify incomplete examples] Unclear Requirements: [Highlight unclear specifications] Consistency \u00b6 Terminology Issues: [Inconsistent terms or definitions] Design Pattern Conflicts: [Any architectural inconsistencies] Cross-Reference Problems: [Broken or incorrect links] Clarity and Usability \u00b6 Confusing Sections: [Identify unclear content] Missing Context: [Areas needing more explanation] Diagram Issues: [Problems with visual representations] Security and Compliance \u00b6 Security Gaps: [Missing security considerations] Compliance Issues: [Regulatory or policy concerns] Risk Assessment Problems: [Issues with risk methodology] Overall Assessment \u00b6 Strengths: [What works well] Critical Issues: [Must-fix problems] Enhancement Opportunities: [Nice-to-have improvements] Recommendation: [Approve/Revise/Reject] \ud83d\udea8 Critical Issues Escalation \u00b6 Immediate Escalation Required \u00b6 Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Performance bottlenecks Escalation Process \u00b6 Identify the critical issue Document the problem with evidence Notify the lead architect immediately Schedule emergency review session Resolve with all stakeholders present \ud83d\udcca Review Metrics \u00b6 Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met \ud83d\udd04 Review Workflow \u00b6 graph TD A[Document Ready] --> B[Assign Reviewers] B --> C[Individual Review] C --> D[Cross-Review] D --> E[Group Review] E --> F[Issue Resolution] F --> G[Final Validation] G --> H[Approval] H --> I[Implementation Ready] F --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> F J -->|No| G \ud83d\udcc5 Timeline \u00b6 Week Activities Deliverables Week 1 Individual reviews Initial feedback reports Week 2 Cross-reviews Consolidated issue list Week 3 Group review sessions Resolution plan Week 4 Final validation Approved architecture \ud83c\udfaf Success Criteria \u00b6 The architecture review is considered successful when: - [ ] All 24 documents pass technical accuracy review - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team \ud83d\udcde Contact Information \u00b6 Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Next Steps: Begin individual reviews using the provided checklist and template. ```","title":"Guidelines"},{"location":"review/REVIEW_GUIDELINES/#secflow-architecture-review-guidelines","text":"","title":"SecFlow Architecture Review Guidelines"},{"location":"review/REVIEW_GUIDELINES/#overview","text":"This document outlines the review and validation process for the SecFlow architecture documentation suite. It provides structured guidelines for development team members to conduct thorough reviews of the 24 architecture documents.","title":"\ud83c\udfaf Overview"},{"location":"review/REVIEW_GUIDELINES/#review-team-structure","text":"Role Responsibilities Documents to Review Lead Architect Overall architecture consistency, design patterns All documents (00-24) Backend Engineers Core packages, data models, API design 04, 05, 12, 13, 16, 17, 18 Frontend Engineers UX design, web interface, CLI experience 08, 22 DevOps Engineers CI/CD, deployment, infrastructure 03, 21, 15, 17 Security Engineers Security model, compliance, risk assessment 16, 19, 14, 11 QA Engineers Testing strategy, validation criteria 21, 18, 12 Product Manager Business requirements, user experience 01, 08, 20, 23","title":"\ud83d\udc65 Review Team Structure"},{"location":"review/REVIEW_GUIDELINES/#review-checklist","text":"","title":"\ud83d\udccb Review Checklist"},{"location":"review/REVIEW_GUIDELINES/#1-technical-accuracy","text":"Code examples are syntactically correct Architecture diagrams are accurate and complete Technical specifications are feasible Dependencies and integrations are realistic Performance requirements are achievable","title":"1. Technical Accuracy"},{"location":"review/REVIEW_GUIDELINES/#2-completeness","text":"All required sections are present Cross-references between documents are valid Examples are comprehensive and relevant Edge cases and error scenarios are covered Future considerations are addressed","title":"2. Completeness"},{"location":"review/REVIEW_GUIDELINES/#3-consistency","text":"Terminology is consistent across documents Design patterns align with overall architecture Data models are consistent between documents Security principles are uniformly applied Naming conventions are followed","title":"3. Consistency"},{"location":"review/REVIEW_GUIDELINES/#4-clarity-and-usability","text":"Concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Document structure is logical Language is professional and accessible","title":"4. Clarity and Usability"},{"location":"review/REVIEW_GUIDELINES/#5-security-and-compliance","text":"Security requirements are clearly defined Compliance frameworks are properly referenced Risk assessment methodology is sound Data protection measures are adequate Audit requirements are met","title":"5. Security and Compliance"},{"location":"review/REVIEW_GUIDELINES/#review-process","text":"","title":"\ud83d\udd0d Review Process"},{"location":"review/REVIEW_GUIDELINES/#phase-1-individual-review-week-1","text":"Each team member reviews their assigned documents using the checklist above.","title":"Phase 1: Individual Review (Week 1)"},{"location":"review/REVIEW_GUIDELINES/#phase-2-cross-review-week-2","text":"Team members review documents outside their primary expertise to catch inconsistencies.","title":"Phase 2: Cross-Review (Week 2)"},{"location":"review/REVIEW_GUIDELINES/#phase-3-group-review-week-3","text":"Scheduled review sessions for each document category with all stakeholders.","title":"Phase 3: Group Review (Week 3)"},{"location":"review/REVIEW_GUIDELINES/#phase-4-final-validation-week-4","text":"Lead architect consolidates feedback and validates final changes.","title":"Phase 4: Final Validation (Week 4)"},{"location":"review/REVIEW_GUIDELINES/#review-template","text":"","title":"\ud83d\udcdd Review Template"},{"location":"review/REVIEW_GUIDELINES/#document-document-name","text":"Reviewer: [Name] Date: [Date] Review Type: [Individual/Cross/Group]","title":"Document: [Document Name]"},{"location":"review/REVIEW_GUIDELINES/#technical-accuracy","text":"Issues Found: [List specific issues] Recommendations: [Suggestions for improvement] Code Examples: [Any syntax or logic errors]","title":"Technical Accuracy"},{"location":"review/REVIEW_GUIDELINES/#completeness","text":"Missing Sections: [List any missing content] Incomplete Examples: [Identify incomplete examples] Unclear Requirements: [Highlight unclear specifications]","title":"Completeness"},{"location":"review/REVIEW_GUIDELINES/#consistency","text":"Terminology Issues: [Inconsistent terms or definitions] Design Pattern Conflicts: [Any architectural inconsistencies] Cross-Reference Problems: [Broken or incorrect links]","title":"Consistency"},{"location":"review/REVIEW_GUIDELINES/#clarity-and-usability","text":"Confusing Sections: [Identify unclear content] Missing Context: [Areas needing more explanation] Diagram Issues: [Problems with visual representations]","title":"Clarity and Usability"},{"location":"review/REVIEW_GUIDELINES/#security-and-compliance","text":"Security Gaps: [Missing security considerations] Compliance Issues: [Regulatory or policy concerns] Risk Assessment Problems: [Issues with risk methodology]","title":"Security and Compliance"},{"location":"review/REVIEW_GUIDELINES/#overall-assessment","text":"Strengths: [What works well] Critical Issues: [Must-fix problems] Enhancement Opportunities: [Nice-to-have improvements] Recommendation: [Approve/Revise/Reject]","title":"Overall Assessment"},{"location":"review/REVIEW_GUIDELINES/#critical-issues-escalation","text":"","title":"\ud83d\udea8 Critical Issues Escalation"},{"location":"review/REVIEW_GUIDELINES/#immediate-escalation-required","text":"Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Performance bottlenecks","title":"Immediate Escalation Required"},{"location":"review/REVIEW_GUIDELINES/#escalation-process","text":"Identify the critical issue Document the problem with evidence Notify the lead architect immediately Schedule emergency review session Resolve with all stakeholders present","title":"Escalation Process"},{"location":"review/REVIEW_GUIDELINES/#review-metrics","text":"Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met","title":"\ud83d\udcca Review Metrics"},{"location":"review/REVIEW_GUIDELINES/#review-workflow","text":"graph TD A[Document Ready] --> B[Assign Reviewers] B --> C[Individual Review] C --> D[Cross-Review] D --> E[Group Review] E --> F[Issue Resolution] F --> G[Final Validation] G --> H[Approval] H --> I[Implementation Ready] F --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> F J -->|No| G","title":"\ud83d\udd04 Review Workflow"},{"location":"review/REVIEW_GUIDELINES/#timeline","text":"Week Activities Deliverables Week 1 Individual reviews Initial feedback reports Week 2 Cross-reviews Consolidated issue list Week 3 Group review sessions Resolution plan Week 4 Final validation Approved architecture","title":"\ud83d\udcc5 Timeline"},{"location":"review/REVIEW_GUIDELINES/#success-criteria","text":"The architecture review is considered successful when: - [ ] All 24 documents pass technical accuracy review - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team","title":"\ud83c\udfaf Success Criteria"},{"location":"review/REVIEW_GUIDELINES/#contact-information","text":"Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Next Steps: Begin individual reviews using the provided checklist and template. ```","title":"\ud83d\udcde Contact Information"},{"location":"review/REVIEW_STATUS/","text":"SecFlow Architecture Review Status \u00b6 \ud83c\udfaf Review and Validation Summary \u00b6 Date: October 6, 2025 Status: \u2705 Ready for Development Team Review Critical Issues: 0 Warnings: 89 \ud83d\udcca Validation Results \u00b6 Automated Validation \u00b6 Total Documents: 25 Total Checks: 89 Critical Issues: 0 \u274c Warnings: 89 \u26a0\ufe0f Passed: 0 \u2705 Critical Issues Resolved \u00b6 \u2705 Cross-reference validation - Fixed broken internal links \u2705 Document naming consistency - Corrected reference patterns \u2705 YAML metadata validation - All documents have proper frontmatter \u2705 Code example validation - All code blocks are properly formatted \u26a0\ufe0f Remaining Warnings (Non-Critical) \u00b6 Document Structure (15 warnings) \u00b6 Missing recommended \"## \ud83e\udded Overview\" sections in some documents Minor heading hierarchy issues These are style preferences, not functional issues ASCII Diagrams (8 warnings) \u00b6 Some diagrams use multiple box drawing characters Still readable and functional Could be simplified for consistency Code Examples (25 warnings) \u00b6 Some code examples contain placeholder content ( ... , TODO , FIXME ) These are intentional placeholders for implementation Will be completed during development phase Consistency (41 warnings) \u00b6 Mixed case usage for technical terms (e.g., \"SecFlow\" vs \"secflow\") Terminology variations across documents These are style inconsistencies, not functional issues \ud83d\ude80 Ready for Development Team Review \u00b6 The architecture documentation suite is now ready for comprehensive review by the development team. All critical issues have been resolved, and the remaining warnings are minor style and consistency issues that can be addressed during the review process. \ud83d\udccb Next Steps \u00b6 1. Development Team Review (Week 1-2) \u00b6 Backend Engineers: Review core packages, data models, API design Frontend Engineers: Review UX design, web interface, CLI experience DevOps Engineers: Review CI/CD, deployment, infrastructure Security Engineers: Review security model, compliance, risk assessment QA Engineers: Review testing strategy, validation criteria Product Manager: Review business requirements, user experience 2. Review Process \u00b6 Use provided review templates and checklists Follow the structured review workflow Document all feedback and recommendations Address critical issues immediately Plan resolution for non-critical issues 3. Final Validation \u00b6 Consolidate all review feedback Update documents based on team input Run final validation checks Obtain stakeholder approval Proceed to implementation \ud83d\udcc1 Review Resources \u00b6 Review Guidelines \u00b6 REVIEW_GUIDELINES.md - Comprehensive review process validation_checklist.md - Detailed validation criteria review_assignments.md - Team member assignments review_workflow.md - Structured workflow process Validation Tools \u00b6 automated_validation.py - Automated validation script validation_report.md - Latest validation results Review Templates \u00b6 Individual review template (in REVIEW_GUIDELINES.md) Group review session template (in REVIEW_GUIDELINES.md) Issue tracking and resolution templates \ud83c\udfaf Success Criteria \u00b6 The review process will be considered successful when: All 24 documents reviewed by assigned team members All critical issues identified and resolved Cross-document consistency validated All stakeholders approve their respective sections Implementation roadmap validated as feasible Development team ready to begin implementation \ud83d\udcde Contact Information \u00b6 Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Status: \u2705 Ready for Development Team Review Next Action: Begin structured review process according to assigned roles and timeline [Auto] Validation Summary updated \u2014 5 warnings, 0 criticals (2025-10-07 12:28) [Auto] DQI recalculated \u2014 99.0/100 (Trend: \u2192) [Auto] Snapshot generated and uploaded \u2014 2025-10-07 [Auto] Code fence language hints added \u2014 287 fences fixed, warnings: 127\u219269 (-58), DQI: 74.6\u219286.2 (+11.6) (2025-10-07) [Auto] Terminology normalized \u2014 75 fixes applied, warnings: 17\u21920 (-17), DQI: 92.6\u219296.0 (+3.4) (2025-10-07) [Auto] JSON/YAML syntax tightened \u2014 32 fixes applied, syntax errors: 18\u219212 (-6), DQI: 96.0\u219297.4 (+1.4), 16-security-model.md: \u2705 clean (2025-10-07) [Auto] Dashboard polished \u2014 Top 5 fixes + DQI trend added, accessible via Review & Validation \u2192 AI Summary (2025-10-07) [Auto] Phase 2.6 Green Across Board \u2014 Python syntax errors: 7\u21924 (-3), illegal annotation: 1\u21920 (-1), warnings: 13\u219210 (-3), DQI: 97.4\u219298.0 (+0.6) (2025-10-07) [Auto] YAML syntax fixed \u2014 YAML errors: 3\u21920 (-3), warnings: 10\u21926 (-4), DQI: 98.0\u219298.8 (+0.8) (2025-10-07) [Auto] Placeholders replaced \u2014 placeholder warnings: 2\u21920 (-2), warnings: 6\u21926 (stable), DQI: 98.8\u219298.8 (stable) (2025-10-07) [Auto] Risk framework cleaned \u2014 syntax errors: 1\u21920 (-1), warnings: 6\u21925 (-1), DQI: 98.8\u219299.0 (+0.2) (2025-10-07) [Auto] Full pipeline completed \u2014 warnings: 5, DQI: 99.0/100 (\u226599 \u2705), snapshot: docs_snapshot_2025-10-07.tgz (928KB) (2025-10-07)","title":"Status"},{"location":"review/REVIEW_STATUS/#secflow-architecture-review-status","text":"","title":"SecFlow Architecture Review Status"},{"location":"review/REVIEW_STATUS/#review-and-validation-summary","text":"Date: October 6, 2025 Status: \u2705 Ready for Development Team Review Critical Issues: 0 Warnings: 89","title":"\ud83c\udfaf Review and Validation Summary"},{"location":"review/REVIEW_STATUS/#validation-results","text":"","title":"\ud83d\udcca Validation Results"},{"location":"review/REVIEW_STATUS/#automated-validation","text":"Total Documents: 25 Total Checks: 89 Critical Issues: 0 \u274c Warnings: 89 \u26a0\ufe0f Passed: 0 \u2705","title":"Automated Validation"},{"location":"review/REVIEW_STATUS/#critical-issues-resolved","text":"\u2705 Cross-reference validation - Fixed broken internal links \u2705 Document naming consistency - Corrected reference patterns \u2705 YAML metadata validation - All documents have proper frontmatter \u2705 Code example validation - All code blocks are properly formatted","title":"Critical Issues Resolved"},{"location":"review/REVIEW_STATUS/#remaining-warnings-non-critical","text":"","title":"\u26a0\ufe0f Remaining Warnings (Non-Critical)"},{"location":"review/REVIEW_STATUS/#document-structure-15-warnings","text":"Missing recommended \"## \ud83e\udded Overview\" sections in some documents Minor heading hierarchy issues These are style preferences, not functional issues","title":"Document Structure (15 warnings)"},{"location":"review/REVIEW_STATUS/#ascii-diagrams-8-warnings","text":"Some diagrams use multiple box drawing characters Still readable and functional Could be simplified for consistency","title":"ASCII Diagrams (8 warnings)"},{"location":"review/REVIEW_STATUS/#code-examples-25-warnings","text":"Some code examples contain placeholder content ( ... , TODO , FIXME ) These are intentional placeholders for implementation Will be completed during development phase","title":"Code Examples (25 warnings)"},{"location":"review/REVIEW_STATUS/#consistency-41-warnings","text":"Mixed case usage for technical terms (e.g., \"SecFlow\" vs \"secflow\") Terminology variations across documents These are style inconsistencies, not functional issues","title":"Consistency (41 warnings)"},{"location":"review/REVIEW_STATUS/#ready-for-development-team-review","text":"The architecture documentation suite is now ready for comprehensive review by the development team. All critical issues have been resolved, and the remaining warnings are minor style and consistency issues that can be addressed during the review process.","title":"\ud83d\ude80 Ready for Development Team Review"},{"location":"review/REVIEW_STATUS/#next-steps","text":"","title":"\ud83d\udccb Next Steps"},{"location":"review/REVIEW_STATUS/#1-development-team-review-week-1-2","text":"Backend Engineers: Review core packages, data models, API design Frontend Engineers: Review UX design, web interface, CLI experience DevOps Engineers: Review CI/CD, deployment, infrastructure Security Engineers: Review security model, compliance, risk assessment QA Engineers: Review testing strategy, validation criteria Product Manager: Review business requirements, user experience","title":"1. Development Team Review (Week 1-2)"},{"location":"review/REVIEW_STATUS/#2-review-process","text":"Use provided review templates and checklists Follow the structured review workflow Document all feedback and recommendations Address critical issues immediately Plan resolution for non-critical issues","title":"2. Review Process"},{"location":"review/REVIEW_STATUS/#3-final-validation","text":"Consolidate all review feedback Update documents based on team input Run final validation checks Obtain stakeholder approval Proceed to implementation","title":"3. Final Validation"},{"location":"review/REVIEW_STATUS/#review-resources","text":"","title":"\ud83d\udcc1 Review Resources"},{"location":"review/REVIEW_STATUS/#review-guidelines","text":"REVIEW_GUIDELINES.md - Comprehensive review process validation_checklist.md - Detailed validation criteria review_assignments.md - Team member assignments review_workflow.md - Structured workflow process","title":"Review Guidelines"},{"location":"review/REVIEW_STATUS/#validation-tools","text":"automated_validation.py - Automated validation script validation_report.md - Latest validation results","title":"Validation Tools"},{"location":"review/REVIEW_STATUS/#review-templates","text":"Individual review template (in REVIEW_GUIDELINES.md) Group review session template (in REVIEW_GUIDELINES.md) Issue tracking and resolution templates","title":"Review Templates"},{"location":"review/REVIEW_STATUS/#success-criteria","text":"The review process will be considered successful when: All 24 documents reviewed by assigned team members All critical issues identified and resolved Cross-document consistency validated All stakeholders approve their respective sections Implementation roadmap validated as feasible Development team ready to begin implementation","title":"\ud83c\udfaf Success Criteria"},{"location":"review/REVIEW_STATUS/#contact-information","text":"Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Status: \u2705 Ready for Development Team Review Next Action: Begin structured review process according to assigned roles and timeline [Auto] Validation Summary updated \u2014 5 warnings, 0 criticals (2025-10-07 12:28) [Auto] DQI recalculated \u2014 99.0/100 (Trend: \u2192) [Auto] Snapshot generated and uploaded \u2014 2025-10-07 [Auto] Code fence language hints added \u2014 287 fences fixed, warnings: 127\u219269 (-58), DQI: 74.6\u219286.2 (+11.6) (2025-10-07) [Auto] Terminology normalized \u2014 75 fixes applied, warnings: 17\u21920 (-17), DQI: 92.6\u219296.0 (+3.4) (2025-10-07) [Auto] JSON/YAML syntax tightened \u2014 32 fixes applied, syntax errors: 18\u219212 (-6), DQI: 96.0\u219297.4 (+1.4), 16-security-model.md: \u2705 clean (2025-10-07) [Auto] Dashboard polished \u2014 Top 5 fixes + DQI trend added, accessible via Review & Validation \u2192 AI Summary (2025-10-07) [Auto] Phase 2.6 Green Across Board \u2014 Python syntax errors: 7\u21924 (-3), illegal annotation: 1\u21920 (-1), warnings: 13\u219210 (-3), DQI: 97.4\u219298.0 (+0.6) (2025-10-07) [Auto] YAML syntax fixed \u2014 YAML errors: 3\u21920 (-3), warnings: 10\u21926 (-4), DQI: 98.0\u219298.8 (+0.8) (2025-10-07) [Auto] Placeholders replaced \u2014 placeholder warnings: 2\u21920 (-2), warnings: 6\u21926 (stable), DQI: 98.8\u219298.8 (stable) (2025-10-07) [Auto] Risk framework cleaned \u2014 syntax errors: 1\u21920 (-1), warnings: 6\u21925 (-1), DQI: 98.8\u219299.0 (+0.2) (2025-10-07) [Auto] Full pipeline completed \u2014 warnings: 5, DQI: 99.0/100 (\u226599 \u2705), snapshot: docs_snapshot_2025-10-07.tgz (928KB) (2025-10-07)","title":"\ud83d\udcde Contact Information"},{"location":"review/VALIDATION_SUMMARY/","text":"AI-Assisted Validation Summary \u00b6 Generated : 2025-10-07 12:28:23 Source : docs/review/validation_report.md \ud83d\udcca Overview \u00b6 Total Documents : 25 Critical Issues : 0 Warnings : 5 Categories : 2 \ud83d\udcc8 Documentation Quality Index (DQI) \u00b6 Score : 99.0/100 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent \ud83c\udff7\ufe0f Warnings by Category \u00b6 Code Block Issues \u00b6 Count : 3 Issues : Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax Terminology / Glossary Inconsistencies \u00b6 Count : 2 Issues : Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) \ud83d\udd25 Top 10 Most Frequent Issues \u00b6 1x - Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 1x - Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 1x - Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 1x - Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 1x - Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value \ud83d\udca1 Suggested Actions \u00b6 Add language hints to code blocks (3 issues): Specify python , yaml , json etc. for better syntax highlighting Standardize terminology (2 issues): Use consistent capitalization (e.g., 'SecFlow' not 'secflow') Run validation regularly : Use make validate before committing changes Review validation summary : Check VALIDATION_SUMMARY.md for detailed insights Follow glossary standards : Use docs/review/glossary.yml for terminology consistency Complete code examples : Replace placeholders with working code snippets \ud83d\udcc8 Document Health Score \u00b6 Document Warnings Criticals Health Score 00-index.md 0 0 \ud83d\udfe2 100% 01-title-and-executive-summary.md 0 0 \ud83d\udfe2 100% 02-architecture-philosophy.md 0 0 \ud83d\udfe2 100% 03-repository-layout.md 0 0 \ud83d\udfe2 100% 04-core-packages-and-responsibilities.md 0 0 \ud83d\udfe2 100% 05-orchestration-and-workflow-engine.md 0 0 \ud83d\udfe2 100% 07-tools-integration-model.md 0 0 \ud83d\udfe2 100% 09-resource-registry.md 0 0 \ud83d\udfe2 100% 10-wordlist-and-output-sharing.md 0 0 \ud83d\udfe2 100% 11-project-isolation-and-data-sharing.md 0 0 \ud83d\udfe2 100% 13-cve-cwe-poc-enrichment-layer.md 0 0 \ud83d\udfe2 100% 15-garbage-collection-and-retention.md 0 0 \ud83d\udfe2 100% 16-security-model.md 0 0 \ud83d\udfe2 100% 18-error-handling-and-recovery.md 0 0 \ud83d\udfe2 100% 19-risk-assessment-framework.md 0 0 \ud83d\udfe2 100% 20-migration-and-implementation-phases.md 0 0 \ud83d\udfe2 100% 21-ci-cd-and-testing-strategy.md 0 0 \ud83d\udfe2 100% 22-developer-experience-and-docs.md 0 0 \ud83d\udfe2 100% 23-future-roadmap.md 0 0 \ud83d\udfe2 100% 24-final-consensus-summary.md 0 0 \ud83d\udfe2 100% 06-plugin-system.md 1 0 \ud83d\udfe2 98% 08-tool-manager-and-ux-design.md 1 0 \ud83d\udfe2 98% 12-findings-model-and-schema.md 1 0 \ud83d\udfe2 98% 14-poc-sources-and-legal-guidelines.md 1 0 \ud83d\udfe2 98% 17-observability-logging-and-metrics.md 1 0 \ud83d\udfe2 98% \ud83c\udfaf Priority Recommendations \u00b6 Focus on high-warning documents : 06-plugin-system.md, 08-tool-manager-and-ux-design.md, 12-findings-model-and-schema.md Standardize terminology : Important for professional presentation Regular validation : Run make validate before each commit Team review : Use structured review process for major changes Continuous improvement : Monitor validation trends over time Generated by AI-Assisted Validation Summary Generator Last updated: 2025-10-07 12:28:23","title":"AI Summary"},{"location":"review/VALIDATION_SUMMARY/#ai-assisted-validation-summary","text":"Generated : 2025-10-07 12:28:23 Source : docs/review/validation_report.md","title":"AI-Assisted Validation Summary"},{"location":"review/VALIDATION_SUMMARY/#overview","text":"Total Documents : 25 Critical Issues : 0 Warnings : 5 Categories : 2","title":"\ud83d\udcca Overview"},{"location":"review/VALIDATION_SUMMARY/#documentation-quality-index-dqi","text":"Score : 99.0/100 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent","title":"\ud83d\udcc8 Documentation Quality Index (DQI)"},{"location":"review/VALIDATION_SUMMARY/#warnings-by-category","text":"","title":"\ud83c\udff7\ufe0f Warnings by Category"},{"location":"review/VALIDATION_SUMMARY/#code-block-issues","text":"Count : 3 Issues : Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax","title":"Code Block Issues"},{"location":"review/VALIDATION_SUMMARY/#terminology-glossary-inconsistencies","text":"Count : 2 Issues : Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow )","title":"Terminology / Glossary Inconsistencies"},{"location":"review/VALIDATION_SUMMARY/#top-10-most-frequent-issues","text":"1x - Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 1x - Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 1x - Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 1x - Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 1x - Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value","title":"\ud83d\udd25 Top 10 Most Frequent Issues"},{"location":"review/VALIDATION_SUMMARY/#suggested-actions","text":"Add language hints to code blocks (3 issues): Specify python , yaml , json etc. for better syntax highlighting Standardize terminology (2 issues): Use consistent capitalization (e.g., 'SecFlow' not 'secflow') Run validation regularly : Use make validate before committing changes Review validation summary : Check VALIDATION_SUMMARY.md for detailed insights Follow glossary standards : Use docs/review/glossary.yml for terminology consistency Complete code examples : Replace placeholders with working code snippets","title":"\ud83d\udca1 Suggested Actions"},{"location":"review/VALIDATION_SUMMARY/#document-health-score","text":"Document Warnings Criticals Health Score 00-index.md 0 0 \ud83d\udfe2 100% 01-title-and-executive-summary.md 0 0 \ud83d\udfe2 100% 02-architecture-philosophy.md 0 0 \ud83d\udfe2 100% 03-repository-layout.md 0 0 \ud83d\udfe2 100% 04-core-packages-and-responsibilities.md 0 0 \ud83d\udfe2 100% 05-orchestration-and-workflow-engine.md 0 0 \ud83d\udfe2 100% 07-tools-integration-model.md 0 0 \ud83d\udfe2 100% 09-resource-registry.md 0 0 \ud83d\udfe2 100% 10-wordlist-and-output-sharing.md 0 0 \ud83d\udfe2 100% 11-project-isolation-and-data-sharing.md 0 0 \ud83d\udfe2 100% 13-cve-cwe-poc-enrichment-layer.md 0 0 \ud83d\udfe2 100% 15-garbage-collection-and-retention.md 0 0 \ud83d\udfe2 100% 16-security-model.md 0 0 \ud83d\udfe2 100% 18-error-handling-and-recovery.md 0 0 \ud83d\udfe2 100% 19-risk-assessment-framework.md 0 0 \ud83d\udfe2 100% 20-migration-and-implementation-phases.md 0 0 \ud83d\udfe2 100% 21-ci-cd-and-testing-strategy.md 0 0 \ud83d\udfe2 100% 22-developer-experience-and-docs.md 0 0 \ud83d\udfe2 100% 23-future-roadmap.md 0 0 \ud83d\udfe2 100% 24-final-consensus-summary.md 0 0 \ud83d\udfe2 100% 06-plugin-system.md 1 0 \ud83d\udfe2 98% 08-tool-manager-and-ux-design.md 1 0 \ud83d\udfe2 98% 12-findings-model-and-schema.md 1 0 \ud83d\udfe2 98% 14-poc-sources-and-legal-guidelines.md 1 0 \ud83d\udfe2 98% 17-observability-logging-and-metrics.md 1 0 \ud83d\udfe2 98%","title":"\ud83d\udcc8 Document Health Score"},{"location":"review/VALIDATION_SUMMARY/#priority-recommendations","text":"Focus on high-warning documents : 06-plugin-system.md, 08-tool-manager-and-ux-design.md, 12-findings-model-and-schema.md Standardize terminology : Important for professional presentation Regular validation : Run make validate before each commit Team review : Use structured review process for major changes Continuous improvement : Monitor validation trends over time Generated by AI-Assisted Validation Summary Generator Last updated: 2025-10-07 12:28:23","title":"\ud83c\udfaf Priority Recommendations"},{"location":"review/mermaid-smoketest/","text":"Mermaid Smoketest \u00b6 This page tests that Mermaid diagrams are rendering correctly using the superfences + local Mermaid approach. \ud83e\uddea Test Diagram \u00b6 flowchart LR A[Start] --> B[Process] B --> C[End] \u2705 Expected Result \u00b6 You should see a flowchart diagram above with three boxes connected by arrows. \ud83d\udd04 Sequence Diagram Test \u00b6 sequenceDiagram participant A as Alice participant B as Bob A->>B: Hello Bob, how are you? B-->>A: Great! A-)B: See you later! \ud83d\udcca Class Diagram Test \u00b6 classDiagram class Animal { +String name +int age +makeSound() } class Dog { +String breed +bark() } Animal <|-- Dog \ud83c\udfaf State Diagram Test \u00b6 stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] \ud83d\udcc8 Expected Results \u00b6 All four diagrams above should render as interactive Mermaid diagrams, not as code blocks. ```","title":"Mermaid Smoketest"},{"location":"review/mermaid-smoketest/#mermaid-smoketest","text":"This page tests that Mermaid diagrams are rendering correctly using the superfences + local Mermaid approach.","title":"Mermaid Smoketest"},{"location":"review/mermaid-smoketest/#test-diagram","text":"flowchart LR A[Start] --> B[Process] B --> C[End]","title":"\ud83e\uddea Test Diagram"},{"location":"review/mermaid-smoketest/#expected-result","text":"You should see a flowchart diagram above with three boxes connected by arrows.","title":"\u2705 Expected Result"},{"location":"review/mermaid-smoketest/#sequence-diagram-test","text":"sequenceDiagram participant A as Alice participant B as Bob A->>B: Hello Bob, how are you? B-->>A: Great! A-)B: See you later!","title":"\ud83d\udd04 Sequence Diagram Test"},{"location":"review/mermaid-smoketest/#class-diagram-test","text":"classDiagram class Animal { +String name +int age +makeSound() } class Dog { +String breed +bark() } Animal <|-- Dog","title":"\ud83d\udcca Class Diagram Test"},{"location":"review/mermaid-smoketest/#state-diagram-test","text":"stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*]","title":"\ud83c\udfaf State Diagram Test"},{"location":"review/mermaid-smoketest/#expected-results","text":"All four diagrams above should render as interactive Mermaid diagrams, not as code blocks. ```","title":"\ud83d\udcc8 Expected Results"},{"location":"review/review_assignments/","text":"SecFlow Architecture Review Assignments \u00b6 \ud83d\udc65 Review Team Assignments \u00b6 This document outlines the specific review assignments for each team member, ensuring comprehensive coverage of all 24 architecture documents. \ud83c\udfaf Lead Architect Review (All Documents) \u00b6 Reviewer: Lead Architect Scope: All documents (00-24) Focus: Overall architecture consistency, design patterns, cross-document coherence Priority Documents \u00b6 00-index.md - Navigation and structure 02-architecture-philosophy.md - Core architectural principles 04-core-packages-and-responsibilities.md - Package structure and relationships 05-orchestration-and-workflow-engine.md - Workflow orchestration design 24-final-consensus-summary.md - Final architectural consensus Review Criteria \u00b6 Architectural consistency across all documents Design pattern alignment with hexagonal architecture Cross-document references and dependencies Overall system coherence and feasibility Strategic alignment with project goals \ud83d\udd27 Backend Engineers Review \u00b6 Reviewers: Senior Backend Engineer, Backend Developer Scope: Core technical implementation documents Focus: Data models, API design, database integration, workflow execution Assigned Documents \u00b6 04-core-packages-and-responsibilities.md Package structure and responsibilities Interface definitions and contracts Cross-package dependencies 05-orchestration-and-workflow-engine.md DAG execution engine design Workflow specification schema Node executor implementation 12-findings-model-and-schema.md Finding data model and normalization Database schema design Data validation and transformation 13-cve-cwe-poc-enrichment-layer.md Enrichment pipeline design External API integrations Data caching and synchronization 16-security-model.md Authentication and authorization implementation Security middleware design Audit logging mechanisms 17-observability-logging-and-metrics.md Logging infrastructure design Metrics collection and export Distributed tracing implementation 18-error-handling-and-recovery.md Error handling strategies Retry logic and circuit breakers Dead letter queue implementation Review Criteria \u00b6 Technical feasibility of implementation Code examples are syntactically correct Data models are complete and consistent API design follows RESTful principles Database schema is normalized and efficient Error handling is comprehensive Performance requirements are achievable \ud83c\udfa8 Frontend Engineers Review \u00b6 Reviewers: Frontend Developer, UX Designer Scope: User interface and experience documents Focus: UI/UX design, web interface, CLI experience, user workflows Assigned Documents \u00b6 08-tool-manager-and-ux-design.md Tool Manager UX design CLI and web interface specifications User workflow design 22-developer-experience-and-docs.md Developer tools and CLI utilities Documentation system design Local development workflow Review Criteria \u00b6 UI/UX design is intuitive and accessible CLI commands are logical and well-organized Web interface specifications are complete User workflows are efficient and clear Developer experience is optimized Documentation is user-friendly \ud83d\ude80 DevOps Engineers Review \u00b6 Reviewers: DevOps Engineer, Infrastructure Engineer Scope: Infrastructure, deployment, and operational documents Focus: CI/CD, deployment strategies, infrastructure requirements, monitoring Assigned Documents \u00b6 03-repository-layout.md Mono-repo structure and tooling Import boundaries and enforcement Development workflow automation 15-garbage-collection-and-retention.md Data lifecycle management Cleanup and retention policies Storage optimization strategies 17-observability-logging-and-metrics.md Monitoring and observability infrastructure Log aggregation and analysis Metrics collection and alerting 21-ci-cd-and-testing-strategy.md CI/CD pipeline design Testing framework and automation Deployment strategies and rollback Review Criteria \u00b6 Infrastructure requirements are realistic CI/CD pipeline is efficient and reliable Deployment strategies are safe and scalable Monitoring and observability are comprehensive Backup and recovery procedures are adequate Security scanning and compliance checks are integrated \ud83d\udd12 Security Engineers Review \u00b6 Reviewers: Security Engineer, Compliance Officer Scope: Security, compliance, and risk assessment documents Focus: Security model, compliance requirements, risk assessment, data protection Assigned Documents \u00b6 11-project-isolation-and-data-sharing.md Project isolation mechanisms Data sharing policies and controls Access control and permissions 14-poc-sources-and-legal-guidelines.md PoC governance and legal compliance Sandbox execution security Legal and ethical guidelines 16-security-model.md Authentication and authorization design Security middleware and controls Audit logging and compliance 19-risk-assessment-framework.md Risk scoring methodology NIST framework implementation CVSS and CWE integration Review Criteria \u00b6 Security controls are comprehensive and effective Compliance requirements are met Risk assessment methodology is sound Data protection measures are adequate Audit requirements are satisfied Legal and ethical guidelines are followed \ud83e\uddea QA Engineers Review \u00b6 Reviewers: QA Engineer, Test Automation Engineer Scope: Testing strategy and quality assurance documents Focus: Testing frameworks, quality gates, validation criteria, test automation Assigned Documents \u00b6 12-findings-model-and-schema.md Data validation and transformation testing Schema validation and error handling Data integrity testing 18-error-handling-and-recovery.md Error handling testing strategies Recovery mechanism validation Fault tolerance testing 21-ci-cd-and-testing-strategy.md Testing framework and automation Quality gates and validation criteria Test coverage and reporting Review Criteria \u00b6 Testing strategies are comprehensive Quality gates are appropriate and measurable Test automation is feasible and maintainable Error scenarios are adequately covered Performance testing requirements are defined Security testing is integrated \ud83d\udcca Product Manager Review \u00b6 Reviewer: Product Manager Scope: Business requirements and user experience documents Focus: Business value, user requirements, market fit, strategic alignment Assigned Documents \u00b6 01-title-and-executive-summary.md Project vision and goals Business value proposition Market positioning 08-tool-manager-and-ux-design.md User experience design Tool management workflows User interface specifications 20-migration-and-implementation-phases.md Implementation roadmap and timeline Phase deliverables and milestones Success criteria and metrics 23-future-roadmap.md Strategic evolution and roadmap AI integration and future capabilities Market trends and competitive analysis Review Criteria \u00b6 Business requirements are clearly defined User experience meets market expectations Implementation timeline is realistic Success criteria are measurable Strategic alignment with company goals Competitive advantages are clear \ud83d\udcc5 Review Schedule \u00b6 Week 1: Individual Reviews \u00b6 Monday-Tuesday: Backend Engineers review assigned documents Wednesday-Thursday: Frontend Engineers review assigned documents Friday: DevOps Engineers review assigned documents Week 2: Cross-Reviews \u00b6 Monday: Security Engineers review assigned documents Tuesday: QA Engineers review assigned documents Wednesday: Product Manager review assigned documents Thursday-Friday: Cross-review sessions (each team reviews outside their expertise) Week 3: Group Review Sessions \u00b6 Monday: Core Architecture Review (Documents 00-05) Tuesday: Implementation Review (Documents 06-12) Wednesday: Operations Review (Documents 13-18) Thursday: Strategy Review (Documents 19-24) Friday: Final Consolidation and Issue Resolution Week 4: Final Validation \u00b6 Monday-Tuesday: Lead Architect final review Wednesday: Stakeholder approval sessions Thursday: Final validation and sign-off Friday: Implementation readiness assessment \ud83d\udcdd Review Deliverables \u00b6 Individual Review Deliverables \u00b6 Completed review checklist for each assigned document Detailed feedback report with specific issues and recommendations Technical accuracy validation results Consistency and completeness assessment Cross-Review Deliverables \u00b6 Cross-document consistency validation Integration point verification Cross-functional requirement validation Stakeholder alignment confirmation Group Review Deliverables \u00b6 Consolidated issue list with priorities Resolution plan with timelines Final approval recommendations Implementation readiness assessment \ud83d\udea8 Escalation Process \u00b6 Critical Issues \u00b6 Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Timeline conflicts Escalation Path \u00b6 Identify critical issue during review Document issue with evidence and impact Notify Lead Architect immediately Schedule emergency review session Resolve with all stakeholders present Update review assignments if needed \u2705 Success Criteria \u00b6 The review process is considered successful when: - [ ] All 24 documents are reviewed by assigned team members - [ ] All critical issues are identified and resolved - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Development team is ready to begin implementation Next Steps: Begin individual reviews according to the assigned schedule and scope.","title":"Assignments"},{"location":"review/review_assignments/#secflow-architecture-review-assignments","text":"","title":"SecFlow Architecture Review Assignments"},{"location":"review/review_assignments/#review-team-assignments","text":"This document outlines the specific review assignments for each team member, ensuring comprehensive coverage of all 24 architecture documents.","title":"\ud83d\udc65 Review Team Assignments"},{"location":"review/review_assignments/#lead-architect-review-all-documents","text":"Reviewer: Lead Architect Scope: All documents (00-24) Focus: Overall architecture consistency, design patterns, cross-document coherence","title":"\ud83c\udfaf Lead Architect Review (All Documents)"},{"location":"review/review_assignments/#priority-documents","text":"00-index.md - Navigation and structure 02-architecture-philosophy.md - Core architectural principles 04-core-packages-and-responsibilities.md - Package structure and relationships 05-orchestration-and-workflow-engine.md - Workflow orchestration design 24-final-consensus-summary.md - Final architectural consensus","title":"Priority Documents"},{"location":"review/review_assignments/#review-criteria","text":"Architectural consistency across all documents Design pattern alignment with hexagonal architecture Cross-document references and dependencies Overall system coherence and feasibility Strategic alignment with project goals","title":"Review Criteria"},{"location":"review/review_assignments/#backend-engineers-review","text":"Reviewers: Senior Backend Engineer, Backend Developer Scope: Core technical implementation documents Focus: Data models, API design, database integration, workflow execution","title":"\ud83d\udd27 Backend Engineers Review"},{"location":"review/review_assignments/#assigned-documents","text":"04-core-packages-and-responsibilities.md Package structure and responsibilities Interface definitions and contracts Cross-package dependencies 05-orchestration-and-workflow-engine.md DAG execution engine design Workflow specification schema Node executor implementation 12-findings-model-and-schema.md Finding data model and normalization Database schema design Data validation and transformation 13-cve-cwe-poc-enrichment-layer.md Enrichment pipeline design External API integrations Data caching and synchronization 16-security-model.md Authentication and authorization implementation Security middleware design Audit logging mechanisms 17-observability-logging-and-metrics.md Logging infrastructure design Metrics collection and export Distributed tracing implementation 18-error-handling-and-recovery.md Error handling strategies Retry logic and circuit breakers Dead letter queue implementation","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_1","text":"Technical feasibility of implementation Code examples are syntactically correct Data models are complete and consistent API design follows RESTful principles Database schema is normalized and efficient Error handling is comprehensive Performance requirements are achievable","title":"Review Criteria"},{"location":"review/review_assignments/#frontend-engineers-review","text":"Reviewers: Frontend Developer, UX Designer Scope: User interface and experience documents Focus: UI/UX design, web interface, CLI experience, user workflows","title":"\ud83c\udfa8 Frontend Engineers Review"},{"location":"review/review_assignments/#assigned-documents_1","text":"08-tool-manager-and-ux-design.md Tool Manager UX design CLI and web interface specifications User workflow design 22-developer-experience-and-docs.md Developer tools and CLI utilities Documentation system design Local development workflow","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_2","text":"UI/UX design is intuitive and accessible CLI commands are logical and well-organized Web interface specifications are complete User workflows are efficient and clear Developer experience is optimized Documentation is user-friendly","title":"Review Criteria"},{"location":"review/review_assignments/#devops-engineers-review","text":"Reviewers: DevOps Engineer, Infrastructure Engineer Scope: Infrastructure, deployment, and operational documents Focus: CI/CD, deployment strategies, infrastructure requirements, monitoring","title":"\ud83d\ude80 DevOps Engineers Review"},{"location":"review/review_assignments/#assigned-documents_2","text":"03-repository-layout.md Mono-repo structure and tooling Import boundaries and enforcement Development workflow automation 15-garbage-collection-and-retention.md Data lifecycle management Cleanup and retention policies Storage optimization strategies 17-observability-logging-and-metrics.md Monitoring and observability infrastructure Log aggregation and analysis Metrics collection and alerting 21-ci-cd-and-testing-strategy.md CI/CD pipeline design Testing framework and automation Deployment strategies and rollback","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_3","text":"Infrastructure requirements are realistic CI/CD pipeline is efficient and reliable Deployment strategies are safe and scalable Monitoring and observability are comprehensive Backup and recovery procedures are adequate Security scanning and compliance checks are integrated","title":"Review Criteria"},{"location":"review/review_assignments/#security-engineers-review","text":"Reviewers: Security Engineer, Compliance Officer Scope: Security, compliance, and risk assessment documents Focus: Security model, compliance requirements, risk assessment, data protection","title":"\ud83d\udd12 Security Engineers Review"},{"location":"review/review_assignments/#assigned-documents_3","text":"11-project-isolation-and-data-sharing.md Project isolation mechanisms Data sharing policies and controls Access control and permissions 14-poc-sources-and-legal-guidelines.md PoC governance and legal compliance Sandbox execution security Legal and ethical guidelines 16-security-model.md Authentication and authorization design Security middleware and controls Audit logging and compliance 19-risk-assessment-framework.md Risk scoring methodology NIST framework implementation CVSS and CWE integration","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_4","text":"Security controls are comprehensive and effective Compliance requirements are met Risk assessment methodology is sound Data protection measures are adequate Audit requirements are satisfied Legal and ethical guidelines are followed","title":"Review Criteria"},{"location":"review/review_assignments/#qa-engineers-review","text":"Reviewers: QA Engineer, Test Automation Engineer Scope: Testing strategy and quality assurance documents Focus: Testing frameworks, quality gates, validation criteria, test automation","title":"\ud83e\uddea QA Engineers Review"},{"location":"review/review_assignments/#assigned-documents_4","text":"12-findings-model-and-schema.md Data validation and transformation testing Schema validation and error handling Data integrity testing 18-error-handling-and-recovery.md Error handling testing strategies Recovery mechanism validation Fault tolerance testing 21-ci-cd-and-testing-strategy.md Testing framework and automation Quality gates and validation criteria Test coverage and reporting","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_5","text":"Testing strategies are comprehensive Quality gates are appropriate and measurable Test automation is feasible and maintainable Error scenarios are adequately covered Performance testing requirements are defined Security testing is integrated","title":"Review Criteria"},{"location":"review/review_assignments/#product-manager-review","text":"Reviewer: Product Manager Scope: Business requirements and user experience documents Focus: Business value, user requirements, market fit, strategic alignment","title":"\ud83d\udcca Product Manager Review"},{"location":"review/review_assignments/#assigned-documents_5","text":"01-title-and-executive-summary.md Project vision and goals Business value proposition Market positioning 08-tool-manager-and-ux-design.md User experience design Tool management workflows User interface specifications 20-migration-and-implementation-phases.md Implementation roadmap and timeline Phase deliverables and milestones Success criteria and metrics 23-future-roadmap.md Strategic evolution and roadmap AI integration and future capabilities Market trends and competitive analysis","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_6","text":"Business requirements are clearly defined User experience meets market expectations Implementation timeline is realistic Success criteria are measurable Strategic alignment with company goals Competitive advantages are clear","title":"Review Criteria"},{"location":"review/review_assignments/#review-schedule","text":"","title":"\ud83d\udcc5 Review Schedule"},{"location":"review/review_assignments/#week-1-individual-reviews","text":"Monday-Tuesday: Backend Engineers review assigned documents Wednesday-Thursday: Frontend Engineers review assigned documents Friday: DevOps Engineers review assigned documents","title":"Week 1: Individual Reviews"},{"location":"review/review_assignments/#week-2-cross-reviews","text":"Monday: Security Engineers review assigned documents Tuesday: QA Engineers review assigned documents Wednesday: Product Manager review assigned documents Thursday-Friday: Cross-review sessions (each team reviews outside their expertise)","title":"Week 2: Cross-Reviews"},{"location":"review/review_assignments/#week-3-group-review-sessions","text":"Monday: Core Architecture Review (Documents 00-05) Tuesday: Implementation Review (Documents 06-12) Wednesday: Operations Review (Documents 13-18) Thursday: Strategy Review (Documents 19-24) Friday: Final Consolidation and Issue Resolution","title":"Week 3: Group Review Sessions"},{"location":"review/review_assignments/#week-4-final-validation","text":"Monday-Tuesday: Lead Architect final review Wednesday: Stakeholder approval sessions Thursday: Final validation and sign-off Friday: Implementation readiness assessment","title":"Week 4: Final Validation"},{"location":"review/review_assignments/#review-deliverables","text":"","title":"\ud83d\udcdd Review Deliverables"},{"location":"review/review_assignments/#individual-review-deliverables","text":"Completed review checklist for each assigned document Detailed feedback report with specific issues and recommendations Technical accuracy validation results Consistency and completeness assessment","title":"Individual Review Deliverables"},{"location":"review/review_assignments/#cross-review-deliverables","text":"Cross-document consistency validation Integration point verification Cross-functional requirement validation Stakeholder alignment confirmation","title":"Cross-Review Deliverables"},{"location":"review/review_assignments/#group-review-deliverables","text":"Consolidated issue list with priorities Resolution plan with timelines Final approval recommendations Implementation readiness assessment","title":"Group Review Deliverables"},{"location":"review/review_assignments/#escalation-process","text":"","title":"\ud83d\udea8 Escalation Process"},{"location":"review/review_assignments/#critical-issues","text":"Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Timeline conflicts","title":"Critical Issues"},{"location":"review/review_assignments/#escalation-path","text":"Identify critical issue during review Document issue with evidence and impact Notify Lead Architect immediately Schedule emergency review session Resolve with all stakeholders present Update review assignments if needed","title":"Escalation Path"},{"location":"review/review_assignments/#success-criteria","text":"The review process is considered successful when: - [ ] All 24 documents are reviewed by assigned team members - [ ] All critical issues are identified and resolved - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Development team is ready to begin implementation Next Steps: Begin individual reviews according to the assigned schedule and scope.","title":"\u2705 Success Criteria"},{"location":"review/review_workflow/","text":"SecFlow Architecture Review Workflow \u00b6 \ud83c\udfaf Overview \u00b6 This document defines the structured review workflow for the SecFlow architecture documentation suite, ensuring comprehensive validation and stakeholder approval before implementation begins. \ud83d\udccb Workflow Phases \u00b6 Phase 1: Pre-Review Setup (Day 1) \u00b6 Duration: 1 day Participants: Lead Architect, Review Coordinator Activities \u00b6 Review Environment Setup Create review workspace with all documents Set up collaboration tools (GitHub, Slack, etc.) Configure review tracking system Prepare review templates and checklists Stakeholder Notification Send review invitations to all team members Distribute review assignments and timelines Schedule review sessions and meetings Provide access to review tools and resources Review Preparation Run automated validation checks Generate initial validation report Identify any obvious issues for quick fixes Prepare review guidelines and instructions Deliverables \u00b6 Review workspace configured All stakeholders notified and onboarded Review tools and templates ready Initial validation report generated Phase 2: Individual Reviews (Days 2-6) \u00b6 Duration: 5 days Participants: All assigned reviewers Activities \u00b6 Document-Specific Reviews Each reviewer reviews their assigned documents Use provided checklists and templates Document all issues, suggestions, and feedback Validate technical accuracy and completeness Cross-Document Reviews Review documents outside primary expertise Check for consistency and integration issues Validate cross-references and dependencies Identify architectural inconsistencies Feedback Documentation Complete review templates for each document Categorize issues by severity and type Provide specific recommendations for improvements Document approval status for each document Deliverables \u00b6 Individual review reports for all documents Consolidated issue list with priorities Cross-document consistency validation Initial approval recommendations Phase 3: Cross-Review Sessions (Days 7-8) \u00b6 Duration: 2 days Participants: All reviewers Activities \u00b6 Cross-Functional Reviews Review documents outside primary expertise Validate integration points and dependencies Check for cross-functional requirements Ensure stakeholder alignment Consistency Validation Validate terminology and naming conventions Check architectural pattern consistency Verify data model consistency Validate security model alignment Issue Resolution Planning Prioritize issues by severity and impact Assign resolution responsibilities Set timelines for issue resolution Plan for emergency escalations Deliverables \u00b6 Cross-review validation reports Prioritized issue resolution plan Consistency validation results Stakeholder alignment confirmation Phase 4: Group Review Sessions (Days 9-11) \u00b6 Duration: 3 days Participants: All stakeholders Activities \u00b6 Document Category Reviews Day 9: Core Architecture (Documents 00-05) Day 10: Implementation (Documents 06-12) Day 11: Operations & Strategy (Documents 13-24) Stakeholder Approval Sessions Present review findings for each category Discuss critical issues and resolutions Validate implementation feasibility Obtain formal approval from stakeholders Issue Resolution Address critical issues immediately Plan resolution for non-critical issues Update documents based on feedback Validate changes and improvements Deliverables \u00b6 Group review session reports Stakeholder approval confirmations Resolved critical issues Updated documents with improvements Phase 5: Final Validation (Days 12-14) \u00b6 Duration: 3 days Participants: Lead Architect, Review Coordinator Activities \u00b6 Final Validation Run comprehensive validation checks Verify all issues have been addressed Validate document consistency and accuracy Confirm stakeholder approvals Implementation Readiness Assessment Validate technical feasibility Confirm resource availability Verify timeline alignment Assess risk mitigation strategies Documentation Finalization Update documents with final changes Generate final validation report Prepare implementation guidelines Create handoff documentation Deliverables \u00b6 Final validation report Implementation readiness assessment Updated architecture documentation Implementation handoff package \ud83d\udd04 Review Process Flow \u00b6 graph TD A[Pre-Review Setup] --> B[Individual Reviews] B --> C[Cross-Review Sessions] C --> D[Group Review Sessions] D --> E[Final Validation] E --> F[Implementation Ready] B --> G[Issue Identification] G --> H[Issue Resolution] H --> I[Document Updates] I --> C D --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> H J -->|No| E ```text ## \ud83d\udcca Review Metrics and KPIs ### Quality Metrics | Metric | Target | Measurement | |--------|--------|-------------| | **Review Coverage** | 100% | All documents reviewed by assigned roles | | **Issue Resolution** | 95% | Critical issues resolved before approval | | **Consistency Score** | \u2265 90% | Cross-document consistency rating | | **Technical Accuracy** | 100% | All code examples and diagrams validated | | **Security Compliance** | 100% | All security requirements met | ### Process Metrics | Metric | Target | Measurement | |--------|--------|-------------| | **Review Completion** | 100% | All assigned reviews completed on time | | **Stakeholder Approval** | 100% | All stakeholders approve their sections | | **Issue Response Time** | < 24 hours | Time to respond to critical issues | | **Document Update Time** | < 48 hours | Time to update documents after feedback | ## \ud83d\udea8 Escalation Procedures ### Critical Issues **Definition:** Issues that could prevent implementation or compromise security **Examples:** - Security vulnerabilities in design - Infeasible technical requirements - Major architectural inconsistencies - Compliance violations - Timeline conflicts **Escalation Process:** 1. **Identify** critical issue during review 2. **Document** issue with evidence and impact 3. **Notify** Lead Architect immediately 4. **Schedule** emergency review session within 4 hours 5. **Resolve** with all stakeholders present 6. **Update** review assignments if needed ### Non-Critical Issues **Definition:** Issues that should be addressed but don't block implementation **Examples:** - Minor formatting inconsistencies - Documentation improvements - Code style suggestions - Performance optimizations **Resolution Process:** 1. **Document** issue in review report 2. **Assign** resolution responsibility 3. **Set** timeline for resolution 4. **Track** progress in issue tracking system 5. **Validate** resolution before final approval ## \ud83d\udcdd Review Templates and Tools ### Individual Review Template ```markdown ## Document Review: [Document Name] **Reviewer:** [Name] **Date:** [Date] **Review Type:** [Individual/Cross/Group] ### Technical Accuracy - **Issues Found:** [List specific issues] - **Recommendations:** [Suggestions for improvement] - **Code Examples:** [Any syntax or logic errors] ### Completeness - **Missing Sections:** [List any missing content] - **Incomplete Examples:** [Identify incomplete examples] - **Unclear Requirements:** [Highlight unclear specifications] ### Consistency - **Terminology Issues:** [Inconsistent terms or definitions] - **Design Pattern Conflicts:** [Any architectural inconsistencies] - **Cross-Reference Problems:** [Broken or incorrect links] ### Overall Assessment - **Strengths:** [What works well] - **Critical Issues:** [Must-fix problems] - **Enhancement Opportunities:** [Nice-to-have improvements] - **Recommendation:** [Approve/Revise/Reject] Group Review Session Template \u00b6 ## Group Review Session: [Category] **Date:** [Date] **Participants:** [List participants] ### Documents Reviewed - [List documents reviewed] ### Key Issues Identified - [List critical issues] - [List non-critical issues] ### Resolutions Agreed - [List agreed resolutions] - [Assign responsibilities] - [Set timelines] ### Approval Status - [Document approval status] - [Stakeholder confirmations] - [Next steps] \ud83c\udfaf Success Criteria \u00b6 The review workflow is considered successful when: Quality Criteria \u00b6 All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Process Criteria \u00b6 All review phases completed on schedule All stakeholders actively participated All issues properly documented and resolved All approvals formally obtained Implementation team ready to begin Deliverable Criteria \u00b6 Final validation report generated Implementation readiness assessment completed Updated architecture documentation ready Implementation handoff package prepared Development team onboarded and ready \ud83d\udcde Contact Information \u00b6 Review Team \u00b6 Lead Architect: [Name] - [email] - [phone] Review Coordinator: [Name] - [email] - [phone] Backend Lead: [Name] - [email] - [phone] Frontend Lead: [Name] - [email] - [phone] DevOps Lead: [Name] - [email] - [phone] Security Lead: [Name] - [email] - [phone] Emergency Contacts \u00b6 Critical Issues: Lead Architect - [phone] Process Issues: Review Coordinator - [phone] Technical Issues: Backend Lead - [phone] \ud83d\udcc5 Timeline Summary \u00b6 Phase Duration Key Activities Deliverables Phase 1 1 day Pre-review setup Review environment ready Phase 2 5 days Individual reviews Review reports completed Phase 3 2 days Cross-review sessions Consistency validated Phase 4 3 days Group review sessions Stakeholder approvals Phase 5 3 days Final validation Implementation ready Total Duration: 14 days Target Completion: [Date] Next Steps: Begin Phase 1 - Pre-Review Setup","title":"Workflow"},{"location":"review/review_workflow/#secflow-architecture-review-workflow","text":"","title":"SecFlow Architecture Review Workflow"},{"location":"review/review_workflow/#overview","text":"This document defines the structured review workflow for the SecFlow architecture documentation suite, ensuring comprehensive validation and stakeholder approval before implementation begins.","title":"\ud83c\udfaf Overview"},{"location":"review/review_workflow/#workflow-phases","text":"","title":"\ud83d\udccb Workflow Phases"},{"location":"review/review_workflow/#phase-1-pre-review-setup-day-1","text":"Duration: 1 day Participants: Lead Architect, Review Coordinator","title":"Phase 1: Pre-Review Setup (Day 1)"},{"location":"review/review_workflow/#activities","text":"Review Environment Setup Create review workspace with all documents Set up collaboration tools (GitHub, Slack, etc.) Configure review tracking system Prepare review templates and checklists Stakeholder Notification Send review invitations to all team members Distribute review assignments and timelines Schedule review sessions and meetings Provide access to review tools and resources Review Preparation Run automated validation checks Generate initial validation report Identify any obvious issues for quick fixes Prepare review guidelines and instructions","title":"Activities"},{"location":"review/review_workflow/#deliverables","text":"Review workspace configured All stakeholders notified and onboarded Review tools and templates ready Initial validation report generated","title":"Deliverables"},{"location":"review/review_workflow/#phase-2-individual-reviews-days-2-6","text":"Duration: 5 days Participants: All assigned reviewers","title":"Phase 2: Individual Reviews (Days 2-6)"},{"location":"review/review_workflow/#activities_1","text":"Document-Specific Reviews Each reviewer reviews their assigned documents Use provided checklists and templates Document all issues, suggestions, and feedback Validate technical accuracy and completeness Cross-Document Reviews Review documents outside primary expertise Check for consistency and integration issues Validate cross-references and dependencies Identify architectural inconsistencies Feedback Documentation Complete review templates for each document Categorize issues by severity and type Provide specific recommendations for improvements Document approval status for each document","title":"Activities"},{"location":"review/review_workflow/#deliverables_1","text":"Individual review reports for all documents Consolidated issue list with priorities Cross-document consistency validation Initial approval recommendations","title":"Deliverables"},{"location":"review/review_workflow/#phase-3-cross-review-sessions-days-7-8","text":"Duration: 2 days Participants: All reviewers","title":"Phase 3: Cross-Review Sessions (Days 7-8)"},{"location":"review/review_workflow/#activities_2","text":"Cross-Functional Reviews Review documents outside primary expertise Validate integration points and dependencies Check for cross-functional requirements Ensure stakeholder alignment Consistency Validation Validate terminology and naming conventions Check architectural pattern consistency Verify data model consistency Validate security model alignment Issue Resolution Planning Prioritize issues by severity and impact Assign resolution responsibilities Set timelines for issue resolution Plan for emergency escalations","title":"Activities"},{"location":"review/review_workflow/#deliverables_2","text":"Cross-review validation reports Prioritized issue resolution plan Consistency validation results Stakeholder alignment confirmation","title":"Deliverables"},{"location":"review/review_workflow/#phase-4-group-review-sessions-days-9-11","text":"Duration: 3 days Participants: All stakeholders","title":"Phase 4: Group Review Sessions (Days 9-11)"},{"location":"review/review_workflow/#activities_3","text":"Document Category Reviews Day 9: Core Architecture (Documents 00-05) Day 10: Implementation (Documents 06-12) Day 11: Operations & Strategy (Documents 13-24) Stakeholder Approval Sessions Present review findings for each category Discuss critical issues and resolutions Validate implementation feasibility Obtain formal approval from stakeholders Issue Resolution Address critical issues immediately Plan resolution for non-critical issues Update documents based on feedback Validate changes and improvements","title":"Activities"},{"location":"review/review_workflow/#deliverables_3","text":"Group review session reports Stakeholder approval confirmations Resolved critical issues Updated documents with improvements","title":"Deliverables"},{"location":"review/review_workflow/#phase-5-final-validation-days-12-14","text":"Duration: 3 days Participants: Lead Architect, Review Coordinator","title":"Phase 5: Final Validation (Days 12-14)"},{"location":"review/review_workflow/#activities_4","text":"Final Validation Run comprehensive validation checks Verify all issues have been addressed Validate document consistency and accuracy Confirm stakeholder approvals Implementation Readiness Assessment Validate technical feasibility Confirm resource availability Verify timeline alignment Assess risk mitigation strategies Documentation Finalization Update documents with final changes Generate final validation report Prepare implementation guidelines Create handoff documentation","title":"Activities"},{"location":"review/review_workflow/#deliverables_4","text":"Final validation report Implementation readiness assessment Updated architecture documentation Implementation handoff package","title":"Deliverables"},{"location":"review/review_workflow/#review-process-flow","text":"graph TD A[Pre-Review Setup] --> B[Individual Reviews] B --> C[Cross-Review Sessions] C --> D[Group Review Sessions] D --> E[Final Validation] E --> F[Implementation Ready] B --> G[Issue Identification] G --> H[Issue Resolution] H --> I[Document Updates] I --> C D --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> H J -->|No| E ```text ## \ud83d\udcca Review Metrics and KPIs ### Quality Metrics | Metric | Target | Measurement | |--------|--------|-------------| | **Review Coverage** | 100% | All documents reviewed by assigned roles | | **Issue Resolution** | 95% | Critical issues resolved before approval | | **Consistency Score** | \u2265 90% | Cross-document consistency rating | | **Technical Accuracy** | 100% | All code examples and diagrams validated | | **Security Compliance** | 100% | All security requirements met | ### Process Metrics | Metric | Target | Measurement | |--------|--------|-------------| | **Review Completion** | 100% | All assigned reviews completed on time | | **Stakeholder Approval** | 100% | All stakeholders approve their sections | | **Issue Response Time** | < 24 hours | Time to respond to critical issues | | **Document Update Time** | < 48 hours | Time to update documents after feedback | ## \ud83d\udea8 Escalation Procedures ### Critical Issues **Definition:** Issues that could prevent implementation or compromise security **Examples:** - Security vulnerabilities in design - Infeasible technical requirements - Major architectural inconsistencies - Compliance violations - Timeline conflicts **Escalation Process:** 1. **Identify** critical issue during review 2. **Document** issue with evidence and impact 3. **Notify** Lead Architect immediately 4. **Schedule** emergency review session within 4 hours 5. **Resolve** with all stakeholders present 6. **Update** review assignments if needed ### Non-Critical Issues **Definition:** Issues that should be addressed but don't block implementation **Examples:** - Minor formatting inconsistencies - Documentation improvements - Code style suggestions - Performance optimizations **Resolution Process:** 1. **Document** issue in review report 2. **Assign** resolution responsibility 3. **Set** timeline for resolution 4. **Track** progress in issue tracking system 5. **Validate** resolution before final approval ## \ud83d\udcdd Review Templates and Tools ### Individual Review Template ```markdown ## Document Review: [Document Name] **Reviewer:** [Name] **Date:** [Date] **Review Type:** [Individual/Cross/Group] ### Technical Accuracy - **Issues Found:** [List specific issues] - **Recommendations:** [Suggestions for improvement] - **Code Examples:** [Any syntax or logic errors] ### Completeness - **Missing Sections:** [List any missing content] - **Incomplete Examples:** [Identify incomplete examples] - **Unclear Requirements:** [Highlight unclear specifications] ### Consistency - **Terminology Issues:** [Inconsistent terms or definitions] - **Design Pattern Conflicts:** [Any architectural inconsistencies] - **Cross-Reference Problems:** [Broken or incorrect links] ### Overall Assessment - **Strengths:** [What works well] - **Critical Issues:** [Must-fix problems] - **Enhancement Opportunities:** [Nice-to-have improvements] - **Recommendation:** [Approve/Revise/Reject]","title":"\ud83d\udd04 Review Process Flow"},{"location":"review/review_workflow/#group-review-session-template","text":"## Group Review Session: [Category] **Date:** [Date] **Participants:** [List participants] ### Documents Reviewed - [List documents reviewed] ### Key Issues Identified - [List critical issues] - [List non-critical issues] ### Resolutions Agreed - [List agreed resolutions] - [Assign responsibilities] - [Set timelines] ### Approval Status - [Document approval status] - [Stakeholder confirmations] - [Next steps]","title":"Group Review Session Template"},{"location":"review/review_workflow/#success-criteria","text":"The review workflow is considered successful when:","title":"\ud83c\udfaf Success Criteria"},{"location":"review/review_workflow/#quality-criteria","text":"All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible","title":"Quality Criteria"},{"location":"review/review_workflow/#process-criteria","text":"All review phases completed on schedule All stakeholders actively participated All issues properly documented and resolved All approvals formally obtained Implementation team ready to begin","title":"Process Criteria"},{"location":"review/review_workflow/#deliverable-criteria","text":"Final validation report generated Implementation readiness assessment completed Updated architecture documentation ready Implementation handoff package prepared Development team onboarded and ready","title":"Deliverable Criteria"},{"location":"review/review_workflow/#contact-information","text":"","title":"\ud83d\udcde Contact Information"},{"location":"review/review_workflow/#review-team","text":"Lead Architect: [Name] - [email] - [phone] Review Coordinator: [Name] - [email] - [phone] Backend Lead: [Name] - [email] - [phone] Frontend Lead: [Name] - [email] - [phone] DevOps Lead: [Name] - [email] - [phone] Security Lead: [Name] - [email] - [phone]","title":"Review Team"},{"location":"review/review_workflow/#emergency-contacts","text":"Critical Issues: Lead Architect - [phone] Process Issues: Review Coordinator - [phone] Technical Issues: Backend Lead - [phone]","title":"Emergency Contacts"},{"location":"review/review_workflow/#timeline-summary","text":"Phase Duration Key Activities Deliverables Phase 1 1 day Pre-review setup Review environment ready Phase 2 5 days Individual reviews Review reports completed Phase 3 2 days Cross-review sessions Consistency validated Phase 4 3 days Group review sessions Stakeholder approvals Phase 5 3 days Final validation Implementation ready Total Duration: 14 days Target Completion: [Date] Next Steps: Begin Phase 1 - Pre-Review Setup","title":"\ud83d\udcc5 Timeline Summary"},{"location":"review/validation_checklist/","text":"SecFlow Architecture Validation Checklist \u00b6 \ud83c\udfaf Overview \u00b6 This comprehensive validation checklist ensures all architecture documents meet quality standards before implementation begins. \ud83d\udccb Document Structure Validation \u00b6 YAML Metadata Validation \u00b6 All documents have required YAML frontmatter Title, author, codename, version, and date are present Version format follows semantic versioning (e.g., \"1.0\") Date format is consistent (YYYY-MM-DD) Content Structure Validation \u00b6 Each document has a clear overview section ASCII diagrams are properly formatted Code examples are syntax-highlighted Tables are properly formatted Internal links use correct MkDocs format Section headers follow consistent hierarchy \ud83d\udd0d Technical Content Validation \u00b6 Code Examples \u00b6 Python code examples are syntactically correct YAML configurations are valid JSON examples are properly formatted Import statements are accurate Function signatures are complete Error handling is demonstrated Architecture Diagrams \u00b6 ASCII diagrams are readable and accurate Component relationships are clearly shown Data flow directions are indicated Layer boundaries are defined External dependencies are identified Data Models \u00b6 Pydantic models are complete and valid Field types are correctly specified Validation rules are appropriate Relationships between models are clear Database schemas are consistent \ud83c\udfd7\ufe0f Architecture Consistency Validation \u00b6 Design Patterns \u00b6 Hexagonal architecture principles are consistently applied Port and adapter patterns are properly implemented Event-driven design is consistently used Immutable data flow is maintained Dependency inversion is followed Naming Conventions \u00b6 Package names follow Python conventions Class names use PascalCase Function names use snake_case Constants use UPPER_CASE File names are descriptive and consistent Cross-Document References \u00b6 All internal links are valid Referenced concepts are defined Data model references are consistent API endpoint references are accurate Configuration references are valid \ud83d\udd12 Security Validation \u00b6 Security Model \u00b6 Authentication mechanisms are clearly defined Authorization model is comprehensive Data encryption requirements are specified Audit logging is properly designed Security boundaries are clearly defined Compliance Requirements \u00b6 NIST framework references are accurate CVSS scoring methodology is correct CWE/OWASP mappings are valid MITRE ATT&CK references are current Data protection requirements are met Risk Assessment \u00b6 Risk scoring formulas are mathematically sound Risk factors are properly weighted Risk thresholds are realistic Risk mitigation strategies are feasible Risk reporting is comprehensive \ud83d\ude80 Implementation Feasibility Validation \u00b6 Technical Requirements \u00b6 All dependencies are available and maintained Performance requirements are achievable Scalability requirements are realistic Integration points are well-defined Error handling strategies are complete Resource Requirements \u00b6 Infrastructure requirements are realistic Development effort estimates are accurate Testing requirements are comprehensive Deployment complexity is manageable Maintenance requirements are clear Migration Strategy \u00b6 Phase transitions are clearly defined Rollback strategies are specified Data migration plans are feasible Testing strategies are comprehensive Success criteria are measurable \ud83d\udcca Quality Metrics Validation \u00b6 Completeness \u00b6 All required sections are present Examples cover all major use cases Edge cases are addressed Error scenarios are documented Future considerations are included Clarity \u00b6 Technical concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Language is professional and accessible Document structure is logical Accuracy \u00b6 Technical specifications are correct Code examples are functional Architecture diagrams are accurate Data models are consistent Integration points are valid \ud83d\udd04 Process Validation \u00b6 Review Process \u00b6 All stakeholders have reviewed their sections Cross-reviews have been completed Group review sessions have been conducted All issues have been resolved Final approval has been obtained Documentation Standards \u00b6 All documents follow the established template Formatting is consistent across documents Links and references are valid Version control is properly maintained Change tracking is documented \u2705 Final Validation Checklist \u00b6 Pre-Implementation Validation \u00b6 All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Risk assessment methodology is approved by security team Implementation Readiness \u00b6 Development environment setup is documented CI/CD pipeline configuration is ready Testing framework is established Security scanning tools are configured Monitoring and observability tools are ready Documentation generation process is automated \ud83d\udea8 Critical Issues Requiring Immediate Attention \u00b6 Security Issues \u00b6 Any security vulnerabilities in design Missing security controls Inadequate data protection measures Compliance violations Audit trail gaps Technical Issues \u00b6 Infeasible technical requirements Major architectural inconsistencies Performance bottlenecks Integration failures Data model conflicts Process Issues \u00b6 Missing stakeholder approvals Incomplete review process Unresolved critical issues Timeline conflicts Resource constraints \ud83d\udcdd Validation Report Template \u00b6 Document: [Document Name] \u00b6 Validator: [Name] Date: [Date] Status: [Pass/Fail/Needs Revision] Technical Validation \u00b6 Code Examples: [Pass/Fail] - [Comments] Architecture Diagrams: [Pass/Fail] - [Comments] Data Models: [Pass/Fail] - [Comments] Integration Points: [Pass/Fail] - [Comments] Security Validation \u00b6 Security Model: [Pass/Fail] - [Comments] Compliance Requirements: [Pass/Fail] - [Comments] Risk Assessment: [Pass/Fail] - [Comments] Data Protection: [Pass/Fail] - [Comments] Process Validation \u00b6 Review Process: [Pass/Fail] - [Comments] Documentation Standards: [Pass/Fail] - [Comments] Stakeholder Approval: [Pass/Fail] - [Comments] Implementation Readiness: [Pass/Fail] - [Comments] Overall Assessment \u00b6 Strengths: [List strengths] Issues Found: [List issues] Recommendations: [List recommendations] Final Recommendation: [Approve/Revise/Reject] \ud83c\udfaf Success Criteria \u00b6 The architecture validation is considered successful when: - [ ] All documents pass technical accuracy validation - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team - [ ] Development team is ready to begin implementation Next Steps: Use this checklist to validate each document systematically before proceeding to implementation.","title":"Checklist"},{"location":"review/validation_checklist/#secflow-architecture-validation-checklist","text":"","title":"SecFlow Architecture Validation Checklist"},{"location":"review/validation_checklist/#overview","text":"This comprehensive validation checklist ensures all architecture documents meet quality standards before implementation begins.","title":"\ud83c\udfaf Overview"},{"location":"review/validation_checklist/#document-structure-validation","text":"","title":"\ud83d\udccb Document Structure Validation"},{"location":"review/validation_checklist/#yaml-metadata-validation","text":"All documents have required YAML frontmatter Title, author, codename, version, and date are present Version format follows semantic versioning (e.g., \"1.0\") Date format is consistent (YYYY-MM-DD)","title":"YAML Metadata Validation"},{"location":"review/validation_checklist/#content-structure-validation","text":"Each document has a clear overview section ASCII diagrams are properly formatted Code examples are syntax-highlighted Tables are properly formatted Internal links use correct MkDocs format Section headers follow consistent hierarchy","title":"Content Structure Validation"},{"location":"review/validation_checklist/#technical-content-validation","text":"","title":"\ud83d\udd0d Technical Content Validation"},{"location":"review/validation_checklist/#code-examples","text":"Python code examples are syntactically correct YAML configurations are valid JSON examples are properly formatted Import statements are accurate Function signatures are complete Error handling is demonstrated","title":"Code Examples"},{"location":"review/validation_checklist/#architecture-diagrams","text":"ASCII diagrams are readable and accurate Component relationships are clearly shown Data flow directions are indicated Layer boundaries are defined External dependencies are identified","title":"Architecture Diagrams"},{"location":"review/validation_checklist/#data-models","text":"Pydantic models are complete and valid Field types are correctly specified Validation rules are appropriate Relationships between models are clear Database schemas are consistent","title":"Data Models"},{"location":"review/validation_checklist/#architecture-consistency-validation","text":"","title":"\ud83c\udfd7\ufe0f Architecture Consistency Validation"},{"location":"review/validation_checklist/#design-patterns","text":"Hexagonal architecture principles are consistently applied Port and adapter patterns are properly implemented Event-driven design is consistently used Immutable data flow is maintained Dependency inversion is followed","title":"Design Patterns"},{"location":"review/validation_checklist/#naming-conventions","text":"Package names follow Python conventions Class names use PascalCase Function names use snake_case Constants use UPPER_CASE File names are descriptive and consistent","title":"Naming Conventions"},{"location":"review/validation_checklist/#cross-document-references","text":"All internal links are valid Referenced concepts are defined Data model references are consistent API endpoint references are accurate Configuration references are valid","title":"Cross-Document References"},{"location":"review/validation_checklist/#security-validation","text":"","title":"\ud83d\udd12 Security Validation"},{"location":"review/validation_checklist/#security-model","text":"Authentication mechanisms are clearly defined Authorization model is comprehensive Data encryption requirements are specified Audit logging is properly designed Security boundaries are clearly defined","title":"Security Model"},{"location":"review/validation_checklist/#compliance-requirements","text":"NIST framework references are accurate CVSS scoring methodology is correct CWE/OWASP mappings are valid MITRE ATT&CK references are current Data protection requirements are met","title":"Compliance Requirements"},{"location":"review/validation_checklist/#risk-assessment","text":"Risk scoring formulas are mathematically sound Risk factors are properly weighted Risk thresholds are realistic Risk mitigation strategies are feasible Risk reporting is comprehensive","title":"Risk Assessment"},{"location":"review/validation_checklist/#implementation-feasibility-validation","text":"","title":"\ud83d\ude80 Implementation Feasibility Validation"},{"location":"review/validation_checklist/#technical-requirements","text":"All dependencies are available and maintained Performance requirements are achievable Scalability requirements are realistic Integration points are well-defined Error handling strategies are complete","title":"Technical Requirements"},{"location":"review/validation_checklist/#resource-requirements","text":"Infrastructure requirements are realistic Development effort estimates are accurate Testing requirements are comprehensive Deployment complexity is manageable Maintenance requirements are clear","title":"Resource Requirements"},{"location":"review/validation_checklist/#migration-strategy","text":"Phase transitions are clearly defined Rollback strategies are specified Data migration plans are feasible Testing strategies are comprehensive Success criteria are measurable","title":"Migration Strategy"},{"location":"review/validation_checklist/#quality-metrics-validation","text":"","title":"\ud83d\udcca Quality Metrics Validation"},{"location":"review/validation_checklist/#completeness","text":"All required sections are present Examples cover all major use cases Edge cases are addressed Error scenarios are documented Future considerations are included","title":"Completeness"},{"location":"review/validation_checklist/#clarity","text":"Technical concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Language is professional and accessible Document structure is logical","title":"Clarity"},{"location":"review/validation_checklist/#accuracy","text":"Technical specifications are correct Code examples are functional Architecture diagrams are accurate Data models are consistent Integration points are valid","title":"Accuracy"},{"location":"review/validation_checklist/#process-validation","text":"","title":"\ud83d\udd04 Process Validation"},{"location":"review/validation_checklist/#review-process","text":"All stakeholders have reviewed their sections Cross-reviews have been completed Group review sessions have been conducted All issues have been resolved Final approval has been obtained","title":"Review Process"},{"location":"review/validation_checklist/#documentation-standards","text":"All documents follow the established template Formatting is consistent across documents Links and references are valid Version control is properly maintained Change tracking is documented","title":"Documentation Standards"},{"location":"review/validation_checklist/#final-validation-checklist","text":"","title":"\u2705 Final Validation Checklist"},{"location":"review/validation_checklist/#pre-implementation-validation","text":"All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Risk assessment methodology is approved by security team","title":"Pre-Implementation Validation"},{"location":"review/validation_checklist/#implementation-readiness","text":"Development environment setup is documented CI/CD pipeline configuration is ready Testing framework is established Security scanning tools are configured Monitoring and observability tools are ready Documentation generation process is automated","title":"Implementation Readiness"},{"location":"review/validation_checklist/#critical-issues-requiring-immediate-attention","text":"","title":"\ud83d\udea8 Critical Issues Requiring Immediate Attention"},{"location":"review/validation_checklist/#security-issues","text":"Any security vulnerabilities in design Missing security controls Inadequate data protection measures Compliance violations Audit trail gaps","title":"Security Issues"},{"location":"review/validation_checklist/#technical-issues","text":"Infeasible technical requirements Major architectural inconsistencies Performance bottlenecks Integration failures Data model conflicts","title":"Technical Issues"},{"location":"review/validation_checklist/#process-issues","text":"Missing stakeholder approvals Incomplete review process Unresolved critical issues Timeline conflicts Resource constraints","title":"Process Issues"},{"location":"review/validation_checklist/#validation-report-template","text":"","title":"\ud83d\udcdd Validation Report Template"},{"location":"review/validation_checklist/#document-document-name","text":"Validator: [Name] Date: [Date] Status: [Pass/Fail/Needs Revision]","title":"Document: [Document Name]"},{"location":"review/validation_checklist/#technical-validation","text":"Code Examples: [Pass/Fail] - [Comments] Architecture Diagrams: [Pass/Fail] - [Comments] Data Models: [Pass/Fail] - [Comments] Integration Points: [Pass/Fail] - [Comments]","title":"Technical Validation"},{"location":"review/validation_checklist/#security-validation_1","text":"Security Model: [Pass/Fail] - [Comments] Compliance Requirements: [Pass/Fail] - [Comments] Risk Assessment: [Pass/Fail] - [Comments] Data Protection: [Pass/Fail] - [Comments]","title":"Security Validation"},{"location":"review/validation_checklist/#process-validation_1","text":"Review Process: [Pass/Fail] - [Comments] Documentation Standards: [Pass/Fail] - [Comments] Stakeholder Approval: [Pass/Fail] - [Comments] Implementation Readiness: [Pass/Fail] - [Comments]","title":"Process Validation"},{"location":"review/validation_checklist/#overall-assessment","text":"Strengths: [List strengths] Issues Found: [List issues] Recommendations: [List recommendations] Final Recommendation: [Approve/Revise/Reject]","title":"Overall Assessment"},{"location":"review/validation_checklist/#success-criteria","text":"The architecture validation is considered successful when: - [ ] All documents pass technical accuracy validation - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team - [ ] Development team is ready to begin implementation Next Steps: Use this checklist to validate each document systematically before proceeding to implementation.","title":"\ud83c\udfaf Success Criteria"},{"location":"review/validation_report/","text":"Validation Report \u2014 2025-10-07T09:28:17.078745Z \u00b6 Criticals : 0 Warnings : 5 Document Criticals Warnings 00-index.md 0 0 01-title-and-executive-summary.md 0 0 02-architecture-philosophy.md 0 0 03-repository-layout.md 0 0 04-core-packages-and-responsibilities.md 0 0 05-orchestration-and-workflow-engine.md 0 0 06-plugin-system.md 0 1 07-tools-integration-model.md 0 0 08-tool-manager-and-ux-design.md 0 1 09-resource-registry.md 0 0 10-wordlist-and-output-sharing.md 0 0 11-project-isolation-and-data-sharing.md 0 0 12-findings-model-and-schema.md 0 1 13-cve-cwe-poc-enrichment-layer.md 0 0 14-poc-sources-and-legal-guidelines.md 0 1 15-garbage-collection-and-retention.md 0 0 16-security-model.md 0 0 17-observability-logging-and-metrics.md 0 1 18-error-handling-and-recovery.md 0 0 19-risk-assessment-framework.md 0 0 20-migration-and-implementation-phases.md 0 0 21-ci-cd-and-testing-strategy.md 0 0 22-developer-experience-and-docs.md 0 0 23-future-roadmap.md 0 0 24-final-consensus-summary.md 0 0 06-plugin-system.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 08-tool-manager-and-ux-design.md \u00b6 \u26a0\ufe0f Warnings Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 12-findings-model-and-schema.md \u00b6 \u26a0\ufe0f Warnings Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 14-poc-sources-and-legal-guidelines.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 17-observability-logging-and-metrics.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value \ud83d\udcc8 Documentation Quality Index \u00b6 Score : 99.0/100 Criticals : 0 Warnings : 5 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent Trend : \u27a1\ufe0f Stable","title":"Report"},{"location":"review/validation_report/#validation-report-2025-10-07t092817078745z","text":"Criticals : 0 Warnings : 5 Document Criticals Warnings 00-index.md 0 0 01-title-and-executive-summary.md 0 0 02-architecture-philosophy.md 0 0 03-repository-layout.md 0 0 04-core-packages-and-responsibilities.md 0 0 05-orchestration-and-workflow-engine.md 0 0 06-plugin-system.md 0 1 07-tools-integration-model.md 0 0 08-tool-manager-and-ux-design.md 0 1 09-resource-registry.md 0 0 10-wordlist-and-output-sharing.md 0 0 11-project-isolation-and-data-sharing.md 0 0 12-findings-model-and-schema.md 0 1 13-cve-cwe-poc-enrichment-layer.md 0 0 14-poc-sources-and-legal-guidelines.md 0 1 15-garbage-collection-and-retention.md 0 0 16-security-model.md 0 0 17-observability-logging-and-metrics.md 0 1 18-error-handling-and-recovery.md 0 0 19-risk-assessment-framework.md 0 0 20-migration-and-implementation-phases.md 0 0 21-ci-cd-and-testing-strategy.md 0 0 22-developer-experience-and-docs.md 0 0 23-future-roadmap.md 0 0 24-final-consensus-summary.md 0 0","title":"Validation Report \u2014 2025-10-07T09:28:17.078745Z"},{"location":"review/validation_report/#06-plugin-systemmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax","title":"06-plugin-system.md"},{"location":"review/validation_report/#08-tool-manager-and-ux-designmd","text":"\u26a0\ufe0f Warnings Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow )","title":"08-tool-manager-and-ux-design.md"},{"location":"review/validation_report/#12-findings-model-and-schemamd","text":"\u26a0\ufe0f Warnings Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS )","title":"12-findings-model-and-schema.md"},{"location":"review/validation_report/#14-poc-sources-and-legal-guidelinesmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax","title":"14-poc-sources-and-legal-guidelines.md"},{"location":"review/validation_report/#17-observability-logging-and-metricsmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value","title":"17-observability-logging-and-metrics.md"},{"location":"review/validation_report/#documentation-quality-index","text":"Score : 99.0/100 Criticals : 0 Warnings : 5 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent Trend : \u27a1\ufe0f Stable","title":"\ud83d\udcc8 Documentation Quality Index"}]}