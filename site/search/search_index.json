{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SecFlow Documentation \u00b6 This is the comprehensive documentation portal for the SecFlow security toolkit orchestration framework. \ud83d\udcda Documentation Sections \u00b6 \ud83c\udfd7\ufe0f Architecture \u00b6 Complete technical architecture documentation covering: - Repository layout and package structure - Core packages and responsibilities - Workflow orchestration and plugin systems - Tools integration and resource management - Security models and observability \ud83d\udd0d Review & Validation \u00b6 Documentation validation and review tools: - Review guidelines and workflow - Validation checklists and reports - AI-assisted validation summaries - Mermaid diagram testing \ud83d\ude80 Quick Start \u00b6 Architecture Overview : Start with Home for the high-level system overview Executive Summary : Read the Executive Summary for key concepts Validation Portal : Check the Review & Validation section for documentation standards \ud83d\udcca Current Status \u00b6 \u2705 Mermaid Diagrams : All diagrams rendering correctly \u2705 ASCII Artifacts : Eliminated from documentation \u2705 CI Gates : Automated validation in place \u2705 Navigation : All pages accessible and properly linked Built with MkDocs | Last Updated : 2025-10-08","title":"SecFlow Documentation"},{"location":"#welcome-to-secflow-documentation","text":"This is the comprehensive documentation portal for the SecFlow security toolkit orchestration framework.","title":"Welcome to SecFlow Documentation"},{"location":"#documentation-sections","text":"","title":"\ud83d\udcda Documentation Sections"},{"location":"#architecture","text":"Complete technical architecture documentation covering: - Repository layout and package structure - Core packages and responsibilities - Workflow orchestration and plugin systems - Tools integration and resource management - Security models and observability","title":"\ud83c\udfd7\ufe0f Architecture"},{"location":"#review-validation","text":"Documentation validation and review tools: - Review guidelines and workflow - Validation checklists and reports - AI-assisted validation summaries - Mermaid diagram testing","title":"\ud83d\udd0d Review &amp; Validation"},{"location":"#quick-start","text":"Architecture Overview : Start with Home for the high-level system overview Executive Summary : Read the Executive Summary for key concepts Validation Portal : Check the Review & Validation section for documentation standards","title":"\ud83d\ude80 Quick Start"},{"location":"#current-status","text":"\u2705 Mermaid Diagrams : All diagrams rendering correctly \u2705 ASCII Artifacts : Eliminated from documentation \u2705 CI Gates : Automated validation in place \u2705 Navigation : All pages accessible and properly linked Built with MkDocs | Last Updated : 2025-10-08","title":"\ud83d\udcca Current Status"},{"location":"developer-start-here/","text":"Developer Start Here \u00b6 Quick Setup \u00b6 Clone repo, install Poetry, poetry install --no-root Branching: trunk-based; feature branches feat/... ; PR \u2264400 LOC or 2 approvals Run locally: make test ; fast loop: make quick-test Docs: mkdocs serve (local), make health before committing docs Read: docs/architecture/00-index.md , storage contracts, finding schema DoD checklist is in PR template; paste validation evidence in PRs Essential Reading \u00b6 Read this first: Governance & Engineering Standards This page contains the complete Definition of Done checklist, branching rules, CI pipeline order, coverage requirements, and documentation health gates that all contributors must follow. Development Practices: Development Conventions This page covers daily development workflows, SOD/EOD rituals, branch naming conventions, PR rules, and CI fast-fail order that every developer needs to follow. Getting Started \u00b6 Prerequisites \u00b6 Poetry : Install Poetry for dependency management Git : For version control and branching Make : For running build commands Installation \u00b6 # Clone the repository git clone <repository-url> cd secflow # Install dependencies (without root package) poetry install --no-root Development Workflow \u00b6 Branching Strategy \u00b6 Trunk-based development : Main branch is the source of truth Feature branches : Use feat/... prefix for new features Pull Request limits : \u2264400 LOC or require 2 approvals Testing \u00b6 Full test suite : make test Quick iteration : make quick-test for faster feedback Documentation \u00b6 Local development : mkdocs serve to preview docs locally Health checks : Run make health before committing documentation changes Essential Reading \u00b6 Architecture Overview : Start with docs/architecture/00-index.md Storage Contracts : Understand data persistence patterns Finding Schema : Review the findings data model Definition of Done \u00b6 The DoD checklist is available in the PR template. Always paste validation evidence in your pull requests to demonstrate compliance. Next Steps \u00b6 After completing the setup, explore the architecture documentation to understand the system design and begin contributing to the project.","title":"Developer Start Here"},{"location":"developer-start-here/#developer-start-here","text":"","title":"Developer Start Here"},{"location":"developer-start-here/#quick-setup","text":"Clone repo, install Poetry, poetry install --no-root Branching: trunk-based; feature branches feat/... ; PR \u2264400 LOC or 2 approvals Run locally: make test ; fast loop: make quick-test Docs: mkdocs serve (local), make health before committing docs Read: docs/architecture/00-index.md , storage contracts, finding schema DoD checklist is in PR template; paste validation evidence in PRs","title":"Quick Setup"},{"location":"developer-start-here/#essential-reading","text":"Read this first: Governance & Engineering Standards This page contains the complete Definition of Done checklist, branching rules, CI pipeline order, coverage requirements, and documentation health gates that all contributors must follow. Development Practices: Development Conventions This page covers daily development workflows, SOD/EOD rituals, branch naming conventions, PR rules, and CI fast-fail order that every developer needs to follow.","title":"Essential Reading"},{"location":"developer-start-here/#getting-started","text":"","title":"Getting Started"},{"location":"developer-start-here/#prerequisites","text":"Poetry : Install Poetry for dependency management Git : For version control and branching Make : For running build commands","title":"Prerequisites"},{"location":"developer-start-here/#installation","text":"# Clone the repository git clone <repository-url> cd secflow # Install dependencies (without root package) poetry install --no-root","title":"Installation"},{"location":"developer-start-here/#development-workflow","text":"","title":"Development Workflow"},{"location":"developer-start-here/#branching-strategy","text":"Trunk-based development : Main branch is the source of truth Feature branches : Use feat/... prefix for new features Pull Request limits : \u2264400 LOC or require 2 approvals","title":"Branching Strategy"},{"location":"developer-start-here/#testing","text":"Full test suite : make test Quick iteration : make quick-test for faster feedback","title":"Testing"},{"location":"developer-start-here/#documentation","text":"Local development : mkdocs serve to preview docs locally Health checks : Run make health before committing documentation changes","title":"Documentation"},{"location":"developer-start-here/#essential-reading_1","text":"Architecture Overview : Start with docs/architecture/00-index.md Storage Contracts : Understand data persistence patterns Finding Schema : Review the findings data model","title":"Essential Reading"},{"location":"developer-start-here/#definition-of-done","text":"The DoD checklist is available in the PR template. Always paste validation evidence in your pull requests to demonstrate compliance.","title":"Definition of Done"},{"location":"developer-start-here/#next-steps","text":"After completing the setup, explore the architecture documentation to understand the system design and begin contributing to the project.","title":"Next Steps"},{"location":"_includes/dod-checklist/","text":"Definition of Done Checklist \u00b6 Code Quality \u00b6 All tests pass ( make test ) Code coverage meets current milestone threshold No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes Documentation \u00b6 Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check) Testing \u00b6 Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold Security & Compliance \u00b6 No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed Review Process \u00b6 PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments","title":"Dod checklist"},{"location":"_includes/dod-checklist/#definition-of-done-checklist","text":"","title":"Definition of Done Checklist"},{"location":"_includes/dod-checklist/#code-quality","text":"All tests pass ( make test ) Code coverage meets current milestone threshold No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes","title":"Code Quality"},{"location":"_includes/dod-checklist/#documentation","text":"Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check)","title":"Documentation"},{"location":"_includes/dod-checklist/#testing","text":"Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold","title":"Testing"},{"location":"_includes/dod-checklist/#security-compliance","text":"No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed","title":"Security &amp; Compliance"},{"location":"_includes/dod-checklist/#review-process","text":"PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments","title":"Review Process"},{"location":"adr/0000-adr-template/","text":"ADR-0000: \u00b6 Status \u00b6 Proposed | Accepted | Superseded Context \u00b6 Decision \u00b6 Consequences \u00b6 Alternatives Considered \u00b6 Links \u00b6","title":"ADR-0000:"},{"location":"adr/0000-adr-template/#adr-0000","text":"","title":"ADR-0000:"},{"location":"adr/0000-adr-template/#status","text":"Proposed | Accepted | Superseded","title":"Status"},{"location":"adr/0000-adr-template/#context","text":"","title":"Context"},{"location":"adr/0000-adr-template/#decision","text":"","title":"Decision"},{"location":"adr/0000-adr-template/#consequences","text":"","title":"Consequences"},{"location":"adr/0000-adr-template/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"adr/0000-adr-template/#links","text":"","title":"Links"},{"location":"architecture/00-index/","text":"SecFlow \u2014 Architecture & Design Documentation Index \u00b6 Welcome to the SecFlow technical documentation suite. This collection defines the complete internal architecture of the Security Toolkit platform \u2014 covering everything from orchestration engines to data models, plugin systems, and CVE enrichment. Each document in this /docs/architecture/ folder represents a bounded architectural domain , written in deep-technical mode to serve engineers and architects implementing or extending the system. \ud83d\udcda Document Navigation \u00b6 # File Description 01 Title & Executive Summary Project mission, audience, and objectives. 02 Architecture Philosophy Core tenets, design rationale, and constraints. 03 Repository Layout Folder hierarchy, import rules, and tooling. 04 Core Packages & Responsibilities Module roles, data contracts, and dependencies. 05 Orchestration & Workflow Engine DAG-based tool chaining and execution logic. 06 Plugin System Plugin registration, lifecycle, and testing. 07 Tools Integration Model Wrappers, manifests, and sandbox execution. 08 Tool Manager & UX Design How users manage tools and workflows. 09 Resource Registry Global wordlists, payloads, templates, and configs. 10 Wordlist & Output Sharing Rules for shared and isolated resources. 11 Project Isolation & Data Sharing Workspace boundaries, access control, linking. 12 Findings Model & Schema Data models, normalization, and persistence. 13 CVE/CWE/POC Enrichment Layer External vulnerability mapping and enrichment. 14 POC Sources & Legal Guidelines Governance and sandbox policies for proof-of-concepts. 15 Garbage Collection & Retention Data lifecycle, cleanup tasks, and recovery. 16 Security Model Auth, RBAC, sandboxing, and auditing. 17 Observability, Logging & Metrics Telemetry, structured logs, and OpenTelemetry. 18 Error Handling & Recovery Centralized error strategies and resilience patterns. 19 Risk Assessment Framework NIST 5\u00d75 Matrix, CVSS, and MITRE mapping. 20 Migration & Implementation Phases Step-by-step rollout plan. 21 CI/CD & Testing Strategy Build matrix, pipelines, and test automation. 22 Developer Experience & Docs Local setup, MkDocs, and contribution workflow. 23 Future Roadmap AI triage, automation, and next-gen features. \ud83e\udded Reading Order \u00b6 While each document is standalone, recommended order for new contributors: Start with 01\u201304 to grasp architecture and structure. Proceed to 05\u201307 for orchestration and integration. Review 09\u201313 for data, findings, and enrichment logic. Study 16\u201318 for security and observability. Conclude with 19\u201323 for governance, testing, and roadmap. \ud83e\udde9 Versioning & Maintenance \u00b6 Documentation version follows codebase major versions (e.g., 1.0 = v1.x releases). Each architectural change must update its corresponding file and cross-links. Use mkdocs serve locally to preview rendered docs. Changes to interfaces or DTOs require schema diff logs. \ud83e\uddf1 Example Structure Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/\"] C[\"00-index.md\"] D[\"01-title-and-executive-summary.md\"] E[\"02-architecture-philosophy.md\"] F[\"03-repository-layout.md\"] G[\"04-core-packages-and-responsibilities.md\"] H[\"05-orchestration-and-workflow-engine.md\"] I[\"06-plugin-system.md\"] J[\"07-tools-integration-model.md\"] K[\"08-tool-manager-and-ux-design.md\"] L[\"09-resource-registry.md\"] M[\"10-wordlist-and-output-sharing.md\"] N[\"11-project-isolation-and-data-sharing.md\"] O[\"12-findings-model-and-schema.md\"] P[\"13-cve-cwe-poc-enrichment-layer.md\"] Q[\"14-poc-sources-and-legal-guidelines.md\"] R[\"15-garbage-collection-and-retention.md\"] S[\"16-security-model.md\"] T[\"17-observability-logging-and-metrics.md\"] U[\"18-error-handling-and-recovery.md\"] V[\"19-risk-assessment-framework.md\"] W[\"20-migration-and-implementation-phases.md\"] X[\"21-ci-cd-and-testing-strategy.md\"] Y[\"22-developer-experience-and-docs.md\"] Z[\"23-future-roadmap.md\"] A --> B B --> C B --> D B --> E B --> F B --> G B --> H B --> I B --> J B --> K B --> L B --> M B --> N B --> O B --> P B --> Q B --> R B --> S B --> T B --> U B --> V B --> W B --> X B --> Y B --> Z Next: Title & Executive Summary ```","title":"Home"},{"location":"architecture/00-index/#secflow-architecture-design-documentation-index","text":"Welcome to the SecFlow technical documentation suite. This collection defines the complete internal architecture of the Security Toolkit platform \u2014 covering everything from orchestration engines to data models, plugin systems, and CVE enrichment. Each document in this /docs/architecture/ folder represents a bounded architectural domain , written in deep-technical mode to serve engineers and architects implementing or extending the system.","title":"SecFlow \u2014 Architecture &amp; Design Documentation Index"},{"location":"architecture/00-index/#document-navigation","text":"# File Description 01 Title & Executive Summary Project mission, audience, and objectives. 02 Architecture Philosophy Core tenets, design rationale, and constraints. 03 Repository Layout Folder hierarchy, import rules, and tooling. 04 Core Packages & Responsibilities Module roles, data contracts, and dependencies. 05 Orchestration & Workflow Engine DAG-based tool chaining and execution logic. 06 Plugin System Plugin registration, lifecycle, and testing. 07 Tools Integration Model Wrappers, manifests, and sandbox execution. 08 Tool Manager & UX Design How users manage tools and workflows. 09 Resource Registry Global wordlists, payloads, templates, and configs. 10 Wordlist & Output Sharing Rules for shared and isolated resources. 11 Project Isolation & Data Sharing Workspace boundaries, access control, linking. 12 Findings Model & Schema Data models, normalization, and persistence. 13 CVE/CWE/POC Enrichment Layer External vulnerability mapping and enrichment. 14 POC Sources & Legal Guidelines Governance and sandbox policies for proof-of-concepts. 15 Garbage Collection & Retention Data lifecycle, cleanup tasks, and recovery. 16 Security Model Auth, RBAC, sandboxing, and auditing. 17 Observability, Logging & Metrics Telemetry, structured logs, and OpenTelemetry. 18 Error Handling & Recovery Centralized error strategies and resilience patterns. 19 Risk Assessment Framework NIST 5\u00d75 Matrix, CVSS, and MITRE mapping. 20 Migration & Implementation Phases Step-by-step rollout plan. 21 CI/CD & Testing Strategy Build matrix, pipelines, and test automation. 22 Developer Experience & Docs Local setup, MkDocs, and contribution workflow. 23 Future Roadmap AI triage, automation, and next-gen features.","title":"\ud83d\udcda Document Navigation"},{"location":"architecture/00-index/#reading-order","text":"While each document is standalone, recommended order for new contributors: Start with 01\u201304 to grasp architecture and structure. Proceed to 05\u201307 for orchestration and integration. Review 09\u201313 for data, findings, and enrichment logic. Study 16\u201318 for security and observability. Conclude with 19\u201323 for governance, testing, and roadmap.","title":"\ud83e\udded Reading Order"},{"location":"architecture/00-index/#versioning-maintenance","text":"Documentation version follows codebase major versions (e.g., 1.0 = v1.x releases). Each architectural change must update its corresponding file and cross-links. Use mkdocs serve locally to preview rendered docs. Changes to interfaces or DTOs require schema diff logs.","title":"\ud83e\udde9 Versioning &amp; Maintenance"},{"location":"architecture/00-index/#example-structure-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/\"] C[\"00-index.md\"] D[\"01-title-and-executive-summary.md\"] E[\"02-architecture-philosophy.md\"] F[\"03-repository-layout.md\"] G[\"04-core-packages-and-responsibilities.md\"] H[\"05-orchestration-and-workflow-engine.md\"] I[\"06-plugin-system.md\"] J[\"07-tools-integration-model.md\"] K[\"08-tool-manager-and-ux-design.md\"] L[\"09-resource-registry.md\"] M[\"10-wordlist-and-output-sharing.md\"] N[\"11-project-isolation-and-data-sharing.md\"] O[\"12-findings-model-and-schema.md\"] P[\"13-cve-cwe-poc-enrichment-layer.md\"] Q[\"14-poc-sources-and-legal-guidelines.md\"] R[\"15-garbage-collection-and-retention.md\"] S[\"16-security-model.md\"] T[\"17-observability-logging-and-metrics.md\"] U[\"18-error-handling-and-recovery.md\"] V[\"19-risk-assessment-framework.md\"] W[\"20-migration-and-implementation-phases.md\"] X[\"21-ci-cd-and-testing-strategy.md\"] Y[\"22-developer-experience-and-docs.md\"] Z[\"23-future-roadmap.md\"] A --> B B --> C B --> D B --> E B --> F B --> G B --> H B --> I B --> J B --> K B --> L B --> M B --> N B --> O B --> P B --> Q B --> R B --> S B --> T B --> U B --> V B --> W B --> X B --> Y B --> Z Next: Title & Executive Summary ```","title":"\ud83e\uddf1 Example Structure Diagram"},{"location":"architecture/01-title-and-executive-summary/","text":"01 \u2014 Title & Executive Summary \u00b6 \ud83e\udde9 Project Name \u00b6 SecFlow \u2014 Security Toolkit Orchestration Framework \ud83e\udded Executive Summary \u00b6 SecFlow is a modular, extensible, and automation-first security orchestration platform designed to unify multiple open-source and proprietary tools for penetration testing, vulnerability management, and risk analytics. It merges dynamic scanning , tool orchestration , data normalization , and risk intelligence into a single cohesive system \u2014 optimized for both solo pentesters and enterprise red-team workflows. Core Vision \u00b6 \"One orchestrator to chain, enrich, and analyze all security tools \u2014 safely, intelligently, and reproducibly.\" Core Differentiators \u00b6 \ud83d\udd04 End-to-end orchestration: Tools (Nuclei, Feroxbuster, Katana, ZAP, Burp, Caido, etc.) communicate via normalized JSON pipelines. \ud83e\uddf1 Hexagonal architecture: Strict separation between core logic , adapters , and interfaces . \ud83e\udde0 Data-driven intelligence: Findings automatically enriched with CVE/CWE/CVSS and MITRE mapping. \u2699\ufe0f Resource registry: Shared wordlists, templates, and payloads under a versioned resource management system. \ud83e\uddf0 AI-ready foundation: All findings and telemetry are machine-readable for later AI triage and summarization. \ud83c\udfaf Project Goals \u00b6 Category Objective Engineering Create a modular mono-repo structure enabling multiple apps (CLI, API, Worker, UI). Automation Allow users to chain discovery \u2192 scanning \u2192 enrichment workflows via declarative YAML recipes. Data Consistency Standardize tool outputs into common Finding DTOs for unified triage and metrics. Usability Provide a CLI and web UI for project management, orchestration, and analytics. Security Guarantee safe sandboxing, configuration isolation, and audit logging. Scalability Support multi-tenant projects with configurable resource scopes. \ud83e\uddee Key Capabilities Overview \u00b6 Layer Functionality Description Core-Lib Schema & DTOs Defines data models, ports, and contracts. Findings-Engine Normalization Converts raw tool outputs into structured findings. Wrappers Tool Adapters Execute, monitor, and parse tool output. Worker Orchestration Runs workflows (DAG execution, concurrency, retry policies). Web-API REST + WS Interface Exposes project, scan, and findings endpoints. UI HTMX/React Web dashboard for triage and insights. Plugins Extensibility Detectors, enrichers, or analytics extensions. \ud83e\uddf1 Architecture Summary \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Web-API / UI\"] B[\"REST / WebSocket\"] C[\"Worker Engine<br/>(Workflows, DAG, Queues)\"] D[\"Ports / DTOs\"] E[\"Core-Lib<br/>(Models, Ports, Repositories)\"] F[\"Wrappers / Plugins<br/>(Nuclei, Ferox, Katana\u2026)\"] G[\"Tool Output\"] H[\"Findings Engine<br/>(Normalization, Metrics)\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H \u2699\ufe0f Toolchain & Technology Stack \u00b6 Category Tools / Frameworks Language Python 3.11+ Frameworks FastAPI / Flask (API), HTMX / React (UI) ORM / DB SQLModel (SQLite / PostgreSQL) Task Queue Celery / RQ (Redis backend) Caching Redis Testing Pytest + Tox + Coverage CI/CD GitHub Actions + Import-Linter + Ruff + Pyright Packaging Poetry Docs MkDocs + Material theme \ud83e\udde9 Intended Audience \u00b6 Audience Needs Red-Team / Pentesters Automate repetitive workflows, triage large data, correlate findings. Security Engineers Integrate with CI/CD, enrich findings with vulnerability intelligence. Researchers Extend plugins, add enrichment sources, or create experimental scanners. Developers Contribute to wrappers, tooling, and orchestration improvements. \ud83e\udde0 Guiding Principles \u00b6 Automation-First \u2013 Everything that can be automated should be . Safe by Default \u2013 Every scan runs in isolation, respecting sandbox and audit policies. Extensible by Design \u2013 Each tool or feature lives as a plugin or wrapper. Transparent \u2013 Clear data lineage and provenance for every finding. Declarative Configuration \u2013 Tools and workflows defined in YAML/JSON, not hardcoded logic. Deterministic Execution \u2013 Same input, same results. All randomness is logged. \ud83d\udd2e Strategic Vision \u00b6 SecFlow is built not as a single pentest tool, but as a foundation for orchestrated cybersecurity automation . It aims to support: - Scalable enterprise deployments. - Cloud-native workloads (Kubernetes, containerized workers). - AI-driven vulnerability triage. - Integration with bug bounty, CI/CD, and SIEM pipelines. Next: Architecture Philosophy ```","title":"Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#01-title-executive-summary","text":"","title":"01 \u2014 Title &amp; Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#project-name","text":"SecFlow \u2014 Security Toolkit Orchestration Framework","title":"\ud83e\udde9 Project Name"},{"location":"architecture/01-title-and-executive-summary/#executive-summary","text":"SecFlow is a modular, extensible, and automation-first security orchestration platform designed to unify multiple open-source and proprietary tools for penetration testing, vulnerability management, and risk analytics. It merges dynamic scanning , tool orchestration , data normalization , and risk intelligence into a single cohesive system \u2014 optimized for both solo pentesters and enterprise red-team workflows.","title":"\ud83e\udded Executive Summary"},{"location":"architecture/01-title-and-executive-summary/#core-vision","text":"\"One orchestrator to chain, enrich, and analyze all security tools \u2014 safely, intelligently, and reproducibly.\"","title":"Core Vision"},{"location":"architecture/01-title-and-executive-summary/#core-differentiators","text":"\ud83d\udd04 End-to-end orchestration: Tools (Nuclei, Feroxbuster, Katana, ZAP, Burp, Caido, etc.) communicate via normalized JSON pipelines. \ud83e\uddf1 Hexagonal architecture: Strict separation between core logic , adapters , and interfaces . \ud83e\udde0 Data-driven intelligence: Findings automatically enriched with CVE/CWE/CVSS and MITRE mapping. \u2699\ufe0f Resource registry: Shared wordlists, templates, and payloads under a versioned resource management system. \ud83e\uddf0 AI-ready foundation: All findings and telemetry are machine-readable for later AI triage and summarization.","title":"Core Differentiators"},{"location":"architecture/01-title-and-executive-summary/#project-goals","text":"Category Objective Engineering Create a modular mono-repo structure enabling multiple apps (CLI, API, Worker, UI). Automation Allow users to chain discovery \u2192 scanning \u2192 enrichment workflows via declarative YAML recipes. Data Consistency Standardize tool outputs into common Finding DTOs for unified triage and metrics. Usability Provide a CLI and web UI for project management, orchestration, and analytics. Security Guarantee safe sandboxing, configuration isolation, and audit logging. Scalability Support multi-tenant projects with configurable resource scopes.","title":"\ud83c\udfaf Project Goals"},{"location":"architecture/01-title-and-executive-summary/#key-capabilities-overview","text":"Layer Functionality Description Core-Lib Schema & DTOs Defines data models, ports, and contracts. Findings-Engine Normalization Converts raw tool outputs into structured findings. Wrappers Tool Adapters Execute, monitor, and parse tool output. Worker Orchestration Runs workflows (DAG execution, concurrency, retry policies). Web-API REST + WS Interface Exposes project, scan, and findings endpoints. UI HTMX/React Web dashboard for triage and insights. Plugins Extensibility Detectors, enrichers, or analytics extensions.","title":"\ud83e\uddee Key Capabilities Overview"},{"location":"architecture/01-title-and-executive-summary/#architecture-summary","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Web-API / UI\"] B[\"REST / WebSocket\"] C[\"Worker Engine<br/>(Workflows, DAG, Queues)\"] D[\"Ports / DTOs\"] E[\"Core-Lib<br/>(Models, Ports, Repositories)\"] F[\"Wrappers / Plugins<br/>(Nuclei, Ferox, Katana\u2026)\"] G[\"Tool Output\"] H[\"Findings Engine<br/>(Normalization, Metrics)\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H","title":"\ud83e\uddf1 Architecture Summary"},{"location":"architecture/01-title-and-executive-summary/#toolchain-technology-stack","text":"Category Tools / Frameworks Language Python 3.11+ Frameworks FastAPI / Flask (API), HTMX / React (UI) ORM / DB SQLModel (SQLite / PostgreSQL) Task Queue Celery / RQ (Redis backend) Caching Redis Testing Pytest + Tox + Coverage CI/CD GitHub Actions + Import-Linter + Ruff + Pyright Packaging Poetry Docs MkDocs + Material theme","title":"\u2699\ufe0f Toolchain &amp; Technology Stack"},{"location":"architecture/01-title-and-executive-summary/#intended-audience","text":"Audience Needs Red-Team / Pentesters Automate repetitive workflows, triage large data, correlate findings. Security Engineers Integrate with CI/CD, enrich findings with vulnerability intelligence. Researchers Extend plugins, add enrichment sources, or create experimental scanners. Developers Contribute to wrappers, tooling, and orchestration improvements.","title":"\ud83e\udde9 Intended Audience"},{"location":"architecture/01-title-and-executive-summary/#guiding-principles","text":"Automation-First \u2013 Everything that can be automated should be . Safe by Default \u2013 Every scan runs in isolation, respecting sandbox and audit policies. Extensible by Design \u2013 Each tool or feature lives as a plugin or wrapper. Transparent \u2013 Clear data lineage and provenance for every finding. Declarative Configuration \u2013 Tools and workflows defined in YAML/JSON, not hardcoded logic. Deterministic Execution \u2013 Same input, same results. All randomness is logged.","title":"\ud83e\udde0 Guiding Principles"},{"location":"architecture/01-title-and-executive-summary/#strategic-vision","text":"SecFlow is built not as a single pentest tool, but as a foundation for orchestrated cybersecurity automation . It aims to support: - Scalable enterprise deployments. - Cloud-native workloads (Kubernetes, containerized workers). - AI-driven vulnerability triage. - Integration with bug bounty, CI/CD, and SIEM pipelines. Next: Architecture Philosophy ```","title":"\ud83d\udd2e Strategic Vision"},{"location":"architecture/02-architecture-philosophy/","text":"02 \u2014 Architecture Philosophy \u00b6 \ud83e\udded Overview \u00b6 SecFlow's architecture follows the Hexagonal Architecture (Ports & Adapters) paradigm, enforcing a clear separation of concerns between business logic, I/O mechanisms, and user interfaces. It is designed for tool orchestration, data normalization, and future AI enrichment \u2014 with extensibility, safety, and testability at its core. \ud83e\uddf1 Architectural Tenets \u00b6 Principle Description Hexagonal Isolation All business logic lives in core modules with no framework dependencies. I/O adapters (DB, CLI, API) communicate via interfaces (Ports). Event-Driven Orchestration Workflows operate as Directed Acyclic Graphs (DAGs) executed asynchronously, allowing parallel chaining of tools. Immutable Data Flow Each execution step produces structured, immutable output for auditing and replay. Composable Pipelines Discovery \u2192 Scanning \u2192 Enrichment workflows can be combined declaratively. Strong Typing All data contracts use Pydantic models for type safety and schema validation. Declarative Configuration YAML or JSON defines tools, wordlists, templates, and workflows. Safe Execution Every external tool runs within sandbox boundaries (filesystem + network isolation). \u2699\ufe0f Core Architectural Layers \u00b6 +---------------------------------------------------------------+ | Applications | | Web-API | +---------------------------------------------------------------+ | Services Layer | | Workflow Engine | +---------------------------------------------------------------+ | Core-Lib | | DTOs | +---------------------------------------------------------------+ | Infrastructure | | Database | +---------------------------------------------------------------+ | External | | Tools | +---------------------------------------------------------------+ Each layer exposes well-defined boundaries : - Applications depend only on Services through ports . - Services interact with Core logic through domain interfaces . - Core-Lib defines pure domain logic with zero external imports. - Infrastructure implements adapters to databases, cache, and external tools. \ud83e\udde9 Design Goals \u00b6 Goal Implementation Strategy Maintainability Modular mono-repo with strict import boundaries ( import-linter ). Scalability Async task execution (Celery/RQ) and worker-based orchestration. Observability Structured logs, Prometheus metrics, and OpenTelemetry tracing. Reproducibility Deterministic workflows with cached configuration + results. Security Sandboxed subprocesses, limited file system access, and tokenized configuration. Extensibility Plugin registry and manifest-based tool definitions. \ud83e\udde9 Why Hexagonal Architecture? \u00b6 Aspect Traditional Architecture SecFlow Approach Dependencies Frameworks import core directly Core is framework-agnostic Testing Hard to isolate logic Core modules unit-tested independently Tool Integration Ad-hoc scripts Formal wrappers with contracts Maintainability Spaghetti imports Controlled boundaries with Import-Linter Extensibility Static toolset Plugin & manifest system \ud83e\udde9 Component Responsibility Map \u00b6 Component Responsibility Core-Lib Defines domain models ( Finding , Project , Resource ) and interfaces ( ToolPort , StoragePort ). Findings-Engine Normalizes raw scan data into standardized Finding objects. Wrappers Execute external tools (Nuclei, Ferox, etc.) via manifest-driven configs. Worker Executes workflows, manages concurrency, caching, and cleanup. API Exposes endpoints for managing projects, workflows, and triage. Triage-UI Visual interface for findings review, filtering, and reporting. Plugins Optional modules extending detection or enrichment logic. Resource Registry Central management of wordlists, templates, and payloads. \ud83e\udde9 Data Flow Model \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project\"] B[\"Workflow DAG\"] C[\"Tool Wrappers\"] D[\"Runs (execution logs)\"] E[\"Findings Engine\"] F[\"Findings (normalized)\"] G[\"Enrichment Layer\"] H[\"CVE/CWE/POC metadata\"] I[\"Triage / Metrics\"] J[\"Analytics, Dashboards\"] A --> B B --> C C --> D C --> E E --> F E --> G G --> H G --> I I --> J \ud83e\udde0 Architectural Patterns Used \u00b6 Pattern Purpose Repository Pattern Abstract data access (DB, file, memory). DTO (Data Transfer Object) Enforce schema boundaries between layers. Command Pattern Workflow node execution commands. Strategy Pattern Dynamic selection of enrichment or parsing strategies. Observer Pattern Metrics and event hooks on scan completion. Decorator Pattern Reusable middleware for retries, timeouts, and auditing. \ud83d\udd10 Security-by-Design Integration \u00b6 Every architectural decision includes a security review: - Minimal privilege principle (scoped resource access). - Tokenized secrets (no plaintext credentials). - Environment isolation (Docker/Kubernetes runtime sandboxes). - Mandatory audit trail for findings modification. \ud83e\udde0 Future-Proofing Considerations \u00b6 AI Integrations: Workflow outputs use JSON-LD style structures, allowing LLMs to reason over findings safely. Multi-Tenant Mode: Namespaced projects ensure logical and data isolation. Plugin Safety: Plugins are signed, versioned, and validated before loading. Extensible Schema: Findings model allows additional enrichment fields via metadata dicts. Next: Repository Layout","title":"Philosophy"},{"location":"architecture/02-architecture-philosophy/#02-architecture-philosophy","text":"","title":"02 \u2014 Architecture Philosophy"},{"location":"architecture/02-architecture-philosophy/#overview","text":"SecFlow's architecture follows the Hexagonal Architecture (Ports & Adapters) paradigm, enforcing a clear separation of concerns between business logic, I/O mechanisms, and user interfaces. It is designed for tool orchestration, data normalization, and future AI enrichment \u2014 with extensibility, safety, and testability at its core.","title":"\ud83e\udded Overview"},{"location":"architecture/02-architecture-philosophy/#architectural-tenets","text":"Principle Description Hexagonal Isolation All business logic lives in core modules with no framework dependencies. I/O adapters (DB, CLI, API) communicate via interfaces (Ports). Event-Driven Orchestration Workflows operate as Directed Acyclic Graphs (DAGs) executed asynchronously, allowing parallel chaining of tools. Immutable Data Flow Each execution step produces structured, immutable output for auditing and replay. Composable Pipelines Discovery \u2192 Scanning \u2192 Enrichment workflows can be combined declaratively. Strong Typing All data contracts use Pydantic models for type safety and schema validation. Declarative Configuration YAML or JSON defines tools, wordlists, templates, and workflows. Safe Execution Every external tool runs within sandbox boundaries (filesystem + network isolation).","title":"\ud83e\uddf1 Architectural Tenets"},{"location":"architecture/02-architecture-philosophy/#core-architectural-layers","text":"+---------------------------------------------------------------+ | Applications | | Web-API | +---------------------------------------------------------------+ | Services Layer | | Workflow Engine | +---------------------------------------------------------------+ | Core-Lib | | DTOs | +---------------------------------------------------------------+ | Infrastructure | | Database | +---------------------------------------------------------------+ | External | | Tools | +---------------------------------------------------------------+ Each layer exposes well-defined boundaries : - Applications depend only on Services through ports . - Services interact with Core logic through domain interfaces . - Core-Lib defines pure domain logic with zero external imports. - Infrastructure implements adapters to databases, cache, and external tools.","title":"\u2699\ufe0f Core Architectural Layers"},{"location":"architecture/02-architecture-philosophy/#design-goals","text":"Goal Implementation Strategy Maintainability Modular mono-repo with strict import boundaries ( import-linter ). Scalability Async task execution (Celery/RQ) and worker-based orchestration. Observability Structured logs, Prometheus metrics, and OpenTelemetry tracing. Reproducibility Deterministic workflows with cached configuration + results. Security Sandboxed subprocesses, limited file system access, and tokenized configuration. Extensibility Plugin registry and manifest-based tool definitions.","title":"\ud83e\udde9 Design Goals"},{"location":"architecture/02-architecture-philosophy/#why-hexagonal-architecture","text":"Aspect Traditional Architecture SecFlow Approach Dependencies Frameworks import core directly Core is framework-agnostic Testing Hard to isolate logic Core modules unit-tested independently Tool Integration Ad-hoc scripts Formal wrappers with contracts Maintainability Spaghetti imports Controlled boundaries with Import-Linter Extensibility Static toolset Plugin & manifest system","title":"\ud83e\udde9 Why Hexagonal Architecture?"},{"location":"architecture/02-architecture-philosophy/#component-responsibility-map","text":"Component Responsibility Core-Lib Defines domain models ( Finding , Project , Resource ) and interfaces ( ToolPort , StoragePort ). Findings-Engine Normalizes raw scan data into standardized Finding objects. Wrappers Execute external tools (Nuclei, Ferox, etc.) via manifest-driven configs. Worker Executes workflows, manages concurrency, caching, and cleanup. API Exposes endpoints for managing projects, workflows, and triage. Triage-UI Visual interface for findings review, filtering, and reporting. Plugins Optional modules extending detection or enrichment logic. Resource Registry Central management of wordlists, templates, and payloads.","title":"\ud83e\udde9 Component Responsibility Map"},{"location":"architecture/02-architecture-philosophy/#data-flow-model","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project\"] B[\"Workflow DAG\"] C[\"Tool Wrappers\"] D[\"Runs (execution logs)\"] E[\"Findings Engine\"] F[\"Findings (normalized)\"] G[\"Enrichment Layer\"] H[\"CVE/CWE/POC metadata\"] I[\"Triage / Metrics\"] J[\"Analytics, Dashboards\"] A --> B B --> C C --> D C --> E E --> F E --> G G --> H G --> I I --> J","title":"\ud83e\udde9 Data Flow Model"},{"location":"architecture/02-architecture-philosophy/#architectural-patterns-used","text":"Pattern Purpose Repository Pattern Abstract data access (DB, file, memory). DTO (Data Transfer Object) Enforce schema boundaries between layers. Command Pattern Workflow node execution commands. Strategy Pattern Dynamic selection of enrichment or parsing strategies. Observer Pattern Metrics and event hooks on scan completion. Decorator Pattern Reusable middleware for retries, timeouts, and auditing.","title":"\ud83e\udde0 Architectural Patterns Used"},{"location":"architecture/02-architecture-philosophy/#security-by-design-integration","text":"Every architectural decision includes a security review: - Minimal privilege principle (scoped resource access). - Tokenized secrets (no plaintext credentials). - Environment isolation (Docker/Kubernetes runtime sandboxes). - Mandatory audit trail for findings modification.","title":"\ud83d\udd10 Security-by-Design Integration"},{"location":"architecture/02-architecture-philosophy/#future-proofing-considerations","text":"AI Integrations: Workflow outputs use JSON-LD style structures, allowing LLMs to reason over findings safely. Multi-Tenant Mode: Namespaced projects ensure logical and data isolation. Plugin Safety: Plugins are signed, versioned, and validated before loading. Extensible Schema: Findings model allows additional enrichment fields via metadata dicts. Next: Repository Layout","title":"\ud83e\udde0 Future-Proofing Considerations"},{"location":"architecture/03-repository-layout/","text":"03 \u2014 Repository Layout \u00b6 \ud83e\udded Overview \u00b6 The SecFlow repository is structured as a Python mono-repo workspace housing multiple interdependent packages and apps. This allows us to: - Share interfaces and DTOs across services. - Enforce architecture boundaries centrally. - Run unified CI/CD pipelines and dependency resolution through Poetry . \ud83e\uddf1 Directory Structure \u00b6 flowchart TD root[\"SecFlow/\"] root --> pkgs[\"packages/\"] pkgs --> core[\"core-lib/\"] pkgs --> wf[\"workflow-engine/\"] pkgs --> tm[\"tool-manager/\"] pkgs --> rr[\"resource-registry/\"] pkgs --> obs[\"observability/\"] pkgs --> sec[\"security/\"] root --> apps[\"apps/\"] apps --> cli[\"cli/\"] apps --> api[\"api-server/\"] apps --> web[\"web-ui/\"] root --> docs[\"docs/\"] docs --> arch[\"architecture/\"] docs --> review[\"review/\"] docs --> diagrams[\"diagrams/\"] root --> tools[\"tools/\"] tools --> scripts[\"scripts/\"] tools --> linters[\"linters/\"] root --> github[\".github/\"] github --> workflows[\"workflows/\"] \u2699\ufe0f Python Workspace Configuration \u00b6 pyproject.toml Snippet \u00b6 [tool.poetry] name = \"SecFlow\" version = \"1.0.0\" description = \"Security Toolkit Orchestration Framework\" authors = [\"Hernan Trajtemberg <hernan.trajtemberg@domain>\"] [tool.poetry.dependencies] python = \"^3.11\" fastapi = \"^0.115\" sqlmodel = \"^0.0.22\" celery = \"^5.3\" redis = \"^5.0\" pydantic = \"^2.7\" import-linter = \"^1.5\" ruff = \"^0.7\" pytest = \"^8.3\" [tool.poetry.group.dev.dependencies] pyright = \"*\" coverage = \"*\" [build-system] requires = [\"poetry-core>=1.6\"] build-backend = \"poetry.core.masonry.api\" \ud83e\udde9 Application Layering \u00b6 Each app in /apps/ uses internal packages exclusively via ports, ensuring loose coupling. Layer Directory Import Rules Core packages/core-lib No external imports Findings Engine packages/findings-engine May import core-lib Wrappers packages/wrappers May import core-lib, utils API / Worker apps/web-api , apps/worker May import via ports only Plugins packages/plugins Dynamically loaded at runtime \ud83e\udde9 Import-Linter Configuration \u00b6 importlinter.ini enforces import boundaries automatically: [importlinter] root_package = SecFlow [contract: core-isolation] name = Core-Lib Is Independent type = forbidden source_modules = SecFlow.packages.core_lib forbidden_modules = SecFlow.apps SecFlow.packages.wrappers SecFlow.packages.findings_engine [contract: adapters-only] name = Adapters Only Depend On Ports type = layers layers = SecFlow.packages.core_lib SecFlow.packages.findings_engine SecFlow.packages.wrappers SecFlow.apps If violated, the CI pipeline fails the build. \ud83e\udde0 Developer Workflow \u00b6 Local Development \u00b6 poetry install poetry run pre-commit install poetry run pytest Run the Worker \u00b6 poetry run celery -A SecFlow.apps.worker worker --loglevel=info Run the Web API \u00b6 poetry run uvicorn SecFlow.apps.web_api.main:app --reload \ud83e\udde9 Continuous Integration Pipeline \u00b6 GitHub Actions ( .github/workflows/ci.yml ): name: SecFlow CI on: [push, pull_request] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-python@v5 with: python-version: '3.11' - name: Install dependencies run: | pip install poetry poetry install - name: Lint & Type Check run: | poetry run ruff check . poetry run pyright - name: Run Tests run: poetry run pytest --maxfail=1 --disable-warnings -q \ud83e\uddf0 Tooling & Developer Aids \u00b6 Tool Purpose Ruff Linting, formatting enforcement Pyright Static type checking Import-Linter Architecture enforcement Poetry Dependency & build management Tox Multi-environment testing MkDocs Documentation site generation Coverage.py Test coverage reports \ud83e\udde9 ASCII Diagram \u2014 High-Level View \u00b6 +-----------------------------+ | SecFlow/ | +-------------+---------------+ | +------------v-------------+ | packages/ | | core-lib, findings, etc. | +------------+-------------+ | +------------v-------------+ | apps/ | | web-api, worker, cli, ui | +------------+-------------+ | +------------v-------------+ | tests/ | +--------------------------+ \ud83e\udde0 Future Enhancements \u00b6 Monorepo Versioning: Each package versioned via poetry-dynamic-versioning. Documentation Pipeline: Auto-regenerate schema docs ( mkdocs build ) on merge. Pre-Commit Hooks: Validate imports and enforce ruff formatting pre-push. Code Owners: Assign maintainers per package via .github/CODEOWNERS . Next: Core Packages & Responsibilities","title":"Repository Layout"},{"location":"architecture/03-repository-layout/#03-repository-layout","text":"","title":"03 \u2014 Repository Layout"},{"location":"architecture/03-repository-layout/#overview","text":"The SecFlow repository is structured as a Python mono-repo workspace housing multiple interdependent packages and apps. This allows us to: - Share interfaces and DTOs across services. - Enforce architecture boundaries centrally. - Run unified CI/CD pipelines and dependency resolution through Poetry .","title":"\ud83e\udded Overview"},{"location":"architecture/03-repository-layout/#directory-structure","text":"flowchart TD root[\"SecFlow/\"] root --> pkgs[\"packages/\"] pkgs --> core[\"core-lib/\"] pkgs --> wf[\"workflow-engine/\"] pkgs --> tm[\"tool-manager/\"] pkgs --> rr[\"resource-registry/\"] pkgs --> obs[\"observability/\"] pkgs --> sec[\"security/\"] root --> apps[\"apps/\"] apps --> cli[\"cli/\"] apps --> api[\"api-server/\"] apps --> web[\"web-ui/\"] root --> docs[\"docs/\"] docs --> arch[\"architecture/\"] docs --> review[\"review/\"] docs --> diagrams[\"diagrams/\"] root --> tools[\"tools/\"] tools --> scripts[\"scripts/\"] tools --> linters[\"linters/\"] root --> github[\".github/\"] github --> workflows[\"workflows/\"]","title":"\ud83e\uddf1 Directory Structure"},{"location":"architecture/03-repository-layout/#python-workspace-configuration","text":"","title":"\u2699\ufe0f Python Workspace Configuration"},{"location":"architecture/03-repository-layout/#pyprojecttoml-snippet","text":"[tool.poetry] name = \"SecFlow\" version = \"1.0.0\" description = \"Security Toolkit Orchestration Framework\" authors = [\"Hernan Trajtemberg <hernan.trajtemberg@domain>\"] [tool.poetry.dependencies] python = \"^3.11\" fastapi = \"^0.115\" sqlmodel = \"^0.0.22\" celery = \"^5.3\" redis = \"^5.0\" pydantic = \"^2.7\" import-linter = \"^1.5\" ruff = \"^0.7\" pytest = \"^8.3\" [tool.poetry.group.dev.dependencies] pyright = \"*\" coverage = \"*\" [build-system] requires = [\"poetry-core>=1.6\"] build-backend = \"poetry.core.masonry.api\"","title":"pyproject.toml Snippet"},{"location":"architecture/03-repository-layout/#application-layering","text":"Each app in /apps/ uses internal packages exclusively via ports, ensuring loose coupling. Layer Directory Import Rules Core packages/core-lib No external imports Findings Engine packages/findings-engine May import core-lib Wrappers packages/wrappers May import core-lib, utils API / Worker apps/web-api , apps/worker May import via ports only Plugins packages/plugins Dynamically loaded at runtime","title":"\ud83e\udde9 Application Layering"},{"location":"architecture/03-repository-layout/#import-linter-configuration","text":"importlinter.ini enforces import boundaries automatically: [importlinter] root_package = SecFlow [contract: core-isolation] name = Core-Lib Is Independent type = forbidden source_modules = SecFlow.packages.core_lib forbidden_modules = SecFlow.apps SecFlow.packages.wrappers SecFlow.packages.findings_engine [contract: adapters-only] name = Adapters Only Depend On Ports type = layers layers = SecFlow.packages.core_lib SecFlow.packages.findings_engine SecFlow.packages.wrappers SecFlow.apps If violated, the CI pipeline fails the build.","title":"\ud83e\udde9 Import-Linter Configuration"},{"location":"architecture/03-repository-layout/#developer-workflow","text":"","title":"\ud83e\udde0 Developer Workflow"},{"location":"architecture/03-repository-layout/#local-development","text":"poetry install poetry run pre-commit install poetry run pytest","title":"Local Development"},{"location":"architecture/03-repository-layout/#run-the-worker","text":"poetry run celery -A SecFlow.apps.worker worker --loglevel=info","title":"Run the Worker"},{"location":"architecture/03-repository-layout/#run-the-web-api","text":"poetry run uvicorn SecFlow.apps.web_api.main:app --reload","title":"Run the Web API"},{"location":"architecture/03-repository-layout/#continuous-integration-pipeline","text":"GitHub Actions ( .github/workflows/ci.yml ): name: SecFlow CI on: [push, pull_request] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-python@v5 with: python-version: '3.11' - name: Install dependencies run: | pip install poetry poetry install - name: Lint & Type Check run: | poetry run ruff check . poetry run pyright - name: Run Tests run: poetry run pytest --maxfail=1 --disable-warnings -q","title":"\ud83e\udde9 Continuous Integration Pipeline"},{"location":"architecture/03-repository-layout/#tooling-developer-aids","text":"Tool Purpose Ruff Linting, formatting enforcement Pyright Static type checking Import-Linter Architecture enforcement Poetry Dependency & build management Tox Multi-environment testing MkDocs Documentation site generation Coverage.py Test coverage reports","title":"\ud83e\uddf0 Tooling &amp; Developer Aids"},{"location":"architecture/03-repository-layout/#ascii-diagram-high-level-view","text":"+-----------------------------+ | SecFlow/ | +-------------+---------------+ | +------------v-------------+ | packages/ | | core-lib, findings, etc. | +------------+-------------+ | +------------v-------------+ | apps/ | | web-api, worker, cli, ui | +------------+-------------+ | +------------v-------------+ | tests/ | +--------------------------+","title":"\ud83e\udde9 ASCII Diagram \u2014 High-Level View"},{"location":"architecture/03-repository-layout/#future-enhancements","text":"Monorepo Versioning: Each package versioned via poetry-dynamic-versioning. Documentation Pipeline: Auto-regenerate schema docs ( mkdocs build ) on merge. Pre-Commit Hooks: Validate imports and enforce ruff formatting pre-push. Code Owners: Assign maintainers per package via .github/CODEOWNERS . Next: Core Packages & Responsibilities","title":"\ud83e\udde0 Future Enhancements"},{"location":"architecture/04-core-packages-and-responsibilities/","text":"04 \u2014 Core Packages & Responsibilities \u00b6 \ud83e\udded Overview \u00b6 This document describes the core packages inside the packages/ directory and explains the roles, contracts, and relationships that define the heart of SecFlow. The architecture is intentionally hexagonal : - core-lib defines domain models and ports (interfaces). - Other packages such as findings-engine , wrappers , and storage implement those ports. - Application layers ( web-api , worker , cli ) use the ports but never touch implementations directly. \ud83e\udde9 Package Overview Table \u00b6 Package Responsibility Depends On core-lib Domain models, DTOs, Ports, and Service Interfaces. None findings-engine Normalization and enrichment of scan data. core-lib wrappers Executes external tools and parses results. core-lib , utils resources Manages reusable assets (wordlists, payloads, templates). core-lib storage Implements persistence and caching (Postgres, SQLite, Redis). core-lib plugins Houses extension modules (detectors, enrichers, analytics). core-lib utils Generic utilities: logging, validation, subprocess helpers. None \u2699\ufe0f Core-Lib \u00b6 Purpose \u00b6 core-lib is the foundation of SecFlow. It defines all domain entities, type contracts, and abstract interfaces (ports) used throughout the system. Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"core-lib/\"] B[\"models/\"] C[\"finding.py\"] D[\"project.py\"] E[\"resource.py\"] F[\"run.py\"] G[\"ports/\"] H[\"storage_port.py\"] I[\"tool_port.py\"] J[\"resource_port.py\"] K[\"findings_port.py\"] L[\"services/\"] M[\"workflow_service.py\"] N[\"triage_service.py\"] O[\"exceptions.py\"] A --> B A --> G A --> L A --> O B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K L --> M L --> N Example Model \u00b6 # core-lib/models/finding.py from pydantic import BaseModel from datetime import datetime from typing import List, Optional, Dict class Finding(BaseModel): id: str project_id: str detector_id: str title: str severity: str path: str evidence: Dict[str, str] created_at: datetime cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] enrichment: Dict[str, any] = {} Example Port \u00b6 # core-lib/ports/tool_port.py from typing import Protocol, List from core_lib.models.finding import Finding class ToolPort(Protocol): def prepare(self, config: dict) -> None: \"\"\"Prepare the tool with given configuration.\"\"\" pass def execute(self) -> List[Finding]: \"\"\"Execute the tool and return findings.\"\"\" pass def validate_output(self, raw_output: str) -> bool: \"\"\"Validate tool output format.\"\"\" pass This abstraction allows any external tool to be integrated simply by implementing ToolPort . \ud83e\udde0 Findings-Engine \u00b6 Purpose \u00b6 Responsible for normalizing, deduplicating, and enriching findings produced by wrappers. Example Normalization Flow \u00b6 def normalize(raw_data: str, tool: str) -> Finding: if tool == \"nuclei\": return _normalize_nuclei_output(raw_data) elif tool == \"ferox\": return _normalize_ferox_output(raw_data) Capabilities \u00b6 Parse multiple output formats (JSON, XML, plain text). Deduplicate based on fingerprint (host + path + vuln_id). Attach CWE, CVSS, and severity from enrichment sources. Store normalized data through StoragePort . \u2699\ufe0f Wrappers \u00b6 Purpose \u00b6 Wraps and executes third-party tools through a unified interface defined by ToolPort . Example Structure \u00b6 wrappers/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"wrappers/\"] B[\"nuclei_wrapper.py\"] C[\"ferox_wrapper.py\"] D[\"zap_wrapper.py\"] E[\"base_wrapper.py\"] A --> B A --> C A --> D A --> E Example Base Class \u00b6 # wrappers/base_wrapper.py import subprocess from core_lib.models.finding import Finding class BaseWrapper: def __init__(self, manifest: dict): self.manifest = manifest def run(self, target: str) -> list[Finding]: cmd = [self.manifest[\"binary\"], \"-u\", target] result = subprocess.run(cmd, capture_output=True, text=True) return self.parse_output(result.stdout) def parse_output(self, raw: str) -> list[Finding]: raise NotImplementedError All wrappers inherit from BaseWrapper and override parse_output . \ud83d\udce6 Resources \u00b6 Purpose \u00b6 resources manages global and scoped assets: - Wordlists (directories, subdomains, parameters) - Templates (Nuclei, ZAP) - Payload sets - Custom configurations Example Resource Model \u00b6 # core-lib/models/resource.py class Resource(BaseModel): id: str name: str type: Literal[\"wordlist\", \"template\", \"payload\"] scope: Literal[\"global\", \"group\", \"project\"] hash: str version: str metadata: dict \ud83d\uddc3\ufe0f Storage \u00b6 Purpose \u00b6 Implements all persistence operations via the StoragePort interface. Example Interface \u00b6 # core-lib/ports/storage_port.py class StoragePort(Protocol): def save_finding(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list_findings(self, project_id: str) -> list[Finding]: \"\"\"List all findings for a project.\"\"\" pass def delete_project(self, project_id: str) -> None: \"\"\"Delete a project and all its findings.\"\"\" pass Implementation Examples \u00b6 SQLiteStorageAdapter PostgresStorageAdapter RedisCacheAdapter All registered in the storage.registry . \ud83d\udd0c Plugins \u00b6 Purpose \u00b6 Extend SecFlow with additional detection or enrichment capabilities. Example Plugin Registration \u00b6 # plugins/registry.py from typing import Dict, Callable PLUGIN_REGISTRY: Dict[str, Callable] = {} def register_plugin(name: str): def decorator(func): PLUGIN_REGISTRY[name] = func return func return decorator Plugins can dynamically hook into findings processing, orchestration, or resource management. \ud83e\uddf0 Utils \u00b6 utils contains helper modules that are shared across packages but contain no business logic. Examples: \u00b6 utils.subprocess_safe \u2013 wrapper for secure process spawning. utils.hashing \u2013 generate resource hashes (SHA256). utils.validation \u2013 reusable Pydantic validators. utils.config \u2013 environment-aware configuration loader. \ud83d\udd17 Cross-Package Interaction Diagram \u00b6 +-------------+ +----------------+ +---------------+ | Wrappers | ----> | Findings-Engine| ----> | Storage | +-------------+ +----------------+ +---------------+ | ^ | | | | v | v +-----------+ +----------------+ +----------------+ | Resources | | Core-Lib | | Plugins | +-----------+ +----------------+ +----------------+ Arrows represent dependency flow, not import direction (imports always go downward). \ud83e\udde0 Best Practices \u00b6 Every package must expose a __all__ list for stable imports. Each module must define __version__ for dependency tracking. No package may import directly from /apps/ . Keep adapters stateless; use dependency injection for configuration. Always prefer Pydantic models over raw dictionaries. \ud83e\udde9 Testing Guidelines \u00b6 Package Test Type Focus core-lib Unit DTO validation and port contracts findings-engine Integration Normalization and enrichment correctness wrappers System Command execution, parsing accuracy storage Integration CRUD consistency across backends plugins Unit Hook execution and registry behavior \ud83e\udde0 Future Enhancements \u00b6 Introduce a service layer in core-lib to coordinate between repositories and orchestrators. Implement schema diffing for findings-engine to detect breaking changes. Add async wrappers for high-performance tools (e.g., Katana via aiohttp). Build auto-generated docs from port signatures ( mkdocs-gen-files ). Next: Orchestration & Workflow Engine","title":"Core Packages"},{"location":"architecture/04-core-packages-and-responsibilities/#04-core-packages-responsibilities","text":"","title":"04 \u2014 Core Packages &amp; Responsibilities"},{"location":"architecture/04-core-packages-and-responsibilities/#overview","text":"This document describes the core packages inside the packages/ directory and explains the roles, contracts, and relationships that define the heart of SecFlow. The architecture is intentionally hexagonal : - core-lib defines domain models and ports (interfaces). - Other packages such as findings-engine , wrappers , and storage implement those ports. - Application layers ( web-api , worker , cli ) use the ports but never touch implementations directly.","title":"\ud83e\udded Overview"},{"location":"architecture/04-core-packages-and-responsibilities/#package-overview-table","text":"Package Responsibility Depends On core-lib Domain models, DTOs, Ports, and Service Interfaces. None findings-engine Normalization and enrichment of scan data. core-lib wrappers Executes external tools and parses results. core-lib , utils resources Manages reusable assets (wordlists, payloads, templates). core-lib storage Implements persistence and caching (Postgres, SQLite, Redis). core-lib plugins Houses extension modules (detectors, enrichers, analytics). core-lib utils Generic utilities: logging, validation, subprocess helpers. None","title":"\ud83e\udde9 Package Overview Table"},{"location":"architecture/04-core-packages-and-responsibilities/#core-lib","text":"","title":"\u2699\ufe0f Core-Lib"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose","text":"core-lib is the foundation of SecFlow. It defines all domain entities, type contracts, and abstract interfaces (ports) used throughout the system.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"core-lib/\"] B[\"models/\"] C[\"finding.py\"] D[\"project.py\"] E[\"resource.py\"] F[\"run.py\"] G[\"ports/\"] H[\"storage_port.py\"] I[\"tool_port.py\"] J[\"resource_port.py\"] K[\"findings_port.py\"] L[\"services/\"] M[\"workflow_service.py\"] N[\"triage_service.py\"] O[\"exceptions.py\"] A --> B A --> G A --> L A --> O B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K L --> M L --> N","title":"Structure"},{"location":"architecture/04-core-packages-and-responsibilities/#example-model","text":"# core-lib/models/finding.py from pydantic import BaseModel from datetime import datetime from typing import List, Optional, Dict class Finding(BaseModel): id: str project_id: str detector_id: str title: str severity: str path: str evidence: Dict[str, str] created_at: datetime cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] enrichment: Dict[str, any] = {}","title":"Example Model"},{"location":"architecture/04-core-packages-and-responsibilities/#example-port","text":"# core-lib/ports/tool_port.py from typing import Protocol, List from core_lib.models.finding import Finding class ToolPort(Protocol): def prepare(self, config: dict) -> None: \"\"\"Prepare the tool with given configuration.\"\"\" pass def execute(self) -> List[Finding]: \"\"\"Execute the tool and return findings.\"\"\" pass def validate_output(self, raw_output: str) -> bool: \"\"\"Validate tool output format.\"\"\" pass This abstraction allows any external tool to be integrated simply by implementing ToolPort .","title":"Example Port"},{"location":"architecture/04-core-packages-and-responsibilities/#findings-engine","text":"","title":"\ud83e\udde0 Findings-Engine"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_1","text":"Responsible for normalizing, deduplicating, and enriching findings produced by wrappers.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-normalization-flow","text":"def normalize(raw_data: str, tool: str) -> Finding: if tool == \"nuclei\": return _normalize_nuclei_output(raw_data) elif tool == \"ferox\": return _normalize_ferox_output(raw_data)","title":"Example Normalization Flow"},{"location":"architecture/04-core-packages-and-responsibilities/#capabilities","text":"Parse multiple output formats (JSON, XML, plain text). Deduplicate based on fingerprint (host + path + vuln_id). Attach CWE, CVSS, and severity from enrichment sources. Store normalized data through StoragePort .","title":"Capabilities"},{"location":"architecture/04-core-packages-and-responsibilities/#wrappers","text":"","title":"\u2699\ufe0f Wrappers"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_2","text":"Wraps and executes third-party tools through a unified interface defined by ToolPort .","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-structure","text":"wrappers/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"wrappers/\"] B[\"nuclei_wrapper.py\"] C[\"ferox_wrapper.py\"] D[\"zap_wrapper.py\"] E[\"base_wrapper.py\"] A --> B A --> C A --> D A --> E","title":"Example Structure"},{"location":"architecture/04-core-packages-and-responsibilities/#example-base-class","text":"# wrappers/base_wrapper.py import subprocess from core_lib.models.finding import Finding class BaseWrapper: def __init__(self, manifest: dict): self.manifest = manifest def run(self, target: str) -> list[Finding]: cmd = [self.manifest[\"binary\"], \"-u\", target] result = subprocess.run(cmd, capture_output=True, text=True) return self.parse_output(result.stdout) def parse_output(self, raw: str) -> list[Finding]: raise NotImplementedError All wrappers inherit from BaseWrapper and override parse_output .","title":"Example Base Class"},{"location":"architecture/04-core-packages-and-responsibilities/#resources","text":"","title":"\ud83d\udce6 Resources"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_3","text":"resources manages global and scoped assets: - Wordlists (directories, subdomains, parameters) - Templates (Nuclei, ZAP) - Payload sets - Custom configurations","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-resource-model","text":"# core-lib/models/resource.py class Resource(BaseModel): id: str name: str type: Literal[\"wordlist\", \"template\", \"payload\"] scope: Literal[\"global\", \"group\", \"project\"] hash: str version: str metadata: dict","title":"Example Resource Model"},{"location":"architecture/04-core-packages-and-responsibilities/#storage","text":"","title":"\ud83d\uddc3\ufe0f Storage"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_4","text":"Implements all persistence operations via the StoragePort interface.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-interface","text":"# core-lib/ports/storage_port.py class StoragePort(Protocol): def save_finding(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list_findings(self, project_id: str) -> list[Finding]: \"\"\"List all findings for a project.\"\"\" pass def delete_project(self, project_id: str) -> None: \"\"\"Delete a project and all its findings.\"\"\" pass","title":"Example Interface"},{"location":"architecture/04-core-packages-and-responsibilities/#implementation-examples","text":"SQLiteStorageAdapter PostgresStorageAdapter RedisCacheAdapter All registered in the storage.registry .","title":"Implementation Examples"},{"location":"architecture/04-core-packages-and-responsibilities/#plugins","text":"","title":"\ud83d\udd0c Plugins"},{"location":"architecture/04-core-packages-and-responsibilities/#purpose_5","text":"Extend SecFlow with additional detection or enrichment capabilities.","title":"Purpose"},{"location":"architecture/04-core-packages-and-responsibilities/#example-plugin-registration","text":"# plugins/registry.py from typing import Dict, Callable PLUGIN_REGISTRY: Dict[str, Callable] = {} def register_plugin(name: str): def decorator(func): PLUGIN_REGISTRY[name] = func return func return decorator Plugins can dynamically hook into findings processing, orchestration, or resource management.","title":"Example Plugin Registration"},{"location":"architecture/04-core-packages-and-responsibilities/#utils","text":"utils contains helper modules that are shared across packages but contain no business logic.","title":"\ud83e\uddf0 Utils"},{"location":"architecture/04-core-packages-and-responsibilities/#examples","text":"utils.subprocess_safe \u2013 wrapper for secure process spawning. utils.hashing \u2013 generate resource hashes (SHA256). utils.validation \u2013 reusable Pydantic validators. utils.config \u2013 environment-aware configuration loader.","title":"Examples:"},{"location":"architecture/04-core-packages-and-responsibilities/#cross-package-interaction-diagram","text":"+-------------+ +----------------+ +---------------+ | Wrappers | ----> | Findings-Engine| ----> | Storage | +-------------+ +----------------+ +---------------+ | ^ | | | | v | v +-----------+ +----------------+ +----------------+ | Resources | | Core-Lib | | Plugins | +-----------+ +----------------+ +----------------+ Arrows represent dependency flow, not import direction (imports always go downward).","title":"\ud83d\udd17 Cross-Package Interaction Diagram"},{"location":"architecture/04-core-packages-and-responsibilities/#best-practices","text":"Every package must expose a __all__ list for stable imports. Each module must define __version__ for dependency tracking. No package may import directly from /apps/ . Keep adapters stateless; use dependency injection for configuration. Always prefer Pydantic models over raw dictionaries.","title":"\ud83e\udde0 Best Practices"},{"location":"architecture/04-core-packages-and-responsibilities/#testing-guidelines","text":"Package Test Type Focus core-lib Unit DTO validation and port contracts findings-engine Integration Normalization and enrichment correctness wrappers System Command execution, parsing accuracy storage Integration CRUD consistency across backends plugins Unit Hook execution and registry behavior","title":"\ud83e\udde9 Testing Guidelines"},{"location":"architecture/04-core-packages-and-responsibilities/#future-enhancements","text":"Introduce a service layer in core-lib to coordinate between repositories and orchestrators. Implement schema diffing for findings-engine to detect breaking changes. Add async wrappers for high-performance tools (e.g., Katana via aiohttp). Build auto-generated docs from port signatures ( mkdocs-gen-files ). Next: Orchestration & Workflow Engine","title":"\ud83e\udde0 Future Enhancements"},{"location":"architecture/05-orchestration-and-workflow-engine/","text":"05 \u2014 Orchestration & Workflow Engine \u00b6 \ud83e\udded Overview \u00b6 The Orchestration Engine is the operational core of SecFlow. It executes multi-tool workflows composed of discovery , scanning , filtering , enrichment , and analysis stages. Workflows are defined declaratively using YAML recipes and executed as Directed Acyclic Graphs (DAGs) . Each node represents a tool invocation , and edges define data flow between outputs and inputs. \ud83e\uddf1 Conceptual Model \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Node A<br/>(Ferox)\"] B[\"Node B<br/>(Nuclei)\"] C[\"Node C<br/>(CVE Enr)\"] D[\"Inputs/Outputs\"] E[\"Config/Env\"] F[\"Artifact Storage\"] A --> B B --> C A --> D B --> E C --> F Each node produces one or more datasets that can be consumed by downstream nodes. The workflow engine guarantees: - Topological order of execution - Concurrent execution where possible - Automatic retries, timeouts, and logging per node \u2699\ufe0f Workflow Specification Schema \u00b6 version: \"1.0\" name: \"OWASP Top 10 Scan\" description: \"End-to-end test: discovery \u2192 scan \u2192 enrichment\" nodes: - id: \"discovery\" type: \"discovery.ferox\" config: wordlist: \"res://wordlists/dirb:latest\" threads: 50 outputs: [\"urls\"] - id: \"scan\" type: \"scan.nuclei\" inputs: [\"urls\"] config: templates: \"res://templates/owasp-top10:latest\" rate_limit: 150 outputs: [\"findings\"] - id: \"enrich\" type: \"enrich.cve\" inputs: [\"findings\"] config: sources: [\"nvd\", \"osv\", \"exploitdb\"] outputs: [\"enriched_findings\"] \ud83e\udde9 Workflow Engine Architecture \u00b6 +-------------------------------------------------------------+ | Worker Engine | |-------------------------------------------------------------| | - WorkflowScheduler | | - NodeExecutor | | - ResultCache | | - EventBus | |-------------------------------------------------------------| | Uses: Celery (Redis), asyncio, Pydantic validation | +-------------------------------------------------------------+ Key Components \u00b6 Component Description WorkflowScheduler Parses YAML recipes, builds DAG, submits jobs to queue. NodeExecutor Executes nodes, manages subprocess wrappers. ResultCache Stores intermediate results between nodes. EventBus Publishes events (node_started, node_completed, workflow_failed). WorkflowStore Persists workflow metadata in DB. \u2699\ufe0f Python Model \u2014 DAG Representation \u00b6 # findings-engine/workflow_dag.py from typing import List, Dict, Any from pydantic import BaseModel class Node(BaseModel): id: str type: str config: Dict[str, Any] = {} inputs: List[str] = [] outputs: List[str] = [] class Workflow(BaseModel): id: str name: str description: str nodes: List[Node] DAG Validation Example \u00b6 def validate_dag(workflow: Workflow): ids = [n.id for n in workflow.nodes] for node in workflow.nodes: for inp in node.inputs: if inp not in [out for n in workflow.nodes for out in n.outputs]: raise ValueError(f\"Unresolved input '{inp}' in node {node.id}\") \ud83e\udde0 Execution Flow \u00b6 Parse & Validate YAML workflow using Pydantic schema. Register DAG in database ( WorkflowStore ). Submit nodes to Celery/RQ queue respecting topological order. Execute wrappers through ToolPort interface. Normalize findings via FindingsEngine . Publish events to EventBus . Update metrics and trigger downstream listeners (e.g., triage UI). \u2699\ufe0f Node Executor (Simplified) \u00b6 # worker/executor.py from core_lib.ports.tool_port import ToolPort class NodeExecutor: def __init__(self, node, context): self.node = node self.context = context def run(self): tool: ToolPort = self.context.resolve_tool(self.node.type) tool.prepare(self.node.config) results = tool.execute() self.context.store_results(self.node.outputs, results) return results \ud83d\udd04 Concurrency Model \u00b6 The orchestration engine is designed for asynchronous, multi-tool execution. Execution Mode Description Sequential Enforced by DAG dependencies. Parallel Independent nodes run concurrently (async or Celery workers). Chained Output from one node auto-feeds into next via ResultCache . Example \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"ferox\"] B[\"katana\"] C[\"nuclei\"] D[\"httpx\"] A --> B B --> C A --> D D --> C \ud83e\udde9 Error Handling \u00b6 Error Type Handling Strategy Tool crash / non-zero exit Retry (max=3) then mark node failed. Timeout Kill process, log event, continue DAG. Missing input dataset Block downstream nodes, mark dependency unresolved. Parser error Log raw output, fallback to generic findings schema. Each failure is logged in the workflow_runs table and visible in the UI. \ud83e\udde0 Event System \u00b6 The orchestration layer publishes real-time events to facilitate reactive behavior. Example Event Contract \u00b6 { \"event\": \"node_completed\", \"workflow_id\": \"abc123\", \"node_id\": \"scan\", \"duration\": 12.3, \"findings\": 124 } Events can be consumed by: - WebSocket clients in UI (live progress) - Audit log processors - Metrics collectors \ud83e\udde9 Caching & Reuse \u00b6 Intermediate Data: Stored under /cache/{workflow_id}/{node_id}.json ResultHashing: SHA256 of config + inputs for cache hits Warm Runs: Workflows can resume from cached intermediate outputs cache_key = hashlib.sha256(json.dumps(node.config).encode()).hexdigest() \ud83e\uddf1 Example DAG Execution Trace \u00b6 [2025-10-06 12:01:02] Workflow \"OWASP Top 10 Scan\" started [2025-10-06 12:01:05] Node discovery.ferox completed (urls=356) [2025-10-06 12:01:07] Node scan.nuclei completed (findings=112) [2025-10-06 12:01:10] Node enrich.cve completed (enriched_findings=112) [2025-10-06 12:01:10] Workflow completed successfully \ud83d\udd0c Integration with Other Components \u00b6 Component Interaction Findings Engine Receives raw output for normalization. Wrappers Execute the underlying binaries/tools. Storage Persists workflow runs, logs, results. Plugins Hooks into on_node_complete and on_workflow_complete . UI / API Subscribes to event bus for progress updates. \ud83e\udde9 Monitoring & Metrics \u00b6 Every node execution reports: - Duration (seconds) - Findings count - Exit status - CPU/memory usage - Cache hits These metrics feed Prometheus exporters and the analytics dashboards. \ud83e\udde0 Future Enhancements \u00b6 GraphQL-based workflow builder UI. Dynamic scheduling policies (priority, resource weighting). Conditional branching (if, switch nodes). AI-assisted workflow suggestions based on context and prior runs. Distributed orchestration using Celery groups/chords. Next: Plugin System","title":"Orchestration Engine"},{"location":"architecture/05-orchestration-and-workflow-engine/#05-orchestration-workflow-engine","text":"","title":"05 \u2014 Orchestration &amp; Workflow Engine"},{"location":"architecture/05-orchestration-and-workflow-engine/#overview","text":"The Orchestration Engine is the operational core of SecFlow. It executes multi-tool workflows composed of discovery , scanning , filtering , enrichment , and analysis stages. Workflows are defined declaratively using YAML recipes and executed as Directed Acyclic Graphs (DAGs) . Each node represents a tool invocation , and edges define data flow between outputs and inputs.","title":"\ud83e\udded Overview"},{"location":"architecture/05-orchestration-and-workflow-engine/#conceptual-model","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Node A<br/>(Ferox)\"] B[\"Node B<br/>(Nuclei)\"] C[\"Node C<br/>(CVE Enr)\"] D[\"Inputs/Outputs\"] E[\"Config/Env\"] F[\"Artifact Storage\"] A --> B B --> C A --> D B --> E C --> F Each node produces one or more datasets that can be consumed by downstream nodes. The workflow engine guarantees: - Topological order of execution - Concurrent execution where possible - Automatic retries, timeouts, and logging per node","title":"\ud83e\uddf1 Conceptual Model"},{"location":"architecture/05-orchestration-and-workflow-engine/#workflow-specification-schema","text":"version: \"1.0\" name: \"OWASP Top 10 Scan\" description: \"End-to-end test: discovery \u2192 scan \u2192 enrichment\" nodes: - id: \"discovery\" type: \"discovery.ferox\" config: wordlist: \"res://wordlists/dirb:latest\" threads: 50 outputs: [\"urls\"] - id: \"scan\" type: \"scan.nuclei\" inputs: [\"urls\"] config: templates: \"res://templates/owasp-top10:latest\" rate_limit: 150 outputs: [\"findings\"] - id: \"enrich\" type: \"enrich.cve\" inputs: [\"findings\"] config: sources: [\"nvd\", \"osv\", \"exploitdb\"] outputs: [\"enriched_findings\"]","title":"\u2699\ufe0f Workflow Specification Schema"},{"location":"architecture/05-orchestration-and-workflow-engine/#workflow-engine-architecture","text":"+-------------------------------------------------------------+ | Worker Engine | |-------------------------------------------------------------| | - WorkflowScheduler | | - NodeExecutor | | - ResultCache | | - EventBus | |-------------------------------------------------------------| | Uses: Celery (Redis), asyncio, Pydantic validation | +-------------------------------------------------------------+","title":"\ud83e\udde9 Workflow Engine Architecture"},{"location":"architecture/05-orchestration-and-workflow-engine/#key-components","text":"Component Description WorkflowScheduler Parses YAML recipes, builds DAG, submits jobs to queue. NodeExecutor Executes nodes, manages subprocess wrappers. ResultCache Stores intermediate results between nodes. EventBus Publishes events (node_started, node_completed, workflow_failed). WorkflowStore Persists workflow metadata in DB.","title":"Key Components"},{"location":"architecture/05-orchestration-and-workflow-engine/#python-model-dag-representation","text":"# findings-engine/workflow_dag.py from typing import List, Dict, Any from pydantic import BaseModel class Node(BaseModel): id: str type: str config: Dict[str, Any] = {} inputs: List[str] = [] outputs: List[str] = [] class Workflow(BaseModel): id: str name: str description: str nodes: List[Node]","title":"\u2699\ufe0f Python Model \u2014 DAG Representation"},{"location":"architecture/05-orchestration-and-workflow-engine/#dag-validation-example","text":"def validate_dag(workflow: Workflow): ids = [n.id for n in workflow.nodes] for node in workflow.nodes: for inp in node.inputs: if inp not in [out for n in workflow.nodes for out in n.outputs]: raise ValueError(f\"Unresolved input '{inp}' in node {node.id}\")","title":"DAG Validation Example"},{"location":"architecture/05-orchestration-and-workflow-engine/#execution-flow","text":"Parse & Validate YAML workflow using Pydantic schema. Register DAG in database ( WorkflowStore ). Submit nodes to Celery/RQ queue respecting topological order. Execute wrappers through ToolPort interface. Normalize findings via FindingsEngine . Publish events to EventBus . Update metrics and trigger downstream listeners (e.g., triage UI).","title":"\ud83e\udde0 Execution Flow"},{"location":"architecture/05-orchestration-and-workflow-engine/#node-executor-simplified","text":"# worker/executor.py from core_lib.ports.tool_port import ToolPort class NodeExecutor: def __init__(self, node, context): self.node = node self.context = context def run(self): tool: ToolPort = self.context.resolve_tool(self.node.type) tool.prepare(self.node.config) results = tool.execute() self.context.store_results(self.node.outputs, results) return results","title":"\u2699\ufe0f Node Executor (Simplified)"},{"location":"architecture/05-orchestration-and-workflow-engine/#concurrency-model","text":"The orchestration engine is designed for asynchronous, multi-tool execution. Execution Mode Description Sequential Enforced by DAG dependencies. Parallel Independent nodes run concurrently (async or Celery workers). Chained Output from one node auto-feeds into next via ResultCache .","title":"\ud83d\udd04 Concurrency Model"},{"location":"architecture/05-orchestration-and-workflow-engine/#example","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"ferox\"] B[\"katana\"] C[\"nuclei\"] D[\"httpx\"] A --> B B --> C A --> D D --> C","title":"Example"},{"location":"architecture/05-orchestration-and-workflow-engine/#error-handling","text":"Error Type Handling Strategy Tool crash / non-zero exit Retry (max=3) then mark node failed. Timeout Kill process, log event, continue DAG. Missing input dataset Block downstream nodes, mark dependency unresolved. Parser error Log raw output, fallback to generic findings schema. Each failure is logged in the workflow_runs table and visible in the UI.","title":"\ud83e\udde9 Error Handling"},{"location":"architecture/05-orchestration-and-workflow-engine/#event-system","text":"The orchestration layer publishes real-time events to facilitate reactive behavior.","title":"\ud83e\udde0 Event System"},{"location":"architecture/05-orchestration-and-workflow-engine/#example-event-contract","text":"{ \"event\": \"node_completed\", \"workflow_id\": \"abc123\", \"node_id\": \"scan\", \"duration\": 12.3, \"findings\": 124 } Events can be consumed by: - WebSocket clients in UI (live progress) - Audit log processors - Metrics collectors","title":"Example Event Contract"},{"location":"architecture/05-orchestration-and-workflow-engine/#caching-reuse","text":"Intermediate Data: Stored under /cache/{workflow_id}/{node_id}.json ResultHashing: SHA256 of config + inputs for cache hits Warm Runs: Workflows can resume from cached intermediate outputs cache_key = hashlib.sha256(json.dumps(node.config).encode()).hexdigest()","title":"\ud83e\udde9 Caching &amp; Reuse"},{"location":"architecture/05-orchestration-and-workflow-engine/#example-dag-execution-trace","text":"[2025-10-06 12:01:02] Workflow \"OWASP Top 10 Scan\" started [2025-10-06 12:01:05] Node discovery.ferox completed (urls=356) [2025-10-06 12:01:07] Node scan.nuclei completed (findings=112) [2025-10-06 12:01:10] Node enrich.cve completed (enriched_findings=112) [2025-10-06 12:01:10] Workflow completed successfully","title":"\ud83e\uddf1 Example DAG Execution Trace"},{"location":"architecture/05-orchestration-and-workflow-engine/#integration-with-other-components","text":"Component Interaction Findings Engine Receives raw output for normalization. Wrappers Execute the underlying binaries/tools. Storage Persists workflow runs, logs, results. Plugins Hooks into on_node_complete and on_workflow_complete . UI / API Subscribes to event bus for progress updates.","title":"\ud83d\udd0c Integration with Other Components"},{"location":"architecture/05-orchestration-and-workflow-engine/#monitoring-metrics","text":"Every node execution reports: - Duration (seconds) - Findings count - Exit status - CPU/memory usage - Cache hits These metrics feed Prometheus exporters and the analytics dashboards.","title":"\ud83e\udde9 Monitoring &amp; Metrics"},{"location":"architecture/05-orchestration-and-workflow-engine/#future-enhancements","text":"GraphQL-based workflow builder UI. Dynamic scheduling policies (priority, resource weighting). Conditional branching (if, switch nodes). AI-assisted workflow suggestions based on context and prior runs. Distributed orchestration using Celery groups/chords. Next: Plugin System","title":"\ud83e\udde0 Future Enhancements"},{"location":"architecture/06-plugin-system/","text":"06 \u2014 Plugin System \u00b6 \ud83e\udded Overview \u00b6 The Plugin System in SecFlow provides an extensible runtime for integrating new detection, enrichment, or analytics capabilities without modifying core code. Plugins operate under a controlled lifecycle and interact through strictly defined contracts (ports) . SecFlow supports three primary plugin categories: 1. Detectors \u2014 Introduce new scanning logic or post-processing steps. 2. Enrichers \u2014 Add vulnerability metadata (CVE, CWE, CVSS, etc.). 3. Analytics \u2014 Compute metrics, patterns, or visual insights over findings. \ud83e\udde9 Design Principles \u00b6 Principle Description Isolation Each plugin runs in its own context (thread/process sandbox). Declarative Registration Plugins register via manifests or decorators. Contract Enforcement Plugins implement strict base interfaces ( PluginBase , DetectorPlugin , etc.). Hot Reloading Plugin discovery occurs dynamically at startup. Safety Each plugin is validated, hashed, and versioned. \ud83e\uddf1 Plugin Architecture Overview \u00b6 +-------------------------------------------------------------+ | Plugin Manager | | - Registry | | - Loader | | - Validator | | - Sandbox | +-------------------------------------------------------------+ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Plugin Registry\"] B[\"Detector Plugins\"] C[\"Enricher Plugins\"] D[\"Analytics Plugins\"] A --> B A --> C A --> D \u2699\ufe0f Base Interfaces \u00b6 # core-lib/ports/plugin_port.py from typing import Protocol, Any, Dict class PluginPort(Protocol): name: str version: str category: str # \"detector\" | \"enricher\" | \"analytics\" def initialize(self, context: Dict[str, Any]) -> None: \"\"\"Initialize plugin with execution context.\"\"\" pass def execute(self, data: Any) -> Any: \"\"\"Execute plugin logic on input data.\"\"\" pass def teardown(self) -> None: \"\"\"Clean up plugin resources.\"\"\" pass All plugins must subclass or conform to PluginPort . \ud83e\udde9 Example Plugin Registry \u00b6 # plugins/registry.py from typing import Dict, Type from core_lib.ports.plugin_port import PluginPort class PluginRegistry: _registry: Dict[str, Type[PluginPort]] = {} @classmethod def register(cls, name: str, plugin: Type[PluginPort]): cls._registry[name] = plugin print(f\"[+] Registered plugin: {name}\") @classmethod def get(cls, name: str) -> PluginPort: return cls._registry[name] @classmethod def list_plugins(cls): return list(cls._registry.keys()) Registration via Decorator \u00b6 def register_plugin(name: str): def decorator(cls): PluginRegistry.register(name, cls) return cls return decorator \ud83e\udde0 Plugin Lifecycle \u00b6 Registration: Discovered from manifests or decorators. Validation: Signature & schema verification. Initialization: Context injected (paths, project ID, config). Execution: Process data via execute() . Teardown: Release resources, log metrics. \ud83e\udde9 Plugin Manifest Specification \u00b6 name: nuclei-enricher version: \"1.0.0\" category: \"enricher\" entrypoint: \"plugins.nuclei_enricher:Enricher\" dependencies: - requests - cpe config_schema: \"schemas/nuclei_enricher.json\" sandbox: true Each manifest is stored under /plugins/manifests/ and validated on startup. \u2699\ufe0f Example \u2014 CVE Enricher Plugin \u00b6 # plugins/nuclei_enricher.py import requests from core_lib.models.finding import Finding from core_lib.ports.plugin_port import PluginPort @register_plugin(\"cve_enricher\") class CVEEnricher(PluginPort): name = \"cve_enricher\" version = \"1.0.0\" category = \"enricher\" def initialize(self, context): self.sources = context.get(\"sources\", [\"nvd\"]) def execute(self, finding: Finding) -> Finding: for cve_id in finding.cve_ids: data = self._fetch_cve_data(cve_id) finding.enrichment[\"cve\"][cve_id] = data return finding def _fetch_cve_data(self, cve_id): return requests.get(f\"https://services.nvd.nist.gov/rest/json/cve/1.0/{cve_id}\").json() def teardown(self): pass \ud83d\udd10 Sandbox Model \u00b6 Each plugin executes inside a restricted environment: Control Enforcement Filesystem Read-only mount or temp directory Network Denied by default, opt-in per manifest Memory / CPU Controlled via subprocess resource limits Timeouts Enforced via execution wrapper Audit Every plugin invocation logged with context \ud83e\udde9 Plugin Discovery \u00b6 Directory Layout \u00b6 plugins/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"plugins/\"] B[\"registry.py\"] C[\"manifests/\"] D[\"cve_enricher.yaml\"] E[\"nuclei_detector.yaml\"] F[\"detectors/\"] G[\"nuclei_detector.py\"] H[\"zap_detector.py\"] A --> B A --> C A --> F C --> D C --> E F --> G F --> H Discovery Algorithm \u00b6 def discover_plugins(): for manifest in Path(\"plugins/manifests\").glob(\"*.yaml\"): data = yaml.safe_load(manifest.read_text()) entrypoint = import_string(data[\"entrypoint\"]) PluginRegistry.register(data[\"name\"], entrypoint) \ud83e\udde0 Plugin Telemetry \u00b6 Each plugin emits lifecycle events: { \"event\": \"plugin_executed\", \"plugin\": \"cve_enricher\", \"duration_ms\": 342, \"memory_mb\": 42, \"success\": true } Telemetry is captured by the Observability subsystem (see Observability, Logging & Metrics ). \ud83e\udde9 Error Handling \u00b6 Error Strategy Invalid Manifest Skip plugin, log warning. Dependency ImportError Attempt isolated install if allowed. Execution Timeout Abort plugin, mark node partial-success. Sandbox Violation Terminate process, revoke plugin signature. \ud83e\udde0 Example End-to-End Plugin Flow \u00b6 [Plugin Manifest] \u2192 [Registry Register] \u2192 [Initialize] \u2193 \u2193 \u2193 [Execute via Port] \u2192 [Telemetry + Logging] \u2192 [Teardown] \ud83d\udd2e Future Enhancements \u00b6 Plugin signing (PGP signatures). Remote plugin repository (index + version resolution). In-UI plugin store with validation and ratings. Plugin telemetry aggregation dashboards. Next: Tools Integration Model","title":"Plugin System"},{"location":"architecture/06-plugin-system/#06-plugin-system","text":"","title":"06 \u2014 Plugin System"},{"location":"architecture/06-plugin-system/#overview","text":"The Plugin System in SecFlow provides an extensible runtime for integrating new detection, enrichment, or analytics capabilities without modifying core code. Plugins operate under a controlled lifecycle and interact through strictly defined contracts (ports) . SecFlow supports three primary plugin categories: 1. Detectors \u2014 Introduce new scanning logic or post-processing steps. 2. Enrichers \u2014 Add vulnerability metadata (CVE, CWE, CVSS, etc.). 3. Analytics \u2014 Compute metrics, patterns, or visual insights over findings.","title":"\ud83e\udded Overview"},{"location":"architecture/06-plugin-system/#design-principles","text":"Principle Description Isolation Each plugin runs in its own context (thread/process sandbox). Declarative Registration Plugins register via manifests or decorators. Contract Enforcement Plugins implement strict base interfaces ( PluginBase , DetectorPlugin , etc.). Hot Reloading Plugin discovery occurs dynamically at startup. Safety Each plugin is validated, hashed, and versioned.","title":"\ud83e\udde9 Design Principles"},{"location":"architecture/06-plugin-system/#plugin-architecture-overview","text":"+-------------------------------------------------------------+ | Plugin Manager | | - Registry | | - Loader | | - Validator | | - Sandbox | +-------------------------------------------------------------+ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Plugin Registry\"] B[\"Detector Plugins\"] C[\"Enricher Plugins\"] D[\"Analytics Plugins\"] A --> B A --> C A --> D","title":"\ud83e\uddf1 Plugin Architecture Overview"},{"location":"architecture/06-plugin-system/#base-interfaces","text":"# core-lib/ports/plugin_port.py from typing import Protocol, Any, Dict class PluginPort(Protocol): name: str version: str category: str # \"detector\" | \"enricher\" | \"analytics\" def initialize(self, context: Dict[str, Any]) -> None: \"\"\"Initialize plugin with execution context.\"\"\" pass def execute(self, data: Any) -> Any: \"\"\"Execute plugin logic on input data.\"\"\" pass def teardown(self) -> None: \"\"\"Clean up plugin resources.\"\"\" pass All plugins must subclass or conform to PluginPort .","title":"\u2699\ufe0f Base Interfaces"},{"location":"architecture/06-plugin-system/#example-plugin-registry","text":"# plugins/registry.py from typing import Dict, Type from core_lib.ports.plugin_port import PluginPort class PluginRegistry: _registry: Dict[str, Type[PluginPort]] = {} @classmethod def register(cls, name: str, plugin: Type[PluginPort]): cls._registry[name] = plugin print(f\"[+] Registered plugin: {name}\") @classmethod def get(cls, name: str) -> PluginPort: return cls._registry[name] @classmethod def list_plugins(cls): return list(cls._registry.keys())","title":"\ud83e\udde9 Example Plugin Registry"},{"location":"architecture/06-plugin-system/#registration-via-decorator","text":"def register_plugin(name: str): def decorator(cls): PluginRegistry.register(name, cls) return cls return decorator","title":"Registration via Decorator"},{"location":"architecture/06-plugin-system/#plugin-lifecycle","text":"Registration: Discovered from manifests or decorators. Validation: Signature & schema verification. Initialization: Context injected (paths, project ID, config). Execution: Process data via execute() . Teardown: Release resources, log metrics.","title":"\ud83e\udde0 Plugin Lifecycle"},{"location":"architecture/06-plugin-system/#plugin-manifest-specification","text":"name: nuclei-enricher version: \"1.0.0\" category: \"enricher\" entrypoint: \"plugins.nuclei_enricher:Enricher\" dependencies: - requests - cpe config_schema: \"schemas/nuclei_enricher.json\" sandbox: true Each manifest is stored under /plugins/manifests/ and validated on startup.","title":"\ud83e\udde9 Plugin Manifest Specification"},{"location":"architecture/06-plugin-system/#example-cve-enricher-plugin","text":"# plugins/nuclei_enricher.py import requests from core_lib.models.finding import Finding from core_lib.ports.plugin_port import PluginPort @register_plugin(\"cve_enricher\") class CVEEnricher(PluginPort): name = \"cve_enricher\" version = \"1.0.0\" category = \"enricher\" def initialize(self, context): self.sources = context.get(\"sources\", [\"nvd\"]) def execute(self, finding: Finding) -> Finding: for cve_id in finding.cve_ids: data = self._fetch_cve_data(cve_id) finding.enrichment[\"cve\"][cve_id] = data return finding def _fetch_cve_data(self, cve_id): return requests.get(f\"https://services.nvd.nist.gov/rest/json/cve/1.0/{cve_id}\").json() def teardown(self): pass","title":"\u2699\ufe0f Example \u2014 CVE Enricher Plugin"},{"location":"architecture/06-plugin-system/#sandbox-model","text":"Each plugin executes inside a restricted environment: Control Enforcement Filesystem Read-only mount or temp directory Network Denied by default, opt-in per manifest Memory / CPU Controlled via subprocess resource limits Timeouts Enforced via execution wrapper Audit Every plugin invocation logged with context","title":"\ud83d\udd10 Sandbox Model"},{"location":"architecture/06-plugin-system/#plugin-discovery","text":"","title":"\ud83e\udde9 Plugin Discovery"},{"location":"architecture/06-plugin-system/#directory-layout","text":"plugins/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"plugins/\"] B[\"registry.py\"] C[\"manifests/\"] D[\"cve_enricher.yaml\"] E[\"nuclei_detector.yaml\"] F[\"detectors/\"] G[\"nuclei_detector.py\"] H[\"zap_detector.py\"] A --> B A --> C A --> F C --> D C --> E F --> G F --> H","title":"Directory Layout"},{"location":"architecture/06-plugin-system/#discovery-algorithm","text":"def discover_plugins(): for manifest in Path(\"plugins/manifests\").glob(\"*.yaml\"): data = yaml.safe_load(manifest.read_text()) entrypoint = import_string(data[\"entrypoint\"]) PluginRegistry.register(data[\"name\"], entrypoint)","title":"Discovery Algorithm"},{"location":"architecture/06-plugin-system/#plugin-telemetry","text":"Each plugin emits lifecycle events: { \"event\": \"plugin_executed\", \"plugin\": \"cve_enricher\", \"duration_ms\": 342, \"memory_mb\": 42, \"success\": true } Telemetry is captured by the Observability subsystem (see Observability, Logging & Metrics ).","title":"\ud83e\udde0 Plugin Telemetry"},{"location":"architecture/06-plugin-system/#error-handling","text":"Error Strategy Invalid Manifest Skip plugin, log warning. Dependency ImportError Attempt isolated install if allowed. Execution Timeout Abort plugin, mark node partial-success. Sandbox Violation Terminate process, revoke plugin signature.","title":"\ud83e\udde9 Error Handling"},{"location":"architecture/06-plugin-system/#example-end-to-end-plugin-flow","text":"[Plugin Manifest] \u2192 [Registry Register] \u2192 [Initialize] \u2193 \u2193 \u2193 [Execute via Port] \u2192 [Telemetry + Logging] \u2192 [Teardown]","title":"\ud83e\udde0 Example End-to-End Plugin Flow"},{"location":"architecture/06-plugin-system/#future-enhancements","text":"Plugin signing (PGP signatures). Remote plugin repository (index + version resolution). In-UI plugin store with validation and ratings. Plugin telemetry aggregation dashboards. Next: Tools Integration Model","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/07-tools-integration-model/","text":"07 \u2014 Tools Integration Model \u00b6 \ud83e\udded Overview \u00b6 The Tools Integration Model defines how external security tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.) are integrated and orchestrated inside SecFlow . The system is manifest-driven , allowing tools to be defined declaratively, executed in isolation, and chained dynamically via the Workflow Engine. Each integrated tool is represented as: - A manifest (describes binary, configuration, security policies, I/O schema) - A wrapper (implements the standardized execution interface) - An optional adapter plugin (translates output to normalized Findings) \ud83e\uddf1 Architectural Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry<br/>- Manifest Validator<br/>- Sandbox Executor<br/>- Output Parser\"] B[\"Tool Wrapper<br/>(e.g., Nuclei)\"] C[\"Tool Output Parser\"] D[\"Findings Engine\"] A --> B B --> C C --> D \u2699\ufe0f Tool Manifest Structure \u00b6 Every tool integrated into SecFlow must define a manifest under /wrappers/manifests/<tool>.json . Example: nuclei.json \u00b6 { \"name\": \"nuclei\", \"version\": \"3.2.1\", \"binary\": \"nuclei\", \"capabilities\": [\"scan\"], \"outputs\": {\"dataset\": \"findings\"}, \"defaults\": { \"threads\": 25, \"rate_limit\": 150, \"templates\": \"res://templates/nuclei:latest\" }, \"configSchema\": \"schemas/nuclei-config.json\", \"resource_requirements\": { \"cpu_cores\": 2, \"memory_mb\": 512, \"timeout_seconds\": 300 }, \"security\": { \"sandbox\": true, \"network_access\": true, \"file_system_access\": \"read_only\" }, \"selftest\": { \"args\": [\"-version\"], \"expect\": \"Nuclei\" } } \ud83e\udde9 Wrapper Interface \u00b6 All tool wrappers implement the same contract to ensure consistent orchestration. # core-lib/ports/tool_port.py from typing import Any, Dict, Protocol, List from core_lib.models.finding import Finding class ToolPort(Protocol): def prepare(self, config: Dict[str, Any]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run(self) -> str: \"\"\"Execute tool and return raw output.\"\"\" pass def parse_output(self, raw_output: str) -> List[Finding]: \"\"\"Parse raw output into structured findings.\"\"\" pass \ud83e\udde0 Example Implementation: Nuclei Wrapper \u00b6 # wrappers/nuclei_wrapper.py import subprocess, json from core_lib.models.finding import Finding from core_lib.ports.tool_port import ToolPort class NucleiWrapper(ToolPort): def __init__(self, config): self.config = config def prepare(self, config): self.args = [ \"nuclei\", \"-json\", \"-t\", config.get(\"templates\", \"res://templates/nuclei:latest\"), \"-rl\", str(config.get(\"rate_limit\", 150)), \"-c\", str(config.get(\"threads\", 25)) ] def run(self): result = subprocess.run(self.args, capture_output=True, text=True, timeout=300) return result.stdout def parse_output(self, raw_output): findings = [] for line in raw_output.splitlines(): try: data = json.loads(line) findings.append( Finding( title=data[\"info\"][\"name\"], severity=data[\"info\"][\"severity\"], path=data[\"matched-at\"], detector_id=\"nuclei\", evidence=data ) ) except Exception: continue return findings \ud83d\udd10 Sandbox Execution \u00b6 Tools run through a Sandbox Executor, enforcing CPU, memory, and network constraints. # wrappers/executor.py import resource, subprocess, signal class SandboxExecutor: def __init__(self, cpu_limit=2, mem_limit_mb=512): self.cpu_limit = cpu_limit self.mem_limit_mb = mem_limit_mb def execute(self, args): def set_limits(): resource.setrlimit(resource.RLIMIT_CPU, (self.cpu_limit, self.cpu_limit)) resource.setrlimit(resource.RLIMIT_AS, (self.mem_limit_mb * 1024**2,) * 2) proc = subprocess.Popen(args, preexec_fn=set_limits, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = proc.communicate(timeout=300) return stdout.decode(), stderr.decode(), proc.returncode \ud83e\udde9 Tool Registry \u00b6 # wrappers/registry.py from typing import Dict, Type from core_lib.ports.tool_port import ToolPort class ToolRegistry: _registry: Dict[str, Type[ToolPort]] = {} @classmethod def register(cls, name: str, impl: Type[ToolPort]): cls._registry[name] = impl print(f\"[+] Registered tool: {name}\") @classmethod def get(cls, name: str) -> ToolPort: return cls._registry[name] Tools register via decorators or discovery: from wrappers.registry import ToolRegistry @ToolRegistry.register(\"feroxbuster\") class FeroxWrapper(ToolPort): def prepare(self, config: Dict[str, Any]) -> None: self.wordlist = config.get(\"wordlist\", \"res://wordlists/dirb:latest\") self.threads = config.get(\"threads\", 50) def run(self) -> str: cmd = [\"feroxbuster\", \"-u\", self.target, \"-w\", self.wordlist, \"-t\", str(self.threads)] result = subprocess.run(cmd, capture_output=True, text=True) return result.stdout def parse_output(self, raw_output: str) -> List[Finding]: findings = [] for line in raw_output.split('\\n'): if line.strip() and not line.startswith('#'): findings.append(Finding( id=f\"ferox_{hash(line)}\", url=line.strip(), tool=\"feroxbuster\", severity=\"info\" )) return findings \ud83e\udde0 Example Integration \u2014 Feroxbuster \u00b6 # wrappers/ferox_wrapper.py import subprocess from core_lib.models.finding import Finding class FeroxWrapper(ToolPort): def prepare(self, config): self.target = config[\"target\"] self.wordlist = config.get(\"wordlist\", \"/usr/share/wordlists/dirb/common.txt\") self.args = [\"feroxbuster\", \"-u\", self.target, \"-w\", self.wordlist, \"-o\", \"-\"] def run(self): result = subprocess.run(self.args, capture_output=True, text=True) return result.stdout def parse_output(self, raw): findings = [] for line in raw.splitlines(): if \"200\" in line or \"301\" in line: findings.append( Finding( title=\"Discovered Path\", severity=\"info\", path=line.split()[0], detector_id=\"feroxbuster\" ) ) return findings \ud83e\udde9 Tool Orchestration \u00b6 The Workflow Engine dynamically chains tool executions: - Each node defines a tool (by name), configuration, and expected outputs. - Outputs become inputs for subsequent nodes. Example: \u00b6 nodes: - id: discovery type: discovery.ferox config: wordlist: res://wordlists/dirb:latest outputs: [\"urls\"] - id: scan type: scan.nuclei inputs: [\"urls\"] outputs: [\"findings\"] \ud83e\udde0 Tool Output Normalization \u00b6 All tool outputs are normalized into the Finding schema before storage. Field Description detector_id Tool name (e.g., nuclei, zap) title Human-readable description path Target or endpoint severity Normalized (critical/high/medium/low/info) evidence Raw tool output created_at UTC timestamp provenance Manifest + execution context \ud83e\udde9 Error Handling \u00b6 Condition Behavior Tool binary missing Raise ToolNotFoundError Sandbox violation Terminate subprocess Timeout Abort execution, mark partial-success Parse failure Skip line, log warning Non-zero exit code Log and attach stderr to run context \ud83d\udd2e Future Extensions \u00b6 Interactive tool chaining (UI-based pipeline builder) Tool profiles for project-specific setups Remote execution agents (distributed scanning) Version-aware compatibility checks Auto-updating manifests (self-test validation) Next: Tool Manager & User Experience Design","title":"Tools Integration"},{"location":"architecture/07-tools-integration-model/#07-tools-integration-model","text":"","title":"07 \u2014 Tools Integration Model"},{"location":"architecture/07-tools-integration-model/#overview","text":"The Tools Integration Model defines how external security tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.) are integrated and orchestrated inside SecFlow . The system is manifest-driven , allowing tools to be defined declaratively, executed in isolation, and chained dynamically via the Workflow Engine. Each integrated tool is represented as: - A manifest (describes binary, configuration, security policies, I/O schema) - A wrapper (implements the standardized execution interface) - An optional adapter plugin (translates output to normalized Findings)","title":"\ud83e\udded Overview"},{"location":"architecture/07-tools-integration-model/#architectural-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry<br/>- Manifest Validator<br/>- Sandbox Executor<br/>- Output Parser\"] B[\"Tool Wrapper<br/>(e.g., Nuclei)\"] C[\"Tool Output Parser\"] D[\"Findings Engine\"] A --> B B --> C C --> D","title":"\ud83e\uddf1 Architectural Overview"},{"location":"architecture/07-tools-integration-model/#tool-manifest-structure","text":"Every tool integrated into SecFlow must define a manifest under /wrappers/manifests/<tool>.json .","title":"\u2699\ufe0f Tool Manifest Structure"},{"location":"architecture/07-tools-integration-model/#example-nucleijson","text":"{ \"name\": \"nuclei\", \"version\": \"3.2.1\", \"binary\": \"nuclei\", \"capabilities\": [\"scan\"], \"outputs\": {\"dataset\": \"findings\"}, \"defaults\": { \"threads\": 25, \"rate_limit\": 150, \"templates\": \"res://templates/nuclei:latest\" }, \"configSchema\": \"schemas/nuclei-config.json\", \"resource_requirements\": { \"cpu_cores\": 2, \"memory_mb\": 512, \"timeout_seconds\": 300 }, \"security\": { \"sandbox\": true, \"network_access\": true, \"file_system_access\": \"read_only\" }, \"selftest\": { \"args\": [\"-version\"], \"expect\": \"Nuclei\" } }","title":"Example: nuclei.json"},{"location":"architecture/07-tools-integration-model/#wrapper-interface","text":"All tool wrappers implement the same contract to ensure consistent orchestration. # core-lib/ports/tool_port.py from typing import Any, Dict, Protocol, List from core_lib.models.finding import Finding class ToolPort(Protocol): def prepare(self, config: Dict[str, Any]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run(self) -> str: \"\"\"Execute tool and return raw output.\"\"\" pass def parse_output(self, raw_output: str) -> List[Finding]: \"\"\"Parse raw output into structured findings.\"\"\" pass","title":"\ud83e\udde9 Wrapper Interface"},{"location":"architecture/07-tools-integration-model/#example-implementation-nuclei-wrapper","text":"# wrappers/nuclei_wrapper.py import subprocess, json from core_lib.models.finding import Finding from core_lib.ports.tool_port import ToolPort class NucleiWrapper(ToolPort): def __init__(self, config): self.config = config def prepare(self, config): self.args = [ \"nuclei\", \"-json\", \"-t\", config.get(\"templates\", \"res://templates/nuclei:latest\"), \"-rl\", str(config.get(\"rate_limit\", 150)), \"-c\", str(config.get(\"threads\", 25)) ] def run(self): result = subprocess.run(self.args, capture_output=True, text=True, timeout=300) return result.stdout def parse_output(self, raw_output): findings = [] for line in raw_output.splitlines(): try: data = json.loads(line) findings.append( Finding( title=data[\"info\"][\"name\"], severity=data[\"info\"][\"severity\"], path=data[\"matched-at\"], detector_id=\"nuclei\", evidence=data ) ) except Exception: continue return findings","title":"\ud83e\udde0 Example Implementation: Nuclei Wrapper"},{"location":"architecture/07-tools-integration-model/#sandbox-execution","text":"Tools run through a Sandbox Executor, enforcing CPU, memory, and network constraints. # wrappers/executor.py import resource, subprocess, signal class SandboxExecutor: def __init__(self, cpu_limit=2, mem_limit_mb=512): self.cpu_limit = cpu_limit self.mem_limit_mb = mem_limit_mb def execute(self, args): def set_limits(): resource.setrlimit(resource.RLIMIT_CPU, (self.cpu_limit, self.cpu_limit)) resource.setrlimit(resource.RLIMIT_AS, (self.mem_limit_mb * 1024**2,) * 2) proc = subprocess.Popen(args, preexec_fn=set_limits, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = proc.communicate(timeout=300) return stdout.decode(), stderr.decode(), proc.returncode","title":"\ud83d\udd10 Sandbox Execution"},{"location":"architecture/07-tools-integration-model/#tool-registry","text":"# wrappers/registry.py from typing import Dict, Type from core_lib.ports.tool_port import ToolPort class ToolRegistry: _registry: Dict[str, Type[ToolPort]] = {} @classmethod def register(cls, name: str, impl: Type[ToolPort]): cls._registry[name] = impl print(f\"[+] Registered tool: {name}\") @classmethod def get(cls, name: str) -> ToolPort: return cls._registry[name] Tools register via decorators or discovery: from wrappers.registry import ToolRegistry @ToolRegistry.register(\"feroxbuster\") class FeroxWrapper(ToolPort): def prepare(self, config: Dict[str, Any]) -> None: self.wordlist = config.get(\"wordlist\", \"res://wordlists/dirb:latest\") self.threads = config.get(\"threads\", 50) def run(self) -> str: cmd = [\"feroxbuster\", \"-u\", self.target, \"-w\", self.wordlist, \"-t\", str(self.threads)] result = subprocess.run(cmd, capture_output=True, text=True) return result.stdout def parse_output(self, raw_output: str) -> List[Finding]: findings = [] for line in raw_output.split('\\n'): if line.strip() and not line.startswith('#'): findings.append(Finding( id=f\"ferox_{hash(line)}\", url=line.strip(), tool=\"feroxbuster\", severity=\"info\" )) return findings","title":"\ud83e\udde9 Tool Registry"},{"location":"architecture/07-tools-integration-model/#example-integration-feroxbuster","text":"# wrappers/ferox_wrapper.py import subprocess from core_lib.models.finding import Finding class FeroxWrapper(ToolPort): def prepare(self, config): self.target = config[\"target\"] self.wordlist = config.get(\"wordlist\", \"/usr/share/wordlists/dirb/common.txt\") self.args = [\"feroxbuster\", \"-u\", self.target, \"-w\", self.wordlist, \"-o\", \"-\"] def run(self): result = subprocess.run(self.args, capture_output=True, text=True) return result.stdout def parse_output(self, raw): findings = [] for line in raw.splitlines(): if \"200\" in line or \"301\" in line: findings.append( Finding( title=\"Discovered Path\", severity=\"info\", path=line.split()[0], detector_id=\"feroxbuster\" ) ) return findings","title":"\ud83e\udde0 Example Integration \u2014 Feroxbuster"},{"location":"architecture/07-tools-integration-model/#tool-orchestration","text":"The Workflow Engine dynamically chains tool executions: - Each node defines a tool (by name), configuration, and expected outputs. - Outputs become inputs for subsequent nodes.","title":"\ud83e\udde9 Tool Orchestration"},{"location":"architecture/07-tools-integration-model/#example","text":"nodes: - id: discovery type: discovery.ferox config: wordlist: res://wordlists/dirb:latest outputs: [\"urls\"] - id: scan type: scan.nuclei inputs: [\"urls\"] outputs: [\"findings\"]","title":"Example:"},{"location":"architecture/07-tools-integration-model/#tool-output-normalization","text":"All tool outputs are normalized into the Finding schema before storage. Field Description detector_id Tool name (e.g., nuclei, zap) title Human-readable description path Target or endpoint severity Normalized (critical/high/medium/low/info) evidence Raw tool output created_at UTC timestamp provenance Manifest + execution context","title":"\ud83e\udde0 Tool Output Normalization"},{"location":"architecture/07-tools-integration-model/#error-handling","text":"Condition Behavior Tool binary missing Raise ToolNotFoundError Sandbox violation Terminate subprocess Timeout Abort execution, mark partial-success Parse failure Skip line, log warning Non-zero exit code Log and attach stderr to run context","title":"\ud83e\udde9 Error Handling"},{"location":"architecture/07-tools-integration-model/#future-extensions","text":"Interactive tool chaining (UI-based pipeline builder) Tool profiles for project-specific setups Remote execution agents (distributed scanning) Version-aware compatibility checks Auto-updating manifests (self-test validation) Next: Tool Manager & User Experience Design","title":"\ud83d\udd2e Future Extensions"},{"location":"architecture/08-tool-manager-and-ux-design/","text":"08 \u2014 Tool Manager & User Experience Design \u00b6 \ud83e\udded Overview \u00b6 The Tool Manager serves as the central orchestration and configuration interface for all integrated tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.). It acts as the human-facing control plane that bridges declarative configuration, project-level defaults, and the automated orchestration engine. The UX design ensures that both CLI users and UI users can: - Install or enable tools dynamically. - Configure tool parameters or use defaults. - Chain tools together (e.g., Ferox \u2192 Katana \u2192 Nuclei ). - Reuse shared wordlists, payloads, and templates. - View and modify execution pipelines visually or textually. \ud83e\udde9 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry (runtime manifest cache)<br/>- Config Editor (CLI + UI layer)<br/>- Execution Controller (worker interface)<br/>- Output Collector (findings normalization)\"] B[\"Tool Wrappers Layer\"] C[\"Findings Engine\"] A --> B B --> C \u2699\ufe0f Tool Manager Responsibilities \u00b6 Subsystem Responsibility Registry Keeps in-memory registry of available tools with metadata. Configurator Provides unified interface for tool configuration & validation. Executor Handles actual subprocess calls via SandboxExecutor . Cache Layer Stores tool state, results, and logs for replay. Renderer Generates UI/CLI views for configuration and results. \ud83e\uddf1 UX Goals \u00b6 Goal Description Low Friction User can run default scans without prior configuration. Transparency Full visibility into command-line arguments, logs, and results. Reusability Configurations and results can be reused across projects. Isolation Each project may have independent or shared tool setups. Extensibility Easy to add new tools through manifests. \ud83e\udde0 CLI Experience \u00b6 Example Commands \u00b6 # List available tools SecFlow tools list # Enable Feroxbuster & Nuclei SecFlow tools enable feroxbuster nuclei # Configure a tool SecFlow tools config nuclei --threads 50 --templates owasp-top10 # Run workflow interactively SecFlow run workflow.yaml # Chain tools manually SecFlow run feroxbuster --target https://example.com | SecFlow run nuclei CLI UX Principles \u00b6 YAML/JSON inputs mirror internal manifests. All commands are idempotent \u2014 re-runs use cached configurations. CLI supports both single-tool and multi-node workflow modes. \ud83e\udde0 Web UI Experience \u00b6 Visual Overview \u00b6 Project: Acme Web API %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project: Acme Web API\"] B[\"Tools Panel\"] C[\"Workflow Builder\"] D[\"Execution Log\"] E[\"Results\"] A --> B A --> C A --> D A --> E Key Panels \u00b6 Panel Description Tools Panel Shows installed tools, version, manifest, health check. Workflow Builder Drag-and-drop DAG composer (Ferox \u2192 Katana \u2192 Nuclei). Execution Log Real-time stdout/stderr stream with structured logging. Results Viewer Interactive findings dashboard, linked to triage view. Example Workflow Builder Nodes \u00b6 (Discovery) (Crawler) (Scanner) %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Katana\"] C[\"Nuclei\"] A --> B B --> C Users can: - Configure node properties via sidebar. - Save and export workflows as YAML ( workflow-recipe.yaml ). - Re-run saved workflows directly or modify them visually. \ud83e\udde9 Configuration Persistence \u00b6 Each tool has a persistent JSON/YAML config stored under: ~/.SecFlow/config/tools/<tool>.yaml Example \u00b6 version: \"1.0\" defaults: threads: 25 rate_limit: 150 templates: res://templates/nuclei:latest profiles: aggressive: rate_limit: 300 safe: rate_limit: 50 sandbox: true Profiles allow contextual configurations (e.g., \"aggressive\", \"safe\"). \ud83d\udd17 Tool Chaining (Runtime) \u00b6 Tool chaining uses the Workflow Engine, but users can also trigger ad-hoc sequences. Each output dataset is normalized as JSON Lines, allowing flexible piping. SecFlow run feroxbuster --target https://app.local \\ | SecFlow run katana --stdin \\ | SecFlow run nuclei --stdin Data interoperability: \u00b6 Each tool must produce output in a structured JSON format with required fields: { \"url\": \"https://example.com/login\", \"status\": 200, \"source\": \"feroxbuster\" } \ud83e\udde9 Default Configurations & Templates \u00b6 When the user installs SecFlow: - Default manifests are loaded from /resources/defaults/ - All core tools (Nuclei, Ferox, Katana) have pre-configured templates. - The user can immediately run scans without extra setup. Example command: \u00b6 SecFlow quickscan https://example.com Equivalent to: - Workflow: Ferox \u2192 Nuclei (OWASP templates) \ud83e\udde0 Advanced Features \u00b6 Feature Description Auto-Discovery Detects installed binaries and populates manifest registry automatically. Tool Self-Test Validates binary presence and functionality via manifest-defined tests. Runtime Profiles Switch between safe/aggressive scanning modes. Execution History Stores past runs and parameters for reproducibility. \ud83d\udd10 User Permissions \u00b6 Tool Manager respects project and role isolation: Role Capabilities Admin Install, configure, delete tools. Analyst Execute workflows, view logs, triage findings. Viewer Read-only access to results. All actions are logged to the Audit Log. \ud83e\uddf1 Integration with Resource Registry \u00b6 The Tool Manager automatically pulls shared wordlists, templates, or payloads from the Resource Registry (see 09-resource-registry.md ). Example: \u00b6 wordlist: res://wordlists/dirb:latest templates: res://templates/nuclei:latest \ud83e\udde9 Error Recovery UX \u00b6 If a tool fails: - The UI highlights failed nodes in red. - Logs show stderr output inline. - Users can retry individual nodes. - Failures generate a triage entry (finding type = \"tool error\"). \ud83e\udde0 Example CLI Session \u00b6 # Add Ferox with a project-local override wordlist secflow tools add ferox \\ --version 2.10.0 \\ --scope project \\ --config '{ \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64 }' # Run a two-node workflow directly from CLI (discovery \u2192 nuclei) secflow run \\ --project acme-web \\ --recipe workflows/discovery-to-nuclei.yaml \\ --var target=https://app.acme.com API Response Example \u00b6 { \"tool\": \"feroxbuster\", \"version\": \"2.10.0\", \"scope\": \"project\", \"effective_config\": { \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64, \"rate_limit\": 0 }, \"status\": \"installed\", \"selftest\": { \"ok\": true, \"elapsed_ms\": 143 } } \ud83d\udd2e Future Enhancements \u00b6 UI-based \"Workflow Marketplace\" for community templates. AI-assisted tool parameter tuning based on context. Live terminal dashboard with interactive progress visualization. Integration with Burp/OWASP ZAP APIs for direct import. Next: Resource Registry","title":"Tool Manager & UX"},{"location":"architecture/08-tool-manager-and-ux-design/#08-tool-manager-user-experience-design","text":"","title":"08 \u2014 Tool Manager &amp; User Experience Design"},{"location":"architecture/08-tool-manager-and-ux-design/#overview","text":"The Tool Manager serves as the central orchestration and configuration interface for all integrated tools (Nuclei, Feroxbuster, Katana, ZAP, Caido, etc.). It acts as the human-facing control plane that bridges declarative configuration, project-level defaults, and the automated orchestration engine. The UX design ensures that both CLI users and UI users can: - Install or enable tools dynamically. - Configure tool parameters or use defaults. - Chain tools together (e.g., Ferox \u2192 Katana \u2192 Nuclei ). - Reuse shared wordlists, payloads, and templates. - View and modify execution pipelines visually or textually.","title":"\ud83e\udded Overview"},{"location":"architecture/08-tool-manager-and-ux-design/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Tool Manager<br/>- Tool Registry (runtime manifest cache)<br/>- Config Editor (CLI + UI layer)<br/>- Execution Controller (worker interface)<br/>- Output Collector (findings normalization)\"] B[\"Tool Wrappers Layer\"] C[\"Findings Engine\"] A --> B B --> C","title":"\ud83e\udde9 Architecture Overview"},{"location":"architecture/08-tool-manager-and-ux-design/#tool-manager-responsibilities","text":"Subsystem Responsibility Registry Keeps in-memory registry of available tools with metadata. Configurator Provides unified interface for tool configuration & validation. Executor Handles actual subprocess calls via SandboxExecutor . Cache Layer Stores tool state, results, and logs for replay. Renderer Generates UI/CLI views for configuration and results.","title":"\u2699\ufe0f Tool Manager Responsibilities"},{"location":"architecture/08-tool-manager-and-ux-design/#ux-goals","text":"Goal Description Low Friction User can run default scans without prior configuration. Transparency Full visibility into command-line arguments, logs, and results. Reusability Configurations and results can be reused across projects. Isolation Each project may have independent or shared tool setups. Extensibility Easy to add new tools through manifests.","title":"\ud83e\uddf1 UX Goals"},{"location":"architecture/08-tool-manager-and-ux-design/#cli-experience","text":"","title":"\ud83e\udde0 CLI Experience"},{"location":"architecture/08-tool-manager-and-ux-design/#example-commands","text":"# List available tools SecFlow tools list # Enable Feroxbuster & Nuclei SecFlow tools enable feroxbuster nuclei # Configure a tool SecFlow tools config nuclei --threads 50 --templates owasp-top10 # Run workflow interactively SecFlow run workflow.yaml # Chain tools manually SecFlow run feroxbuster --target https://example.com | SecFlow run nuclei","title":"Example Commands"},{"location":"architecture/08-tool-manager-and-ux-design/#cli-ux-principles","text":"YAML/JSON inputs mirror internal manifests. All commands are idempotent \u2014 re-runs use cached configurations. CLI supports both single-tool and multi-node workflow modes.","title":"CLI UX Principles"},{"location":"architecture/08-tool-manager-and-ux-design/#web-ui-experience","text":"","title":"\ud83e\udde0 Web UI Experience"},{"location":"architecture/08-tool-manager-and-ux-design/#visual-overview","text":"Project: Acme Web API %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project: Acme Web API\"] B[\"Tools Panel\"] C[\"Workflow Builder\"] D[\"Execution Log\"] E[\"Results\"] A --> B A --> C A --> D A --> E","title":"Visual Overview"},{"location":"architecture/08-tool-manager-and-ux-design/#key-panels","text":"Panel Description Tools Panel Shows installed tools, version, manifest, health check. Workflow Builder Drag-and-drop DAG composer (Ferox \u2192 Katana \u2192 Nuclei). Execution Log Real-time stdout/stderr stream with structured logging. Results Viewer Interactive findings dashboard, linked to triage view.","title":"Key Panels"},{"location":"architecture/08-tool-manager-and-ux-design/#example-workflow-builder-nodes","text":"(Discovery) (Crawler) (Scanner) %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Katana\"] C[\"Nuclei\"] A --> B B --> C Users can: - Configure node properties via sidebar. - Save and export workflows as YAML ( workflow-recipe.yaml ). - Re-run saved workflows directly or modify them visually.","title":"Example Workflow Builder Nodes"},{"location":"architecture/08-tool-manager-and-ux-design/#configuration-persistence","text":"Each tool has a persistent JSON/YAML config stored under: ~/.SecFlow/config/tools/<tool>.yaml","title":"\ud83e\udde9 Configuration Persistence"},{"location":"architecture/08-tool-manager-and-ux-design/#example","text":"version: \"1.0\" defaults: threads: 25 rate_limit: 150 templates: res://templates/nuclei:latest profiles: aggressive: rate_limit: 300 safe: rate_limit: 50 sandbox: true Profiles allow contextual configurations (e.g., \"aggressive\", \"safe\").","title":"Example"},{"location":"architecture/08-tool-manager-and-ux-design/#tool-chaining-runtime","text":"Tool chaining uses the Workflow Engine, but users can also trigger ad-hoc sequences. Each output dataset is normalized as JSON Lines, allowing flexible piping. SecFlow run feroxbuster --target https://app.local \\ | SecFlow run katana --stdin \\ | SecFlow run nuclei --stdin","title":"\ud83d\udd17 Tool Chaining (Runtime)"},{"location":"architecture/08-tool-manager-and-ux-design/#data-interoperability","text":"Each tool must produce output in a structured JSON format with required fields: { \"url\": \"https://example.com/login\", \"status\": 200, \"source\": \"feroxbuster\" }","title":"Data interoperability:"},{"location":"architecture/08-tool-manager-and-ux-design/#default-configurations-templates","text":"When the user installs SecFlow: - Default manifests are loaded from /resources/defaults/ - All core tools (Nuclei, Ferox, Katana) have pre-configured templates. - The user can immediately run scans without extra setup.","title":"\ud83e\udde9 Default Configurations &amp; Templates"},{"location":"architecture/08-tool-manager-and-ux-design/#example-command","text":"SecFlow quickscan https://example.com Equivalent to: - Workflow: Ferox \u2192 Nuclei (OWASP templates)","title":"Example command:"},{"location":"architecture/08-tool-manager-and-ux-design/#advanced-features","text":"Feature Description Auto-Discovery Detects installed binaries and populates manifest registry automatically. Tool Self-Test Validates binary presence and functionality via manifest-defined tests. Runtime Profiles Switch between safe/aggressive scanning modes. Execution History Stores past runs and parameters for reproducibility.","title":"\ud83e\udde0 Advanced Features"},{"location":"architecture/08-tool-manager-and-ux-design/#user-permissions","text":"Tool Manager respects project and role isolation: Role Capabilities Admin Install, configure, delete tools. Analyst Execute workflows, view logs, triage findings. Viewer Read-only access to results. All actions are logged to the Audit Log.","title":"\ud83d\udd10 User Permissions"},{"location":"architecture/08-tool-manager-and-ux-design/#integration-with-resource-registry","text":"The Tool Manager automatically pulls shared wordlists, templates, or payloads from the Resource Registry (see 09-resource-registry.md ).","title":"\ud83e\uddf1 Integration with Resource Registry"},{"location":"architecture/08-tool-manager-and-ux-design/#example_1","text":"wordlist: res://wordlists/dirb:latest templates: res://templates/nuclei:latest","title":"Example:"},{"location":"architecture/08-tool-manager-and-ux-design/#error-recovery-ux","text":"If a tool fails: - The UI highlights failed nodes in red. - Logs show stderr output inline. - Users can retry individual nodes. - Failures generate a triage entry (finding type = \"tool error\").","title":"\ud83e\udde9 Error Recovery UX"},{"location":"architecture/08-tool-manager-and-ux-design/#example-cli-session","text":"# Add Ferox with a project-local override wordlist secflow tools add ferox \\ --version 2.10.0 \\ --scope project \\ --config '{ \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64 }' # Run a two-node workflow directly from CLI (discovery \u2192 nuclei) secflow run \\ --project acme-web \\ --recipe workflows/discovery-to-nuclei.yaml \\ --var target=https://app.acme.com","title":"\ud83e\udde0 Example CLI Session"},{"location":"architecture/08-tool-manager-and-ux-design/#api-response-example","text":"{ \"tool\": \"feroxbuster\", \"version\": \"2.10.0\", \"scope\": \"project\", \"effective_config\": { \"wordlist\": \"res://wordlists/raft-medium-directories.txt\", \"threads\": 64, \"rate_limit\": 0 }, \"status\": \"installed\", \"selftest\": { \"ok\": true, \"elapsed_ms\": 143 } }","title":"API Response Example"},{"location":"architecture/08-tool-manager-and-ux-design/#future-enhancements","text":"UI-based \"Workflow Marketplace\" for community templates. AI-assisted tool parameter tuning based on context. Live terminal dashboard with interactive progress visualization. Integration with Burp/OWASP ZAP APIs for direct import. Next: Resource Registry","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/09-resource-registry/","text":"09 \u2014 Resource Registry \u00b6 \ud83e\udded Overview \u00b6 The Resource Registry is SecFlow's shared data layer for managing reusable assets such as: Wordlists Templates (Nuclei, ZAP, custom YAML) Headers and payload sets Configurations and schema files CVE/CWE enrichment databases This registry provides unified access to all resources, manages versioning and scope isolation, and acts as the single source of truth for both CLI and API workflows. \ud83e\uddf1 Design Goals \u00b6 Goal Description Centralized One consistent repository for all reusable data. Scoped Resources may be global, group-specific, or project-specific. Versioned Immutable artifacts with hash-based versioning. Queryable Searchable by tags, type, or metadata. Cacheable Local caching for fast re-use and offline execution. Auditable Every modification is logged in the audit trail. \ud83e\udde9 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource API<br/>- Resource Manager<br/>- Indexer / Search Engine<br/>- Version Resolver<br/>- Storage Adapter (File, DB, Remote)\"] B[\"Resource Blob\"] C[\"Local Cache\"] A --> B B --> C \u2699\ufe0f Resource Model \u00b6 # core-lib/models/resource.py from typing import Literal, Optional from datetime import datetime from pydantic import BaseModel class Resource(BaseModel): id: str name: str type: Literal[\"wordlist\", \"templates\", \"headers\", \"payloads\", \"configs\", \"schema\"] version: str hash: str scope: Literal[\"global\", \"group\", \"project\"] owner: Optional[str] blob_uri: str metadata: dict created_at: datetime updated_at: datetime usage_count: int last_used: Optional[datetime] \ud83e\udde9 Scope Hierarchy \u00b6 Level Description Example Path Global Available across all users and projects. res://wordlists/common.txt Group Shared among specific teams. res://group/redteam/headers.json Project Private to a specific pentest or client project. res://project/acme/templates/custom.yaml Precedence Rules \u00b6 Run-level override Node-level override Project default Group default Global default Example: \u00b6 If project-A has a custom wordlist res://project/acme/dirb.yaml , it overrides res://wordlists/dirb:latest . \ud83e\udde9 Example Registry Entry \u00b6 id: \"res://wordlists/dirb:1.2.0\" name: \"Dirbuster Common\" type: \"wordlist\" version: \"1.2.0\" hash: \"sha256:ea13f3b4c8d9e2f1a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" scope: \"global\" metadata: tags: [\"web\", \"discovery\"] size: 24312 license: \"GPL-3.0\" source: \"https://github.com/digination/dirbuster\" created_at: \"2025-08-01T12:32:00Z\" updated_at: \"2025-08-15T09:21:00Z\" \ud83e\uddf1 Storage Backends \u00b6 The Resource Registry supports multiple backends: Backend Usage Notes Local Filesystem Default for developer use. Fast, no dependencies. SQLite / Postgres Production database. Metadata stored in DB, blob on disk. S3 / MinIO Remote multi-tenant storage. Ideal for distributed environments. Git-backed Repository Version-controlled registry. Enables audit and rollback. \ud83e\udde9 Resource Manager Interface \u00b6 # core-lib/ports/resource_port.py from typing import List from .models import Resource class ResourcePort(Protocol): def register(self, resource: Resource) -> None: \"\"\"Register a new resource in the registry.\"\"\" pass def list(self, scope: str) -> List[Resource]: \"\"\"List resources within a scope.\"\"\" pass def get(self, id: str) -> Resource: \"\"\"Get a resource by ID.\"\"\" pass def resolve(self, name: str, version: str) -> Resource: \"\"\"Resolve a resource by name and version.\"\"\" pass def increment_usage(self, id: str) -> None: \"\"\"Increment usage counter for a resource.\"\"\" pass Example Adapter Implementation \u00b6 # storage/resource_repository.py import json, os from core_lib.models.resource import Resource class FileResourceRepo: def __init__(self, base_path=\"~/.SecFlow/resources\"): self.base = os.path.expanduser(base_path) def register(self, res: Resource): path = os.path.join(self.base, f\"{res.id.replace('res://','')}.json\") os.makedirs(os.path.dirname(path), exist_ok=True) with open(path, \"w\") as f: f.write(res.model_dump_json()) def get(self, id): path = os.path.join(self.base, f\"{id.replace('res://','')}.json\") return Resource.parse_file(path) \ud83e\udde9 Resource Fetching & Caching \u00b6 When a workflow references res://wordlists/dirb:latest : The registry resolves the resource (global/group/project). The manager checks the local cache. If missing or outdated, it downloads or loads the file blob. The reference is then injected into the tool configuration. resolved = ResourceManager.resolve(\"res://wordlists/dirb:latest\") path = CacheManager.fetch(resolved) \ud83e\udde0 Resource Versioning \u00b6 Resources are immutable once published; updates produce new versions. Operation Behavior publish Adds new resource version, preserves old one. retire Marks resource as deprecated (kept for history). promote Moves a project resource to group/global scope. Version Reference Syntax \u00b6 res://wordlists/dirb:1.2.0 res://templates/nuclei:latest \ud83e\udde9 Registry CLI Commands \u00b6 # List all resources SecFlow resources list # Show details for a resource SecFlow resources show res://wordlists/dirb:latest # Add new resource SecFlow resources add wordlist ./custom.txt --scope project # Promote resource SecFlow resources promote res://project/acme/dirb:latest --to global \ud83e\udde0 Integration with Tool Manager \u00b6 Tools reference resources via symbolic URIs. At runtime, the registry automatically resolves URIs into local paths. Example Nuclei config: \u00b6 templates: res://templates/nuclei:latest wordlist: res://wordlists/dirb:latest \ud83d\udd12 Security & Integrity \u00b6 Mechanism Description Hash Validation SHA-256 hashes validated before use. Signed Resources Optional GPG signing for remote sources. Access Control Scoped permissions (Admin / Analyst / Viewer). Audit Trail Every fetch and publish event logged. \ud83e\udde9 Garbage Collection Policy \u00b6 When a resource is deleted: - Orphaned blob files are queued for cleanup by the GarbageCollector . - The audit log retains metadata for traceability. - Cached copies remain valid until TTL expiration. \ud83e\udde0 Example Use Case \u00b6 Global Wordlist Shared by Tools \u00b6 All tools can reference res://wordlists/dirb:latest : - Feroxbuster uses it for directory brute-force. - Katana can reuse it for crawl hints. - Nuclei leverages it for parameter fuzzing. Each tool can still override with its own wordlist if required. \ud83d\udd2e Future Enhancements \u00b6 Remote Resource Index API (REST/GraphQL). Smart caching with usage-based prefetch. Resource metrics: \"most used\", \"last updated\", etc. Tag-based search and semantic discovery. Signed update feeds from verified community sources. Next: Wordlist & Output Sharing Rules","title":"Resource Registry"},{"location":"architecture/09-resource-registry/#09-resource-registry","text":"","title":"09 \u2014 Resource Registry"},{"location":"architecture/09-resource-registry/#overview","text":"The Resource Registry is SecFlow's shared data layer for managing reusable assets such as: Wordlists Templates (Nuclei, ZAP, custom YAML) Headers and payload sets Configurations and schema files CVE/CWE enrichment databases This registry provides unified access to all resources, manages versioning and scope isolation, and acts as the single source of truth for both CLI and API workflows.","title":"\ud83e\udded Overview"},{"location":"architecture/09-resource-registry/#design-goals","text":"Goal Description Centralized One consistent repository for all reusable data. Scoped Resources may be global, group-specific, or project-specific. Versioned Immutable artifacts with hash-based versioning. Queryable Searchable by tags, type, or metadata. Cacheable Local caching for fast re-use and offline execution. Auditable Every modification is logged in the audit trail.","title":"\ud83e\uddf1 Design Goals"},{"location":"architecture/09-resource-registry/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource API<br/>- Resource Manager<br/>- Indexer / Search Engine<br/>- Version Resolver<br/>- Storage Adapter (File, DB, Remote)\"] B[\"Resource Blob\"] C[\"Local Cache\"] A --> B B --> C","title":"\ud83e\udde9 Architecture Overview"},{"location":"architecture/09-resource-registry/#resource-model","text":"# core-lib/models/resource.py from typing import Literal, Optional from datetime import datetime from pydantic import BaseModel class Resource(BaseModel): id: str name: str type: Literal[\"wordlist\", \"templates\", \"headers\", \"payloads\", \"configs\", \"schema\"] version: str hash: str scope: Literal[\"global\", \"group\", \"project\"] owner: Optional[str] blob_uri: str metadata: dict created_at: datetime updated_at: datetime usage_count: int last_used: Optional[datetime]","title":"\u2699\ufe0f Resource Model"},{"location":"architecture/09-resource-registry/#scope-hierarchy","text":"Level Description Example Path Global Available across all users and projects. res://wordlists/common.txt Group Shared among specific teams. res://group/redteam/headers.json Project Private to a specific pentest or client project. res://project/acme/templates/custom.yaml","title":"\ud83e\udde9 Scope Hierarchy"},{"location":"architecture/09-resource-registry/#precedence-rules","text":"Run-level override Node-level override Project default Group default Global default","title":"Precedence Rules"},{"location":"architecture/09-resource-registry/#example","text":"If project-A has a custom wordlist res://project/acme/dirb.yaml , it overrides res://wordlists/dirb:latest .","title":"Example:"},{"location":"architecture/09-resource-registry/#example-registry-entry","text":"id: \"res://wordlists/dirb:1.2.0\" name: \"Dirbuster Common\" type: \"wordlist\" version: \"1.2.0\" hash: \"sha256:ea13f3b4c8d9e2f1a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" scope: \"global\" metadata: tags: [\"web\", \"discovery\"] size: 24312 license: \"GPL-3.0\" source: \"https://github.com/digination/dirbuster\" created_at: \"2025-08-01T12:32:00Z\" updated_at: \"2025-08-15T09:21:00Z\"","title":"\ud83e\udde9 Example Registry Entry"},{"location":"architecture/09-resource-registry/#storage-backends","text":"The Resource Registry supports multiple backends: Backend Usage Notes Local Filesystem Default for developer use. Fast, no dependencies. SQLite / Postgres Production database. Metadata stored in DB, blob on disk. S3 / MinIO Remote multi-tenant storage. Ideal for distributed environments. Git-backed Repository Version-controlled registry. Enables audit and rollback.","title":"\ud83e\uddf1 Storage Backends"},{"location":"architecture/09-resource-registry/#resource-manager-interface","text":"# core-lib/ports/resource_port.py from typing import List from .models import Resource class ResourcePort(Protocol): def register(self, resource: Resource) -> None: \"\"\"Register a new resource in the registry.\"\"\" pass def list(self, scope: str) -> List[Resource]: \"\"\"List resources within a scope.\"\"\" pass def get(self, id: str) -> Resource: \"\"\"Get a resource by ID.\"\"\" pass def resolve(self, name: str, version: str) -> Resource: \"\"\"Resolve a resource by name and version.\"\"\" pass def increment_usage(self, id: str) -> None: \"\"\"Increment usage counter for a resource.\"\"\" pass","title":"\ud83e\udde9 Resource Manager Interface"},{"location":"architecture/09-resource-registry/#example-adapter-implementation","text":"# storage/resource_repository.py import json, os from core_lib.models.resource import Resource class FileResourceRepo: def __init__(self, base_path=\"~/.SecFlow/resources\"): self.base = os.path.expanduser(base_path) def register(self, res: Resource): path = os.path.join(self.base, f\"{res.id.replace('res://','')}.json\") os.makedirs(os.path.dirname(path), exist_ok=True) with open(path, \"w\") as f: f.write(res.model_dump_json()) def get(self, id): path = os.path.join(self.base, f\"{id.replace('res://','')}.json\") return Resource.parse_file(path)","title":"Example Adapter Implementation"},{"location":"architecture/09-resource-registry/#resource-fetching-caching","text":"When a workflow references res://wordlists/dirb:latest : The registry resolves the resource (global/group/project). The manager checks the local cache. If missing or outdated, it downloads or loads the file blob. The reference is then injected into the tool configuration. resolved = ResourceManager.resolve(\"res://wordlists/dirb:latest\") path = CacheManager.fetch(resolved)","title":"\ud83e\udde9 Resource Fetching &amp; Caching"},{"location":"architecture/09-resource-registry/#resource-versioning","text":"Resources are immutable once published; updates produce new versions. Operation Behavior publish Adds new resource version, preserves old one. retire Marks resource as deprecated (kept for history). promote Moves a project resource to group/global scope.","title":"\ud83e\udde0 Resource Versioning"},{"location":"architecture/09-resource-registry/#version-reference-syntax","text":"res://wordlists/dirb:1.2.0 res://templates/nuclei:latest","title":"Version Reference Syntax"},{"location":"architecture/09-resource-registry/#registry-cli-commands","text":"# List all resources SecFlow resources list # Show details for a resource SecFlow resources show res://wordlists/dirb:latest # Add new resource SecFlow resources add wordlist ./custom.txt --scope project # Promote resource SecFlow resources promote res://project/acme/dirb:latest --to global","title":"\ud83e\udde9 Registry CLI Commands"},{"location":"architecture/09-resource-registry/#integration-with-tool-manager","text":"Tools reference resources via symbolic URIs. At runtime, the registry automatically resolves URIs into local paths.","title":"\ud83e\udde0 Integration with Tool Manager"},{"location":"architecture/09-resource-registry/#example-nuclei-config","text":"templates: res://templates/nuclei:latest wordlist: res://wordlists/dirb:latest","title":"Example Nuclei config:"},{"location":"architecture/09-resource-registry/#security-integrity","text":"Mechanism Description Hash Validation SHA-256 hashes validated before use. Signed Resources Optional GPG signing for remote sources. Access Control Scoped permissions (Admin / Analyst / Viewer). Audit Trail Every fetch and publish event logged.","title":"\ud83d\udd12 Security &amp; Integrity"},{"location":"architecture/09-resource-registry/#garbage-collection-policy","text":"When a resource is deleted: - Orphaned blob files are queued for cleanup by the GarbageCollector . - The audit log retains metadata for traceability. - Cached copies remain valid until TTL expiration.","title":"\ud83e\udde9 Garbage Collection Policy"},{"location":"architecture/09-resource-registry/#example-use-case","text":"","title":"\ud83e\udde0 Example Use Case"},{"location":"architecture/09-resource-registry/#global-wordlist-shared-by-tools","text":"All tools can reference res://wordlists/dirb:latest : - Feroxbuster uses it for directory brute-force. - Katana can reuse it for crawl hints. - Nuclei leverages it for parameter fuzzing. Each tool can still override with its own wordlist if required.","title":"Global Wordlist Shared by Tools"},{"location":"architecture/09-resource-registry/#future-enhancements","text":"Remote Resource Index API (REST/GraphQL). Smart caching with usage-based prefetch. Resource metrics: \"most used\", \"last updated\", etc. Tag-based search and semantic discovery. Signed update feeds from verified community sources. Next: Wordlist & Output Sharing Rules","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/10-wordlist-and-output-sharing/","text":"10 \u2014 Wordlist & Output Sharing Rules \u00b6 \ud83e\udded Overview \u00b6 One of SecFlow's key strengths is inter-tool resource sharing \u2014 allowing different scanners and discovery tools to reuse wordlists , templates , and output datasets through a unified and versioned Resource Registry. This enables workflows like: %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster<br/>(directory discovery)\"] B[\"Katana<br/>(crawler + parameter discovery)\"] C[\"Nuclei<br/>(template-driven vulnerability scan)\"] A --> B B --> C Each step consumes outputs and produces standardized findings that can be reused in subsequent stages \u2014 without manual conversions or reconfiguration. \ud83e\uddf1 Design Objectives \u00b6 Objective Description Shared Wordlists All tools can reference the same library of wordlists. Isolated Overrides Each tool may override global wordlists with project-specific ones. Consistent Output Contracts Every wrapper emits normalized JSON output. Dataset Interoperability Discovery results from one tool can feed another automatically. Performance Efficiency Cached resources and deduplication prevent redundant I/O. \u2699\ufe0f Wordlist Management Flow \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource Registry\"] B[\"Wordlist Resolver\"] C[\"Global Cache\"] D[\"Tool Wrapper Config\"] E[\"Execution Context\"] A --> B B --> C B --> D D --> E Each execution context resolves wordlists from the Resource Registry , fetching them locally if needed. \ud83e\udde9 Shared Wordlist Schema \u00b6 id: res://wordlists/dirb:1.2.0 type: wordlist scope: global metadata: description: \"Common web directory wordlist\" format: \"text\" size: 24312 Usage Example \u00b6 tools: feroxbuster: wordlist: res://wordlists/dirb:latest nuclei: templates: res://templates/owasp-top10:latest At runtime, SecFlow resolves these resources to absolute paths and injects them into each wrapper. \ud83e\udde0 Multi-Scope Selection Logic \u00b6 1. Run-Level Override \u00b6 Supplied directly in the workflow recipe or CLI argument. Example: SecFlow run feroxbuster --wordlist ./private.txt 2. Node-Level Configuration \u00b6 Declared inside workflow YAML. Example: nodes: - id: ferox type: discovery.ferox config: wordlist: res://wordlists/dirb:1.0 3. Project Default \u00b6 Stored under ~/.SecFlow/projects/<id>/config.yaml . 4. Group Default \u00b6 Shared among organizational units or red-team groups. 5. Global Default \u00b6 Fallback resource available for all users. \ud83e\udde0 Tool-Specific Overrides \u00b6 Each wrapper can define preferred wordlists and formats in its manifest. Example (feroxbuster.json): \u00b6 { \"defaults\": { \"wordlist\": \"res://wordlists/dirb:latest\" }, \"accepted_formats\": [\"txt\", \"lst\"] } Nuclei Example: \u00b6 { \"defaults\": { \"templates\": \"res://templates/nuclei:latest\" }, \"accepted_formats\": [\"yaml\"] } The Tool Manager dynamically validates that the provided wordlist matches the expected format before execution. \ud83e\udde9 Output Standardization \u00b6 All tools output to a shared JSON Lines dataset ( .jsonl ). Example \u2014 Ferox Output \u00b6 {\"url\": \"https://target.com/login\", \"status\": 200, \"source\": \"feroxbuster\"} Example \u2014 Nuclei Output \u00b6 {\"id\": \"CVE-2024-12345\", \"template\": \"sql-injection\", \"severity\": \"high\", \"matched-at\": \"https://target.com/login\"} These outputs are normalized into the Finding schema by the Findings Engine. \ud83e\udde0 Chained Data Exchange \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Output (urls.jsonl)\"] C[\"Katana\"] D[\"Nuclei\"] A --> B C --> B B --> D Each wrapper declares output channels (urls, parameters, endpoints, etc.). The Workflow Engine automatically maps compatible output types to input fields of the next node. Example Workflow Excerpt \u00b6 nodes: - id: ferox type: discovery.ferox outputs: [\"urls\"] - id: katana type: crawler.katana inputs: [\"urls\"] outputs: [\"urls\", \"params\"] - id: nuclei type: scanner.nuclei inputs: [\"urls\", \"params\"] outputs: [\"findings\"] \ud83e\udde9 Cross-Tool Resource Sharing Rules \u00b6 Rule Description Wordlists All tools can access any registered wordlist; wrappers define which formats they accept. Templates Shared globally between Nuclei, ZAP, or other scanners. Headers Reusable header sets (e.g., API tokens) can be applied per project. Payloads Payload libraries are versioned and accessible to all fuzzers. Findings Outputs Findings may be exported or reused as seed data for enrichment tools. \ud83e\udde0 Example \u2014 Shared Output Dataset \u00b6 Scenario: \u00b6 A user runs Ferox \u2192 Nuclei chain. Nuclei reuses the output from Ferox as its input dataset. SecFlow run feroxbuster --target https://api.company.com \\ | SecFlow run nuclei --stdin Result: \u00b6 Ferox writes discovered URLs to /runs/<uuid>/ferox/urls.jsonl The Workflow Engine pipes this dataset to the Nuclei node. Nuclei scans each URL using selected templates. Normalized findings are saved under /runs/<uuid>/nuclei/findings.jsonl \ud83e\udde9 Shared Dataset Metadata \u00b6 dataset: id: \"runs/2025-10-06T12:31Z-ferox-urls\" type: \"urls\" source: \"feroxbuster\" size: 243 created_at: \"2025-10-06T12:31Z\" This metadata is referenced by downstream nodes to ensure deterministic workflows. \ud83d\udd10 Data Isolation & Sharing Between Projects \u00b6 SecFlow supports granular sharing control for multi-project setups: Mode Description Isolated Each project keeps separate resources and findings. Shared Group Projects under the same group share wordlists and results. Selective User manually links resources or outputs between projects. Example: \u00b6 project: name: acme-api sharing: with: [\"internal-api\", \"dev-api\"] resources: [\"wordlists\", \"templates\"] outputs: [\"urls\", \"parameters\"] \ud83e\udde9 Cache & Deduplication \u00b6 Wordlists and tool outputs are hash-indexed and cached for reuse. Cache Key Formula \u00b6 cache_key = sha256(resource_id + version + scope) This guarantees consistent retrieval and avoids redundant downloads. \ud83e\udde0 Example End-to-End Flow \u00b6 Project: acme-api %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Shared wordlists (global)\"] B[\"Custom payloads (project)\"] C[\"Workflow: Ferox \u2192 Katana \u2192 Nuclei\"] D[\"URLs discovered \u2192 shared dataset\"] E[\"Parameters extracted \u2192 temp dataset\"] F[\"Findings enriched \u2192 persisted\"] G[\"Cached resources \u2192 reused in next run\"] A --> C B --> C C --> D C --> E C --> F C --> G \ud83e\udde9 Validation & Conflict Resolution \u00b6 Conflict Resolution Same resource name, different scope Higher precedence scope wins (project > group > global). Resource deleted mid-run Cached version remains until TTL expires. Output schema mismatch Adapter plugin transforms data. Duplicate URLs or findings De-duplication engine merges entries by hash. \ud83d\udd2e Future Enhancements \u00b6 Distributed cache synchronization (Redis or MinIO). Tag-based linking of related outputs (e.g., same target domain). Automatic scope inference based on project classification. Resource \"diff\" view between runs. AI suggestion for optimal wordlists or template selection. Next: Project Isolation & Data Sharing Controls","title":"Wordlists & Sharing"},{"location":"architecture/10-wordlist-and-output-sharing/#10-wordlist-output-sharing-rules","text":"","title":"10 \u2014 Wordlist &amp; Output Sharing Rules"},{"location":"architecture/10-wordlist-and-output-sharing/#overview","text":"One of SecFlow's key strengths is inter-tool resource sharing \u2014 allowing different scanners and discovery tools to reuse wordlists , templates , and output datasets through a unified and versioned Resource Registry. This enables workflows like: %%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster<br/>(directory discovery)\"] B[\"Katana<br/>(crawler + parameter discovery)\"] C[\"Nuclei<br/>(template-driven vulnerability scan)\"] A --> B B --> C Each step consumes outputs and produces standardized findings that can be reused in subsequent stages \u2014 without manual conversions or reconfiguration.","title":"\ud83e\udded Overview"},{"location":"architecture/10-wordlist-and-output-sharing/#design-objectives","text":"Objective Description Shared Wordlists All tools can reference the same library of wordlists. Isolated Overrides Each tool may override global wordlists with project-specific ones. Consistent Output Contracts Every wrapper emits normalized JSON output. Dataset Interoperability Discovery results from one tool can feed another automatically. Performance Efficiency Cached resources and deduplication prevent redundant I/O.","title":"\ud83e\uddf1 Design Objectives"},{"location":"architecture/10-wordlist-and-output-sharing/#wordlist-management-flow","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Resource Registry\"] B[\"Wordlist Resolver\"] C[\"Global Cache\"] D[\"Tool Wrapper Config\"] E[\"Execution Context\"] A --> B B --> C B --> D D --> E Each execution context resolves wordlists from the Resource Registry , fetching them locally if needed.","title":"\u2699\ufe0f Wordlist Management Flow"},{"location":"architecture/10-wordlist-and-output-sharing/#shared-wordlist-schema","text":"id: res://wordlists/dirb:1.2.0 type: wordlist scope: global metadata: description: \"Common web directory wordlist\" format: \"text\" size: 24312","title":"\ud83e\udde9 Shared Wordlist Schema"},{"location":"architecture/10-wordlist-and-output-sharing/#usage-example","text":"tools: feroxbuster: wordlist: res://wordlists/dirb:latest nuclei: templates: res://templates/owasp-top10:latest At runtime, SecFlow resolves these resources to absolute paths and injects them into each wrapper.","title":"Usage Example"},{"location":"architecture/10-wordlist-and-output-sharing/#multi-scope-selection-logic","text":"","title":"\ud83e\udde0 Multi-Scope Selection Logic"},{"location":"architecture/10-wordlist-and-output-sharing/#1-run-level-override","text":"Supplied directly in the workflow recipe or CLI argument. Example: SecFlow run feroxbuster --wordlist ./private.txt","title":"1. Run-Level Override"},{"location":"architecture/10-wordlist-and-output-sharing/#2-node-level-configuration","text":"Declared inside workflow YAML. Example: nodes: - id: ferox type: discovery.ferox config: wordlist: res://wordlists/dirb:1.0","title":"2. Node-Level Configuration"},{"location":"architecture/10-wordlist-and-output-sharing/#3-project-default","text":"Stored under ~/.SecFlow/projects/<id>/config.yaml .","title":"3. Project Default"},{"location":"architecture/10-wordlist-and-output-sharing/#4-group-default","text":"Shared among organizational units or red-team groups.","title":"4. Group Default"},{"location":"architecture/10-wordlist-and-output-sharing/#5-global-default","text":"Fallback resource available for all users.","title":"5. Global Default"},{"location":"architecture/10-wordlist-and-output-sharing/#tool-specific-overrides","text":"Each wrapper can define preferred wordlists and formats in its manifest.","title":"\ud83e\udde0 Tool-Specific Overrides"},{"location":"architecture/10-wordlist-and-output-sharing/#example-feroxbusterjson","text":"{ \"defaults\": { \"wordlist\": \"res://wordlists/dirb:latest\" }, \"accepted_formats\": [\"txt\", \"lst\"] }","title":"Example (feroxbuster.json):"},{"location":"architecture/10-wordlist-and-output-sharing/#nuclei-example","text":"{ \"defaults\": { \"templates\": \"res://templates/nuclei:latest\" }, \"accepted_formats\": [\"yaml\"] } The Tool Manager dynamically validates that the provided wordlist matches the expected format before execution.","title":"Nuclei Example:"},{"location":"architecture/10-wordlist-and-output-sharing/#output-standardization","text":"All tools output to a shared JSON Lines dataset ( .jsonl ).","title":"\ud83e\udde9 Output Standardization"},{"location":"architecture/10-wordlist-and-output-sharing/#example-ferox-output","text":"{\"url\": \"https://target.com/login\", \"status\": 200, \"source\": \"feroxbuster\"}","title":"Example \u2014 Ferox Output"},{"location":"architecture/10-wordlist-and-output-sharing/#example-nuclei-output","text":"{\"id\": \"CVE-2024-12345\", \"template\": \"sql-injection\", \"severity\": \"high\", \"matched-at\": \"https://target.com/login\"} These outputs are normalized into the Finding schema by the Findings Engine.","title":"Example \u2014 Nuclei Output"},{"location":"architecture/10-wordlist-and-output-sharing/#chained-data-exchange","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart LR A[\"Feroxbuster\"] B[\"Output (urls.jsonl)\"] C[\"Katana\"] D[\"Nuclei\"] A --> B C --> B B --> D Each wrapper declares output channels (urls, parameters, endpoints, etc.). The Workflow Engine automatically maps compatible output types to input fields of the next node.","title":"\ud83e\udde0 Chained Data Exchange"},{"location":"architecture/10-wordlist-and-output-sharing/#example-workflow-excerpt","text":"nodes: - id: ferox type: discovery.ferox outputs: [\"urls\"] - id: katana type: crawler.katana inputs: [\"urls\"] outputs: [\"urls\", \"params\"] - id: nuclei type: scanner.nuclei inputs: [\"urls\", \"params\"] outputs: [\"findings\"]","title":"Example Workflow Excerpt"},{"location":"architecture/10-wordlist-and-output-sharing/#cross-tool-resource-sharing-rules","text":"Rule Description Wordlists All tools can access any registered wordlist; wrappers define which formats they accept. Templates Shared globally between Nuclei, ZAP, or other scanners. Headers Reusable header sets (e.g., API tokens) can be applied per project. Payloads Payload libraries are versioned and accessible to all fuzzers. Findings Outputs Findings may be exported or reused as seed data for enrichment tools.","title":"\ud83e\udde9 Cross-Tool Resource Sharing Rules"},{"location":"architecture/10-wordlist-and-output-sharing/#example-shared-output-dataset","text":"","title":"\ud83e\udde0 Example \u2014 Shared Output Dataset"},{"location":"architecture/10-wordlist-and-output-sharing/#scenario","text":"A user runs Ferox \u2192 Nuclei chain. Nuclei reuses the output from Ferox as its input dataset. SecFlow run feroxbuster --target https://api.company.com \\ | SecFlow run nuclei --stdin","title":"Scenario:"},{"location":"architecture/10-wordlist-and-output-sharing/#result","text":"Ferox writes discovered URLs to /runs/<uuid>/ferox/urls.jsonl The Workflow Engine pipes this dataset to the Nuclei node. Nuclei scans each URL using selected templates. Normalized findings are saved under /runs/<uuid>/nuclei/findings.jsonl","title":"Result:"},{"location":"architecture/10-wordlist-and-output-sharing/#shared-dataset-metadata","text":"dataset: id: \"runs/2025-10-06T12:31Z-ferox-urls\" type: \"urls\" source: \"feroxbuster\" size: 243 created_at: \"2025-10-06T12:31Z\" This metadata is referenced by downstream nodes to ensure deterministic workflows.","title":"\ud83e\udde9 Shared Dataset Metadata"},{"location":"architecture/10-wordlist-and-output-sharing/#data-isolation-sharing-between-projects","text":"SecFlow supports granular sharing control for multi-project setups: Mode Description Isolated Each project keeps separate resources and findings. Shared Group Projects under the same group share wordlists and results. Selective User manually links resources or outputs between projects.","title":"\ud83d\udd10 Data Isolation &amp; Sharing Between Projects"},{"location":"architecture/10-wordlist-and-output-sharing/#example","text":"project: name: acme-api sharing: with: [\"internal-api\", \"dev-api\"] resources: [\"wordlists\", \"templates\"] outputs: [\"urls\", \"parameters\"]","title":"Example:"},{"location":"architecture/10-wordlist-and-output-sharing/#cache-deduplication","text":"Wordlists and tool outputs are hash-indexed and cached for reuse.","title":"\ud83e\udde9 Cache &amp; Deduplication"},{"location":"architecture/10-wordlist-and-output-sharing/#cache-key-formula","text":"cache_key = sha256(resource_id + version + scope) This guarantees consistent retrieval and avoids redundant downloads.","title":"Cache Key Formula"},{"location":"architecture/10-wordlist-and-output-sharing/#example-end-to-end-flow","text":"Project: acme-api %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Shared wordlists (global)\"] B[\"Custom payloads (project)\"] C[\"Workflow: Ferox \u2192 Katana \u2192 Nuclei\"] D[\"URLs discovered \u2192 shared dataset\"] E[\"Parameters extracted \u2192 temp dataset\"] F[\"Findings enriched \u2192 persisted\"] G[\"Cached resources \u2192 reused in next run\"] A --> C B --> C C --> D C --> E C --> F C --> G","title":"\ud83e\udde0 Example End-to-End Flow"},{"location":"architecture/10-wordlist-and-output-sharing/#validation-conflict-resolution","text":"Conflict Resolution Same resource name, different scope Higher precedence scope wins (project > group > global). Resource deleted mid-run Cached version remains until TTL expires. Output schema mismatch Adapter plugin transforms data. Duplicate URLs or findings De-duplication engine merges entries by hash.","title":"\ud83e\udde9 Validation &amp; Conflict Resolution"},{"location":"architecture/10-wordlist-and-output-sharing/#future-enhancements","text":"Distributed cache synchronization (Redis or MinIO). Tag-based linking of related outputs (e.g., same target domain). Automatic scope inference based on project classification. Resource \"diff\" view between runs. AI suggestion for optimal wordlists or template selection. Next: Project Isolation & Data Sharing Controls","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/11-project-isolation-and-data-sharing/","text":"11 \u2014 Project Isolation & Data Sharing Controls \u00b6 \ud83e\udded Overview \u00b6 The Project Isolation & Sharing subsystem defines how multiple SecFlow projects coexist securely within the same environment. Each project has its own namespace , storage domain , and execution context , but can optionally share data or resources under controlled policies. SecFlow's isolation model is designed to: - Protect sensitive customer or target data. - Enable reuse of discovery outputs between related engagements. - Support team-based collaboration while maintaining auditability. \ud83e\uddf1 Design Principles \u00b6 Principle Description Default Isolation Every project operates in its own workspace and database namespace. Explicit Sharing No cross-project access occurs unless explicitly declared. Scoped Resources Shared assets must be tagged and versioned. Least Privilege Access Users only see or manage projects they are assigned to. Full Traceability Every cross-project data access is logged and auditable. \u2699\ufe0f Architectural Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project Manager<br/>- Workspace Manager (namespacing, FS isolation)<br/>- Data Access Layer (scope resolution)<br/>- Sharing Policy Engine (group + project-level rules)<br/>- Audit Log (cross-project actions)\"] \ud83e\udde9 Project Data Model \u00b6 # core-lib/models/project.py from typing import List, Optional from datetime import datetime from pydantic import BaseModel class Project(BaseModel): id: str name: str owner: str group: Optional[str] description: Optional[str] created_at: datetime updated_at: datetime sharing: Optional[dict] = { \"enabled\": False, \"with\": [], \"resources\": [], \"outputs\": [] } \ud83e\udde9 Workspace Isolation \u00b6 Each project is backed by its own filesystem and database schema. Example Directory Layout \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/projects/\"] B[\"acme-api/\"] C[\"config.yaml\"] D[\"runs/\"] E[\"findings/\"] F[\"cache/\"] G[\"finance-portal/\"] H[\"config.yaml\"] I[\"runs/\"] J[\"findings/\"] K[\"cache/\"] A --> B A --> G B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K Database Schema Isolation \u00b6 Each project gets a dedicated schema: public.findings_acme_api public.findings_finance_portal This allows multiple concurrent engagements with strict data boundaries. \ud83e\udde0 Data Sharing Configuration \u00b6 Example: Controlled Cross-Project Sharing \u00b6 project: name: \"acme-api\" sharing: enabled: true with: - \"internal-api\" - \"qa-staging\" resources: - \"wordlists\" - \"templates\" outputs: - \"urls\" - \"parameters\" In this configuration: - The acme-api project shares wordlists and templates with two sibling projects. - The outputs (e.g., discovered URLs) are made available for enrichment or scanning reuse. \ud83e\udde9 Sharing Policy Engine \u00b6 Logic Flow \u00b6 User Request \u2193 Policy Check (Project A \u2192 Project B) \u2193 Scope Validation \u2193 Access Decision (allow | deny) Policy Structure \u00b6 Field Description resource_type What type of data is being shared (wordlist, output, finding). scope Allowed scope (project, group, global). mode Access type (read-only, read-write, clone). ttl Time-to-live for shared access. Example Policy Definition \u00b6 policies: - resource_type: \"outputs\" scope: \"group\" mode: \"read-only\" ttl: 30d \ud83e\udde9 Isolation Enforcement Mechanisms \u00b6 Layer Enforcement Filesystem Each project path is sandboxed under ~/.SecFlow/projects/<id> . Database Separate schema per project (namespaced tables). Cache Project-specific cache directories. Runtime Context Workers run with PROJECT_ID environment variable. Authorization API tokens include project_id scope claim. \ud83e\udde0 Access Token Scoping \u00b6 API tokens encode the project scope: { \"sub\": \"hernan.trajtemberg\", \"project_id\": \"acme-api\", \"roles\": [\"analyst\"], \"exp\": 1759870400 } Tokens can be project-scoped or group-scoped. The access control middleware rejects out-of-scope operations. \ud83e\udde9 Resource Linking Between Projects \u00b6 Projects can import shared assets from another project's registry. Example Command \u00b6 SecFlow projects link internal-api --resources wordlists templates Example Output \u00b6 Linked resources: \u2714 wordlists (3) \u2714 templates (5) Linked resources are marked in metadata: linked_from: \"project:internal-api\" mode: \"read-only\" \ud83e\uddf1 Output Sharing \u00b6 Outputs (datasets or findings) can also be shared for cross-project correlation or enrichment. Example Workflow: \u00b6 Project A \u2192 Discovery + Scan \u2193 Shared Outputs (URLs, endpoints) \u2193 Project B \u2192 Enrichment / Retesting Sharing Command \u00b6 SecFlow share outputs acme-api --with finance-portal --types urls parameters The receiving project's engine imports the shared dataset as a read-only reference. \ud83e\udde9 Audit Logging \u00b6 Every cross-project access event is logged. Example Log Entry \u00b6 { \"event\": \"resource_access\", \"actor\": \"hernan.trajtemberg\", \"source_project\": \"acme-api\", \"target_project\": \"internal-api\", \"resource\": \"wordlists\", \"timestamp\": \"2025-10-06T11:42:21Z\", \"action\": \"read\" } \ud83e\udde0 Isolation Example Scenarios \u00b6 1. Strict Isolation (Default) \u00b6 Each project operates completely independently. Useful for sensitive pentests or regulated environments. 2. Group-Level Sharing \u00b6 Multiple analysts share discovery data across projects within the same team. 3. Selective Sharing \u00b6 A red-team lead selectively shares Nuclei findings between \"internal-api\" and \"staging-api\". \ud83d\udd10 Security Considerations \u00b6 Risk Mitigation Unauthorized access to shared data Token-scoped enforcement and audit logging Resource version drift Immutable hashes + version pinning Data leakage across clients No implicit sharing; explicit only Lateral movement between project schemas Database role isolation Policy misconfiguration Policy validation + test harness \ud83e\udde9 Example Policy Validation Script \u00b6 def validate_policy(policy): assert policy[\"mode\"] in (\"read-only\", \"read-write\", \"clone\") assert policy[\"scope\"] in (\"project\", \"group\", \"global\") if policy[\"ttl\"]: assert re.match(r\"^\\d+[dh]$\", policy[\"ttl\"]) \ud83d\udd2e Future Enhancements \u00b6 Group-level \"Shared Intelligence Pool\" for recurring findings. Automatic synchronization of enrichment data across related projects. AI-based data deduplication and anomaly detection. Visual dependency graph of shared resources in UI. Temporal sharing policies (\"auto-expire after 30 days\"). Next: Findings Model & Schema Normalization","title":"Project Isolation & Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#11-project-isolation-data-sharing-controls","text":"","title":"11 \u2014 Project Isolation &amp; Data Sharing Controls"},{"location":"architecture/11-project-isolation-and-data-sharing/#overview","text":"The Project Isolation & Sharing subsystem defines how multiple SecFlow projects coexist securely within the same environment. Each project has its own namespace , storage domain , and execution context , but can optionally share data or resources under controlled policies. SecFlow's isolation model is designed to: - Protect sensitive customer or target data. - Enable reuse of discovery outputs between related engagements. - Support team-based collaboration while maintaining auditability.","title":"\ud83e\udded Overview"},{"location":"architecture/11-project-isolation-and-data-sharing/#design-principles","text":"Principle Description Default Isolation Every project operates in its own workspace and database namespace. Explicit Sharing No cross-project access occurs unless explicitly declared. Scoped Resources Shared assets must be tagged and versioned. Least Privilege Access Users only see or manage projects they are assigned to. Full Traceability Every cross-project data access is logged and auditable.","title":"\ud83e\uddf1 Design Principles"},{"location":"architecture/11-project-isolation-and-data-sharing/#architectural-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Project Manager<br/>- Workspace Manager (namespacing, FS isolation)<br/>- Data Access Layer (scope resolution)<br/>- Sharing Policy Engine (group + project-level rules)<br/>- Audit Log (cross-project actions)\"]","title":"\u2699\ufe0f Architectural Overview"},{"location":"architecture/11-project-isolation-and-data-sharing/#project-data-model","text":"# core-lib/models/project.py from typing import List, Optional from datetime import datetime from pydantic import BaseModel class Project(BaseModel): id: str name: str owner: str group: Optional[str] description: Optional[str] created_at: datetime updated_at: datetime sharing: Optional[dict] = { \"enabled\": False, \"with\": [], \"resources\": [], \"outputs\": [] }","title":"\ud83e\udde9 Project Data Model"},{"location":"architecture/11-project-isolation-and-data-sharing/#workspace-isolation","text":"Each project is backed by its own filesystem and database schema.","title":"\ud83e\udde9 Workspace Isolation"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-directory-layout","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/projects/\"] B[\"acme-api/\"] C[\"config.yaml\"] D[\"runs/\"] E[\"findings/\"] F[\"cache/\"] G[\"finance-portal/\"] H[\"config.yaml\"] I[\"runs/\"] J[\"findings/\"] K[\"cache/\"] A --> B A --> G B --> C B --> D B --> E B --> F G --> H G --> I G --> J G --> K","title":"Example Directory Layout"},{"location":"architecture/11-project-isolation-and-data-sharing/#database-schema-isolation","text":"Each project gets a dedicated schema: public.findings_acme_api public.findings_finance_portal This allows multiple concurrent engagements with strict data boundaries.","title":"Database Schema Isolation"},{"location":"architecture/11-project-isolation-and-data-sharing/#data-sharing-configuration","text":"","title":"\ud83e\udde0 Data Sharing Configuration"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-controlled-cross-project-sharing","text":"project: name: \"acme-api\" sharing: enabled: true with: - \"internal-api\" - \"qa-staging\" resources: - \"wordlists\" - \"templates\" outputs: - \"urls\" - \"parameters\" In this configuration: - The acme-api project shares wordlists and templates with two sibling projects. - The outputs (e.g., discovered URLs) are made available for enrichment or scanning reuse.","title":"Example: Controlled Cross-Project Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#sharing-policy-engine","text":"","title":"\ud83e\udde9 Sharing Policy Engine"},{"location":"architecture/11-project-isolation-and-data-sharing/#logic-flow","text":"User Request \u2193 Policy Check (Project A \u2192 Project B) \u2193 Scope Validation \u2193 Access Decision (allow | deny)","title":"Logic Flow"},{"location":"architecture/11-project-isolation-and-data-sharing/#policy-structure","text":"Field Description resource_type What type of data is being shared (wordlist, output, finding). scope Allowed scope (project, group, global). mode Access type (read-only, read-write, clone). ttl Time-to-live for shared access.","title":"Policy Structure"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-policy-definition","text":"policies: - resource_type: \"outputs\" scope: \"group\" mode: \"read-only\" ttl: 30d","title":"Example Policy Definition"},{"location":"architecture/11-project-isolation-and-data-sharing/#isolation-enforcement-mechanisms","text":"Layer Enforcement Filesystem Each project path is sandboxed under ~/.SecFlow/projects/<id> . Database Separate schema per project (namespaced tables). Cache Project-specific cache directories. Runtime Context Workers run with PROJECT_ID environment variable. Authorization API tokens include project_id scope claim.","title":"\ud83e\udde9 Isolation Enforcement Mechanisms"},{"location":"architecture/11-project-isolation-and-data-sharing/#access-token-scoping","text":"API tokens encode the project scope: { \"sub\": \"hernan.trajtemberg\", \"project_id\": \"acme-api\", \"roles\": [\"analyst\"], \"exp\": 1759870400 } Tokens can be project-scoped or group-scoped. The access control middleware rejects out-of-scope operations.","title":"\ud83e\udde0 Access Token Scoping"},{"location":"architecture/11-project-isolation-and-data-sharing/#resource-linking-between-projects","text":"Projects can import shared assets from another project's registry.","title":"\ud83e\udde9 Resource Linking Between Projects"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-command","text":"SecFlow projects link internal-api --resources wordlists templates","title":"Example Command"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-output","text":"Linked resources: \u2714 wordlists (3) \u2714 templates (5) Linked resources are marked in metadata: linked_from: \"project:internal-api\" mode: \"read-only\"","title":"Example Output"},{"location":"architecture/11-project-isolation-and-data-sharing/#output-sharing","text":"Outputs (datasets or findings) can also be shared for cross-project correlation or enrichment.","title":"\ud83e\uddf1 Output Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-workflow","text":"Project A \u2192 Discovery + Scan \u2193 Shared Outputs (URLs, endpoints) \u2193 Project B \u2192 Enrichment / Retesting","title":"Example Workflow:"},{"location":"architecture/11-project-isolation-and-data-sharing/#sharing-command","text":"SecFlow share outputs acme-api --with finance-portal --types urls parameters The receiving project's engine imports the shared dataset as a read-only reference.","title":"Sharing Command"},{"location":"architecture/11-project-isolation-and-data-sharing/#audit-logging","text":"Every cross-project access event is logged.","title":"\ud83e\udde9 Audit Logging"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-log-entry","text":"{ \"event\": \"resource_access\", \"actor\": \"hernan.trajtemberg\", \"source_project\": \"acme-api\", \"target_project\": \"internal-api\", \"resource\": \"wordlists\", \"timestamp\": \"2025-10-06T11:42:21Z\", \"action\": \"read\" }","title":"Example Log Entry"},{"location":"architecture/11-project-isolation-and-data-sharing/#isolation-example-scenarios","text":"","title":"\ud83e\udde0 Isolation Example Scenarios"},{"location":"architecture/11-project-isolation-and-data-sharing/#1-strict-isolation-default","text":"Each project operates completely independently. Useful for sensitive pentests or regulated environments.","title":"1. Strict Isolation (Default)"},{"location":"architecture/11-project-isolation-and-data-sharing/#2-group-level-sharing","text":"Multiple analysts share discovery data across projects within the same team.","title":"2. Group-Level Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#3-selective-sharing","text":"A red-team lead selectively shares Nuclei findings between \"internal-api\" and \"staging-api\".","title":"3. Selective Sharing"},{"location":"architecture/11-project-isolation-and-data-sharing/#security-considerations","text":"Risk Mitigation Unauthorized access to shared data Token-scoped enforcement and audit logging Resource version drift Immutable hashes + version pinning Data leakage across clients No implicit sharing; explicit only Lateral movement between project schemas Database role isolation Policy misconfiguration Policy validation + test harness","title":"\ud83d\udd10 Security Considerations"},{"location":"architecture/11-project-isolation-and-data-sharing/#example-policy-validation-script","text":"def validate_policy(policy): assert policy[\"mode\"] in (\"read-only\", \"read-write\", \"clone\") assert policy[\"scope\"] in (\"project\", \"group\", \"global\") if policy[\"ttl\"]: assert re.match(r\"^\\d+[dh]$\", policy[\"ttl\"])","title":"\ud83e\udde9 Example Policy Validation Script"},{"location":"architecture/11-project-isolation-and-data-sharing/#future-enhancements","text":"Group-level \"Shared Intelligence Pool\" for recurring findings. Automatic synchronization of enrichment data across related projects. AI-based data deduplication and anomaly detection. Visual dependency graph of shared resources in UI. Temporal sharing policies (\"auto-expire after 30 days\"). Next: Findings Model & Schema Normalization","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/12-findings-model-and-schema/","text":"12 \u2014 Findings Model & Schema Normalization \u00b6 \ud83e\udded Overview \u00b6 The Findings Schema is the canonical data contract for SecFlow \u2014 it defines how vulnerabilities, anomalies, and tool outputs are normalized, enriched, and persisted. Every discovery, scan, or enrichment step in the workflow produces Findings , which are standardized across tools (Nuclei, Ferox, ZAP, Caido, custom detectors). The schema ensures: - Cross-tool consistency - Automated CVE/CWE/OWASP correlation - Risk scoring alignment (CVSS/NIST) - Deterministic enrichment for analytics and triage \ud83e\uddf1 Data Flow Summary \u00b6 [ Tool Output ] \u2193 [ Findings Engine ] \u2193 [ Normalization Layer ] \u2193 [ Enrichment Layer (CVE/CWE/OWASP) ] \u2193 [ Persistent Store + Cache + Analytics ] \u2699\ufe0f Findings Core Model \u00b6 from pydantic import BaseModel, Field from typing import Optional, List, Dict from uuid import UUID from datetime import datetime class Finding(BaseModel): id: UUID project_id: UUID run_id: Optional[UUID] detector_id: str = Field(description=\"canonical tool or plugin id, e.g. scan.nuclei\") title: str severity: str # enum: info, low, medium, high, critical resource: str # canonical URL or identifier evidence: Dict[str, object] = {} cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] cvss: Optional[float] mitre_attack: List[str] = [] poc_links: List[str] = [] created_at: datetime provenance: Dict[str, object] = {} \ud83e\udde9 Normalization Process \u00b6 Each wrapper or plugin output goes through the Findings Normalizer, which performs: Schema validation Severity normalization Field mapping Evidence compression Deduplication Example Normalized Finding \u00b6 { \"id\": \"7a3d9f8e-1f83-4a55-9a6c-5ea9e1a3b8d2\", \"project_id\": \"b0b8b6e0-7c6a-43e8-8b2f-8a0d0e8b0f3c\", \"run_id\": \"2a2b3c4d-5e6f-7081-92a3-b4c5d6e7f809\", \"detector_id\": \"scan.nuclei\", \"title\": \"X-Frame-Options header missing\", \"severity\": \"low\", \"resource\": \"https://app.acme.com/\", \"evidence\": { \"request_id\": \"r-01HXXQ5AKP0NWS\", \"template\": \"misconfig/headers/xfo-missing.yaml\" }, \"cwe\": 693, \"owasp\": \"A05\", \"cve_ids\": [], \"cvss\": 3.7, \"mitre_attack\": [\"T1190\"], \"poc_links\": [], \"created_at\": \"2025-10-07T08:22:31Z\", \"provenance\": { \"tool_version\": \"nuclei 3.1.0\", \"host\": \"runner-12\" } } def normalize(raw: dict, source: str) -> Finding: severity_map = {\"info\": \"info\", \"low\": \"low\", \"medium\": \"medium\", \"high\": \"high\", \"critical\": \"critical\"} return Finding( id=str(uuid4()), project_id=raw.get(\"project_id\"), detector_id=source, title=raw.get(\"info\", {}).get(\"name\") or raw.get(\"title\", \"Unnamed Finding\"), severity=severity_map.get(raw.get(\"severity\", \"info\")), path=raw.get(\"matched-at\") or raw.get(\"url\"), evidence=raw, created_at=datetime.utcnow(), provenance={\"tool\": source, \"version\": raw.get(\"version\", \"unknown\")} ) \ud83e\udde9 Normalization Rules by Source \u00b6 Tool Input Format Mapping Nuclei JSON lines info.name \u2192 title , info.severity \u2192 severity , matched-at \u2192 path Feroxbuster Text URL \u2192 path , status \u2192 evidence.status ZAP/Burp XML/JSON PluginId \u2192 cwe , RiskDesc \u2192 severity Caido SQLite Vulnerability.name \u2192 title , score \u2192 cvss_score Custom Detectors Python dict Arbitrary fields normalized via schema mapping Normalization is performed by findings-engine using source-specific adapters. \ud83e\udde0 Severity Mapping \u00b6 Raw Severity Normalized CVSS Equivalent informational info 0.0\u20133.9 low low 4.0\u20136.9 medium medium 6.0\u20137.4 high high 7.5\u20138.9 critical critical 9.0\u201310.0 \ud83e\udde0 Deduplication Strategy \u00b6 Findings are hashed on a deterministic fingerprint: hash_input = f\"{finding.path}:{finding.title}:{finding.detector_id}\" finding_hash = hashlib.sha256(hash_input.encode()).hexdigest() If the hash already exists in the same project and run scope, the finding is merged rather than duplicated. \ud83e\udde9 Enrichment Metadata Structure \u00b6 finding.enrichment = { \"cwe\": {\"id\": 89, \"name\": \"SQL Injection\"}, \"owasp\": {\"category\": \"A03\", \"name\": \"Injection\"}, \"CVSS\": {\"score\": 9.1, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\"}, \"mitre\": {\"technique\": \"T1505\", \"tactic\": \"Persistence\"}, \"poc\": {\"source\": \"exploitdb\", \"link\": \"https://www.exploit-db.com/exploits/52341\"} } This metadata is populated during the Enrichment Phase (CVE/CWE/CVSS sync). \ud83e\udde9 CVE / CWE / OWASP Mapping \u00b6 CVE \u2192 CWE \u00b6 NVD API provides cve.affects.vendor.vendor_data \u2192 cwe.id Mapping stored in local SQLite cache. CWE \u2192 OWASP \u00b6 CWE OWASP Category CWE-79 A03: Injection CWE-89 A03: Injection CWE-287 A07: Identification and Authentication Failures CWE-601 A10: Server-Side Request Forgery (SSRF) Example Mapping Resolver \u00b6 def resolve_owasp(cwe_id): mapping = {\"79\": \"A03\", \"89\": \"A03\", \"287\": \"A07\", \"601\": \"A10\"} return mapping.get(str(cwe_id)) \ud83e\udde9 Confidence & Risk Scoring \u00b6 Confidence combines tool reliability, correlation consistency, and enrichment coverage. Formula \u00b6 confidence = (tool_weight * 0.5) + (enrichment_score * 0.3) + (cross_source_count * 0.2) Risk Score Calculation \u00b6 risk_score = CVSS * confidence This allows probabilistic triage prioritization. \ud83e\udde0 Evidence Normalization \u00b6 Evidence is stored in compact, structured form for indexing: { \"request\": { \"method\": \"POST\", \"url\": \"https://target.com/login\", \"headers\": {\"Content-Type\": \"application/json\"}, \"body\": \"{\\\"username\\\":\\\"admin\\\"}\" }, \"response\": { \"status\": 500, \"headers\": {\"Server\": \"Apache\"}, \"body_snippet\": \"SQL syntax error\" } } Large payloads are truncated or compressed to avoid storage overhead. \ud83e\udde9 Finding Status Lifecycle \u00b6 Status Meaning Managed By open Newly discovered issue Scanner triaged Analyst reviewed Analyst resolved Fixed or confirmed Analyst false_positive Invalid finding Analyst archived Expired or obsolete System (GC) Each status change triggers an audit log event and optional webhook notification. \ud83e\uddf1 Storage Layer Integration \u00b6 Findings are persisted via the StoragePort interface: class FindingsRepository(Protocol): def save(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list(self, project_id: str) -> List[Finding]: \"\"\"List findings for a project.\"\"\" pass def get(self, id: str) -> Finding: \"\"\"Get a finding by ID.\"\"\" pass Supported backends: \u00b6 SQLite (default local mode) PostgreSQL (production multi-project) JSON (testing or demo mode) \ud83e\udde9 Findings Export Schema \u00b6 SecFlow exports findings in structured formats for interoperability: Format Command JSON SecFlow export findings --format json CSV SecFlow export findings --format csv HTML SecFlow report findings --template summary.html SARIF SecFlow export findings --format sarif Example JSON export: \u00b6 { \"project\": \"acme-api\", \"findings\": [ { \"id\": \"a2b4f3c8-d9e2-4f1a-5b6c-7d8e9f0a1b2c\", \"title\": \"SQL Injection\", \"severity\": \"critical\", \"cwe\": 89, \"cvss_score\": 9.1, \"path\": \"/login\", \"created_at\": \"2025-10-06T10:15:00Z\" } ] } \ud83e\udde0 Indexing & Analytics \u00b6 Each finding is indexed in the analytics database: Primary Index: project_id + severity + cwe Full-Text Search: title , path , evidence.body_snippet Aggregations: count by severity, top affected endpoints, common CWE classes. The metrics system (see 06-plugin-system.md & 17-observability-logging-and-metrics.md ) uses these indexes to generate dashboards. \ud83d\udd2e Future Enhancements \u00b6 Graph-based finding correlation (attack paths). AI-driven risk clustering (\"find similar vulnerabilities\"). Contextual auto-triage (OWASP/NIST mapping feedback loops). Delta reports between runs ( run_id diff). Live sync to vulnerability management platforms (DefectDojo, VulnDB). Next: CVE/CWE/POC Enrichment Layer","title":"Findings Schema"},{"location":"architecture/12-findings-model-and-schema/#12-findings-model-schema-normalization","text":"","title":"12 \u2014 Findings Model &amp; Schema Normalization"},{"location":"architecture/12-findings-model-and-schema/#overview","text":"The Findings Schema is the canonical data contract for SecFlow \u2014 it defines how vulnerabilities, anomalies, and tool outputs are normalized, enriched, and persisted. Every discovery, scan, or enrichment step in the workflow produces Findings , which are standardized across tools (Nuclei, Ferox, ZAP, Caido, custom detectors). The schema ensures: - Cross-tool consistency - Automated CVE/CWE/OWASP correlation - Risk scoring alignment (CVSS/NIST) - Deterministic enrichment for analytics and triage","title":"\ud83e\udded Overview"},{"location":"architecture/12-findings-model-and-schema/#data-flow-summary","text":"[ Tool Output ] \u2193 [ Findings Engine ] \u2193 [ Normalization Layer ] \u2193 [ Enrichment Layer (CVE/CWE/OWASP) ] \u2193 [ Persistent Store + Cache + Analytics ]","title":"\ud83e\uddf1 Data Flow Summary"},{"location":"architecture/12-findings-model-and-schema/#findings-core-model","text":"from pydantic import BaseModel, Field from typing import Optional, List, Dict from uuid import UUID from datetime import datetime class Finding(BaseModel): id: UUID project_id: UUID run_id: Optional[UUID] detector_id: str = Field(description=\"canonical tool or plugin id, e.g. scan.nuclei\") title: str severity: str # enum: info, low, medium, high, critical resource: str # canonical URL or identifier evidence: Dict[str, object] = {} cwe: Optional[int] owasp: Optional[str] cve_ids: List[str] = [] cvss: Optional[float] mitre_attack: List[str] = [] poc_links: List[str] = [] created_at: datetime provenance: Dict[str, object] = {}","title":"\u2699\ufe0f Findings Core Model"},{"location":"architecture/12-findings-model-and-schema/#normalization-process","text":"Each wrapper or plugin output goes through the Findings Normalizer, which performs: Schema validation Severity normalization Field mapping Evidence compression Deduplication","title":"\ud83e\udde9 Normalization Process"},{"location":"architecture/12-findings-model-and-schema/#example-normalized-finding","text":"{ \"id\": \"7a3d9f8e-1f83-4a55-9a6c-5ea9e1a3b8d2\", \"project_id\": \"b0b8b6e0-7c6a-43e8-8b2f-8a0d0e8b0f3c\", \"run_id\": \"2a2b3c4d-5e6f-7081-92a3-b4c5d6e7f809\", \"detector_id\": \"scan.nuclei\", \"title\": \"X-Frame-Options header missing\", \"severity\": \"low\", \"resource\": \"https://app.acme.com/\", \"evidence\": { \"request_id\": \"r-01HXXQ5AKP0NWS\", \"template\": \"misconfig/headers/xfo-missing.yaml\" }, \"cwe\": 693, \"owasp\": \"A05\", \"cve_ids\": [], \"cvss\": 3.7, \"mitre_attack\": [\"T1190\"], \"poc_links\": [], \"created_at\": \"2025-10-07T08:22:31Z\", \"provenance\": { \"tool_version\": \"nuclei 3.1.0\", \"host\": \"runner-12\" } } def normalize(raw: dict, source: str) -> Finding: severity_map = {\"info\": \"info\", \"low\": \"low\", \"medium\": \"medium\", \"high\": \"high\", \"critical\": \"critical\"} return Finding( id=str(uuid4()), project_id=raw.get(\"project_id\"), detector_id=source, title=raw.get(\"info\", {}).get(\"name\") or raw.get(\"title\", \"Unnamed Finding\"), severity=severity_map.get(raw.get(\"severity\", \"info\")), path=raw.get(\"matched-at\") or raw.get(\"url\"), evidence=raw, created_at=datetime.utcnow(), provenance={\"tool\": source, \"version\": raw.get(\"version\", \"unknown\")} )","title":"Example Normalized Finding"},{"location":"architecture/12-findings-model-and-schema/#normalization-rules-by-source","text":"Tool Input Format Mapping Nuclei JSON lines info.name \u2192 title , info.severity \u2192 severity , matched-at \u2192 path Feroxbuster Text URL \u2192 path , status \u2192 evidence.status ZAP/Burp XML/JSON PluginId \u2192 cwe , RiskDesc \u2192 severity Caido SQLite Vulnerability.name \u2192 title , score \u2192 cvss_score Custom Detectors Python dict Arbitrary fields normalized via schema mapping Normalization is performed by findings-engine using source-specific adapters.","title":"\ud83e\udde9 Normalization Rules by Source"},{"location":"architecture/12-findings-model-and-schema/#severity-mapping","text":"Raw Severity Normalized CVSS Equivalent informational info 0.0\u20133.9 low low 4.0\u20136.9 medium medium 6.0\u20137.4 high high 7.5\u20138.9 critical critical 9.0\u201310.0","title":"\ud83e\udde0 Severity Mapping"},{"location":"architecture/12-findings-model-and-schema/#deduplication-strategy","text":"Findings are hashed on a deterministic fingerprint: hash_input = f\"{finding.path}:{finding.title}:{finding.detector_id}\" finding_hash = hashlib.sha256(hash_input.encode()).hexdigest() If the hash already exists in the same project and run scope, the finding is merged rather than duplicated.","title":"\ud83e\udde0 Deduplication Strategy"},{"location":"architecture/12-findings-model-and-schema/#enrichment-metadata-structure","text":"finding.enrichment = { \"cwe\": {\"id\": 89, \"name\": \"SQL Injection\"}, \"owasp\": {\"category\": \"A03\", \"name\": \"Injection\"}, \"CVSS\": {\"score\": 9.1, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\"}, \"mitre\": {\"technique\": \"T1505\", \"tactic\": \"Persistence\"}, \"poc\": {\"source\": \"exploitdb\", \"link\": \"https://www.exploit-db.com/exploits/52341\"} } This metadata is populated during the Enrichment Phase (CVE/CWE/CVSS sync).","title":"\ud83e\udde9 Enrichment Metadata Structure"},{"location":"architecture/12-findings-model-and-schema/#cve-cwe-owasp-mapping","text":"","title":"\ud83e\udde9 CVE / CWE / OWASP Mapping"},{"location":"architecture/12-findings-model-and-schema/#cve-cwe","text":"NVD API provides cve.affects.vendor.vendor_data \u2192 cwe.id Mapping stored in local SQLite cache.","title":"CVE \u2192 CWE"},{"location":"architecture/12-findings-model-and-schema/#cwe-owasp","text":"CWE OWASP Category CWE-79 A03: Injection CWE-89 A03: Injection CWE-287 A07: Identification and Authentication Failures CWE-601 A10: Server-Side Request Forgery (SSRF)","title":"CWE \u2192 OWASP"},{"location":"architecture/12-findings-model-and-schema/#example-mapping-resolver","text":"def resolve_owasp(cwe_id): mapping = {\"79\": \"A03\", \"89\": \"A03\", \"287\": \"A07\", \"601\": \"A10\"} return mapping.get(str(cwe_id))","title":"Example Mapping Resolver"},{"location":"architecture/12-findings-model-and-schema/#confidence-risk-scoring","text":"Confidence combines tool reliability, correlation consistency, and enrichment coverage.","title":"\ud83e\udde9 Confidence &amp; Risk Scoring"},{"location":"architecture/12-findings-model-and-schema/#formula","text":"confidence = (tool_weight * 0.5) + (enrichment_score * 0.3) + (cross_source_count * 0.2)","title":"Formula"},{"location":"architecture/12-findings-model-and-schema/#risk-score-calculation","text":"risk_score = CVSS * confidence This allows probabilistic triage prioritization.","title":"Risk Score Calculation"},{"location":"architecture/12-findings-model-and-schema/#evidence-normalization","text":"Evidence is stored in compact, structured form for indexing: { \"request\": { \"method\": \"POST\", \"url\": \"https://target.com/login\", \"headers\": {\"Content-Type\": \"application/json\"}, \"body\": \"{\\\"username\\\":\\\"admin\\\"}\" }, \"response\": { \"status\": 500, \"headers\": {\"Server\": \"Apache\"}, \"body_snippet\": \"SQL syntax error\" } } Large payloads are truncated or compressed to avoid storage overhead.","title":"\ud83e\udde0 Evidence Normalization"},{"location":"architecture/12-findings-model-and-schema/#finding-status-lifecycle","text":"Status Meaning Managed By open Newly discovered issue Scanner triaged Analyst reviewed Analyst resolved Fixed or confirmed Analyst false_positive Invalid finding Analyst archived Expired or obsolete System (GC) Each status change triggers an audit log event and optional webhook notification.","title":"\ud83e\udde9 Finding Status Lifecycle"},{"location":"architecture/12-findings-model-and-schema/#storage-layer-integration","text":"Findings are persisted via the StoragePort interface: class FindingsRepository(Protocol): def save(self, finding: Finding) -> None: \"\"\"Save a finding to storage.\"\"\" pass def list(self, project_id: str) -> List[Finding]: \"\"\"List findings for a project.\"\"\" pass def get(self, id: str) -> Finding: \"\"\"Get a finding by ID.\"\"\" pass","title":"\ud83e\uddf1 Storage Layer Integration"},{"location":"architecture/12-findings-model-and-schema/#supported-backends","text":"SQLite (default local mode) PostgreSQL (production multi-project) JSON (testing or demo mode)","title":"Supported backends:"},{"location":"architecture/12-findings-model-and-schema/#findings-export-schema","text":"SecFlow exports findings in structured formats for interoperability: Format Command JSON SecFlow export findings --format json CSV SecFlow export findings --format csv HTML SecFlow report findings --template summary.html SARIF SecFlow export findings --format sarif","title":"\ud83e\udde9 Findings Export Schema"},{"location":"architecture/12-findings-model-and-schema/#example-json-export","text":"{ \"project\": \"acme-api\", \"findings\": [ { \"id\": \"a2b4f3c8-d9e2-4f1a-5b6c-7d8e9f0a1b2c\", \"title\": \"SQL Injection\", \"severity\": \"critical\", \"cwe\": 89, \"cvss_score\": 9.1, \"path\": \"/login\", \"created_at\": \"2025-10-06T10:15:00Z\" } ] }","title":"Example JSON export:"},{"location":"architecture/12-findings-model-and-schema/#indexing-analytics","text":"Each finding is indexed in the analytics database: Primary Index: project_id + severity + cwe Full-Text Search: title , path , evidence.body_snippet Aggregations: count by severity, top affected endpoints, common CWE classes. The metrics system (see 06-plugin-system.md & 17-observability-logging-and-metrics.md ) uses these indexes to generate dashboards.","title":"\ud83e\udde0 Indexing &amp; Analytics"},{"location":"architecture/12-findings-model-and-schema/#future-enhancements","text":"Graph-based finding correlation (attack paths). AI-driven risk clustering (\"find similar vulnerabilities\"). Contextual auto-triage (OWASP/NIST mapping feedback loops). Delta reports between runs ( run_id diff). Live sync to vulnerability management platforms (DefectDojo, VulnDB). Next: CVE/CWE/POC Enrichment Layer","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/","text":"13 \u2014 CVE/CWE/POC Enrichment Layer \u00b6 \ud83e\udded Overview \u00b6 The Enrichment Layer bridges the gap between raw scanner results and full vulnerability intelligence. It performs automated correlation between findings and vulnerability databases such as: CVE/NVD \u2014 canonical vulnerability records CWE \u2014 weakness categorization CVSS \u2014 scoring and severity models Exploit-DB , Vulners , Metasploit , OSV \u2014 PoC and exploit references MITRE ATT&CK \u2014 behavioral mapping SecFlow treats enrichment as a pluggable knowledge graph layer that can be extended via simple JSON/YAML definitions or API connectors. \u2699\ufe0f Enrichment Pipeline \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Normalized Finding\"] B[\"CPE Identifier Extraction\"] C[\"CVE Resolver (NVD, OSV, Vulners)\"] D[\"CWE / OWASP Mapping\"] E[\"CVSS Calculation\"] F[\"POC + Exploit Lookup\"] G[\"MITRE ATT&CK Correlation\"] H[\"Final Enriched Finding\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H \ud83e\udde9 CPE & Component Extraction \u00b6 The enrichment process begins by fingerprinting software components. def extract_cpe(finding: Finding) -> Optional[str]: headers = finding.evidence.get(\"response\", {}).get(\"headers\", {}) banner = headers.get(\"Server\") or headers.get(\"X-Powered-By\") return cpe_guess(banner) if banner else None Example results: \u00b6 Server: Apache/2.4.54 \u2192 cpe:/a:apache:http_server:2.4.54 X-Powered-By: Express \u2192 cpe:/a:npmjs:express:4.18.2 \ud83e\udde9 CVE Resolution Engine \u00b6 The CVE Resolver queries multiple backends in a failover chain: Source Endpoint Rate Limit Cache TTL NVD https://services.nvd.nist.gov/rest/json/cves/2.0 1000/day 24h OSV.dev https://api.osv.dev/v1/query Unlimited 12h Vulners API https://vulners.com/api/v3/search/lucene/ 2000/day 24h class CVEResolver: def resolve(self, cpe: str) -> List[dict]: cached = self.cache.get(cpe) if cached: return cached results = [] for backend in self.backends: try: results.extend(backend.query(cpe)) except Exception: continue self.cache.set(cpe, results) return results Results are merged and normalized into a unified CVE format. \ud83e\udde0 CVE Normalized Model \u00b6 class CVEEntry(BaseModel): cve_id: str description: str published: datetime cvss_score: float cvss_vector: str cwe_ids: List[int] references: List[str] exploit_refs: List[str] source: str Each finding may be associated with multiple CVE entries. \ud83e\udde9 CWE / OWASP / MITRE Mapping \u00b6 Once CVEs are linked, weaknesses and behavioral context are resolved. Source Purpose Mapping Strategy CWE Weakness classification CVE \u2192 CWE via NVD JSON OWASP Application risk class CWE \u2192 OWASP Top 10 map MITRE ATT&CK Adversary tactics/techniques CWE \u2192 ATT&CK TID correlation def map_cwe_to_owasp(cwe_id: int) -> str: mapping = { 79: \"A03: Injection\", 89: \"A03: Injection\", 787: \"A05: Buffer Overflow\", 601: \"A10: SSRF\" } return mapping.get(cwe_id, \"N/A\") MITRE correlation example: \u00b6 mitre_map = { \"CWE-79\": \"T1059.007 (Cross-Site Scripting)\", \"CWE-89\": \"T1505.003 (SQL Injection)\" } \ud83e\udde9 CVSS Calculation \u00b6 If a finding lacks explicit CVSS scoring, SecFlow derives one via heuristics: def derive_cvss(cwe_id: int, context: dict) -> float: # basic fallback estimation if cwe_id in (79, 89): return 9.0 elif cwe_id in (200, 201): return 7.5 return 5.0 Final score combines: \u00b6 base = CVSS temporal = exploit_availability * 0.2 environmental = exposure_factor * 0.3 final = (base + temporal + environmental) / 1.5 \ud83e\udde0 PoC & Exploit Correlation \u00b6 Data Sources \u00b6 Source Access Notes Exploit-DB Public dump Weekly sync Vulners API Indexed by CVE Metasploit Local metadata Optional GitHub PoC OSV + GitHub GraphQL Filter by repo tags SecurityFocus (legacy) Offline mirror Static references Example Resolver \u00b6 def resolve_poc(cve_id: str) -> list: sources = [exploitdb, vulners, githubpoc] results = [] for s in sources: results.extend(s.search(cve_id)) return list(set(results)) \ud83e\udde9 PoC Safety Governance \u00b6 Because PoCs can contain malicious payloads, SecFlow enforces strict isolation. Policy Enforcement Read-only storage PoCs stored as text blobs, no exec permission Sandbox validation Hash-check before use Legal disclaimer Must be accepted before PoC download Runtime restriction Execution allowed only in --sandbox mode Example governance guard: \u00b6 def safe_open_poc(poc_path: Path): if not user.accepted_disclaimer: raise PermissionError(\"PoC execution disabled until disclaimer accepted.\") subprocess.run([\"sandbox\", \"python3\", poc_path]) \ud83e\udde9 Caching & Synchronization \u00b6 Each enrichment source maintains a versioned local cache: Component Backend Format TTL CVE NVD JSON SQLite 24h CWE MITRE XML JSON 7d PoC Exploit-DB FS/JSON 14d Example cache adapter: \u00b6 class LocalCache: def get(self, key: str) -> Optional[Any]: \"\"\"Get value from cache by key.\"\"\" pass def set(self, key: str, value: Any, ttl: int) -> None: \"\"\"Set value in cache with TTL.\"\"\" pass def purge_expired(self) -> None: \"\"\"Remove expired entries from cache.\"\"\" pass \ud83e\udde9 API Exposure \u00b6 The enrichment system provides a unified query interface: Endpoint Method Description /api/v1/enrich/cve POST Enrich a finding by CPE/CVE /api/v1/enrich/cwe POST Map CWE to OWASP /api/v1/enrich/poc GET Retrieve PoC links /api/v1/enrich/status GET Show cache health Example response: \u00b6 { \"finding_id\": \"1234\", \"cve\": [\"CVE-2024-12345\"], \"CVSS\": 9.8, \"cwe\": 89, \"owasp\": \"A03: Injection\", \"poc_links\": [\"https://exploit-db.com/exploits/52341\"] } \ud83e\udde9 Enrichment Rules & Priority \u00b6 Local cache first API sources (NVD/OSV) second Third-party mirrors (Vulners, Exploit-DB) last Each backend includes retry and circuit-breaker logic via Tenacity. \ud83e\udde9 Parallel Enrichment \u00b6 The enrichment worker uses async pipelines for batch enrichment: async def enrich_findings_batch(findings: list): async with aiohttp.ClientSession() as session: tasks = [enrich_one(f, session) for f in findings] return await asyncio.gather(*tasks) Each finding may take 0.1\u20131.5 seconds depending on CVE count; concurrency keeps throughput high. \ud83e\udde0 Example Enrichment Output \u00b6 { \"finding_id\": \"abcd-1234\", \"cpe\": \"cpe:/a:apache:http_server:2.4.54\", \"cve_ids\": [\"CVE-2023-25690\"], \"cwe\": 89, \"owasp\": \"A03: Injection\", \"cvss_score\": 9.8, \"poc_links\": [\"https://exploit-db.com/exploits/52341\"], \"mitre_tid\": \"T1505.003\", \"last_enriched\": \"2025-10-06T09:43:00Z\" } \ud83d\udd12 Security & Compliance \u00b6 All enrichment data is public-source only. No proprietary or restricted databases. Caching system automatically purges expired CVE entries. Full audit trail for every enrichment event. \ud83d\udd2e Future Enhancements \u00b6 Integrate VulnCheck, CISA KEV, and EPSS for exploit prediction. Introduce AI-based CVE clustering for semantic matching. Enrichment correlation graph for cross-project analysis. Automated risk re-scoring based on KEV updates. Next: POC Sources, Safety & Legal Guidelines","title":"Enrichment Layer"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#13-cvecwepoc-enrichment-layer","text":"","title":"13 \u2014 CVE/CWE/POC Enrichment Layer"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#overview","text":"The Enrichment Layer bridges the gap between raw scanner results and full vulnerability intelligence. It performs automated correlation between findings and vulnerability databases such as: CVE/NVD \u2014 canonical vulnerability records CWE \u2014 weakness categorization CVSS \u2014 scoring and severity models Exploit-DB , Vulners , Metasploit , OSV \u2014 PoC and exploit references MITRE ATT&CK \u2014 behavioral mapping SecFlow treats enrichment as a pluggable knowledge graph layer that can be extended via simple JSON/YAML definitions or API connectors.","title":"\ud83e\udded Overview"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#enrichment-pipeline","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Normalized Finding\"] B[\"CPE Identifier Extraction\"] C[\"CVE Resolver (NVD, OSV, Vulners)\"] D[\"CWE / OWASP Mapping\"] E[\"CVSS Calculation\"] F[\"POC + Exploit Lookup\"] G[\"MITRE ATT&CK Correlation\"] H[\"Final Enriched Finding\"] A --> B B --> C C --> D D --> E E --> F F --> G G --> H","title":"\u2699\ufe0f Enrichment Pipeline"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cpe-component-extraction","text":"The enrichment process begins by fingerprinting software components. def extract_cpe(finding: Finding) -> Optional[str]: headers = finding.evidence.get(\"response\", {}).get(\"headers\", {}) banner = headers.get(\"Server\") or headers.get(\"X-Powered-By\") return cpe_guess(banner) if banner else None","title":"\ud83e\udde9 CPE &amp; Component Extraction"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-results","text":"Server: Apache/2.4.54 \u2192 cpe:/a:apache:http_server:2.4.54 X-Powered-By: Express \u2192 cpe:/a:npmjs:express:4.18.2","title":"Example results:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cve-resolution-engine","text":"The CVE Resolver queries multiple backends in a failover chain: Source Endpoint Rate Limit Cache TTL NVD https://services.nvd.nist.gov/rest/json/cves/2.0 1000/day 24h OSV.dev https://api.osv.dev/v1/query Unlimited 12h Vulners API https://vulners.com/api/v3/search/lucene/ 2000/day 24h class CVEResolver: def resolve(self, cpe: str) -> List[dict]: cached = self.cache.get(cpe) if cached: return cached results = [] for backend in self.backends: try: results.extend(backend.query(cpe)) except Exception: continue self.cache.set(cpe, results) return results Results are merged and normalized into a unified CVE format.","title":"\ud83e\udde9 CVE Resolution Engine"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cve-normalized-model","text":"class CVEEntry(BaseModel): cve_id: str description: str published: datetime cvss_score: float cvss_vector: str cwe_ids: List[int] references: List[str] exploit_refs: List[str] source: str Each finding may be associated with multiple CVE entries.","title":"\ud83e\udde0 CVE Normalized Model"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cwe-owasp-mitre-mapping","text":"Once CVEs are linked, weaknesses and behavioral context are resolved. Source Purpose Mapping Strategy CWE Weakness classification CVE \u2192 CWE via NVD JSON OWASP Application risk class CWE \u2192 OWASP Top 10 map MITRE ATT&CK Adversary tactics/techniques CWE \u2192 ATT&CK TID correlation def map_cwe_to_owasp(cwe_id: int) -> str: mapping = { 79: \"A03: Injection\", 89: \"A03: Injection\", 787: \"A05: Buffer Overflow\", 601: \"A10: SSRF\" } return mapping.get(cwe_id, \"N/A\")","title":"\ud83e\udde9 CWE / OWASP / MITRE Mapping"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#mitre-correlation-example","text":"mitre_map = { \"CWE-79\": \"T1059.007 (Cross-Site Scripting)\", \"CWE-89\": \"T1505.003 (SQL Injection)\" }","title":"MITRE correlation example:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#cvss-calculation","text":"If a finding lacks explicit CVSS scoring, SecFlow derives one via heuristics: def derive_cvss(cwe_id: int, context: dict) -> float: # basic fallback estimation if cwe_id in (79, 89): return 9.0 elif cwe_id in (200, 201): return 7.5 return 5.0","title":"\ud83e\udde9 CVSS Calculation"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#final-score-combines","text":"base = CVSS temporal = exploit_availability * 0.2 environmental = exposure_factor * 0.3 final = (base + temporal + environmental) / 1.5","title":"Final score combines:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#poc-exploit-correlation","text":"","title":"\ud83e\udde0 PoC &amp; Exploit Correlation"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#data-sources","text":"Source Access Notes Exploit-DB Public dump Weekly sync Vulners API Indexed by CVE Metasploit Local metadata Optional GitHub PoC OSV + GitHub GraphQL Filter by repo tags SecurityFocus (legacy) Offline mirror Static references","title":"Data Sources"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-resolver","text":"def resolve_poc(cve_id: str) -> list: sources = [exploitdb, vulners, githubpoc] results = [] for s in sources: results.extend(s.search(cve_id)) return list(set(results))","title":"Example Resolver"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#poc-safety-governance","text":"Because PoCs can contain malicious payloads, SecFlow enforces strict isolation. Policy Enforcement Read-only storage PoCs stored as text blobs, no exec permission Sandbox validation Hash-check before use Legal disclaimer Must be accepted before PoC download Runtime restriction Execution allowed only in --sandbox mode","title":"\ud83e\udde9 PoC Safety Governance"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-governance-guard","text":"def safe_open_poc(poc_path: Path): if not user.accepted_disclaimer: raise PermissionError(\"PoC execution disabled until disclaimer accepted.\") subprocess.run([\"sandbox\", \"python3\", poc_path])","title":"Example governance guard:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#caching-synchronization","text":"Each enrichment source maintains a versioned local cache: Component Backend Format TTL CVE NVD JSON SQLite 24h CWE MITRE XML JSON 7d PoC Exploit-DB FS/JSON 14d","title":"\ud83e\udde9 Caching &amp; Synchronization"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-cache-adapter","text":"class LocalCache: def get(self, key: str) -> Optional[Any]: \"\"\"Get value from cache by key.\"\"\" pass def set(self, key: str, value: Any, ttl: int) -> None: \"\"\"Set value in cache with TTL.\"\"\" pass def purge_expired(self) -> None: \"\"\"Remove expired entries from cache.\"\"\" pass","title":"Example cache adapter:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#api-exposure","text":"The enrichment system provides a unified query interface: Endpoint Method Description /api/v1/enrich/cve POST Enrich a finding by CPE/CVE /api/v1/enrich/cwe POST Map CWE to OWASP /api/v1/enrich/poc GET Retrieve PoC links /api/v1/enrich/status GET Show cache health","title":"\ud83e\udde9 API Exposure"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-response","text":"{ \"finding_id\": \"1234\", \"cve\": [\"CVE-2024-12345\"], \"CVSS\": 9.8, \"cwe\": 89, \"owasp\": \"A03: Injection\", \"poc_links\": [\"https://exploit-db.com/exploits/52341\"] }","title":"Example response:"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#enrichment-rules-priority","text":"Local cache first API sources (NVD/OSV) second Third-party mirrors (Vulners, Exploit-DB) last Each backend includes retry and circuit-breaker logic via Tenacity.","title":"\ud83e\udde9 Enrichment Rules &amp; Priority"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#parallel-enrichment","text":"The enrichment worker uses async pipelines for batch enrichment: async def enrich_findings_batch(findings: list): async with aiohttp.ClientSession() as session: tasks = [enrich_one(f, session) for f in findings] return await asyncio.gather(*tasks) Each finding may take 0.1\u20131.5 seconds depending on CVE count; concurrency keeps throughput high.","title":"\ud83e\udde9 Parallel Enrichment"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#example-enrichment-output","text":"{ \"finding_id\": \"abcd-1234\", \"cpe\": \"cpe:/a:apache:http_server:2.4.54\", \"cve_ids\": [\"CVE-2023-25690\"], \"cwe\": 89, \"owasp\": \"A03: Injection\", \"cvss_score\": 9.8, \"poc_links\": [\"https://exploit-db.com/exploits/52341\"], \"mitre_tid\": \"T1505.003\", \"last_enriched\": \"2025-10-06T09:43:00Z\" }","title":"\ud83e\udde0 Example Enrichment Output"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#security-compliance","text":"All enrichment data is public-source only. No proprietary or restricted databases. Caching system automatically purges expired CVE entries. Full audit trail for every enrichment event.","title":"\ud83d\udd12 Security &amp; Compliance"},{"location":"architecture/13-cve-cwe-poc-enrichment-layer/#future-enhancements","text":"Integrate VulnCheck, CISA KEV, and EPSS for exploit prediction. Introduce AI-based CVE clustering for semantic matching. Enrichment correlation graph for cross-project analysis. Automated risk re-scoring based on KEV updates. Next: POC Sources, Safety & Legal Guidelines","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/14-poc-sources-and-legal-guidelines/","text":"14 \u2014 PoC Governance, Safety, and Legal Framework \u00b6 \ud83e\udded Overview \u00b6 Proof-of-Concept (PoC) execution is the most sensitive aspect of the SecFlow platform. While enrichment and discovery involve data collection, PoC modules may trigger actual exploits \u2014 often on controlled test targets or lab environments. This section defines: - The security model for PoC management and execution. - Legal and ethical boundaries for researchers. - Technical enforcement of sandbox isolation. - Policy hierarchy for PoC lifecycle, provenance, and deletion. \u2696\ufe0f Legal & Ethical Framework \u00b6 1. Authorized Testing Only \u00b6 PoC execution is strictly limited to: - Assets owned or explicitly authorized by the user. - Lab or offline sandbox targets. - Non-production systems unless a signed authorization exists. 2. Compliance with International Norms \u00b6 SecFlow aligns with: - ISO/IEC 29147 \u2014 Vulnerability Disclosure. - ISO/IEC 30111 \u2014 Vulnerability Handling. - NIST SP 800-115 \u2014 Technical Guide to Information Security Testing. - CISA KEV and CVE\u00ae Program standards. 3. Researcher Agreement \u00b6 Before using any exploit-related features: - Users must accept the Responsible Use Agreement (RUA) . - Every PoC download or execution is logged. - No redistribution of PoC code outside permitted licensing (e.g., GPL, BSD). \ud83e\uddf1 PoC Lifecycle Management \u00b6 Stage Description Acquisition PoC fetched from trusted sources (Exploit-DB, Vulners, GitHub PoC, OSV). Verification Signature/hash validated; metadata normalized. Classification Marked as local, remote, DoS, or privilege escalation. Sandbox Execution Run only in isolated containers or VMs. Audit Logging Execution parameters, user, and timestamp recorded. Retention & Purge Expired PoCs automatically archived or deleted. \ud83e\udde9 Storage & Metadata \u00b6 PoCs are stored as immutable artifacts under the internal resource store: %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/resources/poc/\"] B[\"CVE-2024-12345/\"] C[\"exploit.py\"] D[\"metadata.json\"] E[\"README.md\"] F[\"CVE-2023-98765/\"] A --> B A --> F B --> C B --> D B --> E Example metadata.json \u00b6 { \"cve\": \"CVE-2024-12345\", \"source\": \"exploit-db\", \"url\": \"https://www.exploit-db.com/exploits/52341\", \"verified\": true, \"language\": \"python\", \"type\": \"rce\", \"hash\": \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\", \"sandbox_only\": true, \"license\": \"GPLv2\", \"date_fetched\": \"2025-10-06T10:20:00Z\" } \ud83d\udd12 Sandbox Execution Architecture \u00b6 1. Isolation Model \u00b6 All PoC executions occur within the SecFlow Sandbox Runtime, implemented as: Docker container with restricted capabilities ( --cap-drop=ALL ). Read-only mount of PoC files. No network egress unless explicitly allowed. Time & memory quotas enforced by cgroups. 2. Runtime Diagram \u00b6 +-------------------------------------------------------+ | SecFlow Sandbox Runtime | |-------------------------------------------------------| | - Namespace Isolation (PID, NET, MNT) | | - Read-only FS for /poc | | - IPC + SYS_ADMIN disabled | | - AppArmor / SELinux profiles | | - Seccomp filters (deny dangerous syscalls) | +-------------------------------------------------------+ \u2191 | PoC artifact + parameters | [SecFlow Worker] \u2192 [Sandbox Orchestrator] \u2192 [Container Runtime] 3. Sample Sandbox Invocation \u00b6 SecFlow sandbox run poc CVE-2024-12345 --target https://staging.example.com Under the hood: subprocess.run([ \"docker\", \"run\", \"--rm\", \"--network\", \"none\", \"--memory\", \"512m\", \"--cpus\", \"1\", \"-v\", \"/pocstore/CVE-2024-12345:/poc:ro\", \"SecFlow-sandbox:latest\", \"python3\", \"/poc/exploit.py\", \"--target\", \"https://staging.example.com\" ]) \u2699\ufe0f PoC Execution Policy \u00b6 Policy Enforcement Sandbox Only No PoC runs on host system. Read-only Filesystem Prevents code modification or persistence. No Network by Default All outbound connections blocked. User Authorization Each run signed with user identity & timestamp. Logging & Replay Stdout/stderr captured in audit logs. Time-Bound Execution Hard kill if runtime exceeds timeout_seconds . \ud83e\udde0 Policy Configuration Example \u00b6 # ~/.SecFlow/policies/poc.yaml sandbox: image: SecFlow-sandbox:latest max_cpu: 1 max_memory_mb: 512 timeout_seconds: 300 allow_network: false allow_filesystem_write: false compliance: require_disclaimer: true require_project_authorization: true auto_verify_hashes: true \ud83e\udde9 Governance Logging \u00b6 Every PoC-related event is appended to a tamper-resistant audit log: Field Description event_id UUID of the audit entry user_id Executing user timestamp UTC ISO-8601 time action e.g., \"sandbox_run\", \"download\", \"verify\" cve_id Related CVE tool Source or wrapper (e.g., \"exploitdb\") sandbox_id Container identifier hash SHA256 of PoC code Example Log Entry \u00b6 { \"event_id\": \"a1f3b7c2-9f11-45d0-bc97-6c9472cbdcb2\", \"user_id\": \"hernan\", \"action\": \"sandbox_run\", \"cve_id\": \"CVE-2024-12345\", \"timestamp\": \"2025-10-06T10:43:00Z\", \"sandbox_id\": \"sandbox-83214\", \"hash\": \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\", \"tool\": \"exploitdb\" } \ud83e\udde0 Legal Notice Enforcement \u00b6 Before any PoC interaction, users must sign a legal disclaimer: Responsible Use Notice: You acknowledge that PoC exploitation is to be performed exclusively on systems you own or are explicitly authorized to test. SecFlow is not liable for any misuse or damages resulting from unauthorized use. The system stores an acceptance hash: ~/.SecFlow/.disclaimer_accepted \ud83e\udde9 Cross-Project PoC Access \u00b6 To prevent accidental disclosure: - PoC artifacts are scoped per project by default. - Shared PoCs require explicit admin approval. - Access controlled via role-based permissions (RBAC). Role Access Admin Global & project PoCs Analyst Project PoCs only Viewer Read-only (metadata only) \ud83e\uddf1 PoC Verification Workflow \u00b6 graph TD A[Download PoC] --> B[Verify Hash & Signature] B --> C{Trusted Source?} C -->|Yes| D[Mark Verified] C -->|No| E[Quarantine] E --> F[Manual Review Required] D --> G[Available for Sandbox Run] \ud83d\udd12 Quarantine Mechanism \u00b6 Unknown or tampered PoCs are moved to: ~/.SecFlow/resources/poc/quarantine/ Each is tagged with a quarantine reason. Admins can review and promote back to verified status. def quarantine_poc(poc_id: str, reason: str): shutil.move(f\"/pocstore/{poc_id}\", \"/pocstore/quarantine/\") write_log(f\"PoC {poc_id} quarantined: {reason}\") \ud83e\udde0 Example Execution Trace \u00b6 [PoC: CVE-2024-12345] Verified source: ExploitDB [Sandbox] Starting container SecFlow-sandbox:latest [Sandbox] CPU quota: 1 core, Memory: 512MB [Sandbox] Network: disabled [Output] [+] Exploit successful: remote command executed [Cleanup] PoC container destroyed after 120s \ud83d\udd2e Future Enhancements \u00b6 AppArmor enforcement policies with dynamic runtime profiling. PoC provenance blockchain for cryptographic integrity. AI-assisted PoC safety classification (RCE, DoS, PrivEsc). Multi-tenant isolation for collaborative workspaces. Live replay of PoC runs for training and documentation. Next: Garbage Collection & Data Retention Policy","title":"PoC Sources & Legal"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#14-poc-governance-safety-and-legal-framework","text":"","title":"14 \u2014 PoC Governance, Safety, and Legal Framework"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#overview","text":"Proof-of-Concept (PoC) execution is the most sensitive aspect of the SecFlow platform. While enrichment and discovery involve data collection, PoC modules may trigger actual exploits \u2014 often on controlled test targets or lab environments. This section defines: - The security model for PoC management and execution. - Legal and ethical boundaries for researchers. - Technical enforcement of sandbox isolation. - Policy hierarchy for PoC lifecycle, provenance, and deletion.","title":"\ud83e\udded Overview"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#legal-ethical-framework","text":"","title":"\u2696\ufe0f Legal &amp; Ethical Framework"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#1-authorized-testing-only","text":"PoC execution is strictly limited to: - Assets owned or explicitly authorized by the user. - Lab or offline sandbox targets. - Non-production systems unless a signed authorization exists.","title":"1. Authorized Testing Only"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#2-compliance-with-international-norms","text":"SecFlow aligns with: - ISO/IEC 29147 \u2014 Vulnerability Disclosure. - ISO/IEC 30111 \u2014 Vulnerability Handling. - NIST SP 800-115 \u2014 Technical Guide to Information Security Testing. - CISA KEV and CVE\u00ae Program standards.","title":"2. Compliance with International Norms"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#3-researcher-agreement","text":"Before using any exploit-related features: - Users must accept the Responsible Use Agreement (RUA) . - Every PoC download or execution is logged. - No redistribution of PoC code outside permitted licensing (e.g., GPL, BSD).","title":"3. Researcher Agreement"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#poc-lifecycle-management","text":"Stage Description Acquisition PoC fetched from trusted sources (Exploit-DB, Vulners, GitHub PoC, OSV). Verification Signature/hash validated; metadata normalized. Classification Marked as local, remote, DoS, or privilege escalation. Sandbox Execution Run only in isolated containers or VMs. Audit Logging Execution parameters, user, and timestamp recorded. Retention & Purge Expired PoCs automatically archived or deleted.","title":"\ud83e\uddf1 PoC Lifecycle Management"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#storage-metadata","text":"PoCs are stored as immutable artifacts under the internal resource store: %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/resources/poc/\"] B[\"CVE-2024-12345/\"] C[\"exploit.py\"] D[\"metadata.json\"] E[\"README.md\"] F[\"CVE-2023-98765/\"] A --> B A --> F B --> C B --> D B --> E","title":"\ud83e\udde9 Storage &amp; Metadata"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#example-metadatajson","text":"{ \"cve\": \"CVE-2024-12345\", \"source\": \"exploit-db\", \"url\": \"https://www.exploit-db.com/exploits/52341\", \"verified\": true, \"language\": \"python\", \"type\": \"rce\", \"hash\": \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\", \"sandbox_only\": true, \"license\": \"GPLv2\", \"date_fetched\": \"2025-10-06T10:20:00Z\" }","title":"Example metadata.json"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#sandbox-execution-architecture","text":"","title":"\ud83d\udd12 Sandbox Execution Architecture"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#1-isolation-model","text":"All PoC executions occur within the SecFlow Sandbox Runtime, implemented as: Docker container with restricted capabilities ( --cap-drop=ALL ). Read-only mount of PoC files. No network egress unless explicitly allowed. Time & memory quotas enforced by cgroups.","title":"1. Isolation Model"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#2-runtime-diagram","text":"+-------------------------------------------------------+ | SecFlow Sandbox Runtime | |-------------------------------------------------------| | - Namespace Isolation (PID, NET, MNT) | | - Read-only FS for /poc | | - IPC + SYS_ADMIN disabled | | - AppArmor / SELinux profiles | | - Seccomp filters (deny dangerous syscalls) | +-------------------------------------------------------+ \u2191 | PoC artifact + parameters | [SecFlow Worker] \u2192 [Sandbox Orchestrator] \u2192 [Container Runtime]","title":"2. Runtime Diagram"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#3-sample-sandbox-invocation","text":"SecFlow sandbox run poc CVE-2024-12345 --target https://staging.example.com Under the hood: subprocess.run([ \"docker\", \"run\", \"--rm\", \"--network\", \"none\", \"--memory\", \"512m\", \"--cpus\", \"1\", \"-v\", \"/pocstore/CVE-2024-12345:/poc:ro\", \"SecFlow-sandbox:latest\", \"python3\", \"/poc/exploit.py\", \"--target\", \"https://staging.example.com\" ])","title":"3. Sample Sandbox Invocation"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#poc-execution-policy","text":"Policy Enforcement Sandbox Only No PoC runs on host system. Read-only Filesystem Prevents code modification or persistence. No Network by Default All outbound connections blocked. User Authorization Each run signed with user identity & timestamp. Logging & Replay Stdout/stderr captured in audit logs. Time-Bound Execution Hard kill if runtime exceeds timeout_seconds .","title":"\u2699\ufe0f PoC Execution Policy"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#policy-configuration-example","text":"# ~/.SecFlow/policies/poc.yaml sandbox: image: SecFlow-sandbox:latest max_cpu: 1 max_memory_mb: 512 timeout_seconds: 300 allow_network: false allow_filesystem_write: false compliance: require_disclaimer: true require_project_authorization: true auto_verify_hashes: true","title":"\ud83e\udde0 Policy Configuration Example"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#governance-logging","text":"Every PoC-related event is appended to a tamper-resistant audit log: Field Description event_id UUID of the audit entry user_id Executing user timestamp UTC ISO-8601 time action e.g., \"sandbox_run\", \"download\", \"verify\" cve_id Related CVE tool Source or wrapper (e.g., \"exploitdb\") sandbox_id Container identifier hash SHA256 of PoC code","title":"\ud83e\udde9 Governance Logging"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#example-log-entry","text":"{ \"event_id\": \"a1f3b7c2-9f11-45d0-bc97-6c9472cbdcb2\", \"user_id\": \"hernan\", \"action\": \"sandbox_run\", \"cve_id\": \"CVE-2024-12345\", \"timestamp\": \"2025-10-06T10:43:00Z\", \"sandbox_id\": \"sandbox-83214\", \"hash\": \"b3f9f74e89e2a1b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0\", \"tool\": \"exploitdb\" }","title":"Example Log Entry"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#legal-notice-enforcement","text":"Before any PoC interaction, users must sign a legal disclaimer: Responsible Use Notice: You acknowledge that PoC exploitation is to be performed exclusively on systems you own or are explicitly authorized to test. SecFlow is not liable for any misuse or damages resulting from unauthorized use. The system stores an acceptance hash: ~/.SecFlow/.disclaimer_accepted","title":"\ud83e\udde0 Legal Notice Enforcement"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#cross-project-poc-access","text":"To prevent accidental disclosure: - PoC artifacts are scoped per project by default. - Shared PoCs require explicit admin approval. - Access controlled via role-based permissions (RBAC). Role Access Admin Global & project PoCs Analyst Project PoCs only Viewer Read-only (metadata only)","title":"\ud83e\udde9 Cross-Project PoC Access"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#poc-verification-workflow","text":"graph TD A[Download PoC] --> B[Verify Hash & Signature] B --> C{Trusted Source?} C -->|Yes| D[Mark Verified] C -->|No| E[Quarantine] E --> F[Manual Review Required] D --> G[Available for Sandbox Run]","title":"\ud83e\uddf1 PoC Verification Workflow"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#quarantine-mechanism","text":"Unknown or tampered PoCs are moved to: ~/.SecFlow/resources/poc/quarantine/ Each is tagged with a quarantine reason. Admins can review and promote back to verified status. def quarantine_poc(poc_id: str, reason: str): shutil.move(f\"/pocstore/{poc_id}\", \"/pocstore/quarantine/\") write_log(f\"PoC {poc_id} quarantined: {reason}\")","title":"\ud83d\udd12 Quarantine Mechanism"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#example-execution-trace","text":"[PoC: CVE-2024-12345] Verified source: ExploitDB [Sandbox] Starting container SecFlow-sandbox:latest [Sandbox] CPU quota: 1 core, Memory: 512MB [Sandbox] Network: disabled [Output] [+] Exploit successful: remote command executed [Cleanup] PoC container destroyed after 120s","title":"\ud83e\udde0 Example Execution Trace"},{"location":"architecture/14-poc-sources-and-legal-guidelines/#future-enhancements","text":"AppArmor enforcement policies with dynamic runtime profiling. PoC provenance blockchain for cryptographic integrity. AI-assisted PoC safety classification (RCE, DoS, PrivEsc). Multi-tenant isolation for collaborative workspaces. Live replay of PoC runs for training and documentation. Next: Garbage Collection & Data Retention Policy","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/15-garbage-collection-and-retention/","text":"15 \u2014 Garbage Collection & Data Retention Policy \u00b6 \ud83e\udded Overview \u00b6 The Garbage Collection (GC) subsystem ensures long-term maintainability and performance of the SecFlow platform. It automates the cleanup of: - Expired runs and temporary results - Deleted project data (findings, resources, logs) - Old cache entries and transient files - Obsolete resources (e.g., old templates or wordlists) GC operates as an asynchronous background service within the worker app and supports both soft-delete and hard-delete modes. \u2699\ufe0f Core Objectives \u00b6 Objective Description Consistency Prevent orphaned or dangling records (runs, findings). Recoverability Allow temporary undo (via soft delete + TTL). Auditability Maintain logs of every GC event for compliance. Efficiency Keep database and disk footprint minimal. Isolation Per-project retention scopes (independent cleanup). \ud83e\udde9 Architecture Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Worker App<br/>- GC Scheduler (Celery Beat)<br/>- GC Worker (Async Cleanup Tasks)<br/>- Retention Policy Manager<br/>- Audit Trail Logger\"] B[\"Persistent Storage Layer<br/>- DB Tables: projects, runs, findings, logs<br/>- FS Paths: /resources, /cache, /artifacts\"] A --> B \ud83e\udde9 Retention Model \u00b6 Each project defines its retention profile in ~/.SecFlow/projects/<id>/config.yaml : retention: findings_ttl_days: 180 runs_ttl_days: 90 cache_ttl_days: 30 artifacts_ttl_days: 180 soft_delete_ttl_days: 14 auto_cleanup: true \ud83e\uddf1 Data Lifecycle \u00b6 Stage Description Active Data used by ongoing projects or workflows. Soft Deleted Marked for deletion but restorable ( flag: deleted=true ). Expired TTL exceeded; scheduled for cleanup. Hard Deleted Permanently removed after grace period. \ud83e\udde0 Database-Level Soft Delete \u00b6 class BaseModel(SQLModel): id: UUID created_at: datetime updated_at: datetime deleted: bool = False deleted_at: Optional[datetime] = None When a record is soft-deleted: def soft_delete(obj): obj.deleted = True obj.deleted_at = datetime.utcnow() session.add(obj) session.commit() Recovery: def restore(obj): obj.deleted = False obj.deleted_at = None session.commit() \ud83e\udde9 File System Garbage Collector \u00b6 Directory Structure \u00b6 /data/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/\"] B[\"projects/\"] C[\"<project_id>/\"] D[\"findings/\"] E[\"runs/\"] F[\"logs/\"] G[\"artifacts/\"] H[\"cache/\"] I[\"tmp/\"] A --> B A --> H A --> I B --> C C --> D C --> E C --> F C --> G The GC worker traverses these trees periodically: def sweep_directory(base_path: Path, older_than: timedelta): now = datetime.utcnow() for p in base_path.rglob(\"*\"): if p.is_file() and (now - datetime.fromtimestamp(p.stat().st_mtime)) > older_than: p.unlink() \ud83e\udde0 GC Task Scheduling \u00b6 Celery Task Definition \u00b6 @app.task(name=\"gc.cleanup_expired\") def cleanup_expired(): sweep_projects() sweep_cache() sweep_artifacts() Scheduler Configuration \u00b6 CELERY_BEAT_SCHEDULE = { \"cleanup-every-6h\": { \"task\": \"gc.cleanup_expired\", \"schedule\": crontab(hour=\"*/6\"), }, } GC tasks can be triggered manually: SecFlow gc run --project acme-api \ud83e\udde9 Retention Policy Evaluation \u00b6 Example Policy Rules \u00b6 Rule Condition Action Inactive Runs Run ended > 90 days ago Delete run logs Soft-Deleted Findings Deleted > 14 days ago Purge permanently Cache Expired Cache entry older than 30 days Remove Unused Artifacts Artifact not accessed for 180 days Archive or delete Policy Engine Snippet \u00b6 def evaluate_retention(entity, policy): if entity.deleted and expired(entity.deleted_at, policy.soft_delete_ttl_days): hard_delete(entity) elif expired(entity.updated_at, policy.findings_ttl_days): soft_delete(entity) \ud83e\udde9 Audit Logging for GC \u00b6 Each GC operation generates an audit record: { \"event\": \"gc_delete\", \"type\": \"finding\", \"target_id\": \"f123-45ac\", \"project_id\": \"p001\", \"timestamp\": \"2025-10-06T09:30:00Z\", \"user\": \"system\", \"ttl_rule\": \"soft_delete_ttl_days=14\" } Stored in: ~/.SecFlow/audit/gc.log \ud83e\uddf1 Orphan Detection \u00b6 SQL Example \u00b6 SELECT f.id FROM findings f LEFT JOIN runs r ON f.run_id = r.id WHERE r.id IS NULL; Any orphaned findings or artifacts (without associated runs/projects) are purged automatically. \ud83e\udde9 Cache Lifecycle \u00b6 Caches (e.g., CVE data, scan results, tool logs) use a standardized interface: class CacheEntry(BaseModel): key: str value: bytes expires_at: datetime def purge_expired(): session.query(CacheEntry).filter(CacheEntry.expires_at < datetime.utcnow()).delete() \ud83e\udde0 Manual Cleanup Command \u00b6 Users can trigger GC manually via CLI: # Run full cleanup (all projects) SecFlow gc run # Run cleanup for one project SecFlow gc run --project acme-api # Preview what will be deleted SecFlow gc dry-run Example output: \u00b6 [GC] Found 12 expired runs, 4 orphaned findings, 6 stale cache entries [GC] Total reclaimed: 1.2 GB \ud83d\udd10 Security Considerations \u00b6 All deletions (soft or hard) are logged. Data is never removed without audit trace. System prevents GC while a project is locked or running. Manual GC requires Admin role. \ud83d\udd04 GC Metrics & Observability \u00b6 Metric Description gc_runs_total Number of GC cycles executed gc_files_removed_total Number of files deleted gc_bytes_reclaimed_total Storage reclaimed in bytes gc_duration_seconds Time per GC cycle gc_errors_total Failed cleanup operations Exposed via Prometheus at /metrics . \ud83e\udde0 Example GC Cycle Log \u00b6 [GC] Cycle started at 2025-10-06T09:00:00Z [GC] Processed 3 projects [GC] Deleted 15 findings (soft) [GC] Purged 10 runs (hard) [GC] Reclaimed 1.8GB disk space [GC] Cycle completed in 42.3s \ud83d\udd2e Future Enhancements \u00b6 Incremental snapshot pruning \u2014 keep only latest versions of runs. Policy-as-Code \u2014 customizable YAML rulesets. AI-based cleanup predictions \u2014 identify stale datasets dynamically. Storage quota enforcement per user/project. UI dashboard for retention and GC insights. Next: Security Model (RBAC, Authentication, Sandboxing)","title":"Garbage Collection"},{"location":"architecture/15-garbage-collection-and-retention/#15-garbage-collection-data-retention-policy","text":"","title":"15 \u2014 Garbage Collection &amp; Data Retention Policy"},{"location":"architecture/15-garbage-collection-and-retention/#overview","text":"The Garbage Collection (GC) subsystem ensures long-term maintainability and performance of the SecFlow platform. It automates the cleanup of: - Expired runs and temporary results - Deleted project data (findings, resources, logs) - Old cache entries and transient files - Obsolete resources (e.g., old templates or wordlists) GC operates as an asynchronous background service within the worker app and supports both soft-delete and hard-delete modes.","title":"\ud83e\udded Overview"},{"location":"architecture/15-garbage-collection-and-retention/#core-objectives","text":"Objective Description Consistency Prevent orphaned or dangling records (runs, findings). Recoverability Allow temporary undo (via soft delete + TTL). Auditability Maintain logs of every GC event for compliance. Efficiency Keep database and disk footprint minimal. Isolation Per-project retention scopes (independent cleanup).","title":"\u2699\ufe0f Core Objectives"},{"location":"architecture/15-garbage-collection-and-retention/#architecture-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Worker App<br/>- GC Scheduler (Celery Beat)<br/>- GC Worker (Async Cleanup Tasks)<br/>- Retention Policy Manager<br/>- Audit Trail Logger\"] B[\"Persistent Storage Layer<br/>- DB Tables: projects, runs, findings, logs<br/>- FS Paths: /resources, /cache, /artifacts\"] A --> B","title":"\ud83e\udde9 Architecture Diagram"},{"location":"architecture/15-garbage-collection-and-retention/#retention-model","text":"Each project defines its retention profile in ~/.SecFlow/projects/<id>/config.yaml : retention: findings_ttl_days: 180 runs_ttl_days: 90 cache_ttl_days: 30 artifacts_ttl_days: 180 soft_delete_ttl_days: 14 auto_cleanup: true","title":"\ud83e\udde9 Retention Model"},{"location":"architecture/15-garbage-collection-and-retention/#data-lifecycle","text":"Stage Description Active Data used by ongoing projects or workflows. Soft Deleted Marked for deletion but restorable ( flag: deleted=true ). Expired TTL exceeded; scheduled for cleanup. Hard Deleted Permanently removed after grace period.","title":"\ud83e\uddf1 Data Lifecycle"},{"location":"architecture/15-garbage-collection-and-retention/#database-level-soft-delete","text":"class BaseModel(SQLModel): id: UUID created_at: datetime updated_at: datetime deleted: bool = False deleted_at: Optional[datetime] = None When a record is soft-deleted: def soft_delete(obj): obj.deleted = True obj.deleted_at = datetime.utcnow() session.add(obj) session.commit() Recovery: def restore(obj): obj.deleted = False obj.deleted_at = None session.commit()","title":"\ud83e\udde0 Database-Level Soft Delete"},{"location":"architecture/15-garbage-collection-and-retention/#file-system-garbage-collector","text":"","title":"\ud83e\udde9 File System Garbage Collector"},{"location":"architecture/15-garbage-collection-and-retention/#directory-structure","text":"/data/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"~/.SecFlow/\"] B[\"projects/\"] C[\"<project_id>/\"] D[\"findings/\"] E[\"runs/\"] F[\"logs/\"] G[\"artifacts/\"] H[\"cache/\"] I[\"tmp/\"] A --> B A --> H A --> I B --> C C --> D C --> E C --> F C --> G The GC worker traverses these trees periodically: def sweep_directory(base_path: Path, older_than: timedelta): now = datetime.utcnow() for p in base_path.rglob(\"*\"): if p.is_file() and (now - datetime.fromtimestamp(p.stat().st_mtime)) > older_than: p.unlink()","title":"Directory Structure"},{"location":"architecture/15-garbage-collection-and-retention/#gc-task-scheduling","text":"","title":"\ud83e\udde0 GC Task Scheduling"},{"location":"architecture/15-garbage-collection-and-retention/#celery-task-definition","text":"@app.task(name=\"gc.cleanup_expired\") def cleanup_expired(): sweep_projects() sweep_cache() sweep_artifacts()","title":"Celery Task Definition"},{"location":"architecture/15-garbage-collection-and-retention/#scheduler-configuration","text":"CELERY_BEAT_SCHEDULE = { \"cleanup-every-6h\": { \"task\": \"gc.cleanup_expired\", \"schedule\": crontab(hour=\"*/6\"), }, } GC tasks can be triggered manually: SecFlow gc run --project acme-api","title":"Scheduler Configuration"},{"location":"architecture/15-garbage-collection-and-retention/#retention-policy-evaluation","text":"","title":"\ud83e\udde9 Retention Policy Evaluation"},{"location":"architecture/15-garbage-collection-and-retention/#example-policy-rules","text":"Rule Condition Action Inactive Runs Run ended > 90 days ago Delete run logs Soft-Deleted Findings Deleted > 14 days ago Purge permanently Cache Expired Cache entry older than 30 days Remove Unused Artifacts Artifact not accessed for 180 days Archive or delete","title":"Example Policy Rules"},{"location":"architecture/15-garbage-collection-and-retention/#policy-engine-snippet","text":"def evaluate_retention(entity, policy): if entity.deleted and expired(entity.deleted_at, policy.soft_delete_ttl_days): hard_delete(entity) elif expired(entity.updated_at, policy.findings_ttl_days): soft_delete(entity)","title":"Policy Engine Snippet"},{"location":"architecture/15-garbage-collection-and-retention/#audit-logging-for-gc","text":"Each GC operation generates an audit record: { \"event\": \"gc_delete\", \"type\": \"finding\", \"target_id\": \"f123-45ac\", \"project_id\": \"p001\", \"timestamp\": \"2025-10-06T09:30:00Z\", \"user\": \"system\", \"ttl_rule\": \"soft_delete_ttl_days=14\" } Stored in: ~/.SecFlow/audit/gc.log","title":"\ud83e\udde9 Audit Logging for GC"},{"location":"architecture/15-garbage-collection-and-retention/#orphan-detection","text":"","title":"\ud83e\uddf1 Orphan Detection"},{"location":"architecture/15-garbage-collection-and-retention/#sql-example","text":"SELECT f.id FROM findings f LEFT JOIN runs r ON f.run_id = r.id WHERE r.id IS NULL; Any orphaned findings or artifacts (without associated runs/projects) are purged automatically.","title":"SQL Example"},{"location":"architecture/15-garbage-collection-and-retention/#cache-lifecycle","text":"Caches (e.g., CVE data, scan results, tool logs) use a standardized interface: class CacheEntry(BaseModel): key: str value: bytes expires_at: datetime def purge_expired(): session.query(CacheEntry).filter(CacheEntry.expires_at < datetime.utcnow()).delete()","title":"\ud83e\udde9 Cache Lifecycle"},{"location":"architecture/15-garbage-collection-and-retention/#manual-cleanup-command","text":"Users can trigger GC manually via CLI: # Run full cleanup (all projects) SecFlow gc run # Run cleanup for one project SecFlow gc run --project acme-api # Preview what will be deleted SecFlow gc dry-run","title":"\ud83e\udde0 Manual Cleanup Command"},{"location":"architecture/15-garbage-collection-and-retention/#example-output","text":"[GC] Found 12 expired runs, 4 orphaned findings, 6 stale cache entries [GC] Total reclaimed: 1.2 GB","title":"Example output:"},{"location":"architecture/15-garbage-collection-and-retention/#security-considerations","text":"All deletions (soft or hard) are logged. Data is never removed without audit trace. System prevents GC while a project is locked or running. Manual GC requires Admin role.","title":"\ud83d\udd10 Security Considerations"},{"location":"architecture/15-garbage-collection-and-retention/#gc-metrics-observability","text":"Metric Description gc_runs_total Number of GC cycles executed gc_files_removed_total Number of files deleted gc_bytes_reclaimed_total Storage reclaimed in bytes gc_duration_seconds Time per GC cycle gc_errors_total Failed cleanup operations Exposed via Prometheus at /metrics .","title":"\ud83d\udd04 GC Metrics &amp; Observability"},{"location":"architecture/15-garbage-collection-and-retention/#example-gc-cycle-log","text":"[GC] Cycle started at 2025-10-06T09:00:00Z [GC] Processed 3 projects [GC] Deleted 15 findings (soft) [GC] Purged 10 runs (hard) [GC] Reclaimed 1.8GB disk space [GC] Cycle completed in 42.3s","title":"\ud83e\udde0 Example GC Cycle Log"},{"location":"architecture/15-garbage-collection-and-retention/#future-enhancements","text":"Incremental snapshot pruning \u2014 keep only latest versions of runs. Policy-as-Code \u2014 customizable YAML rulesets. AI-based cleanup predictions \u2014 identify stale datasets dynamically. Storage quota enforcement per user/project. UI dashboard for retention and GC insights. Next: Security Model (RBAC, Authentication, Sandboxing)","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/16-security-model/","text":"16 \u2014 Security Model (RBAC, Authentication, and Sandboxing) \u00b6 \ud83e\udded Overview \u00b6 Security is a first-class design goal in the SecFlow architecture. Every user action, process execution, and data mutation is subject to authentication, authorization, and sandboxing. This model ensures: - Strict access separation between users, projects, and data. - Controlled execution of potentially dangerous tools or PoCs. - Immutable auditability of all operations. - Zero-trust posture inside the orchestration environment. \ud83e\udde9 Layers of the Security Model \u00b6 Layer Scope Mechanism Authentication User identity verification JWT / OAuth2 tokens Authorization (RBAC) Access control Role-based & scope-aware Execution Sandboxing Process isolation Containerized runners Secrets Management API keys, credentials Encrypted vault Audit Logging Accountability Immutable append-only logs \ud83e\udde0 Authentication Architecture \u00b6 SecFlow supports two auth models: 1. Local Auth (default): For standalone or on-premise deployments. 2. Federated Auth (optional): OAuth2/OpenID Connect integration (GitHub, Google, Azure AD). Token Model \u00b6 { \"sub\": \"hernan\", \"role\": \"admin\", \"exp\": 1738783200, \"projects\": [\"proj-01\", \"proj-02\"] } Token Flow Diagram \u00b6 [User Login] \u2192 [Auth Provider] \u2192 [JWT Issued] \u2192 [API Gateway] \u2192 [SecFlow Web/API] Each request to /api/* must include: Authorization: Bearer <token> Tokens are verified by the API middleware using RS256 signature validation. \ud83e\udde9 Role-Based Access Control (RBAC) \u00b6 Roles define the scope of capabilities across the platform. Role Permissions Admin Full control \u2014 manage tools, users, projects, retention, policies. Analyst Execute workflows, triage findings, view reports, limited editing. Viewer Read-only access to results and dashboards. Automation (Service) Used by background tasks (limited scoped tokens). Example permission matrix: \u00b6 Action Admin Analyst Viewer Service Run workflow \u2705 \u2705 \u274c \u2705 Modify tool config \u2705 \u274c \u274c \u274c View findings \u2705 \u2705 \u2705 \u2705 Delete project \u2705 \u274c \u274c \u274c Access PoCs \u2705 \u2705 \u274c \u274c Run GC tasks \u2705 \u274c \u274c \u2705 \u2699\ufe0f Policy Enforcement \u00b6 Every endpoint and command passes through an Access Policy Filter: def authorize(action: str, user: User, project: Optional[str] = None): if not user.has_permission(action, project): raise HTTPException(403, detail=f\"Forbidden: {action}\") Example route decorator: \u00b6 @app.get(\"/api/v1/findings\") @require_role([\"admin\", \"analyst\", \"viewer\"]) def list_findings(): def authorize(self, action: str, user: User, project: Optional[str] = None) -> bool: \"\"\"Check if user is authorized for action.\"\"\" return user.has_permission(action, project) \ud83e\uddf1 Secrets Management \u00b6 Secret Types \u00b6 API tokens for external tools (e.g., Shodan, Vulners) Private SSH keys for remote scans Encrypted credentials for authenticated targets Storage Backend \u00b6 All secrets are stored in the SecFlow Vault, an encrypted JSON database backed by Fernet (AES-256). class SecretVault: def __init__(self, keyfile: Path): self.key = load_key(keyfile) def store(self, id: str, data: dict): enc = fernet_encrypt(json.dumps(data), self.key) write_file(f\"/vault/{id}.enc\", enc) Secrets CLI \u00b6 SecFlow secrets add nuclei_api --value \"TOKEN123\" SecFlow secrets list SecFlow secrets remove nuclei_api All access is scoped by user and project context. \ud83d\udd12 Execution Sandboxing \u00b6 All scanner and PoC executions run inside restricted containers or subprocess jails. Isolation Techniques \u00b6 Mechanism Purpose Namespaces (PID, NET, MNT) Process isolation Seccomp Filters Syscall restriction cgroups v2 CPU/memory limits No-root UID mapping Drops privileges AppArmor profiles File access control Read-only FS Prevents persistence Example \u00b6 docker run --rm --cap-drop=ALL \\ --security-opt=no-new-privileges \\ --read-only -m 512m --cpus=1 \\ SecFlow-runner:latest nuclei -t /templates \ud83e\udde9 Network & Data Security \u00b6 Channel Encryption Notes API <-> UI HTTPS (TLS 1.3) Strict transport enforced Worker <-> API Mutual TLS Each worker has its own cert File Sync AES-256 encrypted Optional compression Database At-rest encryption SQLite: SEE, Postgres: TDE Audit Logs Signed + timestamped Prevents tampering \ud83e\udde0 Secure Inter-Process Communication \u00b6 Internal apps communicate via ZeroMQ or Redis queues over TLS. Each message includes a signed envelope: { \"msg_id\": \"uuid\", \"issuer\": \"worker-1\", \"signature\": \"HMAC-SHA256\", \"payload\": {\"data\": \"encrypted_content\"} } This ensures authenticity and non-repudiation. \ud83e\udde9 Security Hooks & Middleware \u00b6 SecFlow injects middleware for: - Request validation (pydantic schemas) - JWT token expiry verification - Role validation (fast-path lookup) - Audit log writing after each mutating operation Example: \u00b6 @app.middleware(\"http\") async def audit_request(request, call_next): response = await call_next(request) if request.method in (\"POST\", \"DELETE\", \"PATCH\"): log_audit(request, response) return response \ud83d\udd10 Audit Trail & Tamper Resistance \u00b6 Log Format \u00b6 { \"timestamp\": \"2025-10-06T09:40:00Z\", \"user\": \"hernan\", \"action\": \"workflow_run\", \"project\": \"api-audit\", \"status\": \"success\", \"hash\": \"sha256:0a3b1c2d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2\" } Storage & Verification \u00b6 Logs stored as JSON lines under /audit/ Each file signed with an HMAC chain: H_i = HMAC(H_prev + log_i, key) Immutable and verifiable chain-of-trust. \ud83e\udde0 Security Monitoring \u00b6 Metric Description auth_failures_total Failed login attempts sandbox_executions_total Containers spawned policy_violations_total Unauthorized actions vault_accesses_total Secret retrievals audit_events_total Log entries recorded \u2699\ufe0f Compliance Framework Alignment \u00b6 SecFlow's security architecture aligns with: Framework Compliance Area NIST SP 800-53 Access control, auditing, system protection ISO/IEC 27001 Information security management OWASP SAMM Secure software development lifecycle MITRE ATT&CK Mapping detection behaviors GDPR Art. 32 Data confidentiality and integrity \ud83d\udd12 Key Rotation & Secrets Expiry \u00b6 Secrets have explicit TTLs (default: 180 days). Vault rotation command: SecFlow vault rotate Rotation regenerates the encryption key and re-encrypts all entries. \ud83e\udde9 Example Access Workflow \u00b6 [Analyst] \u2192 [Login via OIDC] \u2192 [JWT Issued] \u2192 [UI/API Gateway] \u2192 [RBAC Policy Check] \u2192 [Worker Executes Workflow in Sandbox] \u2192 [Findings Logged + Audit Entry Created] \ud83d\udd2e Future Enhancements \u00b6 Hardware-backed encryption (YubiKey / TPM) for Vault keys. FIPS-compliant container sandbox. Behavior-based anomaly detection on audit logs. Single Sign-On (SSO) with just-in-time role provisioning. Runtime policy engine (OPA / Rego integration). Next: Observability, Logging & Metrics","title":"Security Model"},{"location":"architecture/16-security-model/#16-security-model-rbac-authentication-and-sandboxing","text":"","title":"16 \u2014 Security Model (RBAC, Authentication, and Sandboxing)"},{"location":"architecture/16-security-model/#overview","text":"Security is a first-class design goal in the SecFlow architecture. Every user action, process execution, and data mutation is subject to authentication, authorization, and sandboxing. This model ensures: - Strict access separation between users, projects, and data. - Controlled execution of potentially dangerous tools or PoCs. - Immutable auditability of all operations. - Zero-trust posture inside the orchestration environment.","title":"\ud83e\udded Overview"},{"location":"architecture/16-security-model/#layers-of-the-security-model","text":"Layer Scope Mechanism Authentication User identity verification JWT / OAuth2 tokens Authorization (RBAC) Access control Role-based & scope-aware Execution Sandboxing Process isolation Containerized runners Secrets Management API keys, credentials Encrypted vault Audit Logging Accountability Immutable append-only logs","title":"\ud83e\udde9 Layers of the Security Model"},{"location":"architecture/16-security-model/#authentication-architecture","text":"SecFlow supports two auth models: 1. Local Auth (default): For standalone or on-premise deployments. 2. Federated Auth (optional): OAuth2/OpenID Connect integration (GitHub, Google, Azure AD).","title":"\ud83e\udde0 Authentication Architecture"},{"location":"architecture/16-security-model/#token-model","text":"{ \"sub\": \"hernan\", \"role\": \"admin\", \"exp\": 1738783200, \"projects\": [\"proj-01\", \"proj-02\"] }","title":"Token Model"},{"location":"architecture/16-security-model/#token-flow-diagram","text":"[User Login] \u2192 [Auth Provider] \u2192 [JWT Issued] \u2192 [API Gateway] \u2192 [SecFlow Web/API] Each request to /api/* must include: Authorization: Bearer <token> Tokens are verified by the API middleware using RS256 signature validation.","title":"Token Flow Diagram"},{"location":"architecture/16-security-model/#role-based-access-control-rbac","text":"Roles define the scope of capabilities across the platform. Role Permissions Admin Full control \u2014 manage tools, users, projects, retention, policies. Analyst Execute workflows, triage findings, view reports, limited editing. Viewer Read-only access to results and dashboards. Automation (Service) Used by background tasks (limited scoped tokens).","title":"\ud83e\udde9 Role-Based Access Control (RBAC)"},{"location":"architecture/16-security-model/#example-permission-matrix","text":"Action Admin Analyst Viewer Service Run workflow \u2705 \u2705 \u274c \u2705 Modify tool config \u2705 \u274c \u274c \u274c View findings \u2705 \u2705 \u2705 \u2705 Delete project \u2705 \u274c \u274c \u274c Access PoCs \u2705 \u2705 \u274c \u274c Run GC tasks \u2705 \u274c \u274c \u2705","title":"Example permission matrix:"},{"location":"architecture/16-security-model/#policy-enforcement","text":"Every endpoint and command passes through an Access Policy Filter: def authorize(action: str, user: User, project: Optional[str] = None): if not user.has_permission(action, project): raise HTTPException(403, detail=f\"Forbidden: {action}\")","title":"\u2699\ufe0f Policy Enforcement"},{"location":"architecture/16-security-model/#example-route-decorator","text":"@app.get(\"/api/v1/findings\") @require_role([\"admin\", \"analyst\", \"viewer\"]) def list_findings(): def authorize(self, action: str, user: User, project: Optional[str] = None) -> bool: \"\"\"Check if user is authorized for action.\"\"\" return user.has_permission(action, project)","title":"Example route decorator:"},{"location":"architecture/16-security-model/#secrets-management","text":"","title":"\ud83e\uddf1 Secrets Management"},{"location":"architecture/16-security-model/#secret-types","text":"API tokens for external tools (e.g., Shodan, Vulners) Private SSH keys for remote scans Encrypted credentials for authenticated targets","title":"Secret Types"},{"location":"architecture/16-security-model/#storage-backend","text":"All secrets are stored in the SecFlow Vault, an encrypted JSON database backed by Fernet (AES-256). class SecretVault: def __init__(self, keyfile: Path): self.key = load_key(keyfile) def store(self, id: str, data: dict): enc = fernet_encrypt(json.dumps(data), self.key) write_file(f\"/vault/{id}.enc\", enc)","title":"Storage Backend"},{"location":"architecture/16-security-model/#secrets-cli","text":"SecFlow secrets add nuclei_api --value \"TOKEN123\" SecFlow secrets list SecFlow secrets remove nuclei_api All access is scoped by user and project context.","title":"Secrets CLI"},{"location":"architecture/16-security-model/#execution-sandboxing","text":"All scanner and PoC executions run inside restricted containers or subprocess jails.","title":"\ud83d\udd12 Execution Sandboxing"},{"location":"architecture/16-security-model/#isolation-techniques","text":"Mechanism Purpose Namespaces (PID, NET, MNT) Process isolation Seccomp Filters Syscall restriction cgroups v2 CPU/memory limits No-root UID mapping Drops privileges AppArmor profiles File access control Read-only FS Prevents persistence","title":"Isolation Techniques"},{"location":"architecture/16-security-model/#example","text":"docker run --rm --cap-drop=ALL \\ --security-opt=no-new-privileges \\ --read-only -m 512m --cpus=1 \\ SecFlow-runner:latest nuclei -t /templates","title":"Example"},{"location":"architecture/16-security-model/#network-data-security","text":"Channel Encryption Notes API <-> UI HTTPS (TLS 1.3) Strict transport enforced Worker <-> API Mutual TLS Each worker has its own cert File Sync AES-256 encrypted Optional compression Database At-rest encryption SQLite: SEE, Postgres: TDE Audit Logs Signed + timestamped Prevents tampering","title":"\ud83e\udde9 Network &amp; Data Security"},{"location":"architecture/16-security-model/#secure-inter-process-communication","text":"Internal apps communicate via ZeroMQ or Redis queues over TLS. Each message includes a signed envelope: { \"msg_id\": \"uuid\", \"issuer\": \"worker-1\", \"signature\": \"HMAC-SHA256\", \"payload\": {\"data\": \"encrypted_content\"} } This ensures authenticity and non-repudiation.","title":"\ud83e\udde0 Secure Inter-Process Communication"},{"location":"architecture/16-security-model/#security-hooks-middleware","text":"SecFlow injects middleware for: - Request validation (pydantic schemas) - JWT token expiry verification - Role validation (fast-path lookup) - Audit log writing after each mutating operation","title":"\ud83e\udde9 Security Hooks &amp; Middleware"},{"location":"architecture/16-security-model/#example_1","text":"@app.middleware(\"http\") async def audit_request(request, call_next): response = await call_next(request) if request.method in (\"POST\", \"DELETE\", \"PATCH\"): log_audit(request, response) return response","title":"Example:"},{"location":"architecture/16-security-model/#audit-trail-tamper-resistance","text":"","title":"\ud83d\udd10 Audit Trail &amp; Tamper Resistance"},{"location":"architecture/16-security-model/#log-format","text":"{ \"timestamp\": \"2025-10-06T09:40:00Z\", \"user\": \"hernan\", \"action\": \"workflow_run\", \"project\": \"api-audit\", \"status\": \"success\", \"hash\": \"sha256:0a3b1c2d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2\" }","title":"Log Format"},{"location":"architecture/16-security-model/#storage-verification","text":"Logs stored as JSON lines under /audit/ Each file signed with an HMAC chain: H_i = HMAC(H_prev + log_i, key) Immutable and verifiable chain-of-trust.","title":"Storage &amp; Verification"},{"location":"architecture/16-security-model/#security-monitoring","text":"Metric Description auth_failures_total Failed login attempts sandbox_executions_total Containers spawned policy_violations_total Unauthorized actions vault_accesses_total Secret retrievals audit_events_total Log entries recorded","title":"\ud83e\udde0 Security Monitoring"},{"location":"architecture/16-security-model/#compliance-framework-alignment","text":"SecFlow's security architecture aligns with: Framework Compliance Area NIST SP 800-53 Access control, auditing, system protection ISO/IEC 27001 Information security management OWASP SAMM Secure software development lifecycle MITRE ATT&CK Mapping detection behaviors GDPR Art. 32 Data confidentiality and integrity","title":"\u2699\ufe0f Compliance Framework Alignment"},{"location":"architecture/16-security-model/#key-rotation-secrets-expiry","text":"Secrets have explicit TTLs (default: 180 days). Vault rotation command: SecFlow vault rotate Rotation regenerates the encryption key and re-encrypts all entries.","title":"\ud83d\udd12 Key Rotation &amp; Secrets Expiry"},{"location":"architecture/16-security-model/#example-access-workflow","text":"[Analyst] \u2192 [Login via OIDC] \u2192 [JWT Issued] \u2192 [UI/API Gateway] \u2192 [RBAC Policy Check] \u2192 [Worker Executes Workflow in Sandbox] \u2192 [Findings Logged + Audit Entry Created]","title":"\ud83e\udde9 Example Access Workflow"},{"location":"architecture/16-security-model/#future-enhancements","text":"Hardware-backed encryption (YubiKey / TPM) for Vault keys. FIPS-compliant container sandbox. Behavior-based anomaly detection on audit logs. Single Sign-On (SSO) with just-in-time role provisioning. Runtime policy engine (OPA / Rego integration). Next: Observability, Logging & Metrics","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/17-observability-logging-and-metrics/","text":"17 \u2014 Observability, Logging, Metrics & Tracing \u00b6 \ud83e\udded Overview \u00b6 The Observability Layer provides full transparency into the runtime behavior of SecFlow. It enables developers, analysts, and operators to: - Track system health and performance metrics. - Trace workflow execution across tools and workers. - Monitor logs and alerts from all layers (API, Worker, Sandbox). - Correlate findings, runs, and audit trails with unique identifiers. The observability system is standards-based , built upon: - OpenTelemetry (OTel) for tracing and context propagation. - Prometheus for metrics export. - JSON structured logging for event correlation. - Grafana dashboards for visualization. \ud83e\uddf1 Observability Architecture Overview \u00b6 +--------------------------------------------------------------+ | SecFlow Stack | | API Layer \u2192 Structured Logging, Metrics, Tracing | | Worker \u2192 Task Metrics, Workflow Spans, Sandbox Logs | | Sandbox \u2192 Runtime Metrics, Security Telemetry | | Database \u2192 Query Timing, Connection Pool Stats | -------------------------------------------------------------- | Exporters \u2192 Prometheus (Metrics), OTLP (Traces), Loki (Logs)| +--------------------------------------------------------------+ \ud83e\udde9 Logging Subsystem \u00b6 Logging Design Goals \u00b6 Goal Implementation Machine-readable JSON structured format with standard fields Correlated across systems trace_id and span_id included Context-aware User, project, workflow metadata embedded Immutable Append-only, timestamped, HMAC-signed if required Log Record Example \u00b6 { \"timestamp\": \"2025-10-06T09:45:32Z\", \"level\": \"INFO\", \"service\": \"worker\", \"trace_id\": \"cbd82b67-4a2b-4db6-9a90-1c3ed1b7e203\", \"span_id\": \"4f7c2b91\", \"project\": \"acme-api\", \"workflow_id\": \"wf-abc123\", \"message\": \"Nuclei scan completed successfully\", \"duration_ms\": 34215 } Logging Stack \u00b6 Python Logging + Structlog \u2014 base structured logs. OpenTelemetry LoggingHandler \u2014 trace context propagation. Loki Exporter \u2014 for central log aggregation (optional). Configuration snippet: \u00b6 LOGGING = { \"version\": 1, \"formatters\": {\"json\": {\"()\": \"pythonjsonlogger.jsonlogger.JsonFormatter\"}}, \"handlers\": { \"console\": {\"class\": \"logging.StreamHandler\", \"formatter\": \"json\"}, }, \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]}, } \u2699\ufe0f Log Levels & Policies \u00b6 Level Description DEBUG Developer-only context, disabled by default in production. INFO System lifecycle and status messages. WARNING Recoverable issues, retryable errors. ERROR Failures in user-triggered operations. CRITICAL Irrecoverable errors (sandbox isolation breach, DB corruption). Retention policy for logs follows the GC subsystem (see 15-garbage-collection-and-retention.md ). \ud83e\udde0 Trace Propagation & Distributed Context \u00b6 SecFlow uses OpenTelemetry (OTel) for distributed tracing. Every API request, task dispatch, and plugin call generates spans linked under one root trace. Example Trace Structure \u00b6 TraceID: 5b2e4f21c9a344f9 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"api.request (GET /api/v1/workflows)\"] B[\"worker.dispatch\"] C[\"tool.nuclei.run\"] D[\"sandbox.spawn\"] E[\"sandbox.cleanup\"] F[\"tool.ferox.run\"] G[\"findings.persist\"] H[\"api.response\"] A --> B A --> H B --> C B --> F B --> G C --> D C --> E Code Example \u00b6 from opentelemetry import trace tracer = trace.get_tracer(\"SecFlow.worker\") with tracer.start_as_current_span(\"workflow.execute\") as span: span.set_attribute(\"workflow.id\", workflow.id) run_workflow(workflow) All traces are exported through OTLP gRPC to the observability backend (e.g., Tempo, Jaeger). \ud83d\udcca Metrics System \u00b6 SecFlow exposes runtime metrics through Prometheus-compatible endpoints. Default Endpoint \u00b6 /metrics Example Metrics \u00b6 Metric Type Description secflow_requests_total Counter Total API requests handled secflow_active_workflows Gauge Currently running workflows secflow_findings_generated_total Counter Findings created secflow_task_duration_seconds Histogram Time taken by async tasks secflow_gc_bytes_reclaimed_total Counter GC reclaimed bytes secflow_sandbox_executions_total Counter Number of sandbox runs secflow_tool_failures_total Counter Failed tool executions secflow_worker_queue_depth Gauge Pending Celery tasks secflow_cve_enrichment_latency_seconds Histogram Time per CVE query Prometheus Export Example \u00b6 from prometheus_client import Counter, Gauge findings_total = Counter(\"secflow_findings_generated_total\", \"Number of findings created\") active_workflows = Gauge(\"secflow_active_workflows\", \"Currently running workflows\") \ud83d\udd0d Example Grafana Dashboard Panels \u00b6 Panel Visualization Query Workflow Throughput Time series rate(secflow_requests_total[5m]) Average Scan Duration Histogram histogram_quantile(0.9, rate(secflow_task_duration_seconds_bucket[5m])) Findings per Project Bar chart sum by (project)(secflow_findings_generated_total) GC Efficiency SingleStat rate(secflow_gc_bytes_reclaimed_total[1h]) Sandbox Failures Table secflow_tool_failures_total \ud83e\udde9 Error Correlation & Incident Debugging \u00b6 Every finding, workflow, and audit entry includes a trace ID. Errors can be traced back to exact processes and spans. Example correlation: \u00b6 Finding \u2192 Workflow ID: wf-abc123 \u2192 Trace ID: cbd82b67a1e4f9d2c8b5e6f7a3d4c9e2b1f8a5c6d7e9f2a3b4c5d6e7f8a9b2c3d4e5f6 \u2192 Logs: worker.log \u2192 Span: tool.nuclei.run This guarantees reproducibility and fast RCA (root cause analysis). \ud83e\udde0 Alerting & Health Checks \u00b6 Health Endpoints \u00b6 Endpoint Description /healthz Liveness probe (basic app status) /readyz Readiness probe (DB + cache + queue connectivity) Example Output \u00b6 { \"status\": \"ok\", \"services\": { \"database\": \"up\", \"cache\": \"up\", \"worker\": \"idle\" } } Alerts (Prometheus Rules) \u00b6 groups: - name: secflow_alerts rules: - alert: HighErrorRate expr: rate(secflow_tool_failures_total[5m]) > 5 for: 10m labels: { severity: warning } annotations: summary: \"Tool failure rate too high\" \ud83d\udd12 Security of Observability Data \u00b6 Concern Mitigation Sensitive logs Field redaction (password, token, secret) Trace integrity HMAC signing of exported spans Log tampering Append-only JSONL + rotation Metrics abuse Authenticated /metrics endpoint (basic token or mutual TLS) Example redaction middleware: \u00b6 def sanitize(data: dict) -> dict: for key in data.keys(): if \"token\" in key.lower() or \"password\" in key.lower(): data[key] = \"[REDACTED]\" return data \ud83e\uddf1 Correlation Example: End-to-End Trace \u00b6 [TRACE 5b2e4f21c9a344f9] %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"/api/v1/workflows/start\"] B[\"worker.queue.dispatch (duration=12ms)\"] C[\"tool.ferox.run (duration=4.1s)\"] D[\"tool.nuclei.run (duration=9.3s)\"] E[\"enrich.cve (duration=1.4s)\"] F[\"findings.persist (duration=320ms)\"] G[\"audit.log (duration=7ms)\"] A --> B A --> C A --> D A --> E A --> F A --> G \ud83e\udde9 Integration with CI/CD and Testing \u00b6 During CI runs: - Unit and integration tests export OTel traces for regression analysis. - Performance tests measure task durations and error rates. Example CI configuration: \u00b6 env: OTEL_EXPORTER_OTLP_ENDPOINT: \"http://otel-collector:4317\" PROMETHEUS_MULTIPROC_DIR: \"/tmp/metrics\" \ud83d\udd2e Future Enhancements \u00b6 Distributed tracing for multi-cluster deployments. Real-time log streaming to the web UI. AI-assisted anomaly detection for workflow performance. Adaptive sampling for trace volume reduction. On-demand debug mode via CLI flag ( --trace verbose ). Next: Error Handling & Recovery","title":"Observability"},{"location":"architecture/17-observability-logging-and-metrics/#17-observability-logging-metrics-tracing","text":"","title":"17 \u2014 Observability, Logging, Metrics &amp; Tracing"},{"location":"architecture/17-observability-logging-and-metrics/#overview","text":"The Observability Layer provides full transparency into the runtime behavior of SecFlow. It enables developers, analysts, and operators to: - Track system health and performance metrics. - Trace workflow execution across tools and workers. - Monitor logs and alerts from all layers (API, Worker, Sandbox). - Correlate findings, runs, and audit trails with unique identifiers. The observability system is standards-based , built upon: - OpenTelemetry (OTel) for tracing and context propagation. - Prometheus for metrics export. - JSON structured logging for event correlation. - Grafana dashboards for visualization.","title":"\ud83e\udded Overview"},{"location":"architecture/17-observability-logging-and-metrics/#observability-architecture-overview","text":"+--------------------------------------------------------------+ | SecFlow Stack | | API Layer \u2192 Structured Logging, Metrics, Tracing | | Worker \u2192 Task Metrics, Workflow Spans, Sandbox Logs | | Sandbox \u2192 Runtime Metrics, Security Telemetry | | Database \u2192 Query Timing, Connection Pool Stats | -------------------------------------------------------------- | Exporters \u2192 Prometheus (Metrics), OTLP (Traces), Loki (Logs)| +--------------------------------------------------------------+","title":"\ud83e\uddf1 Observability Architecture Overview"},{"location":"architecture/17-observability-logging-and-metrics/#logging-subsystem","text":"","title":"\ud83e\udde9 Logging Subsystem"},{"location":"architecture/17-observability-logging-and-metrics/#logging-design-goals","text":"Goal Implementation Machine-readable JSON structured format with standard fields Correlated across systems trace_id and span_id included Context-aware User, project, workflow metadata embedded Immutable Append-only, timestamped, HMAC-signed if required","title":"Logging Design Goals"},{"location":"architecture/17-observability-logging-and-metrics/#log-record-example","text":"{ \"timestamp\": \"2025-10-06T09:45:32Z\", \"level\": \"INFO\", \"service\": \"worker\", \"trace_id\": \"cbd82b67-4a2b-4db6-9a90-1c3ed1b7e203\", \"span_id\": \"4f7c2b91\", \"project\": \"acme-api\", \"workflow_id\": \"wf-abc123\", \"message\": \"Nuclei scan completed successfully\", \"duration_ms\": 34215 }","title":"Log Record Example"},{"location":"architecture/17-observability-logging-and-metrics/#logging-stack","text":"Python Logging + Structlog \u2014 base structured logs. OpenTelemetry LoggingHandler \u2014 trace context propagation. Loki Exporter \u2014 for central log aggregation (optional).","title":"Logging Stack"},{"location":"architecture/17-observability-logging-and-metrics/#configuration-snippet","text":"LOGGING = { \"version\": 1, \"formatters\": {\"json\": {\"()\": \"pythonjsonlogger.jsonlogger.JsonFormatter\"}}, \"handlers\": { \"console\": {\"class\": \"logging.StreamHandler\", \"formatter\": \"json\"}, }, \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]}, }","title":"Configuration snippet:"},{"location":"architecture/17-observability-logging-and-metrics/#log-levels-policies","text":"Level Description DEBUG Developer-only context, disabled by default in production. INFO System lifecycle and status messages. WARNING Recoverable issues, retryable errors. ERROR Failures in user-triggered operations. CRITICAL Irrecoverable errors (sandbox isolation breach, DB corruption). Retention policy for logs follows the GC subsystem (see 15-garbage-collection-and-retention.md ).","title":"\u2699\ufe0f Log Levels &amp; Policies"},{"location":"architecture/17-observability-logging-and-metrics/#trace-propagation-distributed-context","text":"SecFlow uses OpenTelemetry (OTel) for distributed tracing. Every API request, task dispatch, and plugin call generates spans linked under one root trace.","title":"\ud83e\udde0 Trace Propagation &amp; Distributed Context"},{"location":"architecture/17-observability-logging-and-metrics/#example-trace-structure","text":"TraceID: 5b2e4f21c9a344f9 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"api.request (GET /api/v1/workflows)\"] B[\"worker.dispatch\"] C[\"tool.nuclei.run\"] D[\"sandbox.spawn\"] E[\"sandbox.cleanup\"] F[\"tool.ferox.run\"] G[\"findings.persist\"] H[\"api.response\"] A --> B A --> H B --> C B --> F B --> G C --> D C --> E","title":"Example Trace Structure"},{"location":"architecture/17-observability-logging-and-metrics/#code-example","text":"from opentelemetry import trace tracer = trace.get_tracer(\"SecFlow.worker\") with tracer.start_as_current_span(\"workflow.execute\") as span: span.set_attribute(\"workflow.id\", workflow.id) run_workflow(workflow) All traces are exported through OTLP gRPC to the observability backend (e.g., Tempo, Jaeger).","title":"Code Example"},{"location":"architecture/17-observability-logging-and-metrics/#metrics-system","text":"SecFlow exposes runtime metrics through Prometheus-compatible endpoints.","title":"\ud83d\udcca Metrics System"},{"location":"architecture/17-observability-logging-and-metrics/#default-endpoint","text":"/metrics","title":"Default Endpoint"},{"location":"architecture/17-observability-logging-and-metrics/#example-metrics","text":"Metric Type Description secflow_requests_total Counter Total API requests handled secflow_active_workflows Gauge Currently running workflows secflow_findings_generated_total Counter Findings created secflow_task_duration_seconds Histogram Time taken by async tasks secflow_gc_bytes_reclaimed_total Counter GC reclaimed bytes secflow_sandbox_executions_total Counter Number of sandbox runs secflow_tool_failures_total Counter Failed tool executions secflow_worker_queue_depth Gauge Pending Celery tasks secflow_cve_enrichment_latency_seconds Histogram Time per CVE query","title":"Example Metrics"},{"location":"architecture/17-observability-logging-and-metrics/#prometheus-export-example","text":"from prometheus_client import Counter, Gauge findings_total = Counter(\"secflow_findings_generated_total\", \"Number of findings created\") active_workflows = Gauge(\"secflow_active_workflows\", \"Currently running workflows\")","title":"Prometheus Export Example"},{"location":"architecture/17-observability-logging-and-metrics/#example-grafana-dashboard-panels","text":"Panel Visualization Query Workflow Throughput Time series rate(secflow_requests_total[5m]) Average Scan Duration Histogram histogram_quantile(0.9, rate(secflow_task_duration_seconds_bucket[5m])) Findings per Project Bar chart sum by (project)(secflow_findings_generated_total) GC Efficiency SingleStat rate(secflow_gc_bytes_reclaimed_total[1h]) Sandbox Failures Table secflow_tool_failures_total","title":"\ud83d\udd0d Example Grafana Dashboard Panels"},{"location":"architecture/17-observability-logging-and-metrics/#error-correlation-incident-debugging","text":"Every finding, workflow, and audit entry includes a trace ID. Errors can be traced back to exact processes and spans.","title":"\ud83e\udde9 Error Correlation &amp; Incident Debugging"},{"location":"architecture/17-observability-logging-and-metrics/#example-correlation","text":"Finding \u2192 Workflow ID: wf-abc123 \u2192 Trace ID: cbd82b67a1e4f9d2c8b5e6f7a3d4c9e2b1f8a5c6d7e9f2a3b4c5d6e7f8a9b2c3d4e5f6 \u2192 Logs: worker.log \u2192 Span: tool.nuclei.run This guarantees reproducibility and fast RCA (root cause analysis).","title":"Example correlation:"},{"location":"architecture/17-observability-logging-and-metrics/#alerting-health-checks","text":"","title":"\ud83e\udde0 Alerting &amp; Health Checks"},{"location":"architecture/17-observability-logging-and-metrics/#health-endpoints","text":"Endpoint Description /healthz Liveness probe (basic app status) /readyz Readiness probe (DB + cache + queue connectivity)","title":"Health Endpoints"},{"location":"architecture/17-observability-logging-and-metrics/#example-output","text":"{ \"status\": \"ok\", \"services\": { \"database\": \"up\", \"cache\": \"up\", \"worker\": \"idle\" } }","title":"Example Output"},{"location":"architecture/17-observability-logging-and-metrics/#alerts-prometheus-rules","text":"groups: - name: secflow_alerts rules: - alert: HighErrorRate expr: rate(secflow_tool_failures_total[5m]) > 5 for: 10m labels: { severity: warning } annotations: summary: \"Tool failure rate too high\"","title":"Alerts (Prometheus Rules)"},{"location":"architecture/17-observability-logging-and-metrics/#security-of-observability-data","text":"Concern Mitigation Sensitive logs Field redaction (password, token, secret) Trace integrity HMAC signing of exported spans Log tampering Append-only JSONL + rotation Metrics abuse Authenticated /metrics endpoint (basic token or mutual TLS)","title":"\ud83d\udd12 Security of Observability Data"},{"location":"architecture/17-observability-logging-and-metrics/#example-redaction-middleware","text":"def sanitize(data: dict) -> dict: for key in data.keys(): if \"token\" in key.lower() or \"password\" in key.lower(): data[key] = \"[REDACTED]\" return data","title":"Example redaction middleware:"},{"location":"architecture/17-observability-logging-and-metrics/#correlation-example-end-to-end-trace","text":"[TRACE 5b2e4f21c9a344f9] %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"/api/v1/workflows/start\"] B[\"worker.queue.dispatch (duration=12ms)\"] C[\"tool.ferox.run (duration=4.1s)\"] D[\"tool.nuclei.run (duration=9.3s)\"] E[\"enrich.cve (duration=1.4s)\"] F[\"findings.persist (duration=320ms)\"] G[\"audit.log (duration=7ms)\"] A --> B A --> C A --> D A --> E A --> F A --> G","title":"\ud83e\uddf1 Correlation Example: End-to-End Trace"},{"location":"architecture/17-observability-logging-and-metrics/#integration-with-cicd-and-testing","text":"During CI runs: - Unit and integration tests export OTel traces for regression analysis. - Performance tests measure task durations and error rates.","title":"\ud83e\udde9 Integration with CI/CD and Testing"},{"location":"architecture/17-observability-logging-and-metrics/#example-ci-configuration","text":"env: OTEL_EXPORTER_OTLP_ENDPOINT: \"http://otel-collector:4317\" PROMETHEUS_MULTIPROC_DIR: \"/tmp/metrics\"","title":"Example CI configuration:"},{"location":"architecture/17-observability-logging-and-metrics/#future-enhancements","text":"Distributed tracing for multi-cluster deployments. Real-time log streaming to the web UI. AI-assisted anomaly detection for workflow performance. Adaptive sampling for trace volume reduction. On-demand debug mode via CLI flag ( --trace verbose ). Next: Error Handling & Recovery","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/18-error-handling-and-recovery/","text":"18 \u2014 Error Handling, Fault Tolerance & Recovery Architecture \u00b6 \ud83e\udded Overview \u00b6 SecFlow is designed to operate in unpredictable environments \u2014 remote APIs, external security tools, local sandboxes, and asynchronous task queues. This chapter defines the resilience layer of the platform, ensuring that the system can detect, contain, recover, and self-heal from errors without compromising data integrity or user experience. \ud83e\uddf1 Core Resilience Principles \u00b6 Principle Description Isolation Failures in one tool or workflow should not cascade. Retryability Transient failures should automatically reattempt. Idempotency Repeated executions must produce consistent outcomes. Observability Every error must be traceable with full context. Graceful Degradation System continues partially even under partial failure. \u2699\ufe0f Error Taxonomy \u00b6 Errors are classified to determine handling strategy: Type Example Recovery Strategy Transient Network timeout, rate-limit, temporary unavailability Retry with exponential backoff Permanent Invalid config, missing file, malformed schema Fail fast, log, require user fix External Tool Nuclei crash, Feroxbuster exit code \u2260 0 Capture stdout/stderr, mark node failed Internal Logic Python exception, schema mismatch Rollback transaction, log critical Security Violation Sandbox breakout, unauthorized access Immediate isolation + alert User Abort Manual workflow stop Graceful cancellation, cleanup resources \ud83e\udde9 Error Handling Architecture \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Engine<br/>- Retry Controller<br/>- Error Propagation Manager<br/>- Compensation Handlers<br/>- Dead Letter Queue\"] B[\"Observability & Audit Log\"] A --> B \ud83e\udde0 Exception Handling Model \u00b6 All SecFlow components use a unified exception hierarchy: class SecFlowError(Exception): \"\"\"Base class for all SecFlow exceptions.\"\"\" class TransientError(SecFlowError): \"\"\"Recoverable error, eligible for retry.\"\"\" class PermanentError(SecFlowError): \"\"\"Non-recoverable error, must be logged and halted.\"\"\" class SecurityError(SecFlowError): \"\"\"Unauthorized or unsafe action detected.\"\"\" Every operation that might fail is wrapped in a retry-safe decorator. \ud83d\udd01 Retry Logic & Tenacity Integration \u00b6 SecFlow uses the Tenacity library for intelligent retries. from tenacity import retry, wait_exponential, stop_after_attempt @retry( wait=wait_exponential(multiplier=1, min=2, max=30), stop=stop_after_attempt(5), retry_error_callback=lambda r: log_error(r) ) def run_tool(tool_name, args): return subprocess.run(args, check=True) Retry Rules \u00b6 Context Max Retries Delay Type API HTTP Requests 5 Exponential CVE Enrichment Queries 3 Linear Worker Tasks 3 Exponential File System Operations 2 Immediate PoC Sandbox Launch 1 No retry (for safety) \ud83e\uddf1 Circuit Breaker Pattern \u00b6 SecFlow prevents repeated failures from overloading systems via circuit breakers. Implementation Example \u00b6 class CircuitBreaker: def __init__(self, threshold=5, timeout=60): self.failures = 0 self.opened_at = None self.threshold = threshold self.timeout = timeout def record_failure(self): self.failures += 1 if self.failures >= self.threshold: self.opened_at = datetime.utcnow() def can_execute(self): if not self.opened_at: return True return (datetime.utcnow() - self.opened_at).seconds > self.timeout Used for: - Remote API (NVD, OSV, Exploit-DB) - File I/O saturation - Tool wrappers under repeated crashes \ud83e\udde9 Dead Letter Queue (DLQ) \u00b6 Failed tasks that exceed retry limits are pushed into the DLQ for manual review. @app.task(bind=True, max_retries=3) def run_scan(self, task_id): try: run_workflow(task_id) except Exception as e: if self.request.retries == self.max_retries: enqueue_dlq(task_id, str(e)) raise self.retry(exc=e) DLQ entries include: \u00b6 Task ID Workflow ID Exception message Retry count Timestamp Example DLQ record: \u00b6 { \"task\": \"wf-1234-node-nuclei\", \"error\": \"Connection timeout to target\", \"retries\": 3, \"timestamp\": \"2025-10-06T10:22:00Z\" } \ud83e\udde0 Self-Healing Workflows \u00b6 SecFlow supports automatic node rehydration \u2014 failed steps in a workflow can be restarted independently without restarting the entire pipeline. Rehydration Process \u00b6 Detect failed node. Mark upstream outputs as valid. Restart failed node only. Merge results into workflow graph. CLI example: \u00b6 SecFlow workflow resume --node nuclei --project acme-api \ud83e\udde9 Transactional Integrity \u00b6 Database operations are wrapped in ACID transactions using SQLModel context managers: from sqlmodel import Session def save_finding(finding): with Session(engine) as session: try: session.add(finding) session.commit() except Exception: session.rollback() raise All cross-project mutations (findings, triage, cache) are transactional. \ud83e\udde0 Error Event Logging & Correlation \u00b6 Each exception generates an audit entry: { \"event\": \"error\", \"component\": \"worker\", \"type\": \"TransientError\", \"workflow_id\": \"wf-abc123\", \"trace_id\": \"b73f0b7c-47f9-4bb3-9b9c-ffacbd1d6a67\", \"message\": \"Feroxbuster timeout\", \"retries\": 3 } Errors are correlated with: - Workflow Trace ID - Finding UUID (if relevant) - User and project context This allows full replay and debugging via observability dashboards. \u2699\ufe0f Graceful Degradation \u00b6 If a subsystem fails (e.g., enrichment API offline): - Workflows continue with reduced functionality. - Missing data marked as \"partial\": true . - Users notified in the triage panel: \u26a0 CVE enrichment service temporarily unavailable \u2014 retry later. \ud83e\udde9 Alerting & Notification Hooks \u00b6 Integration with Prometheus Alertmanager for system errors. Optional Slack / Email webhook for high-severity failures. Rate-limited notifications to avoid alert fatigue. Example alert webhook payload: \u00b6 { \"severity\": \"critical\", \"component\": \"sandbox\", \"message\": \"PoC execution timeout\", \"project\": \"api-audit\", \"trace_id\": \"a2c134b5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" } \ud83e\uddf1 Recovery Strategies \u00b6 Context Recovery Action Sandbox crash Auto-restart with clean container API outage Retry with backoff + circuit breaker Tool misconfiguration Disable tool temporarily, notify user Cache corruption Rebuild from source Disk full Trigger GC and alert Worker crash Celery task re-queued DB lock contention Exponential backoff retry \ud83e\udde0 Example Error Lifecycle \u00b6 [Error Detected] \u2192 [Retry 1/3] \u2192 [Retry 2/3] \u2192 [DLQ] \u2192 [Alert sent to Slack] \u2192 [Analyst re-runs workflow node] \u2192 [Recovered] \ud83d\udd12 Security Implications \u00b6 Sensitive stack traces are redacted before exposure. Error details logged internally only. External responses use generic safe messages: {\"error\": \"Internal processing issue, please retry later.\"} \ud83d\udd2e Future Enhancements \u00b6 Adaptive retry policies based on machine learning. AI-powered incident summarization for triage. Transactional outbox pattern for guaranteed task delivery. Fine-grained chaos testing integrated in CI/CD. Next: Risk Assessment & Scoring Framework","title":"Error Handling"},{"location":"architecture/18-error-handling-and-recovery/#18-error-handling-fault-tolerance-recovery-architecture","text":"","title":"18 \u2014 Error Handling, Fault Tolerance &amp; Recovery Architecture"},{"location":"architecture/18-error-handling-and-recovery/#overview","text":"SecFlow is designed to operate in unpredictable environments \u2014 remote APIs, external security tools, local sandboxes, and asynchronous task queues. This chapter defines the resilience layer of the platform, ensuring that the system can detect, contain, recover, and self-heal from errors without compromising data integrity or user experience.","title":"\ud83e\udded Overview"},{"location":"architecture/18-error-handling-and-recovery/#core-resilience-principles","text":"Principle Description Isolation Failures in one tool or workflow should not cascade. Retryability Transient failures should automatically reattempt. Idempotency Repeated executions must produce consistent outcomes. Observability Every error must be traceable with full context. Graceful Degradation System continues partially even under partial failure.","title":"\ud83e\uddf1 Core Resilience Principles"},{"location":"architecture/18-error-handling-and-recovery/#error-taxonomy","text":"Errors are classified to determine handling strategy: Type Example Recovery Strategy Transient Network timeout, rate-limit, temporary unavailability Retry with exponential backoff Permanent Invalid config, missing file, malformed schema Fail fast, log, require user fix External Tool Nuclei crash, Feroxbuster exit code \u2260 0 Capture stdout/stderr, mark node failed Internal Logic Python exception, schema mismatch Rollback transaction, log critical Security Violation Sandbox breakout, unauthorized access Immediate isolation + alert User Abort Manual workflow stop Graceful cancellation, cleanup resources","title":"\u2699\ufe0f Error Taxonomy"},{"location":"architecture/18-error-handling-and-recovery/#error-handling-architecture","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Workflow Engine<br/>- Retry Controller<br/>- Error Propagation Manager<br/>- Compensation Handlers<br/>- Dead Letter Queue\"] B[\"Observability & Audit Log\"] A --> B","title":"\ud83e\udde9 Error Handling Architecture"},{"location":"architecture/18-error-handling-and-recovery/#exception-handling-model","text":"All SecFlow components use a unified exception hierarchy: class SecFlowError(Exception): \"\"\"Base class for all SecFlow exceptions.\"\"\" class TransientError(SecFlowError): \"\"\"Recoverable error, eligible for retry.\"\"\" class PermanentError(SecFlowError): \"\"\"Non-recoverable error, must be logged and halted.\"\"\" class SecurityError(SecFlowError): \"\"\"Unauthorized or unsafe action detected.\"\"\" Every operation that might fail is wrapped in a retry-safe decorator.","title":"\ud83e\udde0 Exception Handling Model"},{"location":"architecture/18-error-handling-and-recovery/#retry-logic-tenacity-integration","text":"SecFlow uses the Tenacity library for intelligent retries. from tenacity import retry, wait_exponential, stop_after_attempt @retry( wait=wait_exponential(multiplier=1, min=2, max=30), stop=stop_after_attempt(5), retry_error_callback=lambda r: log_error(r) ) def run_tool(tool_name, args): return subprocess.run(args, check=True)","title":"\ud83d\udd01 Retry Logic &amp; Tenacity Integration"},{"location":"architecture/18-error-handling-and-recovery/#retry-rules","text":"Context Max Retries Delay Type API HTTP Requests 5 Exponential CVE Enrichment Queries 3 Linear Worker Tasks 3 Exponential File System Operations 2 Immediate PoC Sandbox Launch 1 No retry (for safety)","title":"Retry Rules"},{"location":"architecture/18-error-handling-and-recovery/#circuit-breaker-pattern","text":"SecFlow prevents repeated failures from overloading systems via circuit breakers.","title":"\ud83e\uddf1 Circuit Breaker Pattern"},{"location":"architecture/18-error-handling-and-recovery/#implementation-example","text":"class CircuitBreaker: def __init__(self, threshold=5, timeout=60): self.failures = 0 self.opened_at = None self.threshold = threshold self.timeout = timeout def record_failure(self): self.failures += 1 if self.failures >= self.threshold: self.opened_at = datetime.utcnow() def can_execute(self): if not self.opened_at: return True return (datetime.utcnow() - self.opened_at).seconds > self.timeout Used for: - Remote API (NVD, OSV, Exploit-DB) - File I/O saturation - Tool wrappers under repeated crashes","title":"Implementation Example"},{"location":"architecture/18-error-handling-and-recovery/#dead-letter-queue-dlq","text":"Failed tasks that exceed retry limits are pushed into the DLQ for manual review. @app.task(bind=True, max_retries=3) def run_scan(self, task_id): try: run_workflow(task_id) except Exception as e: if self.request.retries == self.max_retries: enqueue_dlq(task_id, str(e)) raise self.retry(exc=e)","title":"\ud83e\udde9 Dead Letter Queue (DLQ)"},{"location":"architecture/18-error-handling-and-recovery/#dlq-entries-include","text":"Task ID Workflow ID Exception message Retry count Timestamp","title":"DLQ entries include:"},{"location":"architecture/18-error-handling-and-recovery/#example-dlq-record","text":"{ \"task\": \"wf-1234-node-nuclei\", \"error\": \"Connection timeout to target\", \"retries\": 3, \"timestamp\": \"2025-10-06T10:22:00Z\" }","title":"Example DLQ record:"},{"location":"architecture/18-error-handling-and-recovery/#self-healing-workflows","text":"SecFlow supports automatic node rehydration \u2014 failed steps in a workflow can be restarted independently without restarting the entire pipeline.","title":"\ud83e\udde0 Self-Healing Workflows"},{"location":"architecture/18-error-handling-and-recovery/#rehydration-process","text":"Detect failed node. Mark upstream outputs as valid. Restart failed node only. Merge results into workflow graph.","title":"Rehydration Process"},{"location":"architecture/18-error-handling-and-recovery/#cli-example","text":"SecFlow workflow resume --node nuclei --project acme-api","title":"CLI example:"},{"location":"architecture/18-error-handling-and-recovery/#transactional-integrity","text":"Database operations are wrapped in ACID transactions using SQLModel context managers: from sqlmodel import Session def save_finding(finding): with Session(engine) as session: try: session.add(finding) session.commit() except Exception: session.rollback() raise All cross-project mutations (findings, triage, cache) are transactional.","title":"\ud83e\udde9 Transactional Integrity"},{"location":"architecture/18-error-handling-and-recovery/#error-event-logging-correlation","text":"Each exception generates an audit entry: { \"event\": \"error\", \"component\": \"worker\", \"type\": \"TransientError\", \"workflow_id\": \"wf-abc123\", \"trace_id\": \"b73f0b7c-47f9-4bb3-9b9c-ffacbd1d6a67\", \"message\": \"Feroxbuster timeout\", \"retries\": 3 } Errors are correlated with: - Workflow Trace ID - Finding UUID (if relevant) - User and project context This allows full replay and debugging via observability dashboards.","title":"\ud83e\udde0 Error Event Logging &amp; Correlation"},{"location":"architecture/18-error-handling-and-recovery/#graceful-degradation","text":"If a subsystem fails (e.g., enrichment API offline): - Workflows continue with reduced functionality. - Missing data marked as \"partial\": true . - Users notified in the triage panel: \u26a0 CVE enrichment service temporarily unavailable \u2014 retry later.","title":"\u2699\ufe0f Graceful Degradation"},{"location":"architecture/18-error-handling-and-recovery/#alerting-notification-hooks","text":"Integration with Prometheus Alertmanager for system errors. Optional Slack / Email webhook for high-severity failures. Rate-limited notifications to avoid alert fatigue.","title":"\ud83e\udde9 Alerting &amp; Notification Hooks"},{"location":"architecture/18-error-handling-and-recovery/#example-alert-webhook-payload","text":"{ \"severity\": \"critical\", \"component\": \"sandbox\", \"message\": \"PoC execution timeout\", \"project\": \"api-audit\", \"trace_id\": \"a2c134b5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8\" }","title":"Example alert webhook payload:"},{"location":"architecture/18-error-handling-and-recovery/#recovery-strategies","text":"Context Recovery Action Sandbox crash Auto-restart with clean container API outage Retry with backoff + circuit breaker Tool misconfiguration Disable tool temporarily, notify user Cache corruption Rebuild from source Disk full Trigger GC and alert Worker crash Celery task re-queued DB lock contention Exponential backoff retry","title":"\ud83e\uddf1 Recovery Strategies"},{"location":"architecture/18-error-handling-and-recovery/#example-error-lifecycle","text":"[Error Detected] \u2192 [Retry 1/3] \u2192 [Retry 2/3] \u2192 [DLQ] \u2192 [Alert sent to Slack] \u2192 [Analyst re-runs workflow node] \u2192 [Recovered]","title":"\ud83e\udde0 Example Error Lifecycle"},{"location":"architecture/18-error-handling-and-recovery/#security-implications","text":"Sensitive stack traces are redacted before exposure. Error details logged internally only. External responses use generic safe messages: {\"error\": \"Internal processing issue, please retry later.\"}","title":"\ud83d\udd12 Security Implications"},{"location":"architecture/18-error-handling-and-recovery/#future-enhancements","text":"Adaptive retry policies based on machine learning. AI-powered incident summarization for triage. Transactional outbox pattern for guaranteed task delivery. Fine-grained chaos testing integrated in CI/CD. Next: Risk Assessment & Scoring Framework","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/19-risk-assessment-framework/","text":"19 \u2014 Risk Assessment, Scoring & Prioritization Framework \u00b6 \ud83e\udded Overview \u00b6 The Risk Assessment Framework in SecFlow provides a consistent and transparent way to evaluate and prioritize vulnerabilities across multiple projects and tools. It merges four complementary standards: NIST 5\u00d75 Matrix \u2014 contextual risk evaluation (Impact \u00d7 Likelihood). CVSS v3.1 \u2014 standardized vulnerability severity scoring. CWE / OWASP Mapping \u2014 weakness classification. MITRE ATT&CK Correlation \u2014 adversarial behavior context. This layered model supports both quantitative (numeric scores) and qualitative (High/Critical/etc.) analysis for triage and reporting. \ud83e\uddf1 Architecture Overview \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"SecFlow Risk Engine<br/>- CVSS Normalizer (from findings or enrichment)<br/>- NIST 5\u00d75 Contextual Matrix<br/>- MITRE ATT&CK Mapper<br/>- Risk Aggregator & Scorer<br/>- Project Risk Dashboard\"] \u2699\ufe0f Core Objectives \u00b6 Goal Description Standardization Consistent risk scoring across tools and projects. Transparency Traceable logic behind every score. Extensibility Pluggable scoring policies (per organization). Automation Auto-scoring during enrichment and triage. Context-Awareness Includes exploitability, exposure, and asset criticality. \ud83e\uddf1 Scoring Pipeline \u00b6 Finding \u2193 CVSS Vector Parsing \u2193 CWE / OWASP Classification \u2193 Exploit & Exposure Context \u2193 NIST Risk Matrix Evaluation \u2193 Final Risk Score (0\u2013100) + Risk Tier \ud83e\udde0 CVSS Normalization \u00b6 SecFlow ingests CVSS vectors either from NVD (via enrichment) or tool metadata. Example: \u00b6 CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H Converted into internal representation: { \"base_score\": 9.8, \"impact_subscore\": 5.9, \"exploitability_subscore\": 3.9, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\" } If missing, heuristic fallback is applied based on CWE ID or OWASP category (see 13-cve-cwe-poc-enrichment-layer.md ). \ud83e\udde9 CWE / OWASP Mapping \u00b6 CWE ID OWASP Category Default Impact Default Likelihood 79 A03: Injection High High 89 A03: Injection Very High High 200 A01: Broken Access Control High Medium 601 A10: SSRF Medium Medium 787 A05: Buffer Overflow Critical Medium 352 A08: CSRF Medium High Mappings are maintained in /resources/mappings/cwe_owasp.json . \ud83e\udde0 MITRE ATT&CK Mapping \u00b6 Findings enriched with ATT&CK technique IDs (TIDs) during correlation (see 13-cve-cwe-poc-enrichment-layer.md ) are leveraged to infer attack chain context. MITRE Technique ID Tactic Effect T1059.007 Execution Cross-Site Scripting T1505.003 Persistence SQL Injection T1071.001 Command & Control Web Protocols T1190 Initial Access Exploit Public-Facing App This mapping helps classify whether the finding affects initial compromise, lateral movement, or data exfiltration. \u2699\ufe0f NIST 5\u00d75 Risk Matrix \u00b6 Definition \u00b6 Impact \u2193 / Likelihood \u2192 Very Low Low Medium High Very High Very High Medium High High Critical Critical High Low Medium High High Critical Medium Low Low Medium High High Low Low Low Low Medium High Very Low Low Low Low Low Medium Mapping to Severity \u00b6 Result Score Range Label Critical 90\u2013100 \ud83d\udd25 High 70\u201389 \u26a0\ufe0f Medium 40\u201369 \u2696\ufe0f Low 20\u201339 \ud83e\udde9 Informational 0\u201319 \u2139\ufe0f \ud83e\udde9 Likelihood Factors \u00b6 Likelihood is dynamically computed using multiple context sources: Factor Description Weight Exploit Availability Known PoC, KEV presence +0.3 Network Exposure Publicly reachable target +0.25 Authentication Required Lowers likelihood if true -0.15 Complexity Tool-derived complexity \u00b10.1 Detection Confidence Based on finding engine \u00b10.2 Pseudo-code: \u00b6 def likelihood_score(finding): score = 0.3 if finding.poc_available else 0 if finding.exposure == \"internet\": score += 0.25 if finding.auth_required: score -= 0.15 if finding.complexity == \"low\": score += 0.1 return min(max(score, 0), 1) \ud83e\udde0 Impact Factors \u00b6 Impact combines technical and business context: Factor Example Weight Confidentiality Data exposure +0.3 Integrity Tampering possible +0.3 Availability Service crash, DoS +0.2 Privilege Escalation Root/system access +0.2 Asset Criticality System importance +0.4 Final impact = weighted sum normalized to 1.0. \u2699\ufe0f Combined Risk Formula \u00b6 Final quantitative risk score (0\u2013100): risk_score = ((CVSS_base / 10) * 0.6 + impact_factor * 0.25 + likelihood_factor * 0.15) * 100 Rounded to nearest integer. Example \u00b6 Metric Value CVSS Base 9.8 Impact Factor 0.8 Likelihood Factor 0.7 Final Score ((0.98*0.6)+(0.8*0.25)+(0.7*0.15))*100 = 89.7 \u2192 High \ud83e\udde0 Contextual Adjustments \u00b6 Certain contexts modify final risk score: Context Adjustment Active exploit in wild (CISA KEV) +10 Proof-of-concept verified +5 Patched version available -5 Internal-only system -10 Compensating controls present -15 Scores are capped at 100 and floored at 0. \ud83e\udde9 Aggregated Risk Dashboard \u00b6 Each project's analytics tab visualizes: Metric Description Average CVSS per project Top 10 findings by risk score Risk evolution over time Distribution by OWASP category ATT&CK tactics heatmap Example chart: \u00b6 %%{init: {\"theme\":\"neutral\"}}%% xychart-beta title \"Risk Trend (Score over Time)\" x-axis [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"] y-axis \"Risk Score\" 0 --> 100 bar [85, 70, 45, 60, 80] \u2699\ufe0f Risk Normalization Across Tools \u00b6 Different tools report different severity models (e.g., \"Critical\", \"High\", \"Medium\"). SecFlow converts all to internal numeric ranges: Tool Critical High Medium Low Nuclei 90\u2013100 70\u201389 40\u201369 20\u201339 ZAP 85\u2013100 65\u201384 35\u201364 15\u201334 Burp 90\u2013100 75\u201389 45\u201374 25\u201344 All are harmonized via the risk formula to produce consistent prioritization. \ud83e\udde0 Risk Aggregation & Reporting \u00b6 Project-level risk is computed as weighted mean: def project_risk(findings): weights = [f.cvss_score * f.impact_weight for f in findings] return sum(weights) / len(weights) Analytics engine stores snapshots in /analytics/risk_snapshots/ . \ud83e\udde9 Risk API Endpoints \u00b6 Endpoint Method Description /api/v1/risk/score/{finding_id} GET Returns risk vector and classification /api/v1/risk/project/{id} GET Aggregated project risk summary /api/v1/risk/export POST Export risk data to JSON/CSV /api/v1/risk/heatmap GET Generates OWASP \u00d7 ATT&CK matrix Example Response: \u00b6 { \"finding_id\": \"abcd-123\", \"score\": 89.7, \"severity\": \"High\", \"CVSS\": 9.8, \"impact_factor\": 0.8, \"likelihood_factor\": 0.7, \"nist_matrix\": \"High/High \u2192 Critical\", \"owasp\": \"A03: Injection\", \"mitre_tid\": \"T1505.003\" } \ud83d\udd12 Auditability & Traceability \u00b6 Every risk computation is versioned and auditable: - Stored with enrichment metadata hash. - Recomputed automatically if CVSS source data updates. Log entry example: \u00b6 { \"event\": \"risk_recalc\", \"finding_id\": \"abcd-123\", \"old_score\": 78, \"new_score\": 89.7, \"reason\": \"CISA KEV inclusion\" } \ud83d\udd2e Future Enhancements \u00b6 Integration with EPSS (Exploit Prediction Scoring System). ML-based contextual risk forecasting. Auto-adjustment based on exploit telemetry feeds. Risk-driven workflow prioritization for automated scanning. AI-assistant suggestions for mitigations. Next: Migration & Implementation Phases","title":"Risk Assessment"},{"location":"architecture/19-risk-assessment-framework/#19-risk-assessment-scoring-prioritization-framework","text":"","title":"19 \u2014 Risk Assessment, Scoring &amp; Prioritization Framework"},{"location":"architecture/19-risk-assessment-framework/#overview","text":"The Risk Assessment Framework in SecFlow provides a consistent and transparent way to evaluate and prioritize vulnerabilities across multiple projects and tools. It merges four complementary standards: NIST 5\u00d75 Matrix \u2014 contextual risk evaluation (Impact \u00d7 Likelihood). CVSS v3.1 \u2014 standardized vulnerability severity scoring. CWE / OWASP Mapping \u2014 weakness classification. MITRE ATT&CK Correlation \u2014 adversarial behavior context. This layered model supports both quantitative (numeric scores) and qualitative (High/Critical/etc.) analysis for triage and reporting.","title":"\ud83e\udded Overview"},{"location":"architecture/19-risk-assessment-framework/#architecture-overview","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"SecFlow Risk Engine<br/>- CVSS Normalizer (from findings or enrichment)<br/>- NIST 5\u00d75 Contextual Matrix<br/>- MITRE ATT&CK Mapper<br/>- Risk Aggregator & Scorer<br/>- Project Risk Dashboard\"]","title":"\ud83e\uddf1 Architecture Overview"},{"location":"architecture/19-risk-assessment-framework/#core-objectives","text":"Goal Description Standardization Consistent risk scoring across tools and projects. Transparency Traceable logic behind every score. Extensibility Pluggable scoring policies (per organization). Automation Auto-scoring during enrichment and triage. Context-Awareness Includes exploitability, exposure, and asset criticality.","title":"\u2699\ufe0f Core Objectives"},{"location":"architecture/19-risk-assessment-framework/#scoring-pipeline","text":"Finding \u2193 CVSS Vector Parsing \u2193 CWE / OWASP Classification \u2193 Exploit & Exposure Context \u2193 NIST Risk Matrix Evaluation \u2193 Final Risk Score (0\u2013100) + Risk Tier","title":"\ud83e\uddf1 Scoring Pipeline"},{"location":"architecture/19-risk-assessment-framework/#cvss-normalization","text":"SecFlow ingests CVSS vectors either from NVD (via enrichment) or tool metadata.","title":"\ud83e\udde0 CVSS Normalization"},{"location":"architecture/19-risk-assessment-framework/#example","text":"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H Converted into internal representation: { \"base_score\": 9.8, \"impact_subscore\": 5.9, \"exploitability_subscore\": 3.9, \"vector\": \"CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:H\" } If missing, heuristic fallback is applied based on CWE ID or OWASP category (see 13-cve-cwe-poc-enrichment-layer.md ).","title":"Example:"},{"location":"architecture/19-risk-assessment-framework/#cwe-owasp-mapping","text":"CWE ID OWASP Category Default Impact Default Likelihood 79 A03: Injection High High 89 A03: Injection Very High High 200 A01: Broken Access Control High Medium 601 A10: SSRF Medium Medium 787 A05: Buffer Overflow Critical Medium 352 A08: CSRF Medium High Mappings are maintained in /resources/mappings/cwe_owasp.json .","title":"\ud83e\udde9 CWE / OWASP Mapping"},{"location":"architecture/19-risk-assessment-framework/#mitre-attck-mapping","text":"Findings enriched with ATT&CK technique IDs (TIDs) during correlation (see 13-cve-cwe-poc-enrichment-layer.md ) are leveraged to infer attack chain context. MITRE Technique ID Tactic Effect T1059.007 Execution Cross-Site Scripting T1505.003 Persistence SQL Injection T1071.001 Command & Control Web Protocols T1190 Initial Access Exploit Public-Facing App This mapping helps classify whether the finding affects initial compromise, lateral movement, or data exfiltration.","title":"\ud83e\udde0 MITRE ATT&amp;CK Mapping"},{"location":"architecture/19-risk-assessment-framework/#nist-55-risk-matrix","text":"","title":"\u2699\ufe0f NIST 5\u00d75 Risk Matrix"},{"location":"architecture/19-risk-assessment-framework/#definition","text":"Impact \u2193 / Likelihood \u2192 Very Low Low Medium High Very High Very High Medium High High Critical Critical High Low Medium High High Critical Medium Low Low Medium High High Low Low Low Low Medium High Very Low Low Low Low Low Medium","title":"Definition"},{"location":"architecture/19-risk-assessment-framework/#mapping-to-severity","text":"Result Score Range Label Critical 90\u2013100 \ud83d\udd25 High 70\u201389 \u26a0\ufe0f Medium 40\u201369 \u2696\ufe0f Low 20\u201339 \ud83e\udde9 Informational 0\u201319 \u2139\ufe0f","title":"Mapping to Severity"},{"location":"architecture/19-risk-assessment-framework/#likelihood-factors","text":"Likelihood is dynamically computed using multiple context sources: Factor Description Weight Exploit Availability Known PoC, KEV presence +0.3 Network Exposure Publicly reachable target +0.25 Authentication Required Lowers likelihood if true -0.15 Complexity Tool-derived complexity \u00b10.1 Detection Confidence Based on finding engine \u00b10.2","title":"\ud83e\udde9 Likelihood Factors"},{"location":"architecture/19-risk-assessment-framework/#pseudo-code","text":"def likelihood_score(finding): score = 0.3 if finding.poc_available else 0 if finding.exposure == \"internet\": score += 0.25 if finding.auth_required: score -= 0.15 if finding.complexity == \"low\": score += 0.1 return min(max(score, 0), 1)","title":"Pseudo-code:"},{"location":"architecture/19-risk-assessment-framework/#impact-factors","text":"Impact combines technical and business context: Factor Example Weight Confidentiality Data exposure +0.3 Integrity Tampering possible +0.3 Availability Service crash, DoS +0.2 Privilege Escalation Root/system access +0.2 Asset Criticality System importance +0.4 Final impact = weighted sum normalized to 1.0.","title":"\ud83e\udde0 Impact Factors"},{"location":"architecture/19-risk-assessment-framework/#combined-risk-formula","text":"Final quantitative risk score (0\u2013100): risk_score = ((CVSS_base / 10) * 0.6 + impact_factor * 0.25 + likelihood_factor * 0.15) * 100 Rounded to nearest integer.","title":"\u2699\ufe0f Combined Risk Formula"},{"location":"architecture/19-risk-assessment-framework/#example_1","text":"Metric Value CVSS Base 9.8 Impact Factor 0.8 Likelihood Factor 0.7 Final Score ((0.98*0.6)+(0.8*0.25)+(0.7*0.15))*100 = 89.7 \u2192 High","title":"Example"},{"location":"architecture/19-risk-assessment-framework/#contextual-adjustments","text":"Certain contexts modify final risk score: Context Adjustment Active exploit in wild (CISA KEV) +10 Proof-of-concept verified +5 Patched version available -5 Internal-only system -10 Compensating controls present -15 Scores are capped at 100 and floored at 0.","title":"\ud83e\udde0 Contextual Adjustments"},{"location":"architecture/19-risk-assessment-framework/#aggregated-risk-dashboard","text":"Each project's analytics tab visualizes: Metric Description Average CVSS per project Top 10 findings by risk score Risk evolution over time Distribution by OWASP category ATT&CK tactics heatmap","title":"\ud83e\udde9 Aggregated Risk Dashboard"},{"location":"architecture/19-risk-assessment-framework/#example-chart","text":"%%{init: {\"theme\":\"neutral\"}}%% xychart-beta title \"Risk Trend (Score over Time)\" x-axis [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"] y-axis \"Risk Score\" 0 --> 100 bar [85, 70, 45, 60, 80]","title":"Example chart:"},{"location":"architecture/19-risk-assessment-framework/#risk-normalization-across-tools","text":"Different tools report different severity models (e.g., \"Critical\", \"High\", \"Medium\"). SecFlow converts all to internal numeric ranges: Tool Critical High Medium Low Nuclei 90\u2013100 70\u201389 40\u201369 20\u201339 ZAP 85\u2013100 65\u201384 35\u201364 15\u201334 Burp 90\u2013100 75\u201389 45\u201374 25\u201344 All are harmonized via the risk formula to produce consistent prioritization.","title":"\u2699\ufe0f Risk Normalization Across Tools"},{"location":"architecture/19-risk-assessment-framework/#risk-aggregation-reporting","text":"Project-level risk is computed as weighted mean: def project_risk(findings): weights = [f.cvss_score * f.impact_weight for f in findings] return sum(weights) / len(weights) Analytics engine stores snapshots in /analytics/risk_snapshots/ .","title":"\ud83e\udde0 Risk Aggregation &amp; Reporting"},{"location":"architecture/19-risk-assessment-framework/#risk-api-endpoints","text":"Endpoint Method Description /api/v1/risk/score/{finding_id} GET Returns risk vector and classification /api/v1/risk/project/{id} GET Aggregated project risk summary /api/v1/risk/export POST Export risk data to JSON/CSV /api/v1/risk/heatmap GET Generates OWASP \u00d7 ATT&CK matrix","title":"\ud83e\udde9 Risk API Endpoints"},{"location":"architecture/19-risk-assessment-framework/#example-response","text":"{ \"finding_id\": \"abcd-123\", \"score\": 89.7, \"severity\": \"High\", \"CVSS\": 9.8, \"impact_factor\": 0.8, \"likelihood_factor\": 0.7, \"nist_matrix\": \"High/High \u2192 Critical\", \"owasp\": \"A03: Injection\", \"mitre_tid\": \"T1505.003\" }","title":"Example Response:"},{"location":"architecture/19-risk-assessment-framework/#auditability-traceability","text":"Every risk computation is versioned and auditable: - Stored with enrichment metadata hash. - Recomputed automatically if CVSS source data updates.","title":"\ud83d\udd12 Auditability &amp; Traceability"},{"location":"architecture/19-risk-assessment-framework/#log-entry-example","text":"{ \"event\": \"risk_recalc\", \"finding_id\": \"abcd-123\", \"old_score\": 78, \"new_score\": 89.7, \"reason\": \"CISA KEV inclusion\" }","title":"Log entry example:"},{"location":"architecture/19-risk-assessment-framework/#future-enhancements","text":"Integration with EPSS (Exploit Prediction Scoring System). ML-based contextual risk forecasting. Auto-adjustment based on exploit telemetry feeds. Risk-driven workflow prioritization for automated scanning. AI-assistant suggestions for mitigations. Next: Migration & Implementation Phases","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/20-migration-and-implementation-phases/","text":"20 \u2014 Migration & Implementation Phases \u00b6 \ud83e\udded Overview \u00b6 The migration plan defines a controlled, four-phase rollout of SecFlow's refactored architecture \u2014 from scaffolding and core model setup to the full orchestration of tools, UI, and enrichment. This plan emphasizes: - Incremental refactoring (no big-bang rewrite) - Zero architectural drift (guardrails first) - Progressive integration (tools, plugins, UI) - Data isolation and cleanup (garbage collector enabled) \ud83d\udce6 Phase 0 \u2014 Foundation & Guardrails (Week 1) \u00b6 Objective \u00b6 Establish the new repository structure and enforce architectural discipline before any migration work. Tasks \u00b6 Create mono-repo scaffold under /src/SecFlow/ Add dev environment setup via Makefile , pyproject.toml , and Poetry Enable static analysis tooling : ruff for linting pyright for typing import-linter for import boundaries Setup unit testing scaffold: /tests/core , /tests/wrappers , /tests/plugins Define .github/workflows/ci.yml for matrix builds (Python 3.10\u20133.12) Establish base docs/architecture/ folder for ongoing documentation Expected Deliverables \u00b6 Working mono-repo with guardrails CI passing with lint/type/test checks Developer guide for environment setup ( docs/dev-setup.md ) Example Command \u00b6 make init make lint make test \ud83e\uddf1 Phase 1 \u2014 Core Models & Data Persistence (Week 2) \u00b6 Objective \u00b6 Move fundamental entities (Projects, Findings, Resources, Runs) into modular core-lib and storage packages. Tasks \u00b6 Create core-lib/ package: Models for Project, Finding, Resource, Run Pydantic schemas for DTOs Create storage/ package: Database adapters for SQLite (local) and PostgreSQL (production) Repository interfaces (IProjectRepo, IFindingsRepo, etc.) Alembic or SQLModel migrations Implement CRUD API endpoints: /api/v1/projects /api/v1/findings /api/v1/resources Add test fixtures for sample data Expected Deliverables \u00b6 Persistent data layer Core models validated by schema Functional CRUD endpoints 80%+ test coverage on models and repos Example Model \u00b6 class Project(BaseModel): id: UUID name: str owner: str description: Optional[str] created_at: datetime updated_at: datetime \u2699\ufe0f Phase 2 \u2014 Findings Engine, Wrappers & Workflow (Week 3) \u00b6 Objective \u00b6 Integrate scanning tools (Nuclei, Feroxbuster, Katana, etc.) into the new workflow engine and plugin registry. Tasks \u00b6 Implement findings-engine/ : Normalization layer for all scanners Parser adapters for each tool Implement wrappers/ : NucleiWrapper, FeroxWrapper, ZAPWrapper Each using standardized manifest + sandbox Create plugins/ package: Detection and enrichment plugins (e.g., CVEMapper, RiskScorer) Build workflow engine with DAG executor: YAML recipe parsing Input/output data mapping Caching and persistence Integrate tool registry UI in web frontend Expected Deliverables \u00b6 Tool registry and manifest system Workflow DAG execution engine Normalized findings output (JSON schema compliant) Risk engine integration (Phase 1 of enrichment) Example Wrapper Interface \u00b6 class ToolWrapper(Protocol): def prepare(self, config: Dict[str, Any]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run(self) -> ToolOutput: \"\"\"Execute tool and return output.\"\"\" pass def parse_output(self, raw: str) -> List[Finding]: \"\"\"Parse raw output into findings.\"\"\" pass \ud83c\udf10 Phase 3 \u2014 API, Worker, and Triage UI (Week 4) \u00b6 Objective \u00b6 Deliver full orchestration capability through APIs, background workers, and a lightweight HTMX/React UI. Tasks \u00b6 Create web-api/ : REST endpoints for workflows, findings, triage WebSocket for live updates Create worker/ : Celery/asyncio-based job processor Queues for workflow nodes and enrichment Create triage-ui/ : Interactive HTMX dashboard for findings triage Tabs: \"Projects\", \"Findings\", \"Tools\", \"Metrics\" Implement user auth & RBAC JWT + role middleware Add audit logging for all changes Integrate observability stack (Prometheus, OpenTelemetry) Expected Deliverables \u00b6 Full end-to-end scan \u2192 finding \u2192 triage pipeline Live progress dashboard Role-based access and logging Metrics export for dashboards Example Endpoint \u00b6 @app.post(\"/api/v1/workflows/run\") async def run_workflow(workflow_id: str): job_id = await worker.enqueue(workflow_id) return {\"status\": \"queued\", \"job_id\": job_id} \ud83e\uddf9 Phase 4 \u2014 Garbage Collection, AI, and Advanced Analytics (Week 5+) \u00b6 Objective \u00b6 Introduce garbage collection, retention policy enforcement, and AI-assisted triage. Tasks \u00b6 Implement GarbageCollector service: Sweep orphaned runs and findings Archive logs >30 days Introduce CVE/CWE/PoC Enrichment Integration with NVD, OSV, ExploitDB Deploy AI assistant for: Finding summaries Risk triage automation Workflow suggestions Add cross-project analytics dashboard Implement export formats (PDF, CSV, JSON) Expected Deliverables \u00b6 Fully production-ready orchestration platform Retention-safe data lifecycle AI triage beta enabled Analytics module complete \ud83d\udcc8 Migration Timeline Overview \u00b6 Week Phase Key Deliverables 1 Phase 0 \u2014 Scaffold Repo, linting, CI/CD, guardrails 2 Phase 1 \u2014 Core Models, DB, CRUD API 3 Phase 2 \u2014 Engine Wrappers, Plugins, Workflow 4 Phase 3 \u2014 API/UI Worker, Triage UI, Auth 5+ Phase 4 \u2014 AI/GC Retention, Enrichment, Analytics \ud83d\ude80 Deployment Strategy \u00b6 Branch-per-phase workflow ( feature/phase-1-core , etc.) Pre-merge CI enforcement for all phases Feature flags for new modules Nightly build for cross-validation Docker-compose dev stack for quick testing Example Command \u00b6 docker compose up -d pytest --maxfail=1 --disable-warnings \ud83e\udde0 Key Success Metrics \u00b6 Metric Target Test Coverage >90% for core-lib & storage CI Lint Pass Rate 100% Workflow Execution Latency <300ms/node avg Risk Score Accuracy \u00b15% of reference Mean Time to Deploy <10 min via CI/CD \ud83d\udd2e Next Steps \u00b6 After migration completion: - Freeze legacy JSON-store modules. - Enable CVE/PoC enrichment. - Integrate AI triage assistant. - Conduct internal red-team testing. - Prepare open-source release structure. Next: CI/CD & Testing Strategy","title":"Migration Plan"},{"location":"architecture/20-migration-and-implementation-phases/#20-migration-implementation-phases","text":"","title":"20 \u2014 Migration &amp; Implementation Phases"},{"location":"architecture/20-migration-and-implementation-phases/#overview","text":"The migration plan defines a controlled, four-phase rollout of SecFlow's refactored architecture \u2014 from scaffolding and core model setup to the full orchestration of tools, UI, and enrichment. This plan emphasizes: - Incremental refactoring (no big-bang rewrite) - Zero architectural drift (guardrails first) - Progressive integration (tools, plugins, UI) - Data isolation and cleanup (garbage collector enabled)","title":"\ud83e\udded Overview"},{"location":"architecture/20-migration-and-implementation-phases/#phase-0-foundation-guardrails-week-1","text":"","title":"\ud83d\udce6 Phase 0 \u2014 Foundation &amp; Guardrails (Week 1)"},{"location":"architecture/20-migration-and-implementation-phases/#objective","text":"Establish the new repository structure and enforce architectural discipline before any migration work.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks","text":"Create mono-repo scaffold under /src/SecFlow/ Add dev environment setup via Makefile , pyproject.toml , and Poetry Enable static analysis tooling : ruff for linting pyright for typing import-linter for import boundaries Setup unit testing scaffold: /tests/core , /tests/wrappers , /tests/plugins Define .github/workflows/ci.yml for matrix builds (Python 3.10\u20133.12) Establish base docs/architecture/ folder for ongoing documentation","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables","text":"Working mono-repo with guardrails CI passing with lint/type/test checks Developer guide for environment setup ( docs/dev-setup.md )","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-command","text":"make init make lint make test","title":"Example Command"},{"location":"architecture/20-migration-and-implementation-phases/#phase-1-core-models-data-persistence-week-2","text":"","title":"\ud83e\uddf1 Phase 1 \u2014 Core Models &amp; Data Persistence (Week 2)"},{"location":"architecture/20-migration-and-implementation-phases/#objective_1","text":"Move fundamental entities (Projects, Findings, Resources, Runs) into modular core-lib and storage packages.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks_1","text":"Create core-lib/ package: Models for Project, Finding, Resource, Run Pydantic schemas for DTOs Create storage/ package: Database adapters for SQLite (local) and PostgreSQL (production) Repository interfaces (IProjectRepo, IFindingsRepo, etc.) Alembic or SQLModel migrations Implement CRUD API endpoints: /api/v1/projects /api/v1/findings /api/v1/resources Add test fixtures for sample data","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables_1","text":"Persistent data layer Core models validated by schema Functional CRUD endpoints 80%+ test coverage on models and repos","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-model","text":"class Project(BaseModel): id: UUID name: str owner: str description: Optional[str] created_at: datetime updated_at: datetime","title":"Example Model"},{"location":"architecture/20-migration-and-implementation-phases/#phase-2-findings-engine-wrappers-workflow-week-3","text":"","title":"\u2699\ufe0f Phase 2 \u2014 Findings Engine, Wrappers &amp; Workflow (Week 3)"},{"location":"architecture/20-migration-and-implementation-phases/#objective_2","text":"Integrate scanning tools (Nuclei, Feroxbuster, Katana, etc.) into the new workflow engine and plugin registry.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks_2","text":"Implement findings-engine/ : Normalization layer for all scanners Parser adapters for each tool Implement wrappers/ : NucleiWrapper, FeroxWrapper, ZAPWrapper Each using standardized manifest + sandbox Create plugins/ package: Detection and enrichment plugins (e.g., CVEMapper, RiskScorer) Build workflow engine with DAG executor: YAML recipe parsing Input/output data mapping Caching and persistence Integrate tool registry UI in web frontend","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables_2","text":"Tool registry and manifest system Workflow DAG execution engine Normalized findings output (JSON schema compliant) Risk engine integration (Phase 1 of enrichment)","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-wrapper-interface","text":"class ToolWrapper(Protocol): def prepare(self, config: Dict[str, Any]) -> None: \"\"\"Prepare tool with configuration.\"\"\" pass def run(self) -> ToolOutput: \"\"\"Execute tool and return output.\"\"\" pass def parse_output(self, raw: str) -> List[Finding]: \"\"\"Parse raw output into findings.\"\"\" pass","title":"Example Wrapper Interface"},{"location":"architecture/20-migration-and-implementation-phases/#phase-3-api-worker-and-triage-ui-week-4","text":"","title":"\ud83c\udf10 Phase 3 \u2014 API, Worker, and Triage UI (Week 4)"},{"location":"architecture/20-migration-and-implementation-phases/#objective_3","text":"Deliver full orchestration capability through APIs, background workers, and a lightweight HTMX/React UI.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks_3","text":"Create web-api/ : REST endpoints for workflows, findings, triage WebSocket for live updates Create worker/ : Celery/asyncio-based job processor Queues for workflow nodes and enrichment Create triage-ui/ : Interactive HTMX dashboard for findings triage Tabs: \"Projects\", \"Findings\", \"Tools\", \"Metrics\" Implement user auth & RBAC JWT + role middleware Add audit logging for all changes Integrate observability stack (Prometheus, OpenTelemetry)","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables_3","text":"Full end-to-end scan \u2192 finding \u2192 triage pipeline Live progress dashboard Role-based access and logging Metrics export for dashboards","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#example-endpoint","text":"@app.post(\"/api/v1/workflows/run\") async def run_workflow(workflow_id: str): job_id = await worker.enqueue(workflow_id) return {\"status\": \"queued\", \"job_id\": job_id}","title":"Example Endpoint"},{"location":"architecture/20-migration-and-implementation-phases/#phase-4-garbage-collection-ai-and-advanced-analytics-week-5","text":"","title":"\ud83e\uddf9 Phase 4 \u2014 Garbage Collection, AI, and Advanced Analytics (Week 5+)"},{"location":"architecture/20-migration-and-implementation-phases/#objective_4","text":"Introduce garbage collection, retention policy enforcement, and AI-assisted triage.","title":"Objective"},{"location":"architecture/20-migration-and-implementation-phases/#tasks_4","text":"Implement GarbageCollector service: Sweep orphaned runs and findings Archive logs >30 days Introduce CVE/CWE/PoC Enrichment Integration with NVD, OSV, ExploitDB Deploy AI assistant for: Finding summaries Risk triage automation Workflow suggestions Add cross-project analytics dashboard Implement export formats (PDF, CSV, JSON)","title":"Tasks"},{"location":"architecture/20-migration-and-implementation-phases/#expected-deliverables_4","text":"Fully production-ready orchestration platform Retention-safe data lifecycle AI triage beta enabled Analytics module complete","title":"Expected Deliverables"},{"location":"architecture/20-migration-and-implementation-phases/#migration-timeline-overview","text":"Week Phase Key Deliverables 1 Phase 0 \u2014 Scaffold Repo, linting, CI/CD, guardrails 2 Phase 1 \u2014 Core Models, DB, CRUD API 3 Phase 2 \u2014 Engine Wrappers, Plugins, Workflow 4 Phase 3 \u2014 API/UI Worker, Triage UI, Auth 5+ Phase 4 \u2014 AI/GC Retention, Enrichment, Analytics","title":"\ud83d\udcc8 Migration Timeline Overview"},{"location":"architecture/20-migration-and-implementation-phases/#deployment-strategy","text":"Branch-per-phase workflow ( feature/phase-1-core , etc.) Pre-merge CI enforcement for all phases Feature flags for new modules Nightly build for cross-validation Docker-compose dev stack for quick testing","title":"\ud83d\ude80 Deployment Strategy"},{"location":"architecture/20-migration-and-implementation-phases/#example-command_1","text":"docker compose up -d pytest --maxfail=1 --disable-warnings","title":"Example Command"},{"location":"architecture/20-migration-and-implementation-phases/#key-success-metrics","text":"Metric Target Test Coverage >90% for core-lib & storage CI Lint Pass Rate 100% Workflow Execution Latency <300ms/node avg Risk Score Accuracy \u00b15% of reference Mean Time to Deploy <10 min via CI/CD","title":"\ud83e\udde0 Key Success Metrics"},{"location":"architecture/20-migration-and-implementation-phases/#next-steps","text":"After migration completion: - Freeze legacy JSON-store modules. - Enable CVE/PoC enrichment. - Integrate AI triage assistant. - Conduct internal red-team testing. - Prepare open-source release structure. Next: CI/CD & Testing Strategy","title":"\ud83d\udd2e Next Steps"},{"location":"architecture/21-ci-cd-and-testing-strategy/","text":"21 \u2014 CI/CD and Testing Strategy \u00b6 \ud83e\udded Overview \u00b6 The CI/CD and testing strategy ensures SecFlow maintains high reliability, reproducibility, and security compliance across all modules. It defines automated pipelines for build validation, testing, linting, packaging, and deployment to staging and production. SecFlow's architecture enables modular CI pipelines for packages (core-lib, storage, wrappers, plugins) and apps (web-api, worker, triage-ui). \u2699\ufe0f CI/CD Architecture Diagram \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Developer\"] B[\"Commit / PR \u2192 GitHub Repository\"] C[\"GitHub Actions CI Workflow<br/>- Lint (Ruff)<br/>- Type Check (Pyright)<br/>- Test (Pytest Matrix)<br/>- Build (Poetry / Docker)\"] D[\"Artifacts Published \u2192 Container Registry\"] E[\"CD Pipeline (Staging \u2192 Production)\"] A --> B B --> C C --> D D --> E \ud83e\uddf1 CI Pipeline Structure \u00b6 Files \u00b6 .github/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"workflows/\"] B[\"ci.yml<br/>(Main build & test pipeline)\"] C[\"lint.yml<br/>(Fast linting PR checks)\"] D[\"deploy.yml<br/>(CD pipeline to staging/prod)\"] E[\"nightly.yml<br/>(Nightly validation builds)\"] F[\"security-scan.yml<br/>(Dependency & container scanning)\"] A --> B A --> C A --> D A --> E A --> F Environments \u00b6 dev \u2192 local or containerized build staging \u2192 auto-deployed for QA validation production \u2192 manual approval required \ud83e\uddea Test Taxonomy \u00b6 Level Scope Example Unit Tests Function-level logic validation Testing CVSS normalization, config parsing Integration Tests Module interoperability NucleiWrapper + FindingsEngine Functional Tests End-to-end system behavior Workflow execution pipeline Regression Tests Legacy feature coverage Old project import/export Performance Tests Latency and scalability Parallel scan runs Security Tests Dependency & vulnerability checks Pip-audit, Trivy \ud83e\udde9 Test Framework Stack \u00b6 Tool Purpose pytest Primary test runner pytest-asyncio Async tests for API and worker pytest-cov Coverage reports tox Matrix execution (Python 3.10\u20133.12) httpx HTTP API test client sqlite-memory Fast ephemeral DB backend for testing faker Generate synthetic test data pytest-docker Integration tests for containerized tools \ud83e\uddf1 Test Folder Structure \u00b6 tests/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"tests/\"] B[\"core/\"] C[\"test_models.py\"] D[\"test_utils.py\"] E[\"test_ports.py\"] F[\"wrappers/\"] G[\"test_nuclei_wrapper.py\"] H[\"test_ferox_wrapper.py\"] I[\"test_zap_wrapper.py\"] J[\"plugins/\"] K[\"test_cve_enrichment.py\"] L[\"test_risk_scoring.py\"] M[\"api/\"] N[\"test_projects_api.py\"] O[\"test_findings_api.py\"] P[\"test_workflow_api.py\"] Q[\"e2e/\"] R[\"test_workflow_dag_execution.py\"] A --> B A --> F A --> J A --> M A --> Q B --> C B --> D B --> E F --> G F --> H F --> I J --> K J --> L M --> N M --> O M --> P Q --> R \ud83e\uddee CI Matrix Configuration Example \u00b6 # .github/workflows/ci.yml name: CI on: push: branches: [ main ] pull_request: branches: [ main ] jobs: build-test: runs-on: ubuntu-latest strategy: matrix: python-version: [ \"3.10\", \"3.11\", \"3.12\" ] database: [ \"sqlite\", \"postgres\" ] steps: - uses: actions/checkout@v4 - uses: actions/setup-python@v5 with: python-version: ${{ matrix.python-version }} - run: pip install poetry - run: poetry install - run: make lint - run: make test DB=${{ matrix.database }} - run: pytest --cov=src --cov-report=xml \ud83e\udde0 Deployment Pipeline \u00b6 Staging Pipeline (Continuous Deployment) \u00b6 Triggered on merge to main Deploys to staging environment automatically Runs post-deploy smoke tests: /healthz endpoint Workflow execution sanity test Production Pipeline \u00b6 Requires manual approval ( workflow_dispatch ) Signs Docker images before publishing Deploys to Kubernetes or Docker Swarm cluster Monitors deployment via Prometheus metrics Example job snippet: \u00b6 - name: Deploy to Staging run: | docker-compose -f docker-compose.staging.yml up -d pytest tests/e2e/ --maxfail=1 \ud83e\uddf0 Build Artifacts & Packages \u00b6 Type Output Destination Python Wheels dist/*.whl PyPI private index Docker Images SecFlow-api , SecFlow-worker Container registry Reports coverage.xml , lint.txt , typecheck.json GitHub artifacts Documentation mkdocs site/ GitHub Pages \ud83e\udde0 Quality Gates \u00b6 Check Tool Threshold Linting Ruff No errors Type Checking Pyright 100% coverage Test Coverage Pytest + Coverage > 90% Dependency Scan Pip-audit / Trivy 0 Critical Build Size Docker < 400 MB per image Failed gates block merges automatically. \ud83e\uddea Continuous Security Testing \u00b6 Dependency Auditing: via pip-audit and Safety Container Scanning: via Trivy in CI Secrets Detection: via gitleaks pre-commit hook Infrastructure Scan: via tfsec (for IaC configs) pip install pip-audit safety gitleaks trivy make security-scan \ud83d\udd04 Regression & Replay Testing \u00b6 Each workflow run can be recorded and replayed for regression tests. This ensures stability across version upgrades. Example: \u00b6 pytest tests/e2e/test_workflow_dag_execution.py --record pytest --replay last-run Replay data is stored under /tests/artifacts/replays/ . \ud83e\uddf0 Local Developer Testing \u00b6 Developers can run lightweight tests locally: make test pytest -k \"not e2e\" With Docker-enabled integration tests: make test-docker \ud83d\udcca Metrics & Reporting \u00b6 After each CI build: - Coverage report published to Codecov - Lint/type results annotated in GitHub PR - Performance metrics logged to Prometheus Example coverage badge: \u00b6 [![Coverage](https://img.shields.io/badge/coverage-93%25-brightgreen)](https://codecov.io/gh/SecFlow) \ud83e\uddf1 Disaster Recovery & Rollback \u00b6 Every deployment is versioned: - Docker image tags = vX.Y.Z-buildhash Rollback command: \u00b6 docker pull SecFlow-api:v1.3.2 docker compose up -d --no-build Database snapshots every 6h during staging deployment \ud83d\udd12 Contract Testing \u00b6 Contract tests ensure data integrity and API compatibility across system boundaries. They validate invariants that must hold true regardless of implementation changes. Contract Test Categories \u00b6 Category Purpose Location Example Finding Invariants Validate finding data structure tests/contracts/test_finding_invariants.py Detector ID regex, UTC timestamps Storage Layout Validate data persistence format tests/contracts/test_storage_layout.py Schema version, file structure Finding Invariants Contract \u00b6 Purpose: Ensure findings conform to expected data structure and format requirements. Key Validations: - Detector IDs match pattern: ^[A-Za-z0-9_.-]+$ - Timestamps are ISO 8601 UTC format ending with 'Z' - Required fields are present and properly typed Test Structure: # Positive cases - valid data should pass def test_detector_id_regex_valid (): assert DETECTOR_RE . match ( \"EXAMPLE_1\" ) # Negative cases - invalid data should fail def test_detector_id_regex_invalid (): assert not DETECTOR_RE . match ( \"invalid:id\" ) # Future implementation stubs @pytest . mark . xfail ( reason = \"Implementation pending\" ) def test_finding_schema_validation (): # Will pass when schema validation is implemented Storage Layout Contract \u00b6 Purpose: Ensure data persistence maintains expected structure and versioning. Key Validations: - Schema version field present and valid format - File structure supports expected operations - Backward compatibility for data migration Test Structure: # Happy path - valid storage layout def test_store_layout_has_schema_version ( tmp_path ): data = { \"store_schema_version\" : \"1.0.0\" } assert \"store_schema_version\" in data # Edge cases - missing or invalid versions def test_store_layout_missing_schema_version ( tmp_path ): data = { \"detector_id\" : \"test\" } assert \"store_schema_version\" not in data Contract Test Execution \u00b6 Run contract tests independently: # Run all contract tests pytest tests/contracts/ -q # Run specific contract category pytest tests/contracts/test_finding_invariants.py -q pytest tests/contracts/test_storage_layout.py -q Contract Test Principles \u00b6 Fail Fast: Contract tests should fail immediately when invariants are violated Implementation Agnostic: Tests validate contracts, not implementation details Future-Proof: Use @pytest.mark.xfail for pending implementations Comprehensive Coverage: Include both positive and negative test cases Clear Documentation: Each test explains the contract being validated Integration with CI/CD \u00b6 Contract tests run in the main CI pipeline: - name : Run Contract Tests run : pytest tests/contracts/ --tb=short They provide early warning when: - Data format changes break compatibility - Schema evolution violates existing contracts - New implementations don't meet contract requirements \ud83d\udd2e Future Enhancements \u00b6 Integration with GitHub Advanced Security (code scanning) Dynamic Test Selection (test impacted code only) Chaos Testing on worker queue reliability Parallelized build matrix using GitHub Actions caching Next: Developer Experience & Documentation Plan","title":"CI/CD & Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#21-cicd-and-testing-strategy","text":"","title":"21 \u2014 CI/CD and Testing Strategy"},{"location":"architecture/21-ci-cd-and-testing-strategy/#overview","text":"The CI/CD and testing strategy ensures SecFlow maintains high reliability, reproducibility, and security compliance across all modules. It defines automated pipelines for build validation, testing, linting, packaging, and deployment to staging and production. SecFlow's architecture enables modular CI pipelines for packages (core-lib, storage, wrappers, plugins) and apps (web-api, worker, triage-ui).","title":"\ud83e\udded Overview"},{"location":"architecture/21-ci-cd-and-testing-strategy/#cicd-architecture-diagram","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"Developer\"] B[\"Commit / PR \u2192 GitHub Repository\"] C[\"GitHub Actions CI Workflow<br/>- Lint (Ruff)<br/>- Type Check (Pyright)<br/>- Test (Pytest Matrix)<br/>- Build (Poetry / Docker)\"] D[\"Artifacts Published \u2192 Container Registry\"] E[\"CD Pipeline (Staging \u2192 Production)\"] A --> B B --> C C --> D D --> E","title":"\u2699\ufe0f CI/CD Architecture Diagram"},{"location":"architecture/21-ci-cd-and-testing-strategy/#ci-pipeline-structure","text":"","title":"\ud83e\uddf1 CI Pipeline Structure"},{"location":"architecture/21-ci-cd-and-testing-strategy/#files","text":".github/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"workflows/\"] B[\"ci.yml<br/>(Main build & test pipeline)\"] C[\"lint.yml<br/>(Fast linting PR checks)\"] D[\"deploy.yml<br/>(CD pipeline to staging/prod)\"] E[\"nightly.yml<br/>(Nightly validation builds)\"] F[\"security-scan.yml<br/>(Dependency & container scanning)\"] A --> B A --> C A --> D A --> E A --> F","title":"Files"},{"location":"architecture/21-ci-cd-and-testing-strategy/#environments","text":"dev \u2192 local or containerized build staging \u2192 auto-deployed for QA validation production \u2192 manual approval required","title":"Environments"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-taxonomy","text":"Level Scope Example Unit Tests Function-level logic validation Testing CVSS normalization, config parsing Integration Tests Module interoperability NucleiWrapper + FindingsEngine Functional Tests End-to-end system behavior Workflow execution pipeline Regression Tests Legacy feature coverage Old project import/export Performance Tests Latency and scalability Parallel scan runs Security Tests Dependency & vulnerability checks Pip-audit, Trivy","title":"\ud83e\uddea Test Taxonomy"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-framework-stack","text":"Tool Purpose pytest Primary test runner pytest-asyncio Async tests for API and worker pytest-cov Coverage reports tox Matrix execution (Python 3.10\u20133.12) httpx HTTP API test client sqlite-memory Fast ephemeral DB backend for testing faker Generate synthetic test data pytest-docker Integration tests for containerized tools","title":"\ud83e\udde9 Test Framework Stack"},{"location":"architecture/21-ci-cd-and-testing-strategy/#test-folder-structure","text":"tests/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"tests/\"] B[\"core/\"] C[\"test_models.py\"] D[\"test_utils.py\"] E[\"test_ports.py\"] F[\"wrappers/\"] G[\"test_nuclei_wrapper.py\"] H[\"test_ferox_wrapper.py\"] I[\"test_zap_wrapper.py\"] J[\"plugins/\"] K[\"test_cve_enrichment.py\"] L[\"test_risk_scoring.py\"] M[\"api/\"] N[\"test_projects_api.py\"] O[\"test_findings_api.py\"] P[\"test_workflow_api.py\"] Q[\"e2e/\"] R[\"test_workflow_dag_execution.py\"] A --> B A --> F A --> J A --> M A --> Q B --> C B --> D B --> E F --> G F --> H F --> I J --> K J --> L M --> N M --> O M --> P Q --> R","title":"\ud83e\uddf1 Test Folder Structure"},{"location":"architecture/21-ci-cd-and-testing-strategy/#ci-matrix-configuration-example","text":"# .github/workflows/ci.yml name: CI on: push: branches: [ main ] pull_request: branches: [ main ] jobs: build-test: runs-on: ubuntu-latest strategy: matrix: python-version: [ \"3.10\", \"3.11\", \"3.12\" ] database: [ \"sqlite\", \"postgres\" ] steps: - uses: actions/checkout@v4 - uses: actions/setup-python@v5 with: python-version: ${{ matrix.python-version }} - run: pip install poetry - run: poetry install - run: make lint - run: make test DB=${{ matrix.database }} - run: pytest --cov=src --cov-report=xml","title":"\ud83e\uddee CI Matrix Configuration Example"},{"location":"architecture/21-ci-cd-and-testing-strategy/#deployment-pipeline","text":"","title":"\ud83e\udde0 Deployment Pipeline"},{"location":"architecture/21-ci-cd-and-testing-strategy/#staging-pipeline-continuous-deployment","text":"Triggered on merge to main Deploys to staging environment automatically Runs post-deploy smoke tests: /healthz endpoint Workflow execution sanity test","title":"Staging Pipeline (Continuous Deployment)"},{"location":"architecture/21-ci-cd-and-testing-strategy/#production-pipeline","text":"Requires manual approval ( workflow_dispatch ) Signs Docker images before publishing Deploys to Kubernetes or Docker Swarm cluster Monitors deployment via Prometheus metrics","title":"Production Pipeline"},{"location":"architecture/21-ci-cd-and-testing-strategy/#example-job-snippet","text":"- name: Deploy to Staging run: | docker-compose -f docker-compose.staging.yml up -d pytest tests/e2e/ --maxfail=1","title":"Example job snippet:"},{"location":"architecture/21-ci-cd-and-testing-strategy/#build-artifacts-packages","text":"Type Output Destination Python Wheels dist/*.whl PyPI private index Docker Images SecFlow-api , SecFlow-worker Container registry Reports coverage.xml , lint.txt , typecheck.json GitHub artifacts Documentation mkdocs site/ GitHub Pages","title":"\ud83e\uddf0 Build Artifacts &amp; Packages"},{"location":"architecture/21-ci-cd-and-testing-strategy/#quality-gates","text":"Check Tool Threshold Linting Ruff No errors Type Checking Pyright 100% coverage Test Coverage Pytest + Coverage > 90% Dependency Scan Pip-audit / Trivy 0 Critical Build Size Docker < 400 MB per image Failed gates block merges automatically.","title":"\ud83e\udde0 Quality Gates"},{"location":"architecture/21-ci-cd-and-testing-strategy/#continuous-security-testing","text":"Dependency Auditing: via pip-audit and Safety Container Scanning: via Trivy in CI Secrets Detection: via gitleaks pre-commit hook Infrastructure Scan: via tfsec (for IaC configs) pip install pip-audit safety gitleaks trivy make security-scan","title":"\ud83e\uddea Continuous Security Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#regression-replay-testing","text":"Each workflow run can be recorded and replayed for regression tests. This ensures stability across version upgrades.","title":"\ud83d\udd04 Regression &amp; Replay Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#example","text":"pytest tests/e2e/test_workflow_dag_execution.py --record pytest --replay last-run Replay data is stored under /tests/artifacts/replays/ .","title":"Example:"},{"location":"architecture/21-ci-cd-and-testing-strategy/#local-developer-testing","text":"Developers can run lightweight tests locally: make test pytest -k \"not e2e\" With Docker-enabled integration tests: make test-docker","title":"\ud83e\uddf0 Local Developer Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#metrics-reporting","text":"After each CI build: - Coverage report published to Codecov - Lint/type results annotated in GitHub PR - Performance metrics logged to Prometheus","title":"\ud83d\udcca Metrics &amp; Reporting"},{"location":"architecture/21-ci-cd-and-testing-strategy/#example-coverage-badge","text":"[![Coverage](https://img.shields.io/badge/coverage-93%25-brightgreen)](https://codecov.io/gh/SecFlow)","title":"Example coverage badge:"},{"location":"architecture/21-ci-cd-and-testing-strategy/#disaster-recovery-rollback","text":"Every deployment is versioned: - Docker image tags = vX.Y.Z-buildhash","title":"\ud83e\uddf1 Disaster Recovery &amp; Rollback"},{"location":"architecture/21-ci-cd-and-testing-strategy/#rollback-command","text":"docker pull SecFlow-api:v1.3.2 docker compose up -d --no-build Database snapshots every 6h during staging deployment","title":"Rollback command:"},{"location":"architecture/21-ci-cd-and-testing-strategy/#contract-testing","text":"Contract tests ensure data integrity and API compatibility across system boundaries. They validate invariants that must hold true regardless of implementation changes.","title":"\ud83d\udd12 Contract Testing"},{"location":"architecture/21-ci-cd-and-testing-strategy/#contract-test-categories","text":"Category Purpose Location Example Finding Invariants Validate finding data structure tests/contracts/test_finding_invariants.py Detector ID regex, UTC timestamps Storage Layout Validate data persistence format tests/contracts/test_storage_layout.py Schema version, file structure","title":"Contract Test Categories"},{"location":"architecture/21-ci-cd-and-testing-strategy/#finding-invariants-contract","text":"Purpose: Ensure findings conform to expected data structure and format requirements. Key Validations: - Detector IDs match pattern: ^[A-Za-z0-9_.-]+$ - Timestamps are ISO 8601 UTC format ending with 'Z' - Required fields are present and properly typed Test Structure: # Positive cases - valid data should pass def test_detector_id_regex_valid (): assert DETECTOR_RE . match ( \"EXAMPLE_1\" ) # Negative cases - invalid data should fail def test_detector_id_regex_invalid (): assert not DETECTOR_RE . match ( \"invalid:id\" ) # Future implementation stubs @pytest . mark . xfail ( reason = \"Implementation pending\" ) def test_finding_schema_validation (): # Will pass when schema validation is implemented","title":"Finding Invariants Contract"},{"location":"architecture/21-ci-cd-and-testing-strategy/#storage-layout-contract","text":"Purpose: Ensure data persistence maintains expected structure and versioning. Key Validations: - Schema version field present and valid format - File structure supports expected operations - Backward compatibility for data migration Test Structure: # Happy path - valid storage layout def test_store_layout_has_schema_version ( tmp_path ): data = { \"store_schema_version\" : \"1.0.0\" } assert \"store_schema_version\" in data # Edge cases - missing or invalid versions def test_store_layout_missing_schema_version ( tmp_path ): data = { \"detector_id\" : \"test\" } assert \"store_schema_version\" not in data","title":"Storage Layout Contract"},{"location":"architecture/21-ci-cd-and-testing-strategy/#contract-test-execution","text":"Run contract tests independently: # Run all contract tests pytest tests/contracts/ -q # Run specific contract category pytest tests/contracts/test_finding_invariants.py -q pytest tests/contracts/test_storage_layout.py -q","title":"Contract Test Execution"},{"location":"architecture/21-ci-cd-and-testing-strategy/#contract-test-principles","text":"Fail Fast: Contract tests should fail immediately when invariants are violated Implementation Agnostic: Tests validate contracts, not implementation details Future-Proof: Use @pytest.mark.xfail for pending implementations Comprehensive Coverage: Include both positive and negative test cases Clear Documentation: Each test explains the contract being validated","title":"Contract Test Principles"},{"location":"architecture/21-ci-cd-and-testing-strategy/#integration-with-cicd","text":"Contract tests run in the main CI pipeline: - name : Run Contract Tests run : pytest tests/contracts/ --tb=short They provide early warning when: - Data format changes break compatibility - Schema evolution violates existing contracts - New implementations don't meet contract requirements","title":"Integration with CI/CD"},{"location":"architecture/21-ci-cd-and-testing-strategy/#future-enhancements","text":"Integration with GitHub Advanced Security (code scanning) Dynamic Test Selection (test impacted code only) Chaos Testing on worker queue reliability Parallelized build matrix using GitHub Actions caching Next: Developer Experience & Documentation Plan","title":"\ud83d\udd2e Future Enhancements"},{"location":"architecture/22-developer-experience-and-docs/","text":"22 \u2014 Developer Experience & Documentation Plan \u00b6 \ud83e\udded Overview \u00b6 SecFlow prioritizes an efficient, reliable, and enjoyable developer experience (DX) that promotes rapid iteration without compromising architectural integrity. This section outlines the local environment setup, development workflow, CLI tools, documentation strategy, and contribution guidelines. \ud83e\udde9 Core DX Principles \u00b6 Principle Description Fast Feedback Every command ( make test , make dev ) provides results in < 5s. Safe by Default Guardrails enforce architectural discipline (ruff, pyright, import-linter). Visible Architecture Auto-generated MkDocs diagrams and interlinked specs. One-Command Onboarding New developers can start coding in minutes. DevOps Parity Local, CI, and production environments behave identically. \u2699\ufe0f Local Environment Setup \u00b6 Prerequisites \u00b6 Python \u22653.11 Poetry Docker + Docker Compose Node.js \u226518 (for triage-ui builds) Setup Commands \u00b6 git clone https://github.com/SecFlow/security-toolkit.git cd security-toolkit make init make up make init performs: - Poetry virtualenv setup - Dependency installation - Database migration (SQLite dev DB) - Git pre-commit hooks (ruff, pyright, pytest) - Environment validation ( make check ) \ud83e\uddf1 Developer Makefile Commands \u00b6 Command Description make up Start local stack (API, worker, UI) make down Stop containers and cleanup make dev Launch dev server with autoreload make test Run all tests make lint Run lint + type check make docs Build MkDocs documentation make check Validate dependencies and environment make clean Remove caches and build artifacts Example: \u00b6 make dev # http://localhost:8080 \ud83e\uddf0 Developer CLI \u2014 secflowctl \u00b6 SecFlow provides an integrated command-line interface for developers and operators. Example Commands \u00b6 secflowctl project list secflowctl scan start nuclei --project mytest secflowctl workflow run owasp-top10.yaml secflowctl plugin list secflowctl risk report --format table CLI Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"secflowctl/\"] B[\"__main__.py\"] C[\"commands/\"] D[\"project.py\"] E[\"scan.py\"] F[\"workflow.py\"] G[\"plugin.py\"] H[\"risk.py\"] I[\"utils/\"] J[\"formatting.py\"] A --> B A --> C A --> I C --> D C --> E C --> F C --> G C --> H I --> J CLI Design Features \u00b6 Rich TUI (Textual) output for interactive sessions Tab autocompletion JSON/YAML output modes Direct API calls or local orchestration \ud83e\udded Development Workflow \u00b6 Branching Model \u00b6 main \u2192 stable production branch develop \u2192 integration branch feature/* \u2192 new features or refactors fix/* \u2192 bug fixes release/* \u2192 versioned release candidates Pull Request Requirements \u00b6 1 approving review All CI checks passed (lint, test, type, security scan) Linked issue ID Updated changelog entry Commit Style (Conventional Commits) \u00b6 feat(workflow): add nuclei plugin support fix(storage): handle null resource hash docs(readme): update setup instructions \ud83d\udcd8 Documentation System (MkDocs) \u00b6 MkDocs Project Layout \u00b6 docs/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/<br/>(Deep technical docs)\"] C[\"api/<br/>(OpenAPI spec & usage)\"] D[\"dev/<br/>(Developer onboarding)\"] E[\"operations/<br/>(Deployment & monitoring)\"] F[\"plugins/<br/>(Plugin development guide)\"] G[\"index.md<br/>(Landing page)\"] A --> B A --> C A --> D A --> E A --> F A --> G Build Command \u00b6 make docs # Builds into site/ Features \u00b6 Material for MkDocs theme Auto-generated architecture diagrams via Mermaid Built-in search and code highlighting Versioned docs (via mike) for each release Plugin-based navigation for \"core\", \"apps\", \"plugins\", \"API\" Example mkdocs.yml: \u00b6 site_name: \"SecFlow Developer Docs\" theme: name: material features: - navigation.sections - navigation.instant markdown_extensions: - toc: permalink: true - admonition - pymdownx.superfences plugins: - search - mermaid2 - awesome-pages \ud83e\udde0 Architecture Visualization \u00b6 Architecture diagrams are auto-generated from the codebase using diagrams + pydeps. Example script: \u00b6 make diagram Output: /docs/architecture/assets/architecture.svg Example generated image (ASCII simplified): \u00b6 +-------------+ | web-api | +------+------+----+ | | +-------v--+ +----v--------+ | worker | | triage-ui | +----------+ +-------------+ | | +----v---------v----+ | core-lib / engine | +--------------------+ \ud83e\udde9 Developer Onboarding Flow \u00b6 Step Description 1. Clone Repository git clone and make init 2. Run Local Stack make up \u2192 visit localhost:8080 3. Explore CLI secflowctl help 4. Read Docs make docs \u2192 open site/index.html 5. Add Feature Create feature/my-feature branch 6. Submit PR Push to GitHub, run CI, get review 7. Merge & Deploy Auto-deployed to staging \ud83e\uddf0 Tooling Summary \u00b6 Category Tool Purpose Package Management Poetry Dependency control Linting Ruff Code style & hygiene Typing Pyright Static type enforcement Testing Pytest Unit & integration tests Docs MkDocs Documentation Visualization Diagrams Auto-generate architecture maps Security Gitleaks, Safety Prevent secrets & vulns Formatting Black Consistent code format \ud83e\udde9 Developer Guidelines \u00b6 Code Style \u00b6 Follow PEP8 + Ruff config Enforce docstrings for public classes/functions Avoid circular imports (use ports) Use dependency injection where possible Commit Rules \u00b6 Keep commits atomic (1 logical change) Use descriptive messages Reference related issue (#123) Code Review Expectations \u00b6 Small PRs (<500 LOC preferred) Include before/after screenshots for UI changes Add unit tests for every new feature \ud83e\udde0 Local Testing Shortcuts \u00b6 Scenario Command Run single test pytest tests/core/test_models.py::test_project_model Run tests with coverage pytest --cov=src --cov-report=html Run async API tests pytest tests/api -k \"async\" Skip slow tests pytest -m \"not slow\" Lint before commit pre-commit run --all-files \ud83d\udcd8 Developer Documentation Contributions \u00b6 Docs are written in Markdown under docs/ Always include: \u00b6 Code examples Usage samples Config references Build locally via: \u00b6 mkdocs serve For architecture updates: \u00b6 make diagram && make docs \ud83d\udd2e Future DX Enhancements \u00b6 VS Code Dev Containers for instant onboarding CLI Autoupdate System for secflowctl MkDocs AI Search Plugin (semantic search) Interactive Architecture Map (Mermaid + Live API) Unified Dev Dashboard combining logs, metrics, and CI build state Next: Future Roadmap","title":"Dev Experience & Docs"},{"location":"architecture/22-developer-experience-and-docs/#22-developer-experience-documentation-plan","text":"","title":"22 \u2014 Developer Experience &amp; Documentation Plan"},{"location":"architecture/22-developer-experience-and-docs/#overview","text":"SecFlow prioritizes an efficient, reliable, and enjoyable developer experience (DX) that promotes rapid iteration without compromising architectural integrity. This section outlines the local environment setup, development workflow, CLI tools, documentation strategy, and contribution guidelines.","title":"\ud83e\udded Overview"},{"location":"architecture/22-developer-experience-and-docs/#core-dx-principles","text":"Principle Description Fast Feedback Every command ( make test , make dev ) provides results in < 5s. Safe by Default Guardrails enforce architectural discipline (ruff, pyright, import-linter). Visible Architecture Auto-generated MkDocs diagrams and interlinked specs. One-Command Onboarding New developers can start coding in minutes. DevOps Parity Local, CI, and production environments behave identically.","title":"\ud83e\udde9 Core DX Principles"},{"location":"architecture/22-developer-experience-and-docs/#local-environment-setup","text":"","title":"\u2699\ufe0f Local Environment Setup"},{"location":"architecture/22-developer-experience-and-docs/#prerequisites","text":"Python \u22653.11 Poetry Docker + Docker Compose Node.js \u226518 (for triage-ui builds)","title":"Prerequisites"},{"location":"architecture/22-developer-experience-and-docs/#setup-commands","text":"git clone https://github.com/SecFlow/security-toolkit.git cd security-toolkit make init make up make init performs: - Poetry virtualenv setup - Dependency installation - Database migration (SQLite dev DB) - Git pre-commit hooks (ruff, pyright, pytest) - Environment validation ( make check )","title":"Setup Commands"},{"location":"architecture/22-developer-experience-and-docs/#developer-makefile-commands","text":"Command Description make up Start local stack (API, worker, UI) make down Stop containers and cleanup make dev Launch dev server with autoreload make test Run all tests make lint Run lint + type check make docs Build MkDocs documentation make check Validate dependencies and environment make clean Remove caches and build artifacts","title":"\ud83e\uddf1 Developer Makefile Commands"},{"location":"architecture/22-developer-experience-and-docs/#example","text":"make dev # http://localhost:8080","title":"Example:"},{"location":"architecture/22-developer-experience-and-docs/#developer-cli-secflowctl","text":"SecFlow provides an integrated command-line interface for developers and operators.","title":"\ud83e\uddf0 Developer CLI \u2014 secflowctl"},{"location":"architecture/22-developer-experience-and-docs/#example-commands","text":"secflowctl project list secflowctl scan start nuclei --project mytest secflowctl workflow run owasp-top10.yaml secflowctl plugin list secflowctl risk report --format table","title":"Example Commands"},{"location":"architecture/22-developer-experience-and-docs/#cli-structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"secflowctl/\"] B[\"__main__.py\"] C[\"commands/\"] D[\"project.py\"] E[\"scan.py\"] F[\"workflow.py\"] G[\"plugin.py\"] H[\"risk.py\"] I[\"utils/\"] J[\"formatting.py\"] A --> B A --> C A --> I C --> D C --> E C --> F C --> G C --> H I --> J","title":"CLI Structure"},{"location":"architecture/22-developer-experience-and-docs/#cli-design-features","text":"Rich TUI (Textual) output for interactive sessions Tab autocompletion JSON/YAML output modes Direct API calls or local orchestration","title":"CLI Design Features"},{"location":"architecture/22-developer-experience-and-docs/#development-workflow","text":"","title":"\ud83e\udded Development Workflow"},{"location":"architecture/22-developer-experience-and-docs/#branching-model","text":"main \u2192 stable production branch develop \u2192 integration branch feature/* \u2192 new features or refactors fix/* \u2192 bug fixes release/* \u2192 versioned release candidates","title":"Branching Model"},{"location":"architecture/22-developer-experience-and-docs/#pull-request-requirements","text":"1 approving review All CI checks passed (lint, test, type, security scan) Linked issue ID Updated changelog entry","title":"Pull Request Requirements"},{"location":"architecture/22-developer-experience-and-docs/#commit-style-conventional-commits","text":"feat(workflow): add nuclei plugin support fix(storage): handle null resource hash docs(readme): update setup instructions","title":"Commit Style (Conventional Commits)"},{"location":"architecture/22-developer-experience-and-docs/#documentation-system-mkdocs","text":"","title":"\ud83d\udcd8 Documentation System (MkDocs)"},{"location":"architecture/22-developer-experience-and-docs/#mkdocs-project-layout","text":"docs/ %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"docs/\"] B[\"architecture/<br/>(Deep technical docs)\"] C[\"api/<br/>(OpenAPI spec & usage)\"] D[\"dev/<br/>(Developer onboarding)\"] E[\"operations/<br/>(Deployment & monitoring)\"] F[\"plugins/<br/>(Plugin development guide)\"] G[\"index.md<br/>(Landing page)\"] A --> B A --> C A --> D A --> E A --> F A --> G","title":"MkDocs Project Layout"},{"location":"architecture/22-developer-experience-and-docs/#build-command","text":"make docs # Builds into site/","title":"Build Command"},{"location":"architecture/22-developer-experience-and-docs/#features","text":"Material for MkDocs theme Auto-generated architecture diagrams via Mermaid Built-in search and code highlighting Versioned docs (via mike) for each release Plugin-based navigation for \"core\", \"apps\", \"plugins\", \"API\"","title":"Features"},{"location":"architecture/22-developer-experience-and-docs/#example-mkdocsyml","text":"site_name: \"SecFlow Developer Docs\" theme: name: material features: - navigation.sections - navigation.instant markdown_extensions: - toc: permalink: true - admonition - pymdownx.superfences plugins: - search - mermaid2 - awesome-pages","title":"Example mkdocs.yml:"},{"location":"architecture/22-developer-experience-and-docs/#architecture-visualization","text":"Architecture diagrams are auto-generated from the codebase using diagrams + pydeps.","title":"\ud83e\udde0 Architecture Visualization"},{"location":"architecture/22-developer-experience-and-docs/#example-script","text":"make diagram Output: /docs/architecture/assets/architecture.svg","title":"Example script:"},{"location":"architecture/22-developer-experience-and-docs/#example-generated-image-ascii-simplified","text":"+-------------+ | web-api | +------+------+----+ | | +-------v--+ +----v--------+ | worker | | triage-ui | +----------+ +-------------+ | | +----v---------v----+ | core-lib / engine | +--------------------+","title":"Example generated image (ASCII simplified):"},{"location":"architecture/22-developer-experience-and-docs/#developer-onboarding-flow","text":"Step Description 1. Clone Repository git clone and make init 2. Run Local Stack make up \u2192 visit localhost:8080 3. Explore CLI secflowctl help 4. Read Docs make docs \u2192 open site/index.html 5. Add Feature Create feature/my-feature branch 6. Submit PR Push to GitHub, run CI, get review 7. Merge & Deploy Auto-deployed to staging","title":"\ud83e\udde9 Developer Onboarding Flow"},{"location":"architecture/22-developer-experience-and-docs/#tooling-summary","text":"Category Tool Purpose Package Management Poetry Dependency control Linting Ruff Code style & hygiene Typing Pyright Static type enforcement Testing Pytest Unit & integration tests Docs MkDocs Documentation Visualization Diagrams Auto-generate architecture maps Security Gitleaks, Safety Prevent secrets & vulns Formatting Black Consistent code format","title":"\ud83e\uddf0 Tooling Summary"},{"location":"architecture/22-developer-experience-and-docs/#developer-guidelines","text":"","title":"\ud83e\udde9 Developer Guidelines"},{"location":"architecture/22-developer-experience-and-docs/#code-style","text":"Follow PEP8 + Ruff config Enforce docstrings for public classes/functions Avoid circular imports (use ports) Use dependency injection where possible","title":"Code Style"},{"location":"architecture/22-developer-experience-and-docs/#commit-rules","text":"Keep commits atomic (1 logical change) Use descriptive messages Reference related issue (#123)","title":"Commit Rules"},{"location":"architecture/22-developer-experience-and-docs/#code-review-expectations","text":"Small PRs (<500 LOC preferred) Include before/after screenshots for UI changes Add unit tests for every new feature","title":"Code Review Expectations"},{"location":"architecture/22-developer-experience-and-docs/#local-testing-shortcuts","text":"Scenario Command Run single test pytest tests/core/test_models.py::test_project_model Run tests with coverage pytest --cov=src --cov-report=html Run async API tests pytest tests/api -k \"async\" Skip slow tests pytest -m \"not slow\" Lint before commit pre-commit run --all-files","title":"\ud83e\udde0 Local Testing Shortcuts"},{"location":"architecture/22-developer-experience-and-docs/#developer-documentation-contributions","text":"Docs are written in Markdown under docs/","title":"\ud83d\udcd8 Developer Documentation Contributions"},{"location":"architecture/22-developer-experience-and-docs/#always-include","text":"Code examples Usage samples Config references","title":"Always include:"},{"location":"architecture/22-developer-experience-and-docs/#build-locally-via","text":"mkdocs serve","title":"Build locally via:"},{"location":"architecture/22-developer-experience-and-docs/#for-architecture-updates","text":"make diagram && make docs","title":"For architecture updates:"},{"location":"architecture/22-developer-experience-and-docs/#future-dx-enhancements","text":"VS Code Dev Containers for instant onboarding CLI Autoupdate System for secflowctl MkDocs AI Search Plugin (semantic search) Interactive Architecture Map (Mermaid + Live API) Unified Dev Dashboard combining logs, metrics, and CI build state Next: Future Roadmap","title":"\ud83d\udd2e Future DX Enhancements"},{"location":"architecture/23-future-roadmap/","text":"23 \u2014 Future Roadmap & Evolution Strategy \u00b6 \ud83e\udded Overview \u00b6 This roadmap defines the strategic evolution of SecFlow from a modular pentesting orchestration toolkit to a next-generation autonomous security platform . It describes the planned capabilities, milestones, and technical direction for the next 12\u201324 months \u2014 integrating AI, multi-tenant architectures, collaborative triage, and intelligent automation. \ud83e\uddf1 Phase Overview (Long-Term Vision) \u00b6 Phase Focus Area Description Phase 1 (Current) Foundation Mono-repo, hexagonal core, plugin registry, workflow orchestration Phase 2 (6\u201312 months) Intelligence & AI AI triage, enrichment automation, anomaly detection Phase 3 (12\u201318 months) Collaboration Multi-tenant architecture, role-based analytics, red/blue team integration Phase 4 (18\u201324 months) Autonomy Self-optimizing workflows, adaptive scanners, predictive risk modeling \u2699\ufe0f Phase 2 \u2014 Intelligence & AI Integration \u00b6 The AI Integration Phase introduces cognitive automation into all major subsystems of SecFlow. 1. AI-Assisted Triage \u00b6 LLM-based summarization of findings Context inference (vulnerability relevance, duplication detection) Recommendation engine for next actions and prioritization 2. AI-Driven Risk Enrichment \u00b6 Predict missing CVSS scores and exploit likelihood (ML regression models) Ingest external threat feeds (CISA KEV, ExploitDB updates) Leverage EPSS (Exploit Prediction Scoring System) for dynamic risk adjustments 3. Automated Pattern Discovery \u00b6 Cluster recurring issues across multiple projects Identify \"weak controls\" using unsupervised learning Integrate MITRE ATT&CK correlation heatmaps Example architecture: \u00b6 +--------------------------------------------------+ | AI Engine | | - LLM-based Triage Assistant | | - ML Risk Predictor (EPSS + CVSS Hybrid) | | - Anomaly Detector (Outlier Findings) | | - Natural Language Query Interface | +--------------------------------------------------+ 4. Conversational Analysis Interface \u00b6 A secure chat layer connected to core-lib enabling queries like: show me all high-risk CVEs in projects using nginx summarize findings related to broken authentication predict which components will likely fail next pentest \ud83c\udf10 Phase 3 \u2014 Collaboration & Multi-Tenant Platform \u00b6 This phase focuses on scaling SecFlow for enterprise teams, MSSPs, and collaborative pentest environments. 1. Multi-Tenant Architecture \u00b6 Isolated tenants with shared infrastructure RBAC-based access and audit segregation Namespace-aware project storage ( tenant_id/project_id pattern) 2. Centralized Insights Hub \u00b6 Aggregate findings and metrics across tenants Role-based dashboards (CISO, Pentester, Developer) Global intelligence layer \u2014 vulnerability trends, exploit timelines 3. Cross-Project Correlation \u00b6 Federated search across tenants Shared resource registry (optional per policy) Vulnerability fingerprinting and reuse detection 4. Collaboration Tools \u00b6 Comment threads and annotations on findings Assignments and status tracking Shared remediation checklists \ud83e\udd16 Phase 4 \u2014 Autonomous Security Orchestration \u00b6 The final stage evolves SecFlow into an autonomous orchestration and reasoning engine . 1. Adaptive Scanning \u00b6 Automatically adjusts scanning parameters based on previous findings Integrates feedback loop from risk trends Uses Reinforcement Learning (RL) to optimize coverage vs time 2. Self-Healing Workflows \u00b6 Automatically retries failed nodes Re-schedules scans based on impact or SLA breach Learns ideal tool chaining patterns 3. Predictive Risk Forecasting \u00b6 Time-series models predicting vulnerability resurgence Integration with incident response systems for proactive alerts 4. Security Knowledge Graph \u00b6 Unifies all findings, CVEs, CWEs, ATT&CK tactics, and tool metadata Graph queries (Cypher/SPARQL) for advanced relationships Enables semantic enrichment and AI reasoning \ud83e\uddf0 Supporting Infrastructure Evolution \u00b6 Subsystem Upgrade Path Description Database Postgres \u2192 TimescaleDB Time-series analytics and trend computation Cache Layer Redis \u2192 Redis Cluster High-throughput async workflows Task Queue Celery \u2192 Prefect 3 Enhanced DAG orchestration and observability API Gateway FastAPI \u2192 APIStar (optional microgateway) Modular service boundaries UI Stack HTMX \u2192 React + GraphQL Hybrid Real-time triage dashboards \ud83e\udde0 Advanced Integrations \u00b6 Integration Purpose Caido & ZAP Direct import of active scan results GitHub Advanced Security Repo-based vulnerability correlation ExploitDB / Vulners API Continuous POC enrichment CISA KEV Sync Real-time critical vulnerability flagging AI Plugin System Extensible via OpenAI function-style agents \ud83d\udcca Planned Metrics & Analytics Expansion \u00b6 Area Metric Use Case Risk Intelligence Mean CVSS delta over time Track improvement rate Workflow Analytics Avg. scan node latency Optimize orchestration Tool Effectiveness Finding yield per tool ROI assessment Anomaly Detection Outlier frequency Identify misbehaving scans Team Collaboration Comment-to-remediation ratio Operational efficiency \ud83d\udd12 Future Security Enhancements \u00b6 Hardware attestation for sensitive tool execution (YubiHSM/TPM) Encrypted local storage using Fernet or AWS KMS Multi-factor API access tokens Zero-trust plugin sandboxing (seccomp profiles) Supply chain integrity checks (sigstore/cosign) \ud83c\udf0d Open Source & Community Roadmap \u00b6 SecFlow will release a Community Edition (CE) focusing on: - Lightweight orchestration - Built-in nuclei/ferox wrappers - Project & finding dashboards - Local SQLite backend Planned for mid-2026 under an Apache 2.0 license with optional enterprise modules. Community contributions roadmap: \u00b6 Plugin SDK Workflow Recipe Gallery Integration templates (for Burp, ZAP, Caido) AI-Triage contribution guide \ud83d\udcc5 Timeline Summary \u00b6 Quarter Milestone Highlights Q4 2025 Phase 1 \u2013 Finalize core platform API + Worker stable release Q1 2026 Phase 2 \u2013 AI & enrichment rollout LLM triage + CVE prediction Q2 2026 Phase 3 \u2013 Multi-tenant & collaboration Team dashboards, tenant isolation Q3 2026 Phase 4 \u2013 Autonomous orchestration Adaptive scans, AI reasoning graph Q4 2026 Community Edition release OSS launch, plugin marketplace \ud83e\udde9 Success Metrics & KPIs \u00b6 KPI Target Workflow execution success rate \u2265 98% Mean enrichment latency < 500 ms AI triage accuracy \u2265 90% vs manual MTTR (Mean Time to Risk Report) < 10 min Plugin SDK adoption 50+ community plugins Platform uptime (SLA) 99.9% \ud83d\udd2e Long-Term Vision \u00b6 \"From manual scanning to autonomous, continuous, and intelligent security operations.\" SecFlow's long-term mission is to become the AI-native, self-orchestrating red-team assistant , capable of: - Running its own scan campaigns, - Adjusting priorities dynamically, - Explaining vulnerabilities in human language, - Suggesting fixes, - And predicting attack surfaces before exploitation occurs. Next: Final Consensus Summary","title":"Future Roadmap"},{"location":"architecture/23-future-roadmap/#23-future-roadmap-evolution-strategy","text":"","title":"23 \u2014 Future Roadmap &amp; Evolution Strategy"},{"location":"architecture/23-future-roadmap/#overview","text":"This roadmap defines the strategic evolution of SecFlow from a modular pentesting orchestration toolkit to a next-generation autonomous security platform . It describes the planned capabilities, milestones, and technical direction for the next 12\u201324 months \u2014 integrating AI, multi-tenant architectures, collaborative triage, and intelligent automation.","title":"\ud83e\udded Overview"},{"location":"architecture/23-future-roadmap/#phase-overview-long-term-vision","text":"Phase Focus Area Description Phase 1 (Current) Foundation Mono-repo, hexagonal core, plugin registry, workflow orchestration Phase 2 (6\u201312 months) Intelligence & AI AI triage, enrichment automation, anomaly detection Phase 3 (12\u201318 months) Collaboration Multi-tenant architecture, role-based analytics, red/blue team integration Phase 4 (18\u201324 months) Autonomy Self-optimizing workflows, adaptive scanners, predictive risk modeling","title":"\ud83e\uddf1 Phase Overview (Long-Term Vision)"},{"location":"architecture/23-future-roadmap/#phase-2-intelligence-ai-integration","text":"The AI Integration Phase introduces cognitive automation into all major subsystems of SecFlow.","title":"\u2699\ufe0f Phase 2 \u2014 Intelligence &amp; AI Integration"},{"location":"architecture/23-future-roadmap/#1-ai-assisted-triage","text":"LLM-based summarization of findings Context inference (vulnerability relevance, duplication detection) Recommendation engine for next actions and prioritization","title":"1. AI-Assisted Triage"},{"location":"architecture/23-future-roadmap/#2-ai-driven-risk-enrichment","text":"Predict missing CVSS scores and exploit likelihood (ML regression models) Ingest external threat feeds (CISA KEV, ExploitDB updates) Leverage EPSS (Exploit Prediction Scoring System) for dynamic risk adjustments","title":"2. AI-Driven Risk Enrichment"},{"location":"architecture/23-future-roadmap/#3-automated-pattern-discovery","text":"Cluster recurring issues across multiple projects Identify \"weak controls\" using unsupervised learning Integrate MITRE ATT&CK correlation heatmaps","title":"3. Automated Pattern Discovery"},{"location":"architecture/23-future-roadmap/#example-architecture","text":"+--------------------------------------------------+ | AI Engine | | - LLM-based Triage Assistant | | - ML Risk Predictor (EPSS + CVSS Hybrid) | | - Anomaly Detector (Outlier Findings) | | - Natural Language Query Interface | +--------------------------------------------------+","title":"Example architecture:"},{"location":"architecture/23-future-roadmap/#4-conversational-analysis-interface","text":"A secure chat layer connected to core-lib enabling queries like: show me all high-risk CVEs in projects using nginx summarize findings related to broken authentication predict which components will likely fail next pentest","title":"4. Conversational Analysis Interface"},{"location":"architecture/23-future-roadmap/#phase-3-collaboration-multi-tenant-platform","text":"This phase focuses on scaling SecFlow for enterprise teams, MSSPs, and collaborative pentest environments.","title":"\ud83c\udf10 Phase 3 \u2014 Collaboration &amp; Multi-Tenant Platform"},{"location":"architecture/23-future-roadmap/#1-multi-tenant-architecture","text":"Isolated tenants with shared infrastructure RBAC-based access and audit segregation Namespace-aware project storage ( tenant_id/project_id pattern)","title":"1. Multi-Tenant Architecture"},{"location":"architecture/23-future-roadmap/#2-centralized-insights-hub","text":"Aggregate findings and metrics across tenants Role-based dashboards (CISO, Pentester, Developer) Global intelligence layer \u2014 vulnerability trends, exploit timelines","title":"2. Centralized Insights Hub"},{"location":"architecture/23-future-roadmap/#3-cross-project-correlation","text":"Federated search across tenants Shared resource registry (optional per policy) Vulnerability fingerprinting and reuse detection","title":"3. Cross-Project Correlation"},{"location":"architecture/23-future-roadmap/#4-collaboration-tools","text":"Comment threads and annotations on findings Assignments and status tracking Shared remediation checklists","title":"4. Collaboration Tools"},{"location":"architecture/23-future-roadmap/#phase-4-autonomous-security-orchestration","text":"The final stage evolves SecFlow into an autonomous orchestration and reasoning engine .","title":"\ud83e\udd16 Phase 4 \u2014 Autonomous Security Orchestration"},{"location":"architecture/23-future-roadmap/#1-adaptive-scanning","text":"Automatically adjusts scanning parameters based on previous findings Integrates feedback loop from risk trends Uses Reinforcement Learning (RL) to optimize coverage vs time","title":"1. Adaptive Scanning"},{"location":"architecture/23-future-roadmap/#2-self-healing-workflows","text":"Automatically retries failed nodes Re-schedules scans based on impact or SLA breach Learns ideal tool chaining patterns","title":"2. Self-Healing Workflows"},{"location":"architecture/23-future-roadmap/#3-predictive-risk-forecasting","text":"Time-series models predicting vulnerability resurgence Integration with incident response systems for proactive alerts","title":"3. Predictive Risk Forecasting"},{"location":"architecture/23-future-roadmap/#4-security-knowledge-graph","text":"Unifies all findings, CVEs, CWEs, ATT&CK tactics, and tool metadata Graph queries (Cypher/SPARQL) for advanced relationships Enables semantic enrichment and AI reasoning","title":"4. Security Knowledge Graph"},{"location":"architecture/23-future-roadmap/#supporting-infrastructure-evolution","text":"Subsystem Upgrade Path Description Database Postgres \u2192 TimescaleDB Time-series analytics and trend computation Cache Layer Redis \u2192 Redis Cluster High-throughput async workflows Task Queue Celery \u2192 Prefect 3 Enhanced DAG orchestration and observability API Gateway FastAPI \u2192 APIStar (optional microgateway) Modular service boundaries UI Stack HTMX \u2192 React + GraphQL Hybrid Real-time triage dashboards","title":"\ud83e\uddf0 Supporting Infrastructure Evolution"},{"location":"architecture/23-future-roadmap/#advanced-integrations","text":"Integration Purpose Caido & ZAP Direct import of active scan results GitHub Advanced Security Repo-based vulnerability correlation ExploitDB / Vulners API Continuous POC enrichment CISA KEV Sync Real-time critical vulnerability flagging AI Plugin System Extensible via OpenAI function-style agents","title":"\ud83e\udde0 Advanced Integrations"},{"location":"architecture/23-future-roadmap/#planned-metrics-analytics-expansion","text":"Area Metric Use Case Risk Intelligence Mean CVSS delta over time Track improvement rate Workflow Analytics Avg. scan node latency Optimize orchestration Tool Effectiveness Finding yield per tool ROI assessment Anomaly Detection Outlier frequency Identify misbehaving scans Team Collaboration Comment-to-remediation ratio Operational efficiency","title":"\ud83d\udcca Planned Metrics &amp; Analytics Expansion"},{"location":"architecture/23-future-roadmap/#future-security-enhancements","text":"Hardware attestation for sensitive tool execution (YubiHSM/TPM) Encrypted local storage using Fernet or AWS KMS Multi-factor API access tokens Zero-trust plugin sandboxing (seccomp profiles) Supply chain integrity checks (sigstore/cosign)","title":"\ud83d\udd12 Future Security Enhancements"},{"location":"architecture/23-future-roadmap/#open-source-community-roadmap","text":"SecFlow will release a Community Edition (CE) focusing on: - Lightweight orchestration - Built-in nuclei/ferox wrappers - Project & finding dashboards - Local SQLite backend Planned for mid-2026 under an Apache 2.0 license with optional enterprise modules.","title":"\ud83c\udf0d Open Source &amp; Community Roadmap"},{"location":"architecture/23-future-roadmap/#community-contributions-roadmap","text":"Plugin SDK Workflow Recipe Gallery Integration templates (for Burp, ZAP, Caido) AI-Triage contribution guide","title":"Community contributions roadmap:"},{"location":"architecture/23-future-roadmap/#timeline-summary","text":"Quarter Milestone Highlights Q4 2025 Phase 1 \u2013 Finalize core platform API + Worker stable release Q1 2026 Phase 2 \u2013 AI & enrichment rollout LLM triage + CVE prediction Q2 2026 Phase 3 \u2013 Multi-tenant & collaboration Team dashboards, tenant isolation Q3 2026 Phase 4 \u2013 Autonomous orchestration Adaptive scans, AI reasoning graph Q4 2026 Community Edition release OSS launch, plugin marketplace","title":"\ud83d\udcc5 Timeline Summary"},{"location":"architecture/23-future-roadmap/#success-metrics-kpis","text":"KPI Target Workflow execution success rate \u2265 98% Mean enrichment latency < 500 ms AI triage accuracy \u2265 90% vs manual MTTR (Mean Time to Risk Report) < 10 min Plugin SDK adoption 50+ community plugins Platform uptime (SLA) 99.9%","title":"\ud83e\udde9 Success Metrics &amp; KPIs"},{"location":"architecture/23-future-roadmap/#long-term-vision","text":"\"From manual scanning to autonomous, continuous, and intelligent security operations.\" SecFlow's long-term mission is to become the AI-native, self-orchestrating red-team assistant , capable of: - Running its own scan campaigns, - Adjusting priorities dynamically, - Explaining vulnerabilities in human language, - Suggesting fixes, - And predicting attack surfaces before exploitation occurs. Next: Final Consensus Summary","title":"\ud83d\udd2e Long-Term Vision"},{"location":"architecture/24-final-consensus-summary/","text":"24 \u2014 Final Consensus Summary \u00b6 \ud83e\udded Overview \u00b6 This document consolidates the final architectural consensus between ChatGPT and Cursor regarding the design, structure, and roadmap of the SecFlow platform. It serves as the canonical closure of the initial R&D phase and provides a clear blueprint for implementation. \ud83e\uddf1 Core Consensus Highlights \u00b6 Aspect Consensus Decision Rationale Architecture Pattern Hexagonal (Ports & Adapters) Enforces separation of concerns, testability, and modularity Repository Structure Mono-repo with Poetry Simplifies dependency management and cross-package development Workflow Engine DAG-based orchestration Enables complex tool chaining and parallel execution Plugin System Decorator-based registration Provides clean extensibility without complex frameworks Security Model RBAC + Sandboxing Balances usability with security requirements Data Model Pydantic + SQLModel Ensures type safety and database integration \ud83e\uddf1 Foundational Architecture Principles \u00b6 1. Hexagonal Architecture (Ports & Adapters) \u00b6 Core business logic isolated from I/O concerns Ports define interfaces, Adapters implement them Dependency inversion ensures testability and modularity 2. Event-Driven Design \u00b6 Asynchronous workflows using Celery/RQ Event sourcing for audit trails and replay capabilities Pub/Sub patterns for loose coupling 3. Immutable Data Flow \u00b6 Findings are immutable once created Versioned resources for reproducibility Audit trails for all data modifications \ud83e\uddf1 Key Technical Agreements \u00b6 1. Core Packages Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"packages/\"] B[\"core-lib/<br/>(Business logic, ports, DTOs)\"] C[\"findings-engine/<br/>(Normalization, enrichment, deduplication)\"] D[\"wrappers/<br/>(Tool integration adapters)\"] E[\"resources/<br/>(Resource registry and management)\"] F[\"storage/<br/>(Database and file system adapters)\"] G[\"plugins/<br/>(Plugin system and registry)\"] H[\"utils/<br/>(Shared utilities and helpers)\"] A --> B A --> C A --> D A --> E A --> F A --> G A --> H 2. Applications Structure \u00b6 %%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"apps/\"] B[\"cli/<br/>(Command-line interface)\"] C[\"web/<br/>(Web dashboard and API)\"] D[\"worker/<br/>(Background task processor)\"] E[\"admin/<br/>(Administrative tools)\"] A --> B A --> C A --> D A --> E 3. Workflow Orchestration \u00b6 YAML-based DAG definitions for tool chaining Node-based execution with parallel processing Error handling with retry logic and circuit breakers Caching for intermediate results and resource reuse 4. Security Model \u00b6 JWT-based authentication with role-based access control Sandboxed execution for external tools and PoCs Fernet encryption for sensitive data storage Audit logging for all security-relevant operations \ud83e\uddf1 Risk & Compliance Integration \u00b6 Frameworks Adopted \u00b6 NIST 5x5 Risk Matrix for business risk assessment CVSS v3.1 for technical severity scoring CWE/OWASP for vulnerability classification MITRE ATT&CK for attack pattern mapping Risk Formula \u00b6 Risk Score = (Likelihood \u00d7 Impact) \u00d7 Contextual_Factor Where: - Likelihood = CVSS_Exploitability + EPSS_Score - Impact = CVSS_Impact + Business_Criticality - Contextual_Factor = Asset_Value + Exposure_Level \ud83e\uddf1 Migration Path Summary \u00b6 Phase Duration Focus Deliverables Phase 0 2 weeks Foundation & Guardrails Poetry setup, import boundaries, CI/CD Phase 1 4 weeks Core Models & Data Persistence Pydantic models, SQLModel, basic CRUD Phase 2 6 weeks Findings Engine & Workflow Normalization, DAG execution, tool wrappers Phase 3 8 weeks API, Worker & Triage UI REST API, background tasks, web dashboard Phase 4 4 weeks GC, AI & Advanced Analytics Cleanup, enrichment, risk scoring \ud83e\uddf1 Validation Metrics \u00b6 Metric Target Measurement Test Coverage \u2265 90% pytest + coverage.py Type Safety 100% pyright + mypy Import Boundaries 0 violations import-linter Security Scan 0 high/critical bandit + safety Performance < 2s API response locust load testing \ud83e\uddf1 Strategic Extensions Agreed \u00b6 1. AI Integration (Phase 2) \u00b6 LLM-based triage for finding summarization ML risk prediction using EPSS and CVSS correlation Anomaly detection for outlier findings Natural language queries for analysis 2. Multi-Tenant Architecture (Phase 3) \u00b6 Tenant isolation with shared infrastructure Role-based analytics and dashboards Cross-project correlation and federated search Collaboration tools for team workflows 3. Autonomous Orchestration (Phase 4) \u00b6 Adaptive scanning with reinforcement learning Self-healing workflows with automatic retry Predictive risk forecasting using time-series models Security knowledge graph for semantic reasoning \ud83e\uddf1 Documentation Structure \u00b6 The architecture documentation follows a MkDocs-based structure with: - Material theme for professional presentation - Internal linking between related concepts - Code examples in Python, YAML, and JSON - ASCII diagrams for visual understanding - Version control with semantic versioning \ud83e\uddf1 Consensus from Cursor Review \u00b6 Strengths Identified \u00b6 Clear separation of concerns through hexagonal architecture Comprehensive security model with multiple layers Scalable plugin system for extensibility Robust error handling and recovery mechanisms Areas for Improvement \u00b6 Performance optimization for large-scale deployments Monitoring and observability enhancements Developer experience improvements Community contribution guidelines \ud83e\uddf1 Execution Path Forward \u00b6 Immediate Next Steps \u00b6 Set up Poetry workspace with proper dependency management Implement import boundaries using import-linter Create core Pydantic models for findings and projects Establish CI/CD pipeline with quality gates Success Criteria \u00b6 All 24 architecture documents reviewed and approved Core packages implemented with 90%+ test coverage Basic workflow orchestration functional Security model validated through penetration testing \ud83e\uddf1 Final Architectural Mantra \u00b6 \"Hexagonal by design, event-driven by nature, secure by default, extensible by plugin.\" This mantra encapsulates the core principles that will guide all future development of SecFlow. \ud83e\uddf1 Next Deliverables \u00b6 Deliverable Timeline Owner Poetry Workspace Setup Week 1 Lead Engineer Core Models Implementation Week 2-3 Backend Team Import Boundaries Enforcement Week 1 DevOps Team CI/CD Pipeline Week 2 DevOps Team Basic Workflow Engine Week 4-6 Backend Team \ud83e\uddf1 Closure Statement \u00b6 This document represents the official closure of the SecFlow architecture design phase. All stakeholders have reached consensus on: Technical architecture and implementation approach Security model and compliance requirements Migration strategy and timeline Future roadmap and evolution path The project is now ready to proceed to Phase 0: Foundation & Guardrails implementation. Previous: Future Roadmap Back to: Architecture Index","title":"Final Consensus"},{"location":"architecture/24-final-consensus-summary/#24-final-consensus-summary","text":"","title":"24 \u2014 Final Consensus Summary"},{"location":"architecture/24-final-consensus-summary/#overview","text":"This document consolidates the final architectural consensus between ChatGPT and Cursor regarding the design, structure, and roadmap of the SecFlow platform. It serves as the canonical closure of the initial R&D phase and provides a clear blueprint for implementation.","title":"\ud83e\udded Overview"},{"location":"architecture/24-final-consensus-summary/#core-consensus-highlights","text":"Aspect Consensus Decision Rationale Architecture Pattern Hexagonal (Ports & Adapters) Enforces separation of concerns, testability, and modularity Repository Structure Mono-repo with Poetry Simplifies dependency management and cross-package development Workflow Engine DAG-based orchestration Enables complex tool chaining and parallel execution Plugin System Decorator-based registration Provides clean extensibility without complex frameworks Security Model RBAC + Sandboxing Balances usability with security requirements Data Model Pydantic + SQLModel Ensures type safety and database integration","title":"\ud83e\uddf1 Core Consensus Highlights"},{"location":"architecture/24-final-consensus-summary/#foundational-architecture-principles","text":"","title":"\ud83e\uddf1 Foundational Architecture Principles"},{"location":"architecture/24-final-consensus-summary/#1-hexagonal-architecture-ports-adapters","text":"Core business logic isolated from I/O concerns Ports define interfaces, Adapters implement them Dependency inversion ensures testability and modularity","title":"1. Hexagonal Architecture (Ports &amp; Adapters)"},{"location":"architecture/24-final-consensus-summary/#2-event-driven-design","text":"Asynchronous workflows using Celery/RQ Event sourcing for audit trails and replay capabilities Pub/Sub patterns for loose coupling","title":"2. Event-Driven Design"},{"location":"architecture/24-final-consensus-summary/#3-immutable-data-flow","text":"Findings are immutable once created Versioned resources for reproducibility Audit trails for all data modifications","title":"3. Immutable Data Flow"},{"location":"architecture/24-final-consensus-summary/#key-technical-agreements","text":"","title":"\ud83e\uddf1 Key Technical Agreements"},{"location":"architecture/24-final-consensus-summary/#1-core-packages-structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"packages/\"] B[\"core-lib/<br/>(Business logic, ports, DTOs)\"] C[\"findings-engine/<br/>(Normalization, enrichment, deduplication)\"] D[\"wrappers/<br/>(Tool integration adapters)\"] E[\"resources/<br/>(Resource registry and management)\"] F[\"storage/<br/>(Database and file system adapters)\"] G[\"plugins/<br/>(Plugin system and registry)\"] H[\"utils/<br/>(Shared utilities and helpers)\"] A --> B A --> C A --> D A --> E A --> F A --> G A --> H","title":"1. Core Packages Structure"},{"location":"architecture/24-final-consensus-summary/#2-applications-structure","text":"%%{init: {\"theme\":\"neutral\"}}%% flowchart TD A[\"apps/\"] B[\"cli/<br/>(Command-line interface)\"] C[\"web/<br/>(Web dashboard and API)\"] D[\"worker/<br/>(Background task processor)\"] E[\"admin/<br/>(Administrative tools)\"] A --> B A --> C A --> D A --> E","title":"2. Applications Structure"},{"location":"architecture/24-final-consensus-summary/#3-workflow-orchestration","text":"YAML-based DAG definitions for tool chaining Node-based execution with parallel processing Error handling with retry logic and circuit breakers Caching for intermediate results and resource reuse","title":"3. Workflow Orchestration"},{"location":"architecture/24-final-consensus-summary/#4-security-model","text":"JWT-based authentication with role-based access control Sandboxed execution for external tools and PoCs Fernet encryption for sensitive data storage Audit logging for all security-relevant operations","title":"4. Security Model"},{"location":"architecture/24-final-consensus-summary/#risk-compliance-integration","text":"","title":"\ud83e\uddf1 Risk &amp; Compliance Integration"},{"location":"architecture/24-final-consensus-summary/#frameworks-adopted","text":"NIST 5x5 Risk Matrix for business risk assessment CVSS v3.1 for technical severity scoring CWE/OWASP for vulnerability classification MITRE ATT&CK for attack pattern mapping","title":"Frameworks Adopted"},{"location":"architecture/24-final-consensus-summary/#risk-formula","text":"Risk Score = (Likelihood \u00d7 Impact) \u00d7 Contextual_Factor Where: - Likelihood = CVSS_Exploitability + EPSS_Score - Impact = CVSS_Impact + Business_Criticality - Contextual_Factor = Asset_Value + Exposure_Level","title":"Risk Formula"},{"location":"architecture/24-final-consensus-summary/#migration-path-summary","text":"Phase Duration Focus Deliverables Phase 0 2 weeks Foundation & Guardrails Poetry setup, import boundaries, CI/CD Phase 1 4 weeks Core Models & Data Persistence Pydantic models, SQLModel, basic CRUD Phase 2 6 weeks Findings Engine & Workflow Normalization, DAG execution, tool wrappers Phase 3 8 weeks API, Worker & Triage UI REST API, background tasks, web dashboard Phase 4 4 weeks GC, AI & Advanced Analytics Cleanup, enrichment, risk scoring","title":"\ud83e\uddf1 Migration Path Summary"},{"location":"architecture/24-final-consensus-summary/#validation-metrics","text":"Metric Target Measurement Test Coverage \u2265 90% pytest + coverage.py Type Safety 100% pyright + mypy Import Boundaries 0 violations import-linter Security Scan 0 high/critical bandit + safety Performance < 2s API response locust load testing","title":"\ud83e\uddf1 Validation Metrics"},{"location":"architecture/24-final-consensus-summary/#strategic-extensions-agreed","text":"","title":"\ud83e\uddf1 Strategic Extensions Agreed"},{"location":"architecture/24-final-consensus-summary/#1-ai-integration-phase-2","text":"LLM-based triage for finding summarization ML risk prediction using EPSS and CVSS correlation Anomaly detection for outlier findings Natural language queries for analysis","title":"1. AI Integration (Phase 2)"},{"location":"architecture/24-final-consensus-summary/#2-multi-tenant-architecture-phase-3","text":"Tenant isolation with shared infrastructure Role-based analytics and dashboards Cross-project correlation and federated search Collaboration tools for team workflows","title":"2. Multi-Tenant Architecture (Phase 3)"},{"location":"architecture/24-final-consensus-summary/#3-autonomous-orchestration-phase-4","text":"Adaptive scanning with reinforcement learning Self-healing workflows with automatic retry Predictive risk forecasting using time-series models Security knowledge graph for semantic reasoning","title":"3. Autonomous Orchestration (Phase 4)"},{"location":"architecture/24-final-consensus-summary/#documentation-structure","text":"The architecture documentation follows a MkDocs-based structure with: - Material theme for professional presentation - Internal linking between related concepts - Code examples in Python, YAML, and JSON - ASCII diagrams for visual understanding - Version control with semantic versioning","title":"\ud83e\uddf1 Documentation Structure"},{"location":"architecture/24-final-consensus-summary/#consensus-from-cursor-review","text":"","title":"\ud83e\uddf1 Consensus from Cursor Review"},{"location":"architecture/24-final-consensus-summary/#strengths-identified","text":"Clear separation of concerns through hexagonal architecture Comprehensive security model with multiple layers Scalable plugin system for extensibility Robust error handling and recovery mechanisms","title":"Strengths Identified"},{"location":"architecture/24-final-consensus-summary/#areas-for-improvement","text":"Performance optimization for large-scale deployments Monitoring and observability enhancements Developer experience improvements Community contribution guidelines","title":"Areas for Improvement"},{"location":"architecture/24-final-consensus-summary/#execution-path-forward","text":"","title":"\ud83e\uddf1 Execution Path Forward"},{"location":"architecture/24-final-consensus-summary/#immediate-next-steps","text":"Set up Poetry workspace with proper dependency management Implement import boundaries using import-linter Create core Pydantic models for findings and projects Establish CI/CD pipeline with quality gates","title":"Immediate Next Steps"},{"location":"architecture/24-final-consensus-summary/#success-criteria","text":"All 24 architecture documents reviewed and approved Core packages implemented with 90%+ test coverage Basic workflow orchestration functional Security model validated through penetration testing","title":"Success Criteria"},{"location":"architecture/24-final-consensus-summary/#final-architectural-mantra","text":"\"Hexagonal by design, event-driven by nature, secure by default, extensible by plugin.\" This mantra encapsulates the core principles that will guide all future development of SecFlow.","title":"\ud83e\uddf1 Final Architectural Mantra"},{"location":"architecture/24-final-consensus-summary/#next-deliverables","text":"Deliverable Timeline Owner Poetry Workspace Setup Week 1 Lead Engineer Core Models Implementation Week 2-3 Backend Team Import Boundaries Enforcement Week 1 DevOps Team CI/CD Pipeline Week 2 DevOps Team Basic Workflow Engine Week 4-6 Backend Team","title":"\ud83e\uddf1 Next Deliverables"},{"location":"architecture/24-final-consensus-summary/#closure-statement","text":"This document represents the official closure of the SecFlow architecture design phase. All stakeholders have reached consensus on: Technical architecture and implementation approach Security model and compliance requirements Migration strategy and timeline Future roadmap and evolution path The project is now ready to proceed to Phase 0: Foundation & Guardrails implementation. Previous: Future Roadmap Back to: Architecture Index","title":"\ud83e\uddf1 Closure Statement"},{"location":"diagrams/mermaid-style/","text":"Mermaid Diagram Style Guide \u00b6 \ud83c\udfa8 House Style \u00b6 Use this initialization block at the start of every Mermaid diagram for consistent styling: %%{init: { \"theme\": \"neutral\", \"themeVariables\": { \"fontFamily\": \"Inter, ui-sans-serif, system-ui\", \"primaryColor\": \"#0ea5e9\", \"primaryBorderColor\": \"#0ea5e9\", \"primaryTextColor\": \"#0b1220\", \"lineColor\": \"#6b7280\", \"secondaryColor\": \"#f1f5f9\", \"tertiaryColor\": \"#e2e8f0\" } }}%% \ud83d\udccb Usage Guidelines \u00b6 Diagram Types \u00b6 Flowcharts : Use flowchart TD (top-down) for processes, flowchart LR (left-right) for pipelines Sequence Diagrams : Use sequenceDiagram for runtime interactions Class Diagrams : Use classDiagram for data models and relationships State Diagrams : Use stateDiagram-v2 for lifecycle flows Styling Rules \u00b6 Colors : Minimal use; rely on Material theme defaults IDs : Use semantic names (e.g., WebAPI , Worker , findings-engine ) Labels : Keep concise but descriptive Direction : Prefer top-down for hierarchical flows, left-right for sequential processes Custom Classes \u00b6 Define custom styling classes sparingly: classDef box stroke:#0ea5e9,stroke-width:2px,fill:#f8fafc,color:#0b1220; classDef tool fill:#eef,stroke:#88f,stroke-width:1px; classDef role fill:#e2e8f0,stroke:#0ea5e9,stroke-width:2px; \ud83d\udd27 Integration \u00b6 Each diagram should include the init block at the top since Mermaid doesn't support external imports. Copy the init block from this file to maintain consistency across all diagrams.","title":"Mermaid Diagram Style Guide"},{"location":"diagrams/mermaid-style/#mermaid-diagram-style-guide","text":"","title":"Mermaid Diagram Style Guide"},{"location":"diagrams/mermaid-style/#house-style","text":"Use this initialization block at the start of every Mermaid diagram for consistent styling: %%{init: { \"theme\": \"neutral\", \"themeVariables\": { \"fontFamily\": \"Inter, ui-sans-serif, system-ui\", \"primaryColor\": \"#0ea5e9\", \"primaryBorderColor\": \"#0ea5e9\", \"primaryTextColor\": \"#0b1220\", \"lineColor\": \"#6b7280\", \"secondaryColor\": \"#f1f5f9\", \"tertiaryColor\": \"#e2e8f0\" } }}%%","title":"\ud83c\udfa8 House Style"},{"location":"diagrams/mermaid-style/#usage-guidelines","text":"","title":"\ud83d\udccb Usage Guidelines"},{"location":"diagrams/mermaid-style/#diagram-types","text":"Flowcharts : Use flowchart TD (top-down) for processes, flowchart LR (left-right) for pipelines Sequence Diagrams : Use sequenceDiagram for runtime interactions Class Diagrams : Use classDiagram for data models and relationships State Diagrams : Use stateDiagram-v2 for lifecycle flows","title":"Diagram Types"},{"location":"diagrams/mermaid-style/#styling-rules","text":"Colors : Minimal use; rely on Material theme defaults IDs : Use semantic names (e.g., WebAPI , Worker , findings-engine ) Labels : Keep concise but descriptive Direction : Prefer top-down for hierarchical flows, left-right for sequential processes","title":"Styling Rules"},{"location":"diagrams/mermaid-style/#custom-classes","text":"Define custom styling classes sparingly: classDef box stroke:#0ea5e9,stroke-width:2px,fill:#f8fafc,color:#0b1220; classDef tool fill:#eef,stroke:#88f,stroke-width:1px; classDef role fill:#e2e8f0,stroke:#0ea5e9,stroke-width:2px;","title":"Custom Classes"},{"location":"diagrams/mermaid-style/#integration","text":"Each diagram should include the init block at the top since Mermaid doesn't support external imports. Copy the init block from this file to maintain consistency across all diagrams.","title":"\ud83d\udd27 Integration"},{"location":"governance/development-conventions/","text":"Development Conventions \u00b6 Definition of Done (DoD) Checklist \u00b6 Code Quality \u00b6 All tests pass ( make test ) Code coverage meets current milestone threshold (see Coverage Ratchet below) No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes Documentation \u00b6 Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check) Testing \u00b6 Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold Security & Compliance \u00b6 No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed Review Process \u00b6 PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments SOD/EOD Rituals (Coordinator/DevEx Automation) \u00b6 Start of Day (SOD) - Automated by Coordinator \u00b6 CI Status Check : Automated scan of overnight CI results Coverage Monitoring : Automated coverage trend analysis Dependency Alerts : Automated security vulnerability scanning Build Health : Automated build failure notifications Sprint Progress : Automated sprint burn-down updates End of Day (EOD) - DevEx Automation \u00b6 Health Gates : Automated make health execution Coverage Ratchet : Automated coverage threshold validation Documentation Sync : Automated docs build and link validation Metrics Collection : Automated performance and quality metrics Report Generation : Automated EOD summary creation Report Locations \u00b6 Daily Reports : reports/daily/YYYY-MM-DD.md (auto-generated) Sprint Reports : reports/sprints/sprint-N.md (auto-updated) Milestone Reports : reports/milestones/M{N}.md (auto-compiled) Health Reports : reports/health/YYYY-MM-DD-health.md (auto-generated) Branch Naming & PR Rules \u00b6 Branch Naming Convention \u00b6 Features : feat/description-of-feature Bug Fixes : fix/description-of-bug Documentation : docs/description-of-changes Refactoring : refactor/description-of-refactor Hotfixes : hotfix/description-of-fix Security : security/description-of-security-fix PR Size Rule \u00b6 Standard : \u2264400 LOC per PR Large PRs : >400 LOC requires 2 approvals minimum Emergency : Hotfixes may exceed limits with security team approval Documentation : Docs-only PRs have relaxed limits (\u22641000 LOC) Security Review Rule \u00b6 Mandatory Security Review : All PRs touching security-sensitive code Security Team : @security-lead must approve security-related changes Automated Scanning : All PRs scanned for secrets and vulnerabilities Dependency Updates : Security updates get priority review queue CI Order (Fast-Fail) \u00b6 The CI pipeline runs in strict fast-fail order: Phase 1: Code Quality (Fast-Fail) \u00b6 Ruff Linting - Python code style and formatting Pyright Type Checking - Static type analysis Import Linter - Import organization and unused imports Phase 2: Testing (Fast-Fail) \u00b6 Unit Tests - Core functionality testing Coverage Measurement - Code coverage analysis Coverage Ratchet - Threshold enforcement (fail if drop >2%) Phase 3: Integration (Fast-Fail) \u00b6 Contract Tests - API contract validation Integration Tests - Component interaction testing Phase 4: Documentation (Fast-Fail) \u00b6 Docs Health - make health execution Mermaid Parity - Diagram rendering validation ASCII Blocker - ASCII diagram detection Phase 5: End-to-End (Fast-Fail) \u00b6 E2E Tests - Full system testing Performance Tests - Regression detection Coverage Ratchet Ladder \u00b6 Milestone Thresholds \u00b6 M1 : 80% minimum coverage M2 : 82% minimum coverage M3 : 84% minimum coverage M4 : 86% minimum coverage M5 : 88% minimum coverage M6 : 90% minimum coverage Failure Conditions \u00b6 Drop >2% from previous milestone Fall below current milestone threshold New code without corresponding tests Critical paths without test coverage Development Workflow \u00b6 Daily Development Loop \u00b6 Pull Latest : git pull origin main Run Tests : make test (full suite) Quick Iteration : make quick-test (fast feedback) Health Check : make health (before commit) Commit & Push : Follow commit standards Create PR : Include validation evidence Pre-Commit Checklist \u00b6 All tests pass locally Code coverage maintained Linting passes Documentation updated Security review completed (if applicable) PR Creation Process \u00b6 Branch from main : git checkout -b feat/description Implement changes : Follow DoD checklist Run validation : make test && make health Create PR : Include context and evidence Request reviews : Assign appropriate reviewers Monitor CI : Ensure all checks pass Tool Integration \u00b6 Development Tools \u00b6 Poetry : Dependency management Ruff : Code formatting and linting Pyright : Static type checking Pytest : Testing framework MkDocs : Documentation generation CI/CD Tools \u00b6 GitHub Actions : CI pipeline execution Coverage.py : Coverage measurement Mermaid : Diagram rendering Link Checker : Documentation validation Quality Gates \u00b6 Code Quality Gates \u00b6 Linting : Zero ruff/pyright errors Type Safety : Full type coverage for public APIs Import Hygiene : No unused imports Style Consistency : Automated formatting applied Testing Gates \u00b6 Unit Coverage : Minimum threshold per milestone Integration Coverage : Critical path coverage Contract Validation : API compatibility Performance Regression : No performance degradation Documentation Gates \u00b6 Build Success : MkDocs builds without errors Link Validation : No broken internal links Mermaid Parity : Diagrams render correctly ASCII Blocker : No ASCII diagrams allowed Emergency Procedures \u00b6 Hotfix Process \u00b6 Create hotfix branch : hotfix/critical-issue Implement minimal fix : Focus on stability Security review : Mandatory for security issues Fast-track CI : Expedited review process Deploy immediately : After approval Rollback Process \u00b6 Identify issue : Automated monitoring alerts Create rollback PR : Revert problematic changes Emergency review : Security team approval Deploy rollback : Immediate deployment Post-mortem : Document lessons learned Team Coordination \u00b6 Daily Standups \u00b6 SOD Reports : Automated CI status Blockers : Manual escalation process Progress : Sprint burn-down updates Dependencies : Cross-team coordination Weekly Reviews \u00b6 Coverage Trends : Automated analysis Performance Metrics : Automated collection Security Updates : Automated scanning Process Improvements : Team feedback integration Monthly Retrospectives \u00b6 Process Effectiveness : DoD checklist review Tool Updates : CI/CD pipeline improvements Training Needs : Skill gap identification Process Refinement : Continuous improvement","title":"Development Conventions"},{"location":"governance/development-conventions/#development-conventions","text":"","title":"Development Conventions"},{"location":"governance/development-conventions/#definition-of-done-dod-checklist","text":"","title":"Definition of Done (DoD) Checklist"},{"location":"governance/development-conventions/#code-quality","text":"All tests pass ( make test ) Code coverage meets current milestone threshold (see Coverage Ratchet below) No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes","title":"Code Quality"},{"location":"governance/development-conventions/#documentation","text":"Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check)","title":"Documentation"},{"location":"governance/development-conventions/#testing","text":"Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold","title":"Testing"},{"location":"governance/development-conventions/#security-compliance","text":"No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed","title":"Security &amp; Compliance"},{"location":"governance/development-conventions/#review-process","text":"PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments","title":"Review Process"},{"location":"governance/development-conventions/#sodeod-rituals-coordinatordevex-automation","text":"","title":"SOD/EOD Rituals (Coordinator/DevEx Automation)"},{"location":"governance/development-conventions/#start-of-day-sod-automated-by-coordinator","text":"CI Status Check : Automated scan of overnight CI results Coverage Monitoring : Automated coverage trend analysis Dependency Alerts : Automated security vulnerability scanning Build Health : Automated build failure notifications Sprint Progress : Automated sprint burn-down updates","title":"Start of Day (SOD) - Automated by Coordinator"},{"location":"governance/development-conventions/#end-of-day-eod-devex-automation","text":"Health Gates : Automated make health execution Coverage Ratchet : Automated coverage threshold validation Documentation Sync : Automated docs build and link validation Metrics Collection : Automated performance and quality metrics Report Generation : Automated EOD summary creation","title":"End of Day (EOD) - DevEx Automation"},{"location":"governance/development-conventions/#report-locations","text":"Daily Reports : reports/daily/YYYY-MM-DD.md (auto-generated) Sprint Reports : reports/sprints/sprint-N.md (auto-updated) Milestone Reports : reports/milestones/M{N}.md (auto-compiled) Health Reports : reports/health/YYYY-MM-DD-health.md (auto-generated)","title":"Report Locations"},{"location":"governance/development-conventions/#branch-naming-pr-rules","text":"","title":"Branch Naming &amp; PR Rules"},{"location":"governance/development-conventions/#branch-naming-convention","text":"Features : feat/description-of-feature Bug Fixes : fix/description-of-bug Documentation : docs/description-of-changes Refactoring : refactor/description-of-refactor Hotfixes : hotfix/description-of-fix Security : security/description-of-security-fix","title":"Branch Naming Convention"},{"location":"governance/development-conventions/#pr-size-rule","text":"Standard : \u2264400 LOC per PR Large PRs : >400 LOC requires 2 approvals minimum Emergency : Hotfixes may exceed limits with security team approval Documentation : Docs-only PRs have relaxed limits (\u22641000 LOC)","title":"PR Size Rule"},{"location":"governance/development-conventions/#security-review-rule","text":"Mandatory Security Review : All PRs touching security-sensitive code Security Team : @security-lead must approve security-related changes Automated Scanning : All PRs scanned for secrets and vulnerabilities Dependency Updates : Security updates get priority review queue","title":"Security Review Rule"},{"location":"governance/development-conventions/#ci-order-fast-fail","text":"The CI pipeline runs in strict fast-fail order:","title":"CI Order (Fast-Fail)"},{"location":"governance/development-conventions/#phase-1-code-quality-fast-fail","text":"Ruff Linting - Python code style and formatting Pyright Type Checking - Static type analysis Import Linter - Import organization and unused imports","title":"Phase 1: Code Quality (Fast-Fail)"},{"location":"governance/development-conventions/#phase-2-testing-fast-fail","text":"Unit Tests - Core functionality testing Coverage Measurement - Code coverage analysis Coverage Ratchet - Threshold enforcement (fail if drop >2%)","title":"Phase 2: Testing (Fast-Fail)"},{"location":"governance/development-conventions/#phase-3-integration-fast-fail","text":"Contract Tests - API contract validation Integration Tests - Component interaction testing","title":"Phase 3: Integration (Fast-Fail)"},{"location":"governance/development-conventions/#phase-4-documentation-fast-fail","text":"Docs Health - make health execution Mermaid Parity - Diagram rendering validation ASCII Blocker - ASCII diagram detection","title":"Phase 4: Documentation (Fast-Fail)"},{"location":"governance/development-conventions/#phase-5-end-to-end-fast-fail","text":"E2E Tests - Full system testing Performance Tests - Regression detection","title":"Phase 5: End-to-End (Fast-Fail)"},{"location":"governance/development-conventions/#coverage-ratchet-ladder","text":"","title":"Coverage Ratchet Ladder"},{"location":"governance/development-conventions/#milestone-thresholds","text":"M1 : 80% minimum coverage M2 : 82% minimum coverage M3 : 84% minimum coverage M4 : 86% minimum coverage M5 : 88% minimum coverage M6 : 90% minimum coverage","title":"Milestone Thresholds"},{"location":"governance/development-conventions/#failure-conditions","text":"Drop >2% from previous milestone Fall below current milestone threshold New code without corresponding tests Critical paths without test coverage","title":"Failure Conditions"},{"location":"governance/development-conventions/#development-workflow","text":"","title":"Development Workflow"},{"location":"governance/development-conventions/#daily-development-loop","text":"Pull Latest : git pull origin main Run Tests : make test (full suite) Quick Iteration : make quick-test (fast feedback) Health Check : make health (before commit) Commit & Push : Follow commit standards Create PR : Include validation evidence","title":"Daily Development Loop"},{"location":"governance/development-conventions/#pre-commit-checklist","text":"All tests pass locally Code coverage maintained Linting passes Documentation updated Security review completed (if applicable)","title":"Pre-Commit Checklist"},{"location":"governance/development-conventions/#pr-creation-process","text":"Branch from main : git checkout -b feat/description Implement changes : Follow DoD checklist Run validation : make test && make health Create PR : Include context and evidence Request reviews : Assign appropriate reviewers Monitor CI : Ensure all checks pass","title":"PR Creation Process"},{"location":"governance/development-conventions/#tool-integration","text":"","title":"Tool Integration"},{"location":"governance/development-conventions/#development-tools","text":"Poetry : Dependency management Ruff : Code formatting and linting Pyright : Static type checking Pytest : Testing framework MkDocs : Documentation generation","title":"Development Tools"},{"location":"governance/development-conventions/#cicd-tools","text":"GitHub Actions : CI pipeline execution Coverage.py : Coverage measurement Mermaid : Diagram rendering Link Checker : Documentation validation","title":"CI/CD Tools"},{"location":"governance/development-conventions/#quality-gates","text":"","title":"Quality Gates"},{"location":"governance/development-conventions/#code-quality-gates","text":"Linting : Zero ruff/pyright errors Type Safety : Full type coverage for public APIs Import Hygiene : No unused imports Style Consistency : Automated formatting applied","title":"Code Quality Gates"},{"location":"governance/development-conventions/#testing-gates","text":"Unit Coverage : Minimum threshold per milestone Integration Coverage : Critical path coverage Contract Validation : API compatibility Performance Regression : No performance degradation","title":"Testing Gates"},{"location":"governance/development-conventions/#documentation-gates","text":"Build Success : MkDocs builds without errors Link Validation : No broken internal links Mermaid Parity : Diagrams render correctly ASCII Blocker : No ASCII diagrams allowed","title":"Documentation Gates"},{"location":"governance/development-conventions/#emergency-procedures","text":"","title":"Emergency Procedures"},{"location":"governance/development-conventions/#hotfix-process","text":"Create hotfix branch : hotfix/critical-issue Implement minimal fix : Focus on stability Security review : Mandatory for security issues Fast-track CI : Expedited review process Deploy immediately : After approval","title":"Hotfix Process"},{"location":"governance/development-conventions/#rollback-process","text":"Identify issue : Automated monitoring alerts Create rollback PR : Revert problematic changes Emergency review : Security team approval Deploy rollback : Immediate deployment Post-mortem : Document lessons learned","title":"Rollback Process"},{"location":"governance/development-conventions/#team-coordination","text":"","title":"Team Coordination"},{"location":"governance/development-conventions/#daily-standups","text":"SOD Reports : Automated CI status Blockers : Manual escalation process Progress : Sprint burn-down updates Dependencies : Cross-team coordination","title":"Daily Standups"},{"location":"governance/development-conventions/#weekly-reviews","text":"Coverage Trends : Automated analysis Performance Metrics : Automated collection Security Updates : Automated scanning Process Improvements : Team feedback integration","title":"Weekly Reviews"},{"location":"governance/development-conventions/#monthly-retrospectives","text":"Process Effectiveness : DoD checklist review Tool Updates : CI/CD pipeline improvements Training Needs : Skill gap identification Process Refinement : Continuous improvement","title":"Monthly Retrospectives"},{"location":"governance/engineering-standards/","text":"Engineering Standards & Governance \u00b6 Definition of Done (DoD) Checklist \u00b6 Code Quality \u00b6 All tests pass ( make test ) Code coverage meets current milestone threshold (see Coverage Ratchet below) No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes Documentation \u00b6 Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check) Testing \u00b6 Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold Security & Compliance \u00b6 No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed Review Process \u00b6 PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments Branching & Commit Standards \u00b6 Branch Naming \u00b6 Feature branches : feat/description-of-feature Bug fixes : fix/description-of-bug Documentation : docs/description-of-changes Refactoring : refactor/description-of-refactor Hotfixes : hotfix/description-of-fix Commit Standards \u00b6 Format : type(scope): description Types : feat , fix , docs , style , refactor , test , chore Squash : All commits squashed before merge Rebase : Rebase on main before creating PR Protected main : Direct pushes to main are disabled Pull Request Rules \u00b6 Size limit : \u2264400 LOC or requires 2 approvals Description : Must include context, testing instructions, and validation evidence Reviewers : Minimum 1 approval, 2 for large PRs CI : All checks must pass before merge CI Pipeline Order & Gates \u00b6 The CI pipeline runs in strict order with gates that must pass: Code Quality Gates ruff - Python linting and formatting pyright - Static type checking import-linter - Import organization and unused import detection Testing Gates Unit tests + coverage measurement Coverage ratchet enforcement (see below) Contract tests Integration tests Documentation Gates Docs health check ( make health ) Mermaid parity validation ASCII blocker check (no ASCII diagrams) Internal link validation End-to-End Gates E2E test suite Performance regression checks Coverage Ratchet Ladder \u00b6 Coverage thresholds increase by milestone with strict enforcement: M1 : 80% minimum coverage M2 : 82% minimum coverage M3 : 84% minimum coverage M4 : 86% minimum coverage M5 : 88% minimum coverage M6 : 90% minimum coverage Failure conditions : - Drop >2% from previous milestone - Fall below current milestone threshold - New code without tests Documentation Health Gates \u00b6 Mermaid Parity Check \u00b6 All diagrams must render identically in both Mermaid and ASCII ASCII diagrams are blocked (use make health to validate) Local Mermaid JS files only (no CDN dependencies) Superfences Configuration \u00b6 Use pymdownx.superfences for code blocks Mermaid diagrams use mermaid fence type Local JS files: js/mermaid.min.js and js/mermaid-init.js Fence Hygiene Tips \u00b6 Always specify language for code blocks Use proper fence types for diagrams Test rendering with mkdocs serve Validate with make health before commit SOD/EOD Rituals \u00b6 Start of Day (SOD) \u00b6 Review overnight CI results Check for any failed builds or coverage drops Update project status if needed Plan daily tasks based on current sprint goals End of Day (EOD) \u00b6 Run make health to validate docs Create EOD summary report Update project metrics and status Ensure all PRs have proper validation evidence Report Locations \u00b6 Daily reports : reports/daily/YYYY-MM-DD.md Sprint reports : reports/sprints/sprint-N.md Milestone reports : reports/milestones/M{N}.md ADR-Lite Process \u00b6 When ADR-Lite is Required \u00b6 Architecture decisions affecting multiple components Changes to CI/CD pipeline or tooling New dependencies or technology choices Security or compliance policy changes Breaking changes to APIs or data models ADR-Lite Template \u00b6 Reference: docs/adr/0000-adr-template.md Required sections : - Status (Proposed/Accepted/Deprecated/Superseded) - Context and Problem Statement - Decision Drivers - Considered Options - Decision Outcome - Consequences \"Read \u2192 Run \u2192 Update\" Loop \u00b6 Read Phase \u00b6 Read relevant documentation Understand current architecture Review existing patterns and conventions Check for related issues or PRs Run Phase \u00b6 Set up local development environment Run tests to understand current state Experiment with changes locally Validate with make health and make test Update Phase \u00b6 Implement changes following DoD checklist Update documentation as needed Add tests for new functionality Submit PR with validation evidence Adoption & Integration \u00b6 PR Template Integration \u00b6 The DoD checklist should be referenced in all PR templates. Add this to your PR template: ## DoD Checklist Please review and complete the [ Engineering Standards DoD checklist ]( governance/engineering-standards.md#definition-of-done-dod-checklist ). ## Validation Evidence Paste output from: - `make test` - `make health` - Coverage report Team Onboarding \u00b6 New team members must read this page before first PR Link from \"Developer Start Here\" page Include in team onboarding checklist Review quarterly for updates Continuous Improvement \u00b6 Monthly review of DoD effectiveness Quarterly updates to standards based on team feedback Annual review of CI pipeline and coverage thresholds Regular updates to tooling and best practices","title":"Engineering Standards"},{"location":"governance/engineering-standards/#engineering-standards-governance","text":"","title":"Engineering Standards &amp; Governance"},{"location":"governance/engineering-standards/#definition-of-done-dod-checklist","text":"","title":"Definition of Done (DoD) Checklist"},{"location":"governance/engineering-standards/#code-quality","text":"All tests pass ( make test ) Code coverage meets current milestone threshold (see Coverage Ratchet below) No linting errors ( ruff , pyright , import-linter ) All imports properly organized and unused imports removed Code follows project style guidelines Type hints added for all public functions and classes","title":"Code Quality"},{"location":"governance/engineering-standards/#documentation","text":"Documentation updated for any API changes Docstrings added for new functions/classes README updated if installation/usage changed Docs health gates pass ( make health ) No broken internal links Mermaid diagrams render correctly (parity check)","title":"Documentation"},{"location":"governance/engineering-standards/#testing","text":"Unit tests written for new functionality Integration tests updated if needed Contract tests pass E2E tests pass (if applicable) Test coverage doesn't drop below current threshold","title":"Testing"},{"location":"governance/engineering-standards/#security-compliance","text":"No hardcoded secrets or credentials Security-sensitive code reviewed Dependencies updated and vulnerabilities checked Input validation implemented where needed","title":"Security &amp; Compliance"},{"location":"governance/engineering-standards/#review-process","text":"PR description includes context and testing instructions PR size \u2264400 LOC or requires 2 approvals All CI checks pass Code review completed by required reviewers Validation evidence pasted in PR comments","title":"Review Process"},{"location":"governance/engineering-standards/#branching-commit-standards","text":"","title":"Branching &amp; Commit Standards"},{"location":"governance/engineering-standards/#branch-naming","text":"Feature branches : feat/description-of-feature Bug fixes : fix/description-of-bug Documentation : docs/description-of-changes Refactoring : refactor/description-of-refactor Hotfixes : hotfix/description-of-fix","title":"Branch Naming"},{"location":"governance/engineering-standards/#commit-standards","text":"Format : type(scope): description Types : feat , fix , docs , style , refactor , test , chore Squash : All commits squashed before merge Rebase : Rebase on main before creating PR Protected main : Direct pushes to main are disabled","title":"Commit Standards"},{"location":"governance/engineering-standards/#pull-request-rules","text":"Size limit : \u2264400 LOC or requires 2 approvals Description : Must include context, testing instructions, and validation evidence Reviewers : Minimum 1 approval, 2 for large PRs CI : All checks must pass before merge","title":"Pull Request Rules"},{"location":"governance/engineering-standards/#ci-pipeline-order-gates","text":"The CI pipeline runs in strict order with gates that must pass: Code Quality Gates ruff - Python linting and formatting pyright - Static type checking import-linter - Import organization and unused import detection Testing Gates Unit tests + coverage measurement Coverage ratchet enforcement (see below) Contract tests Integration tests Documentation Gates Docs health check ( make health ) Mermaid parity validation ASCII blocker check (no ASCII diagrams) Internal link validation End-to-End Gates E2E test suite Performance regression checks","title":"CI Pipeline Order &amp; Gates"},{"location":"governance/engineering-standards/#coverage-ratchet-ladder","text":"Coverage thresholds increase by milestone with strict enforcement: M1 : 80% minimum coverage M2 : 82% minimum coverage M3 : 84% minimum coverage M4 : 86% minimum coverage M5 : 88% minimum coverage M6 : 90% minimum coverage Failure conditions : - Drop >2% from previous milestone - Fall below current milestone threshold - New code without tests","title":"Coverage Ratchet Ladder"},{"location":"governance/engineering-standards/#documentation-health-gates","text":"","title":"Documentation Health Gates"},{"location":"governance/engineering-standards/#mermaid-parity-check","text":"All diagrams must render identically in both Mermaid and ASCII ASCII diagrams are blocked (use make health to validate) Local Mermaid JS files only (no CDN dependencies)","title":"Mermaid Parity Check"},{"location":"governance/engineering-standards/#superfences-configuration","text":"Use pymdownx.superfences for code blocks Mermaid diagrams use mermaid fence type Local JS files: js/mermaid.min.js and js/mermaid-init.js","title":"Superfences Configuration"},{"location":"governance/engineering-standards/#fence-hygiene-tips","text":"Always specify language for code blocks Use proper fence types for diagrams Test rendering with mkdocs serve Validate with make health before commit","title":"Fence Hygiene Tips"},{"location":"governance/engineering-standards/#sodeod-rituals","text":"","title":"SOD/EOD Rituals"},{"location":"governance/engineering-standards/#start-of-day-sod","text":"Review overnight CI results Check for any failed builds or coverage drops Update project status if needed Plan daily tasks based on current sprint goals","title":"Start of Day (SOD)"},{"location":"governance/engineering-standards/#end-of-day-eod","text":"Run make health to validate docs Create EOD summary report Update project metrics and status Ensure all PRs have proper validation evidence","title":"End of Day (EOD)"},{"location":"governance/engineering-standards/#report-locations","text":"Daily reports : reports/daily/YYYY-MM-DD.md Sprint reports : reports/sprints/sprint-N.md Milestone reports : reports/milestones/M{N}.md","title":"Report Locations"},{"location":"governance/engineering-standards/#adr-lite-process","text":"","title":"ADR-Lite Process"},{"location":"governance/engineering-standards/#when-adr-lite-is-required","text":"Architecture decisions affecting multiple components Changes to CI/CD pipeline or tooling New dependencies or technology choices Security or compliance policy changes Breaking changes to APIs or data models","title":"When ADR-Lite is Required"},{"location":"governance/engineering-standards/#adr-lite-template","text":"Reference: docs/adr/0000-adr-template.md Required sections : - Status (Proposed/Accepted/Deprecated/Superseded) - Context and Problem Statement - Decision Drivers - Considered Options - Decision Outcome - Consequences","title":"ADR-Lite Template"},{"location":"governance/engineering-standards/#read-run-update-loop","text":"","title":"\"Read \u2192 Run \u2192 Update\" Loop"},{"location":"governance/engineering-standards/#read-phase","text":"Read relevant documentation Understand current architecture Review existing patterns and conventions Check for related issues or PRs","title":"Read Phase"},{"location":"governance/engineering-standards/#run-phase","text":"Set up local development environment Run tests to understand current state Experiment with changes locally Validate with make health and make test","title":"Run Phase"},{"location":"governance/engineering-standards/#update-phase","text":"Implement changes following DoD checklist Update documentation as needed Add tests for new functionality Submit PR with validation evidence","title":"Update Phase"},{"location":"governance/engineering-standards/#adoption-integration","text":"","title":"Adoption &amp; Integration"},{"location":"governance/engineering-standards/#pr-template-integration","text":"The DoD checklist should be referenced in all PR templates. Add this to your PR template: ## DoD Checklist Please review and complete the [ Engineering Standards DoD checklist ]( governance/engineering-standards.md#definition-of-done-dod-checklist ). ## Validation Evidence Paste output from: - `make test` - `make health` - Coverage report","title":"PR Template Integration"},{"location":"governance/engineering-standards/#team-onboarding","text":"New team members must read this page before first PR Link from \"Developer Start Here\" page Include in team onboarding checklist Review quarterly for updates","title":"Team Onboarding"},{"location":"governance/engineering-standards/#continuous-improvement","text":"Monthly review of DoD effectiveness Quarterly updates to standards based on team feedback Annual review of CI pipeline and coverage thresholds Regular updates to tooling and best practices","title":"Continuous Improvement"},{"location":"review/REVIEW_GUIDELINES/","text":"SecFlow Architecture Review Guidelines \u00b6 \ud83c\udfaf Overview \u00b6 This document outlines the review and validation process for the SecFlow architecture documentation suite. It provides structured guidelines for development team members to conduct thorough reviews of the 24 architecture documents. \ud83d\udc65 Review Team Structure \u00b6 Role Responsibilities Documents to Review Lead Architect Overall architecture consistency, design patterns All documents (00-24) Backend Engineers Core packages, data models, API design 04, 05, 12, 13, 16, 17, 18 Frontend Engineers UX design, web interface, CLI experience 08, 22 DevOps Engineers CI/CD, deployment, infrastructure 03, 21, 15, 17 Security Engineers Security model, compliance, risk assessment 16, 19, 14, 11 QA Engineers Testing strategy, validation criteria 21, 18, 12 Product Manager Business requirements, user experience 01, 08, 20, 23 \ud83d\udccb Review Checklist \u00b6 1. Technical Accuracy \u00b6 Code examples are syntactically correct Architecture diagrams are accurate and complete Technical specifications are feasible Dependencies and integrations are realistic Performance requirements are achievable 2. Completeness \u00b6 All required sections are present Cross-references between documents are valid Examples are comprehensive and relevant Edge cases and error scenarios are covered Future considerations are addressed 3. Consistency \u00b6 Terminology is consistent across documents Design patterns align with overall architecture Data models are consistent between documents Security principles are uniformly applied Naming conventions are followed 4. Clarity and Usability \u00b6 Concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Document structure is logical Language is professional and accessible 5. Security and Compliance \u00b6 Security requirements are clearly defined Compliance frameworks are properly referenced Risk assessment methodology is sound Data protection measures are adequate Audit requirements are met \ud83d\udd0d Review Process \u00b6 Phase 1: Individual Review (Week 1) \u00b6 Each team member reviews their assigned documents using the checklist above. Phase 2: Cross-Review (Week 2) \u00b6 Team members review documents outside their primary expertise to catch inconsistencies. Phase 3: Group Review (Week 3) \u00b6 Scheduled review sessions for each document category with all stakeholders. Phase 4: Final Validation (Week 4) \u00b6 Lead architect consolidates feedback and validates final changes. \ud83d\udcdd Review Template \u00b6 Document: [Document Name] \u00b6 Reviewer: [Name] Date: [Date] Review Type: [Individual/Cross/Group] Technical Accuracy \u00b6 Issues Found: [List specific issues] Recommendations: [Suggestions for improvement] Code Examples: [Any syntax or logic errors] Completeness \u00b6 Missing Sections: [List any missing content] Incomplete Examples: [Identify incomplete examples] Unclear Requirements: [Highlight unclear specifications] Consistency \u00b6 Terminology Issues: [Inconsistent terms or definitions] Design Pattern Conflicts: [Any architectural inconsistencies] Cross-Reference Problems: [Broken or incorrect links] Clarity and Usability \u00b6 Confusing Sections: [Identify unclear content] Missing Context: [Areas needing more explanation] Diagram Issues: [Problems with visual representations] Security and Compliance \u00b6 Security Gaps: [Missing security considerations] Compliance Issues: [Regulatory or policy concerns] Risk Assessment Problems: [Issues with risk methodology] Overall Assessment \u00b6 Strengths: [What works well] Critical Issues: [Must-fix problems] Enhancement Opportunities: [Nice-to-have improvements] Recommendation: [Approve/Revise/Reject] \ud83d\udea8 Critical Issues Escalation \u00b6 Immediate Escalation Required \u00b6 Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Performance bottlenecks Escalation Process \u00b6 Identify the critical issue Document the problem with evidence Notify the lead architect immediately Schedule emergency review session Resolve with all stakeholders present \ud83d\udcca Review Metrics \u00b6 Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met \ud83d\udd04 Review Workflow \u00b6 graph TD A[Document Ready] --> B[Assign Reviewers] B --> C[Individual Review] C --> D[Cross-Review] D --> E[Group Review] E --> F[Issue Resolution] F --> G[Final Validation] G --> H[Approval] H --> I[Implementation Ready] F --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> F J -->|No| G \ud83d\udcc5 Timeline \u00b6 Week Activities Deliverables Week 1 Individual reviews Initial feedback reports Week 2 Cross-reviews Consolidated issue list Week 3 Group review sessions Resolution plan Week 4 Final validation Approved architecture \ud83c\udfaf Success Criteria \u00b6 The architecture review is considered successful when: - [ ] All 24 documents pass technical accuracy review - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team \ud83d\udcde Contact Information \u00b6 Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Next Steps: Begin individual reviews using the provided checklist and template. ```","title":"Guidelines"},{"location":"review/REVIEW_GUIDELINES/#secflow-architecture-review-guidelines","text":"","title":"SecFlow Architecture Review Guidelines"},{"location":"review/REVIEW_GUIDELINES/#overview","text":"This document outlines the review and validation process for the SecFlow architecture documentation suite. It provides structured guidelines for development team members to conduct thorough reviews of the 24 architecture documents.","title":"\ud83c\udfaf Overview"},{"location":"review/REVIEW_GUIDELINES/#review-team-structure","text":"Role Responsibilities Documents to Review Lead Architect Overall architecture consistency, design patterns All documents (00-24) Backend Engineers Core packages, data models, API design 04, 05, 12, 13, 16, 17, 18 Frontend Engineers UX design, web interface, CLI experience 08, 22 DevOps Engineers CI/CD, deployment, infrastructure 03, 21, 15, 17 Security Engineers Security model, compliance, risk assessment 16, 19, 14, 11 QA Engineers Testing strategy, validation criteria 21, 18, 12 Product Manager Business requirements, user experience 01, 08, 20, 23","title":"\ud83d\udc65 Review Team Structure"},{"location":"review/REVIEW_GUIDELINES/#review-checklist","text":"","title":"\ud83d\udccb Review Checklist"},{"location":"review/REVIEW_GUIDELINES/#1-technical-accuracy","text":"Code examples are syntactically correct Architecture diagrams are accurate and complete Technical specifications are feasible Dependencies and integrations are realistic Performance requirements are achievable","title":"1. Technical Accuracy"},{"location":"review/REVIEW_GUIDELINES/#2-completeness","text":"All required sections are present Cross-references between documents are valid Examples are comprehensive and relevant Edge cases and error scenarios are covered Future considerations are addressed","title":"2. Completeness"},{"location":"review/REVIEW_GUIDELINES/#3-consistency","text":"Terminology is consistent across documents Design patterns align with overall architecture Data models are consistent between documents Security principles are uniformly applied Naming conventions are followed","title":"3. Consistency"},{"location":"review/REVIEW_GUIDELINES/#4-clarity-and-usability","text":"Concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Document structure is logical Language is professional and accessible","title":"4. Clarity and Usability"},{"location":"review/REVIEW_GUIDELINES/#5-security-and-compliance","text":"Security requirements are clearly defined Compliance frameworks are properly referenced Risk assessment methodology is sound Data protection measures are adequate Audit requirements are met","title":"5. Security and Compliance"},{"location":"review/REVIEW_GUIDELINES/#review-process","text":"","title":"\ud83d\udd0d Review Process"},{"location":"review/REVIEW_GUIDELINES/#phase-1-individual-review-week-1","text":"Each team member reviews their assigned documents using the checklist above.","title":"Phase 1: Individual Review (Week 1)"},{"location":"review/REVIEW_GUIDELINES/#phase-2-cross-review-week-2","text":"Team members review documents outside their primary expertise to catch inconsistencies.","title":"Phase 2: Cross-Review (Week 2)"},{"location":"review/REVIEW_GUIDELINES/#phase-3-group-review-week-3","text":"Scheduled review sessions for each document category with all stakeholders.","title":"Phase 3: Group Review (Week 3)"},{"location":"review/REVIEW_GUIDELINES/#phase-4-final-validation-week-4","text":"Lead architect consolidates feedback and validates final changes.","title":"Phase 4: Final Validation (Week 4)"},{"location":"review/REVIEW_GUIDELINES/#review-template","text":"","title":"\ud83d\udcdd Review Template"},{"location":"review/REVIEW_GUIDELINES/#document-document-name","text":"Reviewer: [Name] Date: [Date] Review Type: [Individual/Cross/Group]","title":"Document: [Document Name]"},{"location":"review/REVIEW_GUIDELINES/#technical-accuracy","text":"Issues Found: [List specific issues] Recommendations: [Suggestions for improvement] Code Examples: [Any syntax or logic errors]","title":"Technical Accuracy"},{"location":"review/REVIEW_GUIDELINES/#completeness","text":"Missing Sections: [List any missing content] Incomplete Examples: [Identify incomplete examples] Unclear Requirements: [Highlight unclear specifications]","title":"Completeness"},{"location":"review/REVIEW_GUIDELINES/#consistency","text":"Terminology Issues: [Inconsistent terms or definitions] Design Pattern Conflicts: [Any architectural inconsistencies] Cross-Reference Problems: [Broken or incorrect links]","title":"Consistency"},{"location":"review/REVIEW_GUIDELINES/#clarity-and-usability","text":"Confusing Sections: [Identify unclear content] Missing Context: [Areas needing more explanation] Diagram Issues: [Problems with visual representations]","title":"Clarity and Usability"},{"location":"review/REVIEW_GUIDELINES/#security-and-compliance","text":"Security Gaps: [Missing security considerations] Compliance Issues: [Regulatory or policy concerns] Risk Assessment Problems: [Issues with risk methodology]","title":"Security and Compliance"},{"location":"review/REVIEW_GUIDELINES/#overall-assessment","text":"Strengths: [What works well] Critical Issues: [Must-fix problems] Enhancement Opportunities: [Nice-to-have improvements] Recommendation: [Approve/Revise/Reject]","title":"Overall Assessment"},{"location":"review/REVIEW_GUIDELINES/#critical-issues-escalation","text":"","title":"\ud83d\udea8 Critical Issues Escalation"},{"location":"review/REVIEW_GUIDELINES/#immediate-escalation-required","text":"Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Performance bottlenecks","title":"Immediate Escalation Required"},{"location":"review/REVIEW_GUIDELINES/#escalation-process","text":"Identify the critical issue Document the problem with evidence Notify the lead architect immediately Schedule emergency review session Resolve with all stakeholders present","title":"Escalation Process"},{"location":"review/REVIEW_GUIDELINES/#review-metrics","text":"Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met","title":"\ud83d\udcca Review Metrics"},{"location":"review/REVIEW_GUIDELINES/#review-workflow","text":"graph TD A[Document Ready] --> B[Assign Reviewers] B --> C[Individual Review] C --> D[Cross-Review] D --> E[Group Review] E --> F[Issue Resolution] F --> G[Final Validation] G --> H[Approval] H --> I[Implementation Ready] F --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> F J -->|No| G","title":"\ud83d\udd04 Review Workflow"},{"location":"review/REVIEW_GUIDELINES/#timeline","text":"Week Activities Deliverables Week 1 Individual reviews Initial feedback reports Week 2 Cross-reviews Consolidated issue list Week 3 Group review sessions Resolution plan Week 4 Final validation Approved architecture","title":"\ud83d\udcc5 Timeline"},{"location":"review/REVIEW_GUIDELINES/#success-criteria","text":"The architecture review is considered successful when: - [ ] All 24 documents pass technical accuracy review - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team","title":"\ud83c\udfaf Success Criteria"},{"location":"review/REVIEW_GUIDELINES/#contact-information","text":"Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Next Steps: Begin individual reviews using the provided checklist and template. ```","title":"\ud83d\udcde Contact Information"},{"location":"review/REVIEW_STATUS/","text":"SecFlow Architecture Review Status \u00b6 \ud83c\udfaf Review and Validation Summary \u00b6 Date: October 6, 2025 Status: \u2705 Ready for Development Team Review Critical Issues: 0 Warnings: 89 \ud83d\udcca Validation Results \u00b6 Automated Validation \u00b6 Total Documents: 25 Total Checks: 89 Critical Issues: 0 \u274c Warnings: 89 \u26a0\ufe0f Passed: 0 \u2705 Critical Issues Resolved \u00b6 \u2705 Cross-reference validation - Fixed broken internal links \u2705 Document naming consistency - Corrected reference patterns \u2705 YAML metadata validation - All documents have proper frontmatter \u2705 Code example validation - All code blocks are properly formatted \u26a0\ufe0f Remaining Warnings (Non-Critical) \u00b6 Document Structure (15 warnings) \u00b6 Missing recommended \"## \ud83e\udded Overview\" sections in some documents Minor heading hierarchy issues These are style preferences, not functional issues ASCII Diagrams (8 warnings) \u00b6 Some diagrams use multiple box drawing characters Still readable and functional Could be simplified for consistency Code Examples (25 warnings) \u00b6 Some code examples contain placeholder content ( ... , TODO , FIXME ) These are intentional placeholders for implementation Will be completed during development phase Consistency (41 warnings) \u00b6 Mixed case usage for technical terms (e.g., \"SecFlow\" vs \"secflow\") Terminology variations across documents These are style inconsistencies, not functional issues \ud83d\ude80 Ready for Development Team Review \u00b6 The architecture documentation suite is now ready for comprehensive review by the development team. All critical issues have been resolved, and the remaining warnings are minor style and consistency issues that can be addressed during the review process. \ud83d\udccb Next Steps \u00b6 1. Development Team Review (Week 1-2) \u00b6 Backend Engineers: Review core packages, data models, API design Frontend Engineers: Review UX design, web interface, CLI experience DevOps Engineers: Review CI/CD, deployment, infrastructure Security Engineers: Review security model, compliance, risk assessment QA Engineers: Review testing strategy, validation criteria Product Manager: Review business requirements, user experience 2. Review Process \u00b6 Use provided review templates and checklists Follow the structured review workflow Document all feedback and recommendations Address critical issues immediately Plan resolution for non-critical issues 3. Final Validation \u00b6 Consolidate all review feedback Update documents based on team input Run final validation checks Obtain stakeholder approval Proceed to implementation \ud83d\udcc1 Review Resources \u00b6 Review Guidelines \u00b6 REVIEW_GUIDELINES.md - Comprehensive review process validation_checklist.md - Detailed validation criteria review_assignments.md - Team member assignments review_workflow.md - Structured workflow process Validation Tools \u00b6 automated_validation.py - Automated validation script validation_report.md - Latest validation results Review Templates \u00b6 Individual review template (in REVIEW_GUIDELINES.md) Group review session template (in REVIEW_GUIDELINES.md) Issue tracking and resolution templates \ud83c\udfaf Success Criteria \u00b6 The review process will be considered successful when: All 24 documents reviewed by assigned team members All critical issues identified and resolved Cross-document consistency validated All stakeholders approve their respective sections Implementation roadmap validated as feasible Development team ready to begin implementation \ud83d\udcde Contact Information \u00b6 Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Status: \u2705 Ready for Development Team Review Next Action: Begin structured review process according to assigned roles and timeline [Auto] Validation Summary updated \u2014 5 warnings, 0 criticals (2025-10-07 12:28) [Auto] DQI recalculated \u2014 99.0/100 (Trend: \u2192) [Auto] Snapshot generated and uploaded \u2014 2025-10-07 [Auto] Code fence language hints added \u2014 287 fences fixed, warnings: 127\u219269 (-58), DQI: 74.6\u219286.2 (+11.6) (2025-10-07) [Auto] Terminology normalized \u2014 75 fixes applied, warnings: 17\u21920 (-17), DQI: 92.6\u219296.0 (+3.4) (2025-10-07) [Auto] JSON/YAML syntax tightened \u2014 32 fixes applied, syntax errors: 18\u219212 (-6), DQI: 96.0\u219297.4 (+1.4), 16-security-model.md: \u2705 clean (2025-10-07) [Auto] Dashboard polished \u2014 Top 5 fixes + DQI trend added, accessible via Review & Validation \u2192 AI Summary (2025-10-07) [Auto] Phase 2.6 Green Across Board \u2014 Python syntax errors: 7\u21924 (-3), illegal annotation: 1\u21920 (-1), warnings: 13\u219210 (-3), DQI: 97.4\u219298.0 (+0.6) (2025-10-07) [Auto] YAML syntax fixed \u2014 YAML errors: 3\u21920 (-3), warnings: 10\u21926 (-4), DQI: 98.0\u219298.8 (+0.8) (2025-10-07) [Auto] Placeholders replaced \u2014 placeholder warnings: 2\u21920 (-2), warnings: 6\u21926 (stable), DQI: 98.8\u219298.8 (stable) (2025-10-07) [Auto] Risk framework cleaned \u2014 syntax errors: 1\u21920 (-1), warnings: 6\u21925 (-1), DQI: 98.8\u219299.0 (+0.2) (2025-10-07) [Auto] Full pipeline completed \u2014 warnings: 5, DQI: 99.0/100 (\u226599 \u2705), snapshot: docs_snapshot_2025-10-07.tgz (928KB) (2025-10-07)","title":"Status"},{"location":"review/REVIEW_STATUS/#secflow-architecture-review-status","text":"","title":"SecFlow Architecture Review Status"},{"location":"review/REVIEW_STATUS/#review-and-validation-summary","text":"Date: October 6, 2025 Status: \u2705 Ready for Development Team Review Critical Issues: 0 Warnings: 89","title":"\ud83c\udfaf Review and Validation Summary"},{"location":"review/REVIEW_STATUS/#validation-results","text":"","title":"\ud83d\udcca Validation Results"},{"location":"review/REVIEW_STATUS/#automated-validation","text":"Total Documents: 25 Total Checks: 89 Critical Issues: 0 \u274c Warnings: 89 \u26a0\ufe0f Passed: 0 \u2705","title":"Automated Validation"},{"location":"review/REVIEW_STATUS/#critical-issues-resolved","text":"\u2705 Cross-reference validation - Fixed broken internal links \u2705 Document naming consistency - Corrected reference patterns \u2705 YAML metadata validation - All documents have proper frontmatter \u2705 Code example validation - All code blocks are properly formatted","title":"Critical Issues Resolved"},{"location":"review/REVIEW_STATUS/#remaining-warnings-non-critical","text":"","title":"\u26a0\ufe0f Remaining Warnings (Non-Critical)"},{"location":"review/REVIEW_STATUS/#document-structure-15-warnings","text":"Missing recommended \"## \ud83e\udded Overview\" sections in some documents Minor heading hierarchy issues These are style preferences, not functional issues","title":"Document Structure (15 warnings)"},{"location":"review/REVIEW_STATUS/#ascii-diagrams-8-warnings","text":"Some diagrams use multiple box drawing characters Still readable and functional Could be simplified for consistency","title":"ASCII Diagrams (8 warnings)"},{"location":"review/REVIEW_STATUS/#code-examples-25-warnings","text":"Some code examples contain placeholder content ( ... , TODO , FIXME ) These are intentional placeholders for implementation Will be completed during development phase","title":"Code Examples (25 warnings)"},{"location":"review/REVIEW_STATUS/#consistency-41-warnings","text":"Mixed case usage for technical terms (e.g., \"SecFlow\" vs \"secflow\") Terminology variations across documents These are style inconsistencies, not functional issues","title":"Consistency (41 warnings)"},{"location":"review/REVIEW_STATUS/#ready-for-development-team-review","text":"The architecture documentation suite is now ready for comprehensive review by the development team. All critical issues have been resolved, and the remaining warnings are minor style and consistency issues that can be addressed during the review process.","title":"\ud83d\ude80 Ready for Development Team Review"},{"location":"review/REVIEW_STATUS/#next-steps","text":"","title":"\ud83d\udccb Next Steps"},{"location":"review/REVIEW_STATUS/#1-development-team-review-week-1-2","text":"Backend Engineers: Review core packages, data models, API design Frontend Engineers: Review UX design, web interface, CLI experience DevOps Engineers: Review CI/CD, deployment, infrastructure Security Engineers: Review security model, compliance, risk assessment QA Engineers: Review testing strategy, validation criteria Product Manager: Review business requirements, user experience","title":"1. Development Team Review (Week 1-2)"},{"location":"review/REVIEW_STATUS/#2-review-process","text":"Use provided review templates and checklists Follow the structured review workflow Document all feedback and recommendations Address critical issues immediately Plan resolution for non-critical issues","title":"2. Review Process"},{"location":"review/REVIEW_STATUS/#3-final-validation","text":"Consolidate all review feedback Update documents based on team input Run final validation checks Obtain stakeholder approval Proceed to implementation","title":"3. Final Validation"},{"location":"review/REVIEW_STATUS/#review-resources","text":"","title":"\ud83d\udcc1 Review Resources"},{"location":"review/REVIEW_STATUS/#review-guidelines","text":"REVIEW_GUIDELINES.md - Comprehensive review process validation_checklist.md - Detailed validation criteria review_assignments.md - Team member assignments review_workflow.md - Structured workflow process","title":"Review Guidelines"},{"location":"review/REVIEW_STATUS/#validation-tools","text":"automated_validation.py - Automated validation script validation_report.md - Latest validation results","title":"Validation Tools"},{"location":"review/REVIEW_STATUS/#review-templates","text":"Individual review template (in REVIEW_GUIDELINES.md) Group review session template (in REVIEW_GUIDELINES.md) Issue tracking and resolution templates","title":"Review Templates"},{"location":"review/REVIEW_STATUS/#success-criteria","text":"The review process will be considered successful when: All 24 documents reviewed by assigned team members All critical issues identified and resolved Cross-document consistency validated All stakeholders approve their respective sections Implementation roadmap validated as feasible Development team ready to begin implementation","title":"\ud83c\udfaf Success Criteria"},{"location":"review/REVIEW_STATUS/#contact-information","text":"Lead Architect: [Name] - [email] Review Coordinator: [Name] - [email] Emergency Escalation: [Name] - [phone] Status: \u2705 Ready for Development Team Review Next Action: Begin structured review process according to assigned roles and timeline [Auto] Validation Summary updated \u2014 5 warnings, 0 criticals (2025-10-07 12:28) [Auto] DQI recalculated \u2014 99.0/100 (Trend: \u2192) [Auto] Snapshot generated and uploaded \u2014 2025-10-07 [Auto] Code fence language hints added \u2014 287 fences fixed, warnings: 127\u219269 (-58), DQI: 74.6\u219286.2 (+11.6) (2025-10-07) [Auto] Terminology normalized \u2014 75 fixes applied, warnings: 17\u21920 (-17), DQI: 92.6\u219296.0 (+3.4) (2025-10-07) [Auto] JSON/YAML syntax tightened \u2014 32 fixes applied, syntax errors: 18\u219212 (-6), DQI: 96.0\u219297.4 (+1.4), 16-security-model.md: \u2705 clean (2025-10-07) [Auto] Dashboard polished \u2014 Top 5 fixes + DQI trend added, accessible via Review & Validation \u2192 AI Summary (2025-10-07) [Auto] Phase 2.6 Green Across Board \u2014 Python syntax errors: 7\u21924 (-3), illegal annotation: 1\u21920 (-1), warnings: 13\u219210 (-3), DQI: 97.4\u219298.0 (+0.6) (2025-10-07) [Auto] YAML syntax fixed \u2014 YAML errors: 3\u21920 (-3), warnings: 10\u21926 (-4), DQI: 98.0\u219298.8 (+0.8) (2025-10-07) [Auto] Placeholders replaced \u2014 placeholder warnings: 2\u21920 (-2), warnings: 6\u21926 (stable), DQI: 98.8\u219298.8 (stable) (2025-10-07) [Auto] Risk framework cleaned \u2014 syntax errors: 1\u21920 (-1), warnings: 6\u21925 (-1), DQI: 98.8\u219299.0 (+0.2) (2025-10-07) [Auto] Full pipeline completed \u2014 warnings: 5, DQI: 99.0/100 (\u226599 \u2705), snapshot: docs_snapshot_2025-10-07.tgz (928KB) (2025-10-07)","title":"\ud83d\udcde Contact Information"},{"location":"review/VALIDATION_SUMMARY/","text":"AI-Assisted Validation Summary \u00b6 Generated : 2025-10-07 12:28:23 Source : docs/review/validation_report.md \ud83d\udcca Overview \u00b6 Total Documents : 25 Critical Issues : 0 Warnings : 5 Categories : 2 \ud83d\udcc8 Documentation Quality Index (DQI) \u00b6 Score : 99.0/100 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent \ud83c\udff7\ufe0f Warnings by Category \u00b6 Code Block Issues \u00b6 Count : 3 Issues : Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax Terminology / Glossary Inconsistencies \u00b6 Count : 2 Issues : Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) \ud83d\udd25 Top 10 Most Frequent Issues \u00b6 1x - Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 1x - Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 1x - Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 1x - Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 1x - Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value \ud83d\udca1 Suggested Actions \u00b6 Add language hints to code blocks (3 issues): Specify python , yaml , json etc. for better syntax highlighting Standardize terminology (2 issues): Use consistent capitalization (e.g., 'SecFlow' not 'secflow') Run validation regularly : Use make validate before committing changes Review validation summary : Check VALIDATION_SUMMARY.md for detailed insights Follow glossary standards : Use docs/review/glossary.yml for terminology consistency Complete code examples : Replace placeholders with working code snippets \ud83d\udcc8 Document Health Score \u00b6 Document Warnings Criticals Health Score 00-index.md 0 0 \ud83d\udfe2 100% 01-title-and-executive-summary.md 0 0 \ud83d\udfe2 100% 02-architecture-philosophy.md 0 0 \ud83d\udfe2 100% 03-repository-layout.md 0 0 \ud83d\udfe2 100% 04-core-packages-and-responsibilities.md 0 0 \ud83d\udfe2 100% 05-orchestration-and-workflow-engine.md 0 0 \ud83d\udfe2 100% 07-tools-integration-model.md 0 0 \ud83d\udfe2 100% 09-resource-registry.md 0 0 \ud83d\udfe2 100% 10-wordlist-and-output-sharing.md 0 0 \ud83d\udfe2 100% 11-project-isolation-and-data-sharing.md 0 0 \ud83d\udfe2 100% 13-cve-cwe-poc-enrichment-layer.md 0 0 \ud83d\udfe2 100% 15-garbage-collection-and-retention.md 0 0 \ud83d\udfe2 100% 16-security-model.md 0 0 \ud83d\udfe2 100% 18-error-handling-and-recovery.md 0 0 \ud83d\udfe2 100% 19-risk-assessment-framework.md 0 0 \ud83d\udfe2 100% 20-migration-and-implementation-phases.md 0 0 \ud83d\udfe2 100% 21-ci-cd-and-testing-strategy.md 0 0 \ud83d\udfe2 100% 22-developer-experience-and-docs.md 0 0 \ud83d\udfe2 100% 23-future-roadmap.md 0 0 \ud83d\udfe2 100% 24-final-consensus-summary.md 0 0 \ud83d\udfe2 100% 06-plugin-system.md 1 0 \ud83d\udfe2 98% 08-tool-manager-and-ux-design.md 1 0 \ud83d\udfe2 98% 12-findings-model-and-schema.md 1 0 \ud83d\udfe2 98% 14-poc-sources-and-legal-guidelines.md 1 0 \ud83d\udfe2 98% 17-observability-logging-and-metrics.md 1 0 \ud83d\udfe2 98% \ud83c\udfaf Priority Recommendations \u00b6 Focus on high-warning documents : 06-plugin-system.md, 08-tool-manager-and-ux-design.md, 12-findings-model-and-schema.md Standardize terminology : Important for professional presentation Regular validation : Run make validate before each commit Team review : Use structured review process for major changes Continuous improvement : Monitor validation trends over time Generated by AI-Assisted Validation Summary Generator Last updated: 2025-10-07 12:28:23","title":"AI Summary"},{"location":"review/VALIDATION_SUMMARY/#ai-assisted-validation-summary","text":"Generated : 2025-10-07 12:28:23 Source : docs/review/validation_report.md","title":"AI-Assisted Validation Summary"},{"location":"review/VALIDATION_SUMMARY/#overview","text":"Total Documents : 25 Critical Issues : 0 Warnings : 5 Categories : 2","title":"\ud83d\udcca Overview"},{"location":"review/VALIDATION_SUMMARY/#documentation-quality-index-dqi","text":"Score : 99.0/100 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent","title":"\ud83d\udcc8 Documentation Quality Index (DQI)"},{"location":"review/VALIDATION_SUMMARY/#warnings-by-category","text":"","title":"\ud83c\udff7\ufe0f Warnings by Category"},{"location":"review/VALIDATION_SUMMARY/#code-block-issues","text":"Count : 3 Issues : Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax","title":"Code Block Issues"},{"location":"review/VALIDATION_SUMMARY/#terminology-glossary-inconsistencies","text":"Count : 2 Issues : Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow )","title":"Terminology / Glossary Inconsistencies"},{"location":"review/VALIDATION_SUMMARY/#top-10-most-frequent-issues","text":"1x - Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 1x - Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 1x - Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 1x - Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 1x - Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value","title":"\ud83d\udd25 Top 10 Most Frequent Issues"},{"location":"review/VALIDATION_SUMMARY/#suggested-actions","text":"Add language hints to code blocks (3 issues): Specify python , yaml , json etc. for better syntax highlighting Standardize terminology (2 issues): Use consistent capitalization (e.g., 'SecFlow' not 'secflow') Run validation regularly : Use make validate before committing changes Review validation summary : Check VALIDATION_SUMMARY.md for detailed insights Follow glossary standards : Use docs/review/glossary.yml for terminology consistency Complete code examples : Replace placeholders with working code snippets","title":"\ud83d\udca1 Suggested Actions"},{"location":"review/VALIDATION_SUMMARY/#document-health-score","text":"Document Warnings Criticals Health Score 00-index.md 0 0 \ud83d\udfe2 100% 01-title-and-executive-summary.md 0 0 \ud83d\udfe2 100% 02-architecture-philosophy.md 0 0 \ud83d\udfe2 100% 03-repository-layout.md 0 0 \ud83d\udfe2 100% 04-core-packages-and-responsibilities.md 0 0 \ud83d\udfe2 100% 05-orchestration-and-workflow-engine.md 0 0 \ud83d\udfe2 100% 07-tools-integration-model.md 0 0 \ud83d\udfe2 100% 09-resource-registry.md 0 0 \ud83d\udfe2 100% 10-wordlist-and-output-sharing.md 0 0 \ud83d\udfe2 100% 11-project-isolation-and-data-sharing.md 0 0 \ud83d\udfe2 100% 13-cve-cwe-poc-enrichment-layer.md 0 0 \ud83d\udfe2 100% 15-garbage-collection-and-retention.md 0 0 \ud83d\udfe2 100% 16-security-model.md 0 0 \ud83d\udfe2 100% 18-error-handling-and-recovery.md 0 0 \ud83d\udfe2 100% 19-risk-assessment-framework.md 0 0 \ud83d\udfe2 100% 20-migration-and-implementation-phases.md 0 0 \ud83d\udfe2 100% 21-ci-cd-and-testing-strategy.md 0 0 \ud83d\udfe2 100% 22-developer-experience-and-docs.md 0 0 \ud83d\udfe2 100% 23-future-roadmap.md 0 0 \ud83d\udfe2 100% 24-final-consensus-summary.md 0 0 \ud83d\udfe2 100% 06-plugin-system.md 1 0 \ud83d\udfe2 98% 08-tool-manager-and-ux-design.md 1 0 \ud83d\udfe2 98% 12-findings-model-and-schema.md 1 0 \ud83d\udfe2 98% 14-poc-sources-and-legal-guidelines.md 1 0 \ud83d\udfe2 98% 17-observability-logging-and-metrics.md 1 0 \ud83d\udfe2 98%","title":"\ud83d\udcc8 Document Health Score"},{"location":"review/VALIDATION_SUMMARY/#priority-recommendations","text":"Focus on high-warning documents : 06-plugin-system.md, 08-tool-manager-and-ux-design.md, 12-findings-model-and-schema.md Standardize terminology : Important for professional presentation Regular validation : Run make validate before each commit Team review : Use structured review process for major changes Continuous improvement : Monitor validation trends over time Generated by AI-Assisted Validation Summary Generator Last updated: 2025-10-07 12:28:23","title":"\ud83c\udfaf Priority Recommendations"},{"location":"review/mermaid-smoketest/","text":"Mermaid Smoketest \u00b6 This page tests that Mermaid diagrams are rendering correctly using the superfences + local Mermaid approach. \ud83e\uddea Test Diagram \u00b6 flowchart LR A[Start] --> B[Process] B --> C[End] \u2705 Expected Result \u00b6 You should see a flowchart diagram above with three boxes connected by arrows. \ud83d\udd04 Sequence Diagram Test \u00b6 sequenceDiagram participant A as Alice participant B as Bob A->>B: Hello Bob, how are you? B-->>A: Great! A-)B: See you later! \ud83d\udcca Class Diagram Test \u00b6 classDiagram class Animal { +String name +int age +makeSound() } class Dog { +String breed +bark() } Animal <|-- Dog \ud83c\udfaf State Diagram Test \u00b6 stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] \ud83d\udcc8 Expected Results \u00b6 All four diagrams above should render as interactive Mermaid diagrams, not as code blocks. ```","title":"Mermaid Smoketest"},{"location":"review/mermaid-smoketest/#mermaid-smoketest","text":"This page tests that Mermaid diagrams are rendering correctly using the superfences + local Mermaid approach.","title":"Mermaid Smoketest"},{"location":"review/mermaid-smoketest/#test-diagram","text":"flowchart LR A[Start] --> B[Process] B --> C[End]","title":"\ud83e\uddea Test Diagram"},{"location":"review/mermaid-smoketest/#expected-result","text":"You should see a flowchart diagram above with three boxes connected by arrows.","title":"\u2705 Expected Result"},{"location":"review/mermaid-smoketest/#sequence-diagram-test","text":"sequenceDiagram participant A as Alice participant B as Bob A->>B: Hello Bob, how are you? B-->>A: Great! A-)B: See you later!","title":"\ud83d\udd04 Sequence Diagram Test"},{"location":"review/mermaid-smoketest/#class-diagram-test","text":"classDiagram class Animal { +String name +int age +makeSound() } class Dog { +String breed +bark() } Animal <|-- Dog","title":"\ud83d\udcca Class Diagram Test"},{"location":"review/mermaid-smoketest/#state-diagram-test","text":"stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*]","title":"\ud83c\udfaf State Diagram Test"},{"location":"review/mermaid-smoketest/#expected-results","text":"All four diagrams above should render as interactive Mermaid diagrams, not as code blocks. ```","title":"\ud83d\udcc8 Expected Results"},{"location":"review/review_assignments/","text":"SecFlow Architecture Review Assignments \u00b6 \ud83d\udc65 Review Team Assignments \u00b6 This document outlines the specific review assignments for each team member, ensuring comprehensive coverage of all 24 architecture documents. \ud83c\udfaf Lead Architect Review (All Documents) \u00b6 Reviewer: Lead Architect Scope: All documents (00-24) Focus: Overall architecture consistency, design patterns, cross-document coherence Priority Documents \u00b6 00-index.md - Navigation and structure 02-architecture-philosophy.md - Core architectural principles 04-core-packages-and-responsibilities.md - Package structure and relationships 05-orchestration-and-workflow-engine.md - Workflow orchestration design 24-final-consensus-summary.md - Final architectural consensus Review Criteria \u00b6 Architectural consistency across all documents Design pattern alignment with hexagonal architecture Cross-document references and dependencies Overall system coherence and feasibility Strategic alignment with project goals \ud83d\udd27 Backend Engineers Review \u00b6 Reviewers: Senior Backend Engineer, Backend Developer Scope: Core technical implementation documents Focus: Data models, API design, database integration, workflow execution Assigned Documents \u00b6 04-core-packages-and-responsibilities.md Package structure and responsibilities Interface definitions and contracts Cross-package dependencies 05-orchestration-and-workflow-engine.md DAG execution engine design Workflow specification schema Node executor implementation 12-findings-model-and-schema.md Finding data model and normalization Database schema design Data validation and transformation 13-cve-cwe-poc-enrichment-layer.md Enrichment pipeline design External API integrations Data caching and synchronization 16-security-model.md Authentication and authorization implementation Security middleware design Audit logging mechanisms 17-observability-logging-and-metrics.md Logging infrastructure design Metrics collection and export Distributed tracing implementation 18-error-handling-and-recovery.md Error handling strategies Retry logic and circuit breakers Dead letter queue implementation Review Criteria \u00b6 Technical feasibility of implementation Code examples are syntactically correct Data models are complete and consistent API design follows RESTful principles Database schema is normalized and efficient Error handling is comprehensive Performance requirements are achievable \ud83c\udfa8 Frontend Engineers Review \u00b6 Reviewers: Frontend Developer, UX Designer Scope: User interface and experience documents Focus: UI/UX design, web interface, CLI experience, user workflows Assigned Documents \u00b6 08-tool-manager-and-ux-design.md Tool Manager UX design CLI and web interface specifications User workflow design 22-developer-experience-and-docs.md Developer tools and CLI utilities Documentation system design Local development workflow Review Criteria \u00b6 UI/UX design is intuitive and accessible CLI commands are logical and well-organized Web interface specifications are complete User workflows are efficient and clear Developer experience is optimized Documentation is user-friendly \ud83d\ude80 DevOps Engineers Review \u00b6 Reviewers: DevOps Engineer, Infrastructure Engineer Scope: Infrastructure, deployment, and operational documents Focus: CI/CD, deployment strategies, infrastructure requirements, monitoring Assigned Documents \u00b6 03-repository-layout.md Mono-repo structure and tooling Import boundaries and enforcement Development workflow automation 15-garbage-collection-and-retention.md Data lifecycle management Cleanup and retention policies Storage optimization strategies 17-observability-logging-and-metrics.md Monitoring and observability infrastructure Log aggregation and analysis Metrics collection and alerting 21-ci-cd-and-testing-strategy.md CI/CD pipeline design Testing framework and automation Deployment strategies and rollback Review Criteria \u00b6 Infrastructure requirements are realistic CI/CD pipeline is efficient and reliable Deployment strategies are safe and scalable Monitoring and observability are comprehensive Backup and recovery procedures are adequate Security scanning and compliance checks are integrated \ud83d\udd12 Security Engineers Review \u00b6 Reviewers: Security Engineer, Compliance Officer Scope: Security, compliance, and risk assessment documents Focus: Security model, compliance requirements, risk assessment, data protection Assigned Documents \u00b6 11-project-isolation-and-data-sharing.md Project isolation mechanisms Data sharing policies and controls Access control and permissions 14-poc-sources-and-legal-guidelines.md PoC governance and legal compliance Sandbox execution security Legal and ethical guidelines 16-security-model.md Authentication and authorization design Security middleware and controls Audit logging and compliance 19-risk-assessment-framework.md Risk scoring methodology NIST framework implementation CVSS and CWE integration Review Criteria \u00b6 Security controls are comprehensive and effective Compliance requirements are met Risk assessment methodology is sound Data protection measures are adequate Audit requirements are satisfied Legal and ethical guidelines are followed \ud83e\uddea QA Engineers Review \u00b6 Reviewers: QA Engineer, Test Automation Engineer Scope: Testing strategy and quality assurance documents Focus: Testing frameworks, quality gates, validation criteria, test automation Assigned Documents \u00b6 12-findings-model-and-schema.md Data validation and transformation testing Schema validation and error handling Data integrity testing 18-error-handling-and-recovery.md Error handling testing strategies Recovery mechanism validation Fault tolerance testing 21-ci-cd-and-testing-strategy.md Testing framework and automation Quality gates and validation criteria Test coverage and reporting Review Criteria \u00b6 Testing strategies are comprehensive Quality gates are appropriate and measurable Test automation is feasible and maintainable Error scenarios are adequately covered Performance testing requirements are defined Security testing is integrated \ud83d\udcca Product Manager Review \u00b6 Reviewer: Product Manager Scope: Business requirements and user experience documents Focus: Business value, user requirements, market fit, strategic alignment Assigned Documents \u00b6 01-title-and-executive-summary.md Project vision and goals Business value proposition Market positioning 08-tool-manager-and-ux-design.md User experience design Tool management workflows User interface specifications 20-migration-and-implementation-phases.md Implementation roadmap and timeline Phase deliverables and milestones Success criteria and metrics 23-future-roadmap.md Strategic evolution and roadmap AI integration and future capabilities Market trends and competitive analysis Review Criteria \u00b6 Business requirements are clearly defined User experience meets market expectations Implementation timeline is realistic Success criteria are measurable Strategic alignment with company goals Competitive advantages are clear \ud83d\udcc5 Review Schedule \u00b6 Week 1: Individual Reviews \u00b6 Monday-Tuesday: Backend Engineers review assigned documents Wednesday-Thursday: Frontend Engineers review assigned documents Friday: DevOps Engineers review assigned documents Week 2: Cross-Reviews \u00b6 Monday: Security Engineers review assigned documents Tuesday: QA Engineers review assigned documents Wednesday: Product Manager review assigned documents Thursday-Friday: Cross-review sessions (each team reviews outside their expertise) Week 3: Group Review Sessions \u00b6 Monday: Core Architecture Review (Documents 00-05) Tuesday: Implementation Review (Documents 06-12) Wednesday: Operations Review (Documents 13-18) Thursday: Strategy Review (Documents 19-24) Friday: Final Consolidation and Issue Resolution Week 4: Final Validation \u00b6 Monday-Tuesday: Lead Architect final review Wednesday: Stakeholder approval sessions Thursday: Final validation and sign-off Friday: Implementation readiness assessment \ud83d\udcdd Review Deliverables \u00b6 Individual Review Deliverables \u00b6 Completed review checklist for each assigned document Detailed feedback report with specific issues and recommendations Technical accuracy validation results Consistency and completeness assessment Cross-Review Deliverables \u00b6 Cross-document consistency validation Integration point verification Cross-functional requirement validation Stakeholder alignment confirmation Group Review Deliverables \u00b6 Consolidated issue list with priorities Resolution plan with timelines Final approval recommendations Implementation readiness assessment \ud83d\udea8 Escalation Process \u00b6 Critical Issues \u00b6 Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Timeline conflicts Escalation Path \u00b6 Identify critical issue during review Document issue with evidence and impact Notify Lead Architect immediately Schedule emergency review session Resolve with all stakeholders present Update review assignments if needed \u2705 Success Criteria \u00b6 The review process is considered successful when: - [ ] All 24 documents are reviewed by assigned team members - [ ] All critical issues are identified and resolved - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Development team is ready to begin implementation Next Steps: Begin individual reviews according to the assigned schedule and scope.","title":"Assignments"},{"location":"review/review_assignments/#secflow-architecture-review-assignments","text":"","title":"SecFlow Architecture Review Assignments"},{"location":"review/review_assignments/#review-team-assignments","text":"This document outlines the specific review assignments for each team member, ensuring comprehensive coverage of all 24 architecture documents.","title":"\ud83d\udc65 Review Team Assignments"},{"location":"review/review_assignments/#lead-architect-review-all-documents","text":"Reviewer: Lead Architect Scope: All documents (00-24) Focus: Overall architecture consistency, design patterns, cross-document coherence","title":"\ud83c\udfaf Lead Architect Review (All Documents)"},{"location":"review/review_assignments/#priority-documents","text":"00-index.md - Navigation and structure 02-architecture-philosophy.md - Core architectural principles 04-core-packages-and-responsibilities.md - Package structure and relationships 05-orchestration-and-workflow-engine.md - Workflow orchestration design 24-final-consensus-summary.md - Final architectural consensus","title":"Priority Documents"},{"location":"review/review_assignments/#review-criteria","text":"Architectural consistency across all documents Design pattern alignment with hexagonal architecture Cross-document references and dependencies Overall system coherence and feasibility Strategic alignment with project goals","title":"Review Criteria"},{"location":"review/review_assignments/#backend-engineers-review","text":"Reviewers: Senior Backend Engineer, Backend Developer Scope: Core technical implementation documents Focus: Data models, API design, database integration, workflow execution","title":"\ud83d\udd27 Backend Engineers Review"},{"location":"review/review_assignments/#assigned-documents","text":"04-core-packages-and-responsibilities.md Package structure and responsibilities Interface definitions and contracts Cross-package dependencies 05-orchestration-and-workflow-engine.md DAG execution engine design Workflow specification schema Node executor implementation 12-findings-model-and-schema.md Finding data model and normalization Database schema design Data validation and transformation 13-cve-cwe-poc-enrichment-layer.md Enrichment pipeline design External API integrations Data caching and synchronization 16-security-model.md Authentication and authorization implementation Security middleware design Audit logging mechanisms 17-observability-logging-and-metrics.md Logging infrastructure design Metrics collection and export Distributed tracing implementation 18-error-handling-and-recovery.md Error handling strategies Retry logic and circuit breakers Dead letter queue implementation","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_1","text":"Technical feasibility of implementation Code examples are syntactically correct Data models are complete and consistent API design follows RESTful principles Database schema is normalized and efficient Error handling is comprehensive Performance requirements are achievable","title":"Review Criteria"},{"location":"review/review_assignments/#frontend-engineers-review","text":"Reviewers: Frontend Developer, UX Designer Scope: User interface and experience documents Focus: UI/UX design, web interface, CLI experience, user workflows","title":"\ud83c\udfa8 Frontend Engineers Review"},{"location":"review/review_assignments/#assigned-documents_1","text":"08-tool-manager-and-ux-design.md Tool Manager UX design CLI and web interface specifications User workflow design 22-developer-experience-and-docs.md Developer tools and CLI utilities Documentation system design Local development workflow","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_2","text":"UI/UX design is intuitive and accessible CLI commands are logical and well-organized Web interface specifications are complete User workflows are efficient and clear Developer experience is optimized Documentation is user-friendly","title":"Review Criteria"},{"location":"review/review_assignments/#devops-engineers-review","text":"Reviewers: DevOps Engineer, Infrastructure Engineer Scope: Infrastructure, deployment, and operational documents Focus: CI/CD, deployment strategies, infrastructure requirements, monitoring","title":"\ud83d\ude80 DevOps Engineers Review"},{"location":"review/review_assignments/#assigned-documents_2","text":"03-repository-layout.md Mono-repo structure and tooling Import boundaries and enforcement Development workflow automation 15-garbage-collection-and-retention.md Data lifecycle management Cleanup and retention policies Storage optimization strategies 17-observability-logging-and-metrics.md Monitoring and observability infrastructure Log aggregation and analysis Metrics collection and alerting 21-ci-cd-and-testing-strategy.md CI/CD pipeline design Testing framework and automation Deployment strategies and rollback","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_3","text":"Infrastructure requirements are realistic CI/CD pipeline is efficient and reliable Deployment strategies are safe and scalable Monitoring and observability are comprehensive Backup and recovery procedures are adequate Security scanning and compliance checks are integrated","title":"Review Criteria"},{"location":"review/review_assignments/#security-engineers-review","text":"Reviewers: Security Engineer, Compliance Officer Scope: Security, compliance, and risk assessment documents Focus: Security model, compliance requirements, risk assessment, data protection","title":"\ud83d\udd12 Security Engineers Review"},{"location":"review/review_assignments/#assigned-documents_3","text":"11-project-isolation-and-data-sharing.md Project isolation mechanisms Data sharing policies and controls Access control and permissions 14-poc-sources-and-legal-guidelines.md PoC governance and legal compliance Sandbox execution security Legal and ethical guidelines 16-security-model.md Authentication and authorization design Security middleware and controls Audit logging and compliance 19-risk-assessment-framework.md Risk scoring methodology NIST framework implementation CVSS and CWE integration","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_4","text":"Security controls are comprehensive and effective Compliance requirements are met Risk assessment methodology is sound Data protection measures are adequate Audit requirements are satisfied Legal and ethical guidelines are followed","title":"Review Criteria"},{"location":"review/review_assignments/#qa-engineers-review","text":"Reviewers: QA Engineer, Test Automation Engineer Scope: Testing strategy and quality assurance documents Focus: Testing frameworks, quality gates, validation criteria, test automation","title":"\ud83e\uddea QA Engineers Review"},{"location":"review/review_assignments/#assigned-documents_4","text":"12-findings-model-and-schema.md Data validation and transformation testing Schema validation and error handling Data integrity testing 18-error-handling-and-recovery.md Error handling testing strategies Recovery mechanism validation Fault tolerance testing 21-ci-cd-and-testing-strategy.md Testing framework and automation Quality gates and validation criteria Test coverage and reporting","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_5","text":"Testing strategies are comprehensive Quality gates are appropriate and measurable Test automation is feasible and maintainable Error scenarios are adequately covered Performance testing requirements are defined Security testing is integrated","title":"Review Criteria"},{"location":"review/review_assignments/#product-manager-review","text":"Reviewer: Product Manager Scope: Business requirements and user experience documents Focus: Business value, user requirements, market fit, strategic alignment","title":"\ud83d\udcca Product Manager Review"},{"location":"review/review_assignments/#assigned-documents_5","text":"01-title-and-executive-summary.md Project vision and goals Business value proposition Market positioning 08-tool-manager-and-ux-design.md User experience design Tool management workflows User interface specifications 20-migration-and-implementation-phases.md Implementation roadmap and timeline Phase deliverables and milestones Success criteria and metrics 23-future-roadmap.md Strategic evolution and roadmap AI integration and future capabilities Market trends and competitive analysis","title":"Assigned Documents"},{"location":"review/review_assignments/#review-criteria_6","text":"Business requirements are clearly defined User experience meets market expectations Implementation timeline is realistic Success criteria are measurable Strategic alignment with company goals Competitive advantages are clear","title":"Review Criteria"},{"location":"review/review_assignments/#review-schedule","text":"","title":"\ud83d\udcc5 Review Schedule"},{"location":"review/review_assignments/#week-1-individual-reviews","text":"Monday-Tuesday: Backend Engineers review assigned documents Wednesday-Thursday: Frontend Engineers review assigned documents Friday: DevOps Engineers review assigned documents","title":"Week 1: Individual Reviews"},{"location":"review/review_assignments/#week-2-cross-reviews","text":"Monday: Security Engineers review assigned documents Tuesday: QA Engineers review assigned documents Wednesday: Product Manager review assigned documents Thursday-Friday: Cross-review sessions (each team reviews outside their expertise)","title":"Week 2: Cross-Reviews"},{"location":"review/review_assignments/#week-3-group-review-sessions","text":"Monday: Core Architecture Review (Documents 00-05) Tuesday: Implementation Review (Documents 06-12) Wednesday: Operations Review (Documents 13-18) Thursday: Strategy Review (Documents 19-24) Friday: Final Consolidation and Issue Resolution","title":"Week 3: Group Review Sessions"},{"location":"review/review_assignments/#week-4-final-validation","text":"Monday-Tuesday: Lead Architect final review Wednesday: Stakeholder approval sessions Thursday: Final validation and sign-off Friday: Implementation readiness assessment","title":"Week 4: Final Validation"},{"location":"review/review_assignments/#review-deliverables","text":"","title":"\ud83d\udcdd Review Deliverables"},{"location":"review/review_assignments/#individual-review-deliverables","text":"Completed review checklist for each assigned document Detailed feedback report with specific issues and recommendations Technical accuracy validation results Consistency and completeness assessment","title":"Individual Review Deliverables"},{"location":"review/review_assignments/#cross-review-deliverables","text":"Cross-document consistency validation Integration point verification Cross-functional requirement validation Stakeholder alignment confirmation","title":"Cross-Review Deliverables"},{"location":"review/review_assignments/#group-review-deliverables","text":"Consolidated issue list with priorities Resolution plan with timelines Final approval recommendations Implementation readiness assessment","title":"Group Review Deliverables"},{"location":"review/review_assignments/#escalation-process","text":"","title":"\ud83d\udea8 Escalation Process"},{"location":"review/review_assignments/#critical-issues","text":"Security vulnerabilities in design Infeasible technical requirements Major architectural inconsistencies Compliance violations Timeline conflicts","title":"Critical Issues"},{"location":"review/review_assignments/#escalation-path","text":"Identify critical issue during review Document issue with evidence and impact Notify Lead Architect immediately Schedule emergency review session Resolve with all stakeholders present Update review assignments if needed","title":"Escalation Path"},{"location":"review/review_assignments/#success-criteria","text":"The review process is considered successful when: - [ ] All 24 documents are reviewed by assigned team members - [ ] All critical issues are identified and resolved - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Development team is ready to begin implementation Next Steps: Begin individual reviews according to the assigned schedule and scope.","title":"\u2705 Success Criteria"},{"location":"review/review_workflow/","text":"SecFlow Architecture Review Workflow \u00b6 \ud83c\udfaf Overview \u00b6 This document defines the structured review workflow for the SecFlow architecture documentation suite, ensuring comprehensive validation and stakeholder approval before implementation begins. \ud83d\udccb Workflow Phases \u00b6 Phase 1: Pre-Review Setup (Day 1) \u00b6 Duration: 1 day Participants: Lead Architect, Review Coordinator Activities \u00b6 Review Environment Setup Create review workspace with all documents Set up collaboration tools (GitHub, Slack, etc.) Configure review tracking system Prepare review templates and checklists Stakeholder Notification Send review invitations to all team members Distribute review assignments and timelines Schedule review sessions and meetings Provide access to review tools and resources Review Preparation Run automated validation checks Generate initial validation report Identify any obvious issues for quick fixes Prepare review guidelines and instructions Deliverables \u00b6 Review workspace configured All stakeholders notified and onboarded Review tools and templates ready Initial validation report generated Phase 2: Individual Reviews (Days 2-6) \u00b6 Duration: 5 days Participants: All assigned reviewers Activities \u00b6 Document-Specific Reviews Each reviewer reviews their assigned documents Use provided checklists and templates Document all issues, suggestions, and feedback Validate technical accuracy and completeness Cross-Document Reviews Review documents outside primary expertise Check for consistency and integration issues Validate cross-references and dependencies Identify architectural inconsistencies Feedback Documentation Complete review templates for each document Categorize issues by severity and type Provide specific recommendations for improvements Document approval status for each document Deliverables \u00b6 Individual review reports for all documents Consolidated issue list with priorities Cross-document consistency validation Initial approval recommendations Phase 3: Cross-Review Sessions (Days 7-8) \u00b6 Duration: 2 days Participants: All reviewers Activities \u00b6 Cross-Functional Reviews Review documents outside primary expertise Validate integration points and dependencies Check for cross-functional requirements Ensure stakeholder alignment Consistency Validation Validate terminology and naming conventions Check architectural pattern consistency Verify data model consistency Validate security model alignment Issue Resolution Planning Prioritize issues by severity and impact Assign resolution responsibilities Set timelines for issue resolution Plan for emergency escalations Deliverables \u00b6 Cross-review validation reports Prioritized issue resolution plan Consistency validation results Stakeholder alignment confirmation Phase 4: Group Review Sessions (Days 9-11) \u00b6 Duration: 3 days Participants: All stakeholders Activities \u00b6 Document Category Reviews Day 9: Core Architecture (Documents 00-05) Day 10: Implementation (Documents 06-12) Day 11: Operations & Strategy (Documents 13-24) Stakeholder Approval Sessions Present review findings for each category Discuss critical issues and resolutions Validate implementation feasibility Obtain formal approval from stakeholders Issue Resolution Address critical issues immediately Plan resolution for non-critical issues Update documents based on feedback Validate changes and improvements Deliverables \u00b6 Group review session reports Stakeholder approval confirmations Resolved critical issues Updated documents with improvements Phase 5: Final Validation (Days 12-14) \u00b6 Duration: 3 days Participants: Lead Architect, Review Coordinator Activities \u00b6 Final Validation Run comprehensive validation checks Verify all issues have been addressed Validate document consistency and accuracy Confirm stakeholder approvals Implementation Readiness Assessment Validate technical feasibility Confirm resource availability Verify timeline alignment Assess risk mitigation strategies Documentation Finalization Update documents with final changes Generate final validation report Prepare implementation guidelines Create handoff documentation Deliverables \u00b6 Final validation report Implementation readiness assessment Updated architecture documentation Implementation handoff package \ud83d\udd04 Review Process Flow \u00b6 graph TD A[Pre-Review Setup] --> B[Individual Reviews] B --> C[Cross-Review Sessions] C --> D[Group Review Sessions] D --> E[Final Validation] E --> F[Implementation Ready] B --> G[Issue Identification] G --> H[Issue Resolution] H --> I[Document Updates] I --> C D --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> H J -->|No| E \ud83d\udcca Review Metrics and KPIs \u00b6 Quality Metrics \u00b6 Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met Process Metrics \u00b6 Metric Target Measurement Review Completion 100% All assigned reviews completed on time Stakeholder Approval 100% All stakeholders approve their sections Issue Response Time < 24 hours Time to respond to critical issues Document Update Time < 48 hours Time to update documents after feedback \ud83d\udea8 Escalation Procedures \u00b6 Critical Issues \u00b6 Definition: Issues that could prevent implementation or compromise security Examples: - Security vulnerabilities in design - Infeasible technical requirements - Major architectural inconsistencies - Compliance violations - Timeline conflicts Escalation Process: 1. Identify critical issue during review 2. Document issue with evidence and impact 3. Notify Lead Architect immediately 4. Schedule emergency review session within 4 hours 5. Resolve with all stakeholders present 6. Update review assignments if needed Non-Critical Issues \u00b6 Definition: Issues that should be addressed but don't block implementation Examples: - Minor formatting inconsistencies - Documentation improvements - Code style suggestions - Performance optimizations Resolution Process: 1. Document issue in review report 2. Assign resolution responsibility 3. Set timeline for resolution 4. Track progress in issue tracking system 5. Validate resolution before final approval \ud83d\udcdd Review Templates and Tools \u00b6 Individual Review Template \u00b6 ## Document Review: [Document Name] **Reviewer:** [Name] **Date:** [Date] **Review Type:** [Individual/Cross/Group] ### Technical Accuracy - **Issues Found:** [List specific issues] - **Recommendations:** [Suggestions for improvement] - **Code Examples:** [Any syntax or logic errors] ### Completeness - **Missing Sections:** [List any missing content] - **Incomplete Examples:** [Identify incomplete examples] - **Unclear Requirements:** [Highlight unclear specifications] ### Consistency - **Terminology Issues:** [Inconsistent terms or definitions] - **Design Pattern Conflicts:** [Any architectural inconsistencies] - **Cross-Reference Problems:** [Broken or incorrect links] ### Overall Assessment - **Strengths:** [What works well] - **Critical Issues:** [Must-fix problems] - **Enhancement Opportunities:** [Nice-to-have improvements] - **Recommendation:** [Approve/Revise/Reject] Group Review Session Template \u00b6 ## Group Review Session: [Category] **Date:** [Date] **Participants:** [List participants] ### Documents Reviewed - [List documents reviewed] ### Key Issues Identified - [List critical issues] - [List non-critical issues] ### Resolutions Agreed - [List agreed resolutions] - [Assign responsibilities] - [Set timelines] ### Approval Status - [Document approval status] - [Stakeholder confirmations] - [Next steps] \ud83c\udfaf Success Criteria \u00b6 The review workflow is considered successful when: Quality Criteria \u00b6 All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Process Criteria \u00b6 All review phases completed on schedule All stakeholders actively participated All issues properly documented and resolved All approvals formally obtained Implementation team ready to begin Deliverable Criteria \u00b6 Final validation report generated Implementation readiness assessment completed Updated architecture documentation ready Implementation handoff package prepared Development team onboarded and ready \ud83d\udcde Contact Information \u00b6 Review Team \u00b6 Lead Architect: [Name] - [email] - [phone] Review Coordinator: [Name] - [email] - [phone] Backend Lead: [Name] - [email] - [phone] Frontend Lead: [Name] - [email] - [phone] DevOps Lead: [Name] - [email] - [phone] Security Lead: [Name] - [email] - [phone] Emergency Contacts \u00b6 Critical Issues: Lead Architect - [phone] Process Issues: Review Coordinator - [phone] Technical Issues: Backend Lead - [phone] \ud83d\udcc5 Timeline Summary \u00b6 Phase Duration Key Activities Deliverables Phase 1 1 day Pre-review setup Review environment ready Phase 2 5 days Individual reviews Review reports completed Phase 3 2 days Cross-review sessions Consistency validated Phase 4 3 days Group review sessions Stakeholder approvals Phase 5 3 days Final validation Implementation ready Total Duration: 14 days Target Completion: [Date] Next Steps: Begin Phase 1 - Pre-Review Setup","title":"Workflow"},{"location":"review/review_workflow/#secflow-architecture-review-workflow","text":"","title":"SecFlow Architecture Review Workflow"},{"location":"review/review_workflow/#overview","text":"This document defines the structured review workflow for the SecFlow architecture documentation suite, ensuring comprehensive validation and stakeholder approval before implementation begins.","title":"\ud83c\udfaf Overview"},{"location":"review/review_workflow/#workflow-phases","text":"","title":"\ud83d\udccb Workflow Phases"},{"location":"review/review_workflow/#phase-1-pre-review-setup-day-1","text":"Duration: 1 day Participants: Lead Architect, Review Coordinator","title":"Phase 1: Pre-Review Setup (Day 1)"},{"location":"review/review_workflow/#activities","text":"Review Environment Setup Create review workspace with all documents Set up collaboration tools (GitHub, Slack, etc.) Configure review tracking system Prepare review templates and checklists Stakeholder Notification Send review invitations to all team members Distribute review assignments and timelines Schedule review sessions and meetings Provide access to review tools and resources Review Preparation Run automated validation checks Generate initial validation report Identify any obvious issues for quick fixes Prepare review guidelines and instructions","title":"Activities"},{"location":"review/review_workflow/#deliverables","text":"Review workspace configured All stakeholders notified and onboarded Review tools and templates ready Initial validation report generated","title":"Deliverables"},{"location":"review/review_workflow/#phase-2-individual-reviews-days-2-6","text":"Duration: 5 days Participants: All assigned reviewers","title":"Phase 2: Individual Reviews (Days 2-6)"},{"location":"review/review_workflow/#activities_1","text":"Document-Specific Reviews Each reviewer reviews their assigned documents Use provided checklists and templates Document all issues, suggestions, and feedback Validate technical accuracy and completeness Cross-Document Reviews Review documents outside primary expertise Check for consistency and integration issues Validate cross-references and dependencies Identify architectural inconsistencies Feedback Documentation Complete review templates for each document Categorize issues by severity and type Provide specific recommendations for improvements Document approval status for each document","title":"Activities"},{"location":"review/review_workflow/#deliverables_1","text":"Individual review reports for all documents Consolidated issue list with priorities Cross-document consistency validation Initial approval recommendations","title":"Deliverables"},{"location":"review/review_workflow/#phase-3-cross-review-sessions-days-7-8","text":"Duration: 2 days Participants: All reviewers","title":"Phase 3: Cross-Review Sessions (Days 7-8)"},{"location":"review/review_workflow/#activities_2","text":"Cross-Functional Reviews Review documents outside primary expertise Validate integration points and dependencies Check for cross-functional requirements Ensure stakeholder alignment Consistency Validation Validate terminology and naming conventions Check architectural pattern consistency Verify data model consistency Validate security model alignment Issue Resolution Planning Prioritize issues by severity and impact Assign resolution responsibilities Set timelines for issue resolution Plan for emergency escalations","title":"Activities"},{"location":"review/review_workflow/#deliverables_2","text":"Cross-review validation reports Prioritized issue resolution plan Consistency validation results Stakeholder alignment confirmation","title":"Deliverables"},{"location":"review/review_workflow/#phase-4-group-review-sessions-days-9-11","text":"Duration: 3 days Participants: All stakeholders","title":"Phase 4: Group Review Sessions (Days 9-11)"},{"location":"review/review_workflow/#activities_3","text":"Document Category Reviews Day 9: Core Architecture (Documents 00-05) Day 10: Implementation (Documents 06-12) Day 11: Operations & Strategy (Documents 13-24) Stakeholder Approval Sessions Present review findings for each category Discuss critical issues and resolutions Validate implementation feasibility Obtain formal approval from stakeholders Issue Resolution Address critical issues immediately Plan resolution for non-critical issues Update documents based on feedback Validate changes and improvements","title":"Activities"},{"location":"review/review_workflow/#deliverables_3","text":"Group review session reports Stakeholder approval confirmations Resolved critical issues Updated documents with improvements","title":"Deliverables"},{"location":"review/review_workflow/#phase-5-final-validation-days-12-14","text":"Duration: 3 days Participants: Lead Architect, Review Coordinator","title":"Phase 5: Final Validation (Days 12-14)"},{"location":"review/review_workflow/#activities_4","text":"Final Validation Run comprehensive validation checks Verify all issues have been addressed Validate document consistency and accuracy Confirm stakeholder approvals Implementation Readiness Assessment Validate technical feasibility Confirm resource availability Verify timeline alignment Assess risk mitigation strategies Documentation Finalization Update documents with final changes Generate final validation report Prepare implementation guidelines Create handoff documentation","title":"Activities"},{"location":"review/review_workflow/#deliverables_4","text":"Final validation report Implementation readiness assessment Updated architecture documentation Implementation handoff package","title":"Deliverables"},{"location":"review/review_workflow/#review-process-flow","text":"graph TD A[Pre-Review Setup] --> B[Individual Reviews] B --> C[Cross-Review Sessions] C --> D[Group Review Sessions] D --> E[Final Validation] E --> F[Implementation Ready] B --> G[Issue Identification] G --> H[Issue Resolution] H --> I[Document Updates] I --> C D --> J[Critical Issues?] J -->|Yes| K[Emergency Review] K --> H J -->|No| E","title":"\ud83d\udd04 Review Process Flow"},{"location":"review/review_workflow/#review-metrics-and-kpis","text":"","title":"\ud83d\udcca Review Metrics and KPIs"},{"location":"review/review_workflow/#quality-metrics","text":"Metric Target Measurement Review Coverage 100% All documents reviewed by assigned roles Issue Resolution 95% Critical issues resolved before approval Consistency Score \u2265 90% Cross-document consistency rating Technical Accuracy 100% All code examples and diagrams validated Security Compliance 100% All security requirements met","title":"Quality Metrics"},{"location":"review/review_workflow/#process-metrics","text":"Metric Target Measurement Review Completion 100% All assigned reviews completed on time Stakeholder Approval 100% All stakeholders approve their sections Issue Response Time < 24 hours Time to respond to critical issues Document Update Time < 48 hours Time to update documents after feedback","title":"Process Metrics"},{"location":"review/review_workflow/#escalation-procedures","text":"","title":"\ud83d\udea8 Escalation Procedures"},{"location":"review/review_workflow/#critical-issues","text":"Definition: Issues that could prevent implementation or compromise security Examples: - Security vulnerabilities in design - Infeasible technical requirements - Major architectural inconsistencies - Compliance violations - Timeline conflicts Escalation Process: 1. Identify critical issue during review 2. Document issue with evidence and impact 3. Notify Lead Architect immediately 4. Schedule emergency review session within 4 hours 5. Resolve with all stakeholders present 6. Update review assignments if needed","title":"Critical Issues"},{"location":"review/review_workflow/#non-critical-issues","text":"Definition: Issues that should be addressed but don't block implementation Examples: - Minor formatting inconsistencies - Documentation improvements - Code style suggestions - Performance optimizations Resolution Process: 1. Document issue in review report 2. Assign resolution responsibility 3. Set timeline for resolution 4. Track progress in issue tracking system 5. Validate resolution before final approval","title":"Non-Critical Issues"},{"location":"review/review_workflow/#review-templates-and-tools","text":"","title":"\ud83d\udcdd Review Templates and Tools"},{"location":"review/review_workflow/#individual-review-template","text":"## Document Review: [Document Name] **Reviewer:** [Name] **Date:** [Date] **Review Type:** [Individual/Cross/Group] ### Technical Accuracy - **Issues Found:** [List specific issues] - **Recommendations:** [Suggestions for improvement] - **Code Examples:** [Any syntax or logic errors] ### Completeness - **Missing Sections:** [List any missing content] - **Incomplete Examples:** [Identify incomplete examples] - **Unclear Requirements:** [Highlight unclear specifications] ### Consistency - **Terminology Issues:** [Inconsistent terms or definitions] - **Design Pattern Conflicts:** [Any architectural inconsistencies] - **Cross-Reference Problems:** [Broken or incorrect links] ### Overall Assessment - **Strengths:** [What works well] - **Critical Issues:** [Must-fix problems] - **Enhancement Opportunities:** [Nice-to-have improvements] - **Recommendation:** [Approve/Revise/Reject]","title":"Individual Review Template"},{"location":"review/review_workflow/#group-review-session-template","text":"## Group Review Session: [Category] **Date:** [Date] **Participants:** [List participants] ### Documents Reviewed - [List documents reviewed] ### Key Issues Identified - [List critical issues] - [List non-critical issues] ### Resolutions Agreed - [List agreed resolutions] - [Assign responsibilities] - [Set timelines] ### Approval Status - [Document approval status] - [Stakeholder confirmations] - [Next steps]","title":"Group Review Session Template"},{"location":"review/review_workflow/#success-criteria","text":"The review workflow is considered successful when:","title":"\ud83c\udfaf Success Criteria"},{"location":"review/review_workflow/#quality-criteria","text":"All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible","title":"Quality Criteria"},{"location":"review/review_workflow/#process-criteria","text":"All review phases completed on schedule All stakeholders actively participated All issues properly documented and resolved All approvals formally obtained Implementation team ready to begin","title":"Process Criteria"},{"location":"review/review_workflow/#deliverable-criteria","text":"Final validation report generated Implementation readiness assessment completed Updated architecture documentation ready Implementation handoff package prepared Development team onboarded and ready","title":"Deliverable Criteria"},{"location":"review/review_workflow/#contact-information","text":"","title":"\ud83d\udcde Contact Information"},{"location":"review/review_workflow/#review-team","text":"Lead Architect: [Name] - [email] - [phone] Review Coordinator: [Name] - [email] - [phone] Backend Lead: [Name] - [email] - [phone] Frontend Lead: [Name] - [email] - [phone] DevOps Lead: [Name] - [email] - [phone] Security Lead: [Name] - [email] - [phone]","title":"Review Team"},{"location":"review/review_workflow/#emergency-contacts","text":"Critical Issues: Lead Architect - [phone] Process Issues: Review Coordinator - [phone] Technical Issues: Backend Lead - [phone]","title":"Emergency Contacts"},{"location":"review/review_workflow/#timeline-summary","text":"Phase Duration Key Activities Deliverables Phase 1 1 day Pre-review setup Review environment ready Phase 2 5 days Individual reviews Review reports completed Phase 3 2 days Cross-review sessions Consistency validated Phase 4 3 days Group review sessions Stakeholder approvals Phase 5 3 days Final validation Implementation ready Total Duration: 14 days Target Completion: [Date] Next Steps: Begin Phase 1 - Pre-Review Setup","title":"\ud83d\udcc5 Timeline Summary"},{"location":"review/validation_checklist/","text":"SecFlow Architecture Validation Checklist \u00b6 \ud83c\udfaf Overview \u00b6 This comprehensive validation checklist ensures all architecture documents meet quality standards before implementation begins. \ud83d\udccb Document Structure Validation \u00b6 YAML Metadata Validation \u00b6 All documents have required YAML frontmatter Title, author, codename, version, and date are present Version format follows semantic versioning (e.g., \"1.0\") Date format is consistent (YYYY-MM-DD) Content Structure Validation \u00b6 Each document has a clear overview section ASCII diagrams are properly formatted Code examples are syntax-highlighted Tables are properly formatted Internal links use correct MkDocs format Section headers follow consistent hierarchy \ud83d\udd0d Technical Content Validation \u00b6 Code Examples \u00b6 Python code examples are syntactically correct YAML configurations are valid JSON examples are properly formatted Import statements are accurate Function signatures are complete Error handling is demonstrated Architecture Diagrams \u00b6 ASCII diagrams are readable and accurate Component relationships are clearly shown Data flow directions are indicated Layer boundaries are defined External dependencies are identified Data Models \u00b6 Pydantic models are complete and valid Field types are correctly specified Validation rules are appropriate Relationships between models are clear Database schemas are consistent \ud83c\udfd7\ufe0f Architecture Consistency Validation \u00b6 Design Patterns \u00b6 Hexagonal architecture principles are consistently applied Port and adapter patterns are properly implemented Event-driven design is consistently used Immutable data flow is maintained Dependency inversion is followed Naming Conventions \u00b6 Package names follow Python conventions Class names use PascalCase Function names use snake_case Constants use UPPER_CASE File names are descriptive and consistent Cross-Document References \u00b6 All internal links are valid Referenced concepts are defined Data model references are consistent API endpoint references are accurate Configuration references are valid \ud83d\udd12 Security Validation \u00b6 Security Model \u00b6 Authentication mechanisms are clearly defined Authorization model is comprehensive Data encryption requirements are specified Audit logging is properly designed Security boundaries are clearly defined Compliance Requirements \u00b6 NIST framework references are accurate CVSS scoring methodology is correct CWE/OWASP mappings are valid MITRE ATT&CK references are current Data protection requirements are met Risk Assessment \u00b6 Risk scoring formulas are mathematically sound Risk factors are properly weighted Risk thresholds are realistic Risk mitigation strategies are feasible Risk reporting is comprehensive \ud83d\ude80 Implementation Feasibility Validation \u00b6 Technical Requirements \u00b6 All dependencies are available and maintained Performance requirements are achievable Scalability requirements are realistic Integration points are well-defined Error handling strategies are complete Resource Requirements \u00b6 Infrastructure requirements are realistic Development effort estimates are accurate Testing requirements are comprehensive Deployment complexity is manageable Maintenance requirements are clear Migration Strategy \u00b6 Phase transitions are clearly defined Rollback strategies are specified Data migration plans are feasible Testing strategies are comprehensive Success criteria are measurable \ud83d\udcca Quality Metrics Validation \u00b6 Completeness \u00b6 All required sections are present Examples cover all major use cases Edge cases are addressed Error scenarios are documented Future considerations are included Clarity \u00b6 Technical concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Language is professional and accessible Document structure is logical Accuracy \u00b6 Technical specifications are correct Code examples are functional Architecture diagrams are accurate Data models are consistent Integration points are valid \ud83d\udd04 Process Validation \u00b6 Review Process \u00b6 All stakeholders have reviewed their sections Cross-reviews have been completed Group review sessions have been conducted All issues have been resolved Final approval has been obtained Documentation Standards \u00b6 All documents follow the established template Formatting is consistent across documents Links and references are valid Version control is properly maintained Change tracking is documented \u2705 Final Validation Checklist \u00b6 Pre-Implementation Validation \u00b6 All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Risk assessment methodology is approved by security team Implementation Readiness \u00b6 Development environment setup is documented CI/CD pipeline configuration is ready Testing framework is established Security scanning tools are configured Monitoring and observability tools are ready Documentation generation process is automated \ud83d\udea8 Critical Issues Requiring Immediate Attention \u00b6 Security Issues \u00b6 Any security vulnerabilities in design Missing security controls Inadequate data protection measures Compliance violations Audit trail gaps Technical Issues \u00b6 Infeasible technical requirements Major architectural inconsistencies Performance bottlenecks Integration failures Data model conflicts Process Issues \u00b6 Missing stakeholder approvals Incomplete review process Unresolved critical issues Timeline conflicts Resource constraints \ud83d\udcdd Validation Report Template \u00b6 Document: [Document Name] \u00b6 Validator: [Name] Date: [Date] Status: [Pass/Fail/Needs Revision] Technical Validation \u00b6 Code Examples: [Pass/Fail] - [Comments] Architecture Diagrams: [Pass/Fail] - [Comments] Data Models: [Pass/Fail] - [Comments] Integration Points: [Pass/Fail] - [Comments] Security Validation \u00b6 Security Model: [Pass/Fail] - [Comments] Compliance Requirements: [Pass/Fail] - [Comments] Risk Assessment: [Pass/Fail] - [Comments] Data Protection: [Pass/Fail] - [Comments] Process Validation \u00b6 Review Process: [Pass/Fail] - [Comments] Documentation Standards: [Pass/Fail] - [Comments] Stakeholder Approval: [Pass/Fail] - [Comments] Implementation Readiness: [Pass/Fail] - [Comments] Overall Assessment \u00b6 Strengths: [List strengths] Issues Found: [List issues] Recommendations: [List recommendations] Final Recommendation: [Approve/Revise/Reject] \ud83c\udfaf Success Criteria \u00b6 The architecture validation is considered successful when: - [ ] All documents pass technical accuracy validation - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team - [ ] Development team is ready to begin implementation Next Steps: Use this checklist to validate each document systematically before proceeding to implementation.","title":"Checklist"},{"location":"review/validation_checklist/#secflow-architecture-validation-checklist","text":"","title":"SecFlow Architecture Validation Checklist"},{"location":"review/validation_checklist/#overview","text":"This comprehensive validation checklist ensures all architecture documents meet quality standards before implementation begins.","title":"\ud83c\udfaf Overview"},{"location":"review/validation_checklist/#document-structure-validation","text":"","title":"\ud83d\udccb Document Structure Validation"},{"location":"review/validation_checklist/#yaml-metadata-validation","text":"All documents have required YAML frontmatter Title, author, codename, version, and date are present Version format follows semantic versioning (e.g., \"1.0\") Date format is consistent (YYYY-MM-DD)","title":"YAML Metadata Validation"},{"location":"review/validation_checklist/#content-structure-validation","text":"Each document has a clear overview section ASCII diagrams are properly formatted Code examples are syntax-highlighted Tables are properly formatted Internal links use correct MkDocs format Section headers follow consistent hierarchy","title":"Content Structure Validation"},{"location":"review/validation_checklist/#technical-content-validation","text":"","title":"\ud83d\udd0d Technical Content Validation"},{"location":"review/validation_checklist/#code-examples","text":"Python code examples are syntactically correct YAML configurations are valid JSON examples are properly formatted Import statements are accurate Function signatures are complete Error handling is demonstrated","title":"Code Examples"},{"location":"review/validation_checklist/#architecture-diagrams","text":"ASCII diagrams are readable and accurate Component relationships are clearly shown Data flow directions are indicated Layer boundaries are defined External dependencies are identified","title":"Architecture Diagrams"},{"location":"review/validation_checklist/#data-models","text":"Pydantic models are complete and valid Field types are correctly specified Validation rules are appropriate Relationships between models are clear Database schemas are consistent","title":"Data Models"},{"location":"review/validation_checklist/#architecture-consistency-validation","text":"","title":"\ud83c\udfd7\ufe0f Architecture Consistency Validation"},{"location":"review/validation_checklist/#design-patterns","text":"Hexagonal architecture principles are consistently applied Port and adapter patterns are properly implemented Event-driven design is consistently used Immutable data flow is maintained Dependency inversion is followed","title":"Design Patterns"},{"location":"review/validation_checklist/#naming-conventions","text":"Package names follow Python conventions Class names use PascalCase Function names use snake_case Constants use UPPER_CASE File names are descriptive and consistent","title":"Naming Conventions"},{"location":"review/validation_checklist/#cross-document-references","text":"All internal links are valid Referenced concepts are defined Data model references are consistent API endpoint references are accurate Configuration references are valid","title":"Cross-Document References"},{"location":"review/validation_checklist/#security-validation","text":"","title":"\ud83d\udd12 Security Validation"},{"location":"review/validation_checklist/#security-model","text":"Authentication mechanisms are clearly defined Authorization model is comprehensive Data encryption requirements are specified Audit logging is properly designed Security boundaries are clearly defined","title":"Security Model"},{"location":"review/validation_checklist/#compliance-requirements","text":"NIST framework references are accurate CVSS scoring methodology is correct CWE/OWASP mappings are valid MITRE ATT&CK references are current Data protection requirements are met","title":"Compliance Requirements"},{"location":"review/validation_checklist/#risk-assessment","text":"Risk scoring formulas are mathematically sound Risk factors are properly weighted Risk thresholds are realistic Risk mitigation strategies are feasible Risk reporting is comprehensive","title":"Risk Assessment"},{"location":"review/validation_checklist/#implementation-feasibility-validation","text":"","title":"\ud83d\ude80 Implementation Feasibility Validation"},{"location":"review/validation_checklist/#technical-requirements","text":"All dependencies are available and maintained Performance requirements are achievable Scalability requirements are realistic Integration points are well-defined Error handling strategies are complete","title":"Technical Requirements"},{"location":"review/validation_checklist/#resource-requirements","text":"Infrastructure requirements are realistic Development effort estimates are accurate Testing requirements are comprehensive Deployment complexity is manageable Maintenance requirements are clear","title":"Resource Requirements"},{"location":"review/validation_checklist/#migration-strategy","text":"Phase transitions are clearly defined Rollback strategies are specified Data migration plans are feasible Testing strategies are comprehensive Success criteria are measurable","title":"Migration Strategy"},{"location":"review/validation_checklist/#quality-metrics-validation","text":"","title":"\ud83d\udcca Quality Metrics Validation"},{"location":"review/validation_checklist/#completeness","text":"All required sections are present Examples cover all major use cases Edge cases are addressed Error scenarios are documented Future considerations are included","title":"Completeness"},{"location":"review/validation_checklist/#clarity","text":"Technical concepts are clearly explained Diagrams enhance understanding Code examples are well-commented Language is professional and accessible Document structure is logical","title":"Clarity"},{"location":"review/validation_checklist/#accuracy","text":"Technical specifications are correct Code examples are functional Architecture diagrams are accurate Data models are consistent Integration points are valid","title":"Accuracy"},{"location":"review/validation_checklist/#process-validation","text":"","title":"\ud83d\udd04 Process Validation"},{"location":"review/validation_checklist/#review-process","text":"All stakeholders have reviewed their sections Cross-reviews have been completed Group review sessions have been conducted All issues have been resolved Final approval has been obtained","title":"Review Process"},{"location":"review/validation_checklist/#documentation-standards","text":"All documents follow the established template Formatting is consistent across documents Links and references are valid Version control is properly maintained Change tracking is documented","title":"Documentation Standards"},{"location":"review/validation_checklist/#final-validation-checklist","text":"","title":"\u2705 Final Validation Checklist"},{"location":"review/validation_checklist/#pre-implementation-validation","text":"All 24 documents pass technical accuracy review No critical security or compliance issues remain Cross-document consistency is validated All stakeholders approve their respective sections Implementation roadmap is validated as feasible Risk assessment methodology is approved by security team","title":"Pre-Implementation Validation"},{"location":"review/validation_checklist/#implementation-readiness","text":"Development environment setup is documented CI/CD pipeline configuration is ready Testing framework is established Security scanning tools are configured Monitoring and observability tools are ready Documentation generation process is automated","title":"Implementation Readiness"},{"location":"review/validation_checklist/#critical-issues-requiring-immediate-attention","text":"","title":"\ud83d\udea8 Critical Issues Requiring Immediate Attention"},{"location":"review/validation_checklist/#security-issues","text":"Any security vulnerabilities in design Missing security controls Inadequate data protection measures Compliance violations Audit trail gaps","title":"Security Issues"},{"location":"review/validation_checklist/#technical-issues","text":"Infeasible technical requirements Major architectural inconsistencies Performance bottlenecks Integration failures Data model conflicts","title":"Technical Issues"},{"location":"review/validation_checklist/#process-issues","text":"Missing stakeholder approvals Incomplete review process Unresolved critical issues Timeline conflicts Resource constraints","title":"Process Issues"},{"location":"review/validation_checklist/#validation-report-template","text":"","title":"\ud83d\udcdd Validation Report Template"},{"location":"review/validation_checklist/#document-document-name","text":"Validator: [Name] Date: [Date] Status: [Pass/Fail/Needs Revision]","title":"Document: [Document Name]"},{"location":"review/validation_checklist/#technical-validation","text":"Code Examples: [Pass/Fail] - [Comments] Architecture Diagrams: [Pass/Fail] - [Comments] Data Models: [Pass/Fail] - [Comments] Integration Points: [Pass/Fail] - [Comments]","title":"Technical Validation"},{"location":"review/validation_checklist/#security-validation_1","text":"Security Model: [Pass/Fail] - [Comments] Compliance Requirements: [Pass/Fail] - [Comments] Risk Assessment: [Pass/Fail] - [Comments] Data Protection: [Pass/Fail] - [Comments]","title":"Security Validation"},{"location":"review/validation_checklist/#process-validation_1","text":"Review Process: [Pass/Fail] - [Comments] Documentation Standards: [Pass/Fail] - [Comments] Stakeholder Approval: [Pass/Fail] - [Comments] Implementation Readiness: [Pass/Fail] - [Comments]","title":"Process Validation"},{"location":"review/validation_checklist/#overall-assessment","text":"Strengths: [List strengths] Issues Found: [List issues] Recommendations: [List recommendations] Final Recommendation: [Approve/Revise/Reject]","title":"Overall Assessment"},{"location":"review/validation_checklist/#success-criteria","text":"The architecture validation is considered successful when: - [ ] All documents pass technical accuracy validation - [ ] No critical security or compliance issues remain - [ ] Cross-document consistency is validated - [ ] All stakeholders approve their respective sections - [ ] Implementation roadmap is validated as feasible - [ ] Risk assessment methodology is approved by security team - [ ] Development team is ready to begin implementation Next Steps: Use this checklist to validate each document systematically before proceeding to implementation.","title":"\ud83c\udfaf Success Criteria"},{"location":"review/validation_report/","text":"Validation Report \u2014 2025-10-07T09:28:17.078745Z \u00b6 Criticals : 0 Warnings : 5 Document Criticals Warnings 00-index.md 0 0 01-title-and-executive-summary.md 0 0 02-architecture-philosophy.md 0 0 03-repository-layout.md 0 0 04-core-packages-and-responsibilities.md 0 0 05-orchestration-and-workflow-engine.md 0 0 06-plugin-system.md 0 1 07-tools-integration-model.md 0 0 08-tool-manager-and-ux-design.md 0 1 09-resource-registry.md 0 0 10-wordlist-and-output-sharing.md 0 0 11-project-isolation-and-data-sharing.md 0 0 12-findings-model-and-schema.md 0 1 13-cve-cwe-poc-enrichment-layer.md 0 0 14-poc-sources-and-legal-guidelines.md 0 1 15-garbage-collection-and-retention.md 0 0 16-security-model.md 0 0 17-observability-logging-and-metrics.md 0 1 18-error-handling-and-recovery.md 0 0 19-risk-assessment-framework.md 0 0 20-migration-and-implementation-phases.md 0 0 21-ci-cd-and-testing-strategy.md 0 0 22-developer-experience-and-docs.md 0 0 23-future-roadmap.md 0 0 24-final-consensus-summary.md 0 0 06-plugin-system.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax 08-tool-manager-and-ux-design.md \u00b6 \u26a0\ufe0f Warnings Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow ) 12-findings-model-and-schema.md \u00b6 \u26a0\ufe0f Warnings Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS ) 14-poc-sources-and-legal-guidelines.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax 17-observability-logging-and-metrics.md \u00b6 \u26a0\ufe0f Warnings Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value \ud83d\udcc8 Documentation Quality Index \u00b6 Score : 99.0/100 Criticals : 0 Warnings : 5 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent Trend : \u27a1\ufe0f Stable","title":"Report"},{"location":"review/validation_report/#validation-report-2025-10-07t092817078745z","text":"Criticals : 0 Warnings : 5 Document Criticals Warnings 00-index.md 0 0 01-title-and-executive-summary.md 0 0 02-architecture-philosophy.md 0 0 03-repository-layout.md 0 0 04-core-packages-and-responsibilities.md 0 0 05-orchestration-and-workflow-engine.md 0 0 06-plugin-system.md 0 1 07-tools-integration-model.md 0 0 08-tool-manager-and-ux-design.md 0 1 09-resource-registry.md 0 0 10-wordlist-and-output-sharing.md 0 0 11-project-isolation-and-data-sharing.md 0 0 12-findings-model-and-schema.md 0 1 13-cve-cwe-poc-enrichment-layer.md 0 0 14-poc-sources-and-legal-guidelines.md 0 1 15-garbage-collection-and-retention.md 0 0 16-security-model.md 0 0 17-observability-logging-and-metrics.md 0 1 18-error-handling-and-recovery.md 0 0 19-risk-assessment-framework.md 0 0 20-migration-and-implementation-phases.md 0 0 21-ci-cd-and-testing-strategy.md 0 0 22-developer-experience-and-docs.md 0 0 23-future-roadmap.md 0 0 24-final-consensus-summary.md 0 0","title":"Validation Report \u2014 2025-10-07T09:28:17.078745Z"},{"location":"review/validation_report/#06-plugin-systemmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 06-plugin-system.md: Syntax error at line 1: invalid syntax","title":"06-plugin-system.md"},{"location":"review/validation_report/#08-tool-manager-and-ux-designmd","text":"\u26a0\ufe0f Warnings Forbidden term in 08-tool-manager-and-ux-design.md: secflow (use SecFlow )","title":"08-tool-manager-and-ux-design.md"},{"location":"review/validation_report/#12-findings-model-and-schemamd","text":"\u26a0\ufe0f Warnings Forbidden term in 12-findings-model-and-schema.md: cvss (use CVSS )","title":"12-findings-model-and-schema.md"},{"location":"review/validation_report/#14-poc-sources-and-legal-guidelinesmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 14-poc-sources-and-legal-guidelines.md: Syntax error at line 1: invalid syntax","title":"14-poc-sources-and-legal-guidelines.md"},{"location":"review/validation_report/#17-observability-logging-and-metricsmd","text":"\u26a0\ufe0f Warnings Syntax check failed in 17-observability-logging-and-metrics.md: JSON error at line 1, column 1: Expecting value","title":"17-observability-logging-and-metrics.md"},{"location":"review/validation_report/#documentation-quality-index","text":"Score : 99.0/100 Criticals : 0 Warnings : 5 Trend : \u2192 Quality Level : \ud83d\udfe2 Excellent Trend : \u27a1\ufe0f Stable","title":"\ud83d\udcc8 Documentation Quality Index"}]}